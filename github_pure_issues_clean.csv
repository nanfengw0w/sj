issue_number,title,content,labels,create_time,close_time,comments_count,state
63322,BUG: pd.col does not support & for combining conditions in .loc,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import io


data = '''datetime,cfs
2018-02-28,350
2018-03-15,420
2019-04-10,380
2019-06-01,450
'''
dd = pd.read_csv(io.StringIO(data), parse_dates=['datetime']).set_index('datetime')
(dd
 .reset_index()
 .sort_values('datetime')
 .loc[(pd.col('datetime') > '2018-03-01') & (pd.col('datetime') <= '2019-05-31')]
 .assign(cfs=pd.col('cfs').clip(upper=400))

)
```

### Issue Description

When using `pd.col` inside `.loc` to filter rows based on multiple conditions, combining conditions with `&` does not work as expected. 

### Expected Behavior

It works like this lambda version:

```python
import pandas as pd
import io


data = '''datetime,cfs
2018-02-28,350
2018-03-15,420
2019-04-10,380
2019-06-01,450
'''
dd = pd.read_csv(io.StringIO(data), parse_dates=['datetime']).set_index('datetime')
(dd
 .reset_index()
 .sort_values('datetime')
 #.loc[(pd.col('datetime') > '2018-03-01') & (pd.col('datetime') <= '2019-05-31')]
 .loc[lambda df_: (df_['datetime'] > '2018-03-01') & (df_['datetime'] <= '2019-05-31')]
 .assign(cfs=pd.col('cfs').clip(upper=400))
)

```

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Numeric Operations', 'expressions']",2025-12-11 02:47:49,2025-12-13 16:50:14,2,closed
63315,BUG: OSError 22 (Invalid argument) in tzconversion tests on Windows/Python 3.11,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Steps to Reproduce the Bug
Set up a Conda environment with Python 3.11 on Windows.

Install development dependencies (NumPy, Cython, etc.).

Build Pandas from the main branch using Meson in the Visual Studio Command Prompt (this step completes successfully).

Bash

# Command used for installation:
python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true
Run the test suite:

Bash

pytest pandas
```

### Issue Description

### Summary of Bug

Running the Pandas test suite (`pytest pandas`) immediately fails on my Windows development environment, raising an `OSError: [Errno 22] Invalid argument` originating from the `time.localtime()` function within `python-dateutil` during time zone conversion tests.

This seems to be a platform-specific issue, likely related to a timestamp exceeding the valid range for the C runtime library on Windows for the tested dates.

### Complete Traceback

<details>
<summary>Click to expand full traceback</summary>

```python
FEExceptions caught in Qt event loop:
________________________________________________________________________________
Traceback (most recent call last):
  File ""pandas/_libs/tslibs/tzconversion.pyx"", line 128, in pandas._libs.tslibs.tzconversion.Localizer.utc_val_to_local_val
    return utc_val + _tz_localize_using_tzinfo_api(
  File ""pandas/_libs/tslibs/tzconversion.pyx"", line 759, in pandas._libs.tslibs.tzconversion._tz_localize_using_tzinfo_api
    dt = _astimezone(dts, tz)
  File ""pandas/_libs/tslibs/tzconversion.pyx"", line 791, in pandas._libs.tslibs.tzconversion._astimezone
    return tz.fromutc(result)
  File ""C:\Users\My\.conda\envs\pandas-dev\Lib\site-packages\dateutil\tz\_common.py"", line 144, in fromutc
    return f(self, dt)
  File ""C:\Users\My\.conda\envs\pandas-dev\Lib\site-packages\dateutil\tz\_common.py"", line 261, in fromutc
    _fold = self._fold_status(dt, dt_wall)
  File ""C:\Users\My\.conda\envs\pandas-dev\Lib\site-packages\dateutil\tz\_common.py"", line 196, in _fold_status
    if self.is_ambiguous(dt_wall):
  File ""C:\Users\My\.conda\envs\pandas-dev\Lib\site-packages\dateutil\tz\tz.py"", line 254, in is_ambiguous
    naive_dst = self._naive_is_dst(dt)
  File ""C:\Users\My\.conda\envs\pandas-dev\Lib\site-packages\dateutil\tz\tz.py"", line 260, in _naive_is_dst
    return time.localtime(timestamp + time.timezone).tm_isdst
OSError: [Errno 22] Invalid argument

### Expected Behavior

The test suite should run completely, with no fatal OSError exceptions.

### Installed Versions

<details>

<summary>Click to expand Environment Information</summary>

**OS/Compiler Environment:**
* OS: Windows (Issue reproduced in VS Native Tools Command Prompt)
* Python: 3.11.14 (from Miniforge/Conda)
* Build Method: Meson / pip install -ve . --no-build-isolation
* VS Build Tools: 2022

**Key Dependencies (from `pip show`):**
* pandas: 3.0.0rc0+21.g499c5d4dd5 (Built from `main` branch)
* numpy: 2.3.5
* python-dateutil: 2.9.0.post0
* tzdata: 2025.2

</details>

","['Bug', 'Needs Triage', 'AI Slop']",2025-12-09 20:04:46,2025-12-09 23:22:21,3,closed
63311,BUG: Inconsistent behavior for `step` with slice for label-based indexing,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# /// script
# requires-python = "">=3.13""
# dependencies = [
#     ""pandas==2.3.3"",
# ]
# ///

import numpy as np
import pandas as pd

# Create a Series with non-contiguous integer index (step of 5)
# Index: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, ...
# Values: 0, 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, ...
T = np.arange(0, 100, 5)
series = pd.Series(np.arange(len(T)), index=T)

# Without step: returns all labels from 10 to 50 inclusive (label-based)
result_no_step = series.loc[10:50]
print(""series.loc[10:50] (no step):"")
print(series.loc[10:50])
print()

# With step=1: same as no step
print(""series.loc[10:50:1] (step=1):"")
print(series.loc[10:50:1])
print()

# With step=2: step is applied positionally, start, stop applied to labels
print(""series.loc[10:50:2] (step=2):"")
print(series.loc[10:50:2])
print()

# With step=5: Same behavior as step 2
print(""series.loc[10:50:5] (step=5):"")
print(series.loc[10:50:5])
print()

# Using arange with same arguments as the slice
print(""series.loc[np.arange(10,50,5)] (step=5):"")
print(series.loc[np.arange(10, 50, 5)])
print()
```

### Issue Description

When using `.loc` with a slice   `start/stop` are applied over a different space than `step` which I found very counterintuitive. I also was not able to find any docs (on this admittedly niche use case)

`start`/`stop` are applied over the values of the labels
`step` is applied positionally over the index

The result of the above script is

```
series.loc[10:50] (no step):
10     2
15     3
20     4
25     5
30     6
35     7
40     8
45     9
50    10
dtype: int64

series.loc[10:50:1] (step=1):
10     2
15     3
20     4
25     5
30     6
35     7
40     8
45     9
50    10
dtype: int64

series.loc[10:50:2] (step=2):
10     2
20     4
30     6
40     8
50    10
dtype: int64

series.loc[10:50:5] (step=5):
10    2
35    7
dtype: int64

series.loc[np.arange(10,50,5)] (step=5):
10    2
15    3
20    4
25    5
30    6
35    7
40    8
45    9
dtype: int64
```

### Expected Behavior

I would have expected either of the following:

### error

Throw an error saying that step is ambiguous and cannot be used here. This seems to be the approach of IntervalIndex:
https://github.com/pandas-dev/pandas/blob/499c5d4dd52a8645bf96c39bad60613097e84c06/pandas/core/indexes/interval.py#L978-L982

(Though as a sidenote I wasn't able to hit that code path)

### Step applies to  Label Space

In my example I would expect the slice with `step=5` to behave the same as `step=1` as it should hit each of the same values. My mental model is that for the case of integers as in my example

`series.loc[slice(start, stop, step)]`

should be equivalent to

`series.loc[np.arange(start, stop+step, step)]`
(with a +step to account for pandas inclusivity of bounds in slicing)
and in more amgious cases e.g. `slice(""a"", ""f"", 3)` and error should be thrown


### Installed Versions

<details><summary>INSTALLED VERSIONS</summary>

```

------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.13.0
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:51 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.5
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
```
</details>
","['Docs', 'Indexing']",2025-12-09 17:12:19,2025-12-14 18:12:20,6,closed
63280,"BUG:  `str` dtype does not return pyarrow boolean (`bool[pyarrow]`) output for elementwise checks (equality, etc.)","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Load a string column with pyarrow backend
make = pd.Series([""Alfa Romeo"", ""Ford"", ""Subaru""], dtype=""str"")

# Elementwise equality check
result = make == ""Alfa Romeo""
print(result.dtype)  # Expected: boolean[pyarrow], Actual: bool
```

### Issue Description

When using the new str dtype (backed by pyarrow) in pandas 3.x, elementwise operations such as equality checks (==) do not return a Series with pyarrow boolean dtype. Instead, the output is a Series with numpy boolean dtype. This is inconsistent with other pyarrow-backed dtypes, which return pyarrow booleans for such operations.

### Expected Behavior

This works if casting to `string[pyarrow]`

```
import pandas as pd

# Load a string column with pyarrow backend
make = pd.Series([""Alfa Romeo"", ""Ford"", ""Subaru""], dtype=""str"")

# Elementwise equality check
result = make.astype('string[pyarrow]') == ""Alfa Romeo""
print(result.dtype)  # Got: boolean[pyarrow]
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 1a3230dc5be4c87b8356765ea3b6568d37cb82fd
python                : 3.11.13
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:40 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6041
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_US.UTF-8

pandas                : 3.0.0rc0
numpy                 : 2.4.0rc1
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.8.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.3
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.7
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : 22.0.0
pyiceberg             : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None
</details>
","['Numeric Operations', 'Strings', 'Arrow']",2025-12-05 23:47:24,2025-12-06 13:50:34,3,closed
63279,DOC: Address other string types in Pandas 3 string migration guide,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/migration-3-strings.rst 


### Documentation problem


In Pandas 2.3, we had `'string[pyarrow]'` and `pd.ArrowDtype(pa.string())`

The new guide at https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/migration-3-strings.rst doesn't mention either.

It would be worthwhile to mention them or deprecate them.

I assume the recommended best practice is to just use `'str'` for the type and disregard the other two.



### Suggested fix for documentation

Mention the 2.3 types and what to migrate them to. Also whether or not it makes sense to use them at all.","['Docs', 'Strings']",2025-12-05 23:07:19,2025-12-14 15:31:59,14,closed
63269,BUG: SUpport numpy 2.0 pls,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
andas/compat/_optional.py"", line 135, in import_optional_dependency
    module = importlib.import_module(name)
  File ""/home/x/anaconda3/lib/python3.12/importlib/__init__.py"", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/x/anaconda3/lib/python3.12/site-packages/numexpr/__init__.py"", line 24, in <module>
    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__
AttributeError: _ARRAY_API not found

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.
```

### Issue Description

this is the **Only** lib that didn't supprot numpy 2.0 until 2025

### Expected Behavior

support numpy 2.0

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
Name: pandas
Version: 2.3.3","['Bug', 'Build']",2025-12-05 01:23:41,2025-12-05 12:26:47,3,closed
63262,BUG: `KeyError` in `Series.__getitem__` with datetime index and mixed unit slice in pandas 3.0.0rc0,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

This script succeeded in pandas 2.x, but raises a `KeyError` in pandas 3.0.0rc0:

```python
import pandas as pd

df = pd.Series(1, index=pd.date_range(start=""2000-01-01"", freq=""h"", periods=8))
start = pd.Timestamp(""2000-01-01 01:00:00"")  # unit=us
stop = start + pd.Timedelta(1)               # unit=ns
df.loc[start:stop]

```

The mix of us-resolution datetime index with a ns-resolution slice seems relevant. Casting `stop` to `us` resolution with `stop = (start + pd.Timedelta(1)).as_unit(""us"")` avoids the error.

### Issue Description

`uv run --python=3.13 --with pandas==3.0.0rc0 python bug.py`

fails with

```pytb
Traceback (most recent call last):
  File ""/Users/toaugspurger/gh/bug.py"", line 6, in <module>
    df.loc[start:stop]
    ~~~~~~^^^^^^^^^^^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexing.py"", line 1207, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexing.py"", line 1429, in _getitem_axis
    return self._get_slice_axis(key, axis=axis)
           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexing.py"", line 1461, in _get_slice_axis
    indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop, slice_obj.step)
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexes/datetimes.py"", line 1061, in slice_indexer
    return Index.slice_indexer(self, start, end, step)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexes/base.py"", line 6690, in slice_indexer
    start_slice, end_slice = self.slice_locs(start, end, step=step)
                             ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexes/base.py"", line 6952, in slice_locs
    end_slice = self.get_slice_bound(end, ""right"")
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexes/base.py"", line 6860, in get_slice_bound
    raise err from None
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexes/base.py"", line 6854, in get_slice_bound
    slc = self.get_loc(label)
  File ""/Users/toaugspurger/.cache/uv/archive-v0/GlcV9lNBXMPGHt8xIqa_B/lib/python3.13/site-packages/pandas/core/indexes/datetimes.py"", line 987, in get_loc
    raise KeyError(orig_key) from err
KeyError: Timestamp('2000-01-01 01:00:00.000000001')

```

### Expected Behavior

Return the matching rows:

```
2000-01-01 01:00:00    1
Freq: h, dtype: int64
```

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Non-Nano']",2025-12-04 15:47:32,2025-12-08 18:25:21,2,closed
63258,DOC: Code example doesn't run as shown,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/1a3230dc5be4c87b8356765ea3b6568d37cb82fd/doc/source/user_guide/copy_on_write.rst?plain=1#L30



### Documentation problem

In https://github.com/pandas-dev/pandas/blob/1a3230dc5be4c87b8356765ea3b6568d37cb82fd/doc/source/user_guide/copy_on_write.rst?plain=1#L30 the doc has:
```
In [1]: df = pd.DataFrame({""foo"": [1, 2, 3], ""bar"": [4, 5, 6]})
In [2]: df[""foo""][df[""bar""] > 5] = 100
In [3]: df
Out[3]:
   foo  bar
0  100    4
1    2    5
2    3    6
```

But running it should give
```
   foo  bar
0    1    4
1    2    5
2  100    6
```

### Suggested fix for documentation

Change as above.","['Docs', 'Copy / view semantics']",2025-12-04 01:56:41,2025-12-04 17:30:42,4,closed
63245,Windows Free Thread,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Why don't add ""Free Thread"" For Windows in last version?

### Feature Description

Why don't add ""Free Thread"" For Windows in last version?

### Alternative Solutions

Why don't add ""Free Thread"" For Windows in last version?

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-12-01 19:14:10,2025-12-01 19:30:38,2,closed
63242,BUG: validation merge arguments on/left_on/right_on/left_index/right_index needs improvement,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df1 = pd.DataFrame({'col':[1,2,3]})
df2 = pd.DataFrame({'col':[2,3,4]})

# Example 1: The following correctly raises an exception, since both left_on and left_index are specified
try:
    df1.merge(df2, left_on='col', left_index=True, right_on='col')
except pd.errors.MergeError as e:
    print(e)

# Example 2: The following should raise an exception, since both right_on and right_index are specified, but it doesn't
try:
    df1.merge(df2, left_on='col', right_on='col', right_index=True)
    raise(Exception('Example 2: No MergeError raised'))
except pd.errors.MergeError as e:
    print(e)
except Exception as e:
    print(e)
```

### Issue Description

I was following the course on Datacamp on merging dataframes. I couldn't understand the examples and exercises where left_index/right_index are used, so searched on the web. I then arrived at https://github.com/pandas-dev/pandas/issues/16228 and https://github.com/pandas-dev/pandas/pull/37547. I believe the fix(es) made there/then isn't fully correct. For one, the behavior isn't symmetric regarding left and right. See the code example.

### Expected Behavior

The validation behavior should be identical when left and right are fully swapped, i.e. left_on<->right_on, left_index<->right_index. Currently, this isn't the case.

I believe that (part of) the problem is due to the usage of 'elif' in _validate_left_right_on() https://github.com/pandas-dev/pandas/blob/main/pandas/core/reshape/merge.py#L1881. After validation of left_on/left_index, it's still necessary to also check for right_on/right_index.

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.11.13
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Belgium.1252
pandas                : 2.3.1
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.7.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.5
numba                 : 0.61.2
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : 1.4.6
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.1
sqlalchemy            : 2.0.42
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None","['Bug', 'Reshaping', 'Error Reporting']",2025-12-01 15:16:12,2025-12-05 12:49:13,3,closed
63240,"`gs://` URLs support in `read_feather()`, `read_pickle()` docs","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.read_feather.html

### Documentation problem

Hello!

From [read_parquet()](https://github.com/pandas-dev/pandas/blob/v2.3.3/pandas/io/parquet.py#L500-L678) docs:

> path : str, path object or file-like object
>     String, path object (implementing ``os.PathLike[str]``), or file-like object implementing a binary ``read()`` function. The string could be a URL. **Valid URL schemes include http, ftp, s3, gs, and file.**

From [read_feather()](https://github.com/pandas-dev/pandas/blob/v2.3.3/pandas/io/feather_format.py#L68-L130) docs:

> path : str, path object, or file-like object
>     String, path object (implementing ``os.PathLike[str]``), or file-like object implementing a binary ``read()`` function. The string could be a URL. **Valid URL schemes include http, ftp, s3, and file**. For file URLs, a host is expected. A local file could be: ``file://localhost/path/to/table.feather``.

Q1. Does `read_feather()` not support `gs://` URLs in `path` parameter? If yes then the docs need to be updated for the same.


From [read_pickle()](https://github.com/pandas-dev/pandas/blob/v2.3.3/pandas/io/pickle.py#L114-L210) docs:

> filepath_or_buffer : str, path object, or file-like object
> String, path object (implementing ``os.PathLike[str]``), or file-like object implementing a binary ``readlines()`` function. Also accepts URL. **URL is not limited to S3 and GCS.**

Q2. This means `read_pickle()` does support `gs://` URLs in `filepath_or_buffer` parameter like `read_parquet()`, correct?

### Suggested fix for documentation

Valid URL schemes include http, ftp, s3, **gs** and file.","['Docs', 'IO Parquet']",2025-12-01 11:42:56,2025-12-04 17:32:23,1,closed
63230,BUG: SettingWithCopyWarning is not shown with copy-on-write disabled,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
with pd.option_context(""mode.copy_on_write"", False): 
  df = pd.DataFrame({""a"": [1,2,3], ""b"": [2,4,6]})
  df2 = df.loc[0:1,:]
  df2.iloc[0,0] = 9999
```

### Issue Description

Under both Python 3.12 and 3.14, with the main branch of Pandas (see details below), the reproducible example does not show the `SettingWithCopyWarning`. 

Possibly related to https://github.com/pandas-dev/pandas/issues/61368 (closed), since it probably also has to do with refcounts. This bug happens on multiple versions of Python, though, instead of only with Python 3.14. 

### Expected Behavior

To show the `SettingWithCopyWarning`. Under Pandas 2.3.3 it works fine and does show the warning:

    SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame

    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      df2.iloc[0,0] = 9999

As in the example, users may choose to disable copy-on-write (`pd.options.mode.copy_on_write = False`) in order to use the view of a data frame with working assignment on the original data frame, as is the default in Pandas < 3.0. However, if they choose to disable copy-on-write, they still should be warned that they are modifying the original data frame, just like in Pandas < 3.0.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 809d9b72a05cc1f35bae1cf2865ec9ee19b11d2c
python                : 3.12.12
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26200
machine               : AMD64
processor             : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 3.0.0.dev0+2755.g809d9b72a0
numpy                 : 2.3.5
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.7.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Copy / view semantics']",2025-11-29 22:14:35,2025-12-01 09:01:46,3,closed
63219,BUG: Inconsistent behavior with groupby and copy-on-write,"Grouping by a Series and the mutating that Series can have different impacts whether a view on the data exists.

```python
ser = pd.Series([1, 2, 1])
df = pd.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6]})
gb = df.groupby(ser)
ser.iloc[0] = 100
print(gb.sum())
#      a  b
# 1    3  6
# 2    2  5
# 100  1  4

ser = pd.Series([1, 2, 1])
df = pd.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6]})
ser2 = ser[:]
gb = df.groupby(ser)
ser.iloc[0] = 100
print(gb.sum())
#    a   b
# 1  4  10
# 2  2   5
```

This only happens for certain paths in groupby, e.g. using

    ser = pd.Series(pd.Categorical([1, 2, 1], categories=[1, 2, 100]))

gives the latter behavior. We should be taking a shallow copy of any grouping Series when we create the `DataFrameGroupBy` instance.

Hat-tip to @jorisvandenbossche for constructing the example.
","['Bug', 'Groupby', 'Copy / view semantics']",2025-11-27 09:59:24,2025-12-04 17:57:43,0,closed
63218,BUG: Inconsisten nunique behavior with Intervals on latest nightly,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

print(f""{pd.__version__}"")

# inf in left position
s = pd.Series([pd.Interval(-np.inf, 0), pd.Interval(-np.inf, 1)])
assert s.nunique() == 2
assert len(s.drop_duplicates()) == 2

# inf in right position
s = pd.Series([pd.Interval(0, np.inf), pd.Interval(1, np.inf)])
assert len(s.drop_duplicates()) == 2
assert s.nunique() == 2  # Fails on latest nightly ( == 0)
```

### Issue Description

The semantics of `nunique` appear to have changed on the latest nightly version as demonstrated by the above example which passes on pandas 2.3.3 but fails on the latest nightly. Intervals with `inf` values on the right seem to be treated as `nan` values. Using `-inf` for the left-hand side of an interval works as it used to.

Notably, the nightly version raises the following suspicious warning:
```
[...]/pandas-debug/.pixi/envs/nightly/lib/python3.14/site-packages/pandas/core/arrays/interval.py:2132: RuntimeWarning: invalid value encountered in multiply
  1j * np.array(right.ravel(), dtype=""complex128"")
```

### Expected Behavior

I'd expect the semantics of nunique to be unchanged

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : a885d6796501923905b9381ecafde07ada57a914
python                : 3.14.0
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Aug 11 21:15:09 PDT 2025; root:xnu-11417.140.69.701.11~1/RELEASE_ARM64_T6041
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2752.ga885d67965
numpy                 : 2.3.5
dateutil              : 2.9.0.post0
pip                   : 25.3
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Regression', 'Interval']",2025-11-27 09:01:08,2025-12-05 17:35:25,3,closed
63213,BUG: Inconsistent str implementation for Timestamp with different units,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from datetime import datetime
import pandas as pd


df = pd.DataFrame({'str': ['2020-01-01 00:00:00.123456789+00:00']})
df['ts'] = pd.to_datetime(df['str'])
df['ts_nano'] = df['ts'].astype('datetime64[ns, UTC]')
df['ts_micro'] = df['ts'].astype('datetime64[us, UTC]')
df['ts_milli'] = df['ts'].astype('datetime64[ms, UTC]')
df['ts_s'] = df['ts'].astype('datetime64[s, UTC]')


print(df.dtypes)
print()

for col in df.columns:
    print(f""{col.ljust(10)} {df[col].iloc[0]}"")

print()
print(""CSV"")

csv_str = df.T.to_csv(index=False)
print(csv_str)
```

### Issue Description

In repro we see:

```
str                      object
ts          datetime64[ns, UTC]
ts_nano     datetime64[ns, UTC]
ts_micro    datetime64[us, UTC]
ts_milli    datetime64[ms, UTC]
ts_s         datetime64[s, UTC]
dtype: object

str        2020-01-01 00:00:00.123456789+00:00
ts         2020-01-01 00:00:00.123456789+00:00
ts_nano    2020-01-01 00:00:00.123456789+00:00
ts_micro   2020-01-01 00:00:00.123456+00:00
ts_milli   2020-01-01 00:00:00.123000+00:00
ts_s       2020-01-01 00:00:00+00:00
```

Observations:

`ts_micro` prints as a _truncated string_, i.e. `789` are omitted, but:
`ts_milli` _replaces_ the `456` with `000`.

This behavior is repeated in `to_csv` output, which is where I encountered the inconsistency.

### Expected Behavior

Either truncate the output or replace with zeros, not both.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-1030-azure
Version               : #35~22.04.1-Ubuntu SMP Mon May 26 18:08:30 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.2.2
matplotlib            : None
numba                 : 0.62.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.2
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-11-26 17:32:33,2025-11-26 17:34:50,1,closed
63209,DOC: `Index.putmask` can accept more than `np.ndarray[bool]` for `mask`,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.Index.putmask.html

### Documentation problem

`mask` can accept 1d arrays of numpy, `list`s of `bool` and `np.bool`, pandas `BooleanArray`, `Index[bool]` and `Series[bool]`.

Originated from [pandas-dev/pandas-stubs#1505](https://github.com/pandas-dev/pandas-stubs/pull/1505/files#r2560759551).

### Suggested fix for documentation

`mask : AnyArrayLike of boolean values`","['Docs', 'Index']",2025-11-26 14:38:20,2025-12-01 18:57:25,0,closed
63194,BUG: option_context does not accept a dict,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas import option_context
with option_context(""display.max_rows"", 10, ""display.max_columns"", 5):
    pass
with option_context({""display.max_rows"": 10, ""display.max_columns"": 5}):
    pass
```

### Issue Description

the code from [`option_context`](https://pandas.pydata.org/docs/dev/reference/api/pandas.option_context.html) doesn't work:
```
File ~\AppData\Local\Venvs\p314\Lib\site-packages\pandas\_config\config.py:473, in option_context.__init__(self, *args)
    471 def __init__(self, *args) -> None:
    472     if len(args) % 2 != 0 or len(args) < 2:
--> 473         raise ValueError(
    474             ""Need to invoke as option_context(pat, val, [(pat, val), ...]).""
    475         )
    477     self.ops = list(zip(args[::2], args[1::2]))

ValueError: Need to invoke as option_context(pat, val, [(pat, val), ...]).
```

### Expected Behavior

the code lifted from the doc should work as documented

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.14.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 85 Stepping 7, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.3.3
numpy                 : 2.3.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.3
Cython                : None
sphinx                : None
IPython               : 9.7.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.10.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.7
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.10.0
scipy                 : 1.16.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.2
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
",['Bug'],2025-11-24 21:39:13,2025-11-25 03:11:20,1,closed
63189,"DOC: `Index.take` can have `axis=""index""`, in contrast to the claims `int` and `always 0`","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.Index.take.html

### Documentation problem

`axis` being `""index""` is also accepted at run time, in contrast to the claim of being `int` and `always 0`.

```py
import pandas as pd

pd.Series([1, 2, 3, 4]).take([1, 1, 2, 2], axis=""index"")
```

### Suggested fix for documentation

```
        axis : {0 or 'index'}, optional
            The axis over which to select values, always 0 or 'index'.
```","['Docs', 'Index']",2025-11-24 14:59:26,2025-11-25 18:15:35,1,closed
63188,BUG: read_excel of to_excel not equal to original dataframe,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas import DataFrame, NA, StringDtype, read_excel


df = DataFrame(data={'string': ['None', """", NA], 'float': [float('nan'), float('inf'), float('-inf')], 'None': [None, None, NA]})
df[""string""] = df[""string""].astype(StringDtype())
df[""float""] = df[""float""].astype(float)
df[""None""] = df[""None""].astype(object)


df.to_excel('test.xlsx')

df_import = read_excel('test.xlsx', index_col=0, dtype=df.dtypes.to_dict())

print(df.dtypes)
print('-------------------------')
print(df_import.dtypes)
print('-------------------------')
print(df)
print('-------------------------')
print(df_import)
```

### Issue Description

When passing some objects to to_excel and reading them back the type of the object will change.
I would assume this is unintended edge case behavior. Is there a good way to mitigate this?



### Expected Behavior

Especially the empty string and string containing 'None' should be loaded as strings and not converted to NA as they were assigned. Also the NA in the object column should be read back to NA and not converted to a float. It would also be preferred if None objects would be read as None or NA.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.10.11
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : de_DE.cp1252
pandas                : 2.3.3
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.3
Cython                : None
sphinx                : 8.1.3
IPython               : 8.37.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.7
numba                 : 0.62.1
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : 3.2.9
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO Excel']",2025-11-24 11:00:50,2025-11-25 03:17:27,1,closed
63180,BUG: Wrong Results for `Rolling.sem`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

n = 2
s = pd.Series(range(n))
# mean: 0.5
# sum((x - mean) ** 2) = 2 * .25 = .5
# sample variance: s^2 = .5 / 1
# s = sqrt(s^2) = sqrt(0.5) = 1 / sqrt(2)
# sem = s / sqrt(n) = 1 / (sqrt(2) * sqrt(2)) = 0.5

print(s.sem())  # 0.5
print(s.rolling(n).sem()[1])  # 0.7071067811865476
```

### Issue Description

The rolling standard error of the mean (SEM) produces wrong values compared to the non-rolling version `Series.sem()`.

### Expected Behavior

For the same data and same `ddof`, `Rolling.sem()` and `Series.sem()` should produce identical results.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c582205e5a776de0774b114fd898531f62be7f2
python                : 3.13.9
python-bits           : 64
OS                    : Linux
OS-release            : 6.17.7-300.fc43.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Sun Nov  2 15:30:09 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : pt_BR.UTF-8
LOCALE                : pt_BR.UTF-8

pandas                : 3.0.0.dev0-2729-g9c582205e5
numpy                 : 2.3.4
dateutil              : 2.9.0.post0
pip                   : 25.3
Cython                : 3.2.1
sphinx                : 8.2.3
IPython               : 9.7.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
bottleneck            : 1.6.0
fastparquet           : 2024.11.0
fsspec                : 2025.10.0
html5lib              : 1.1
hypothesis            : 6.148.0
gcsfs                 : 2025.10.0
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.7
numba                 : 0.62.1
numexpr               : 2.14.1
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.11
pymysql               : 1.4.6
pyarrow               : 22.0.0
pyiceberg             : 0.10.0
pyreadstat            : 1.3.2
pytest                : 9.0.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.10.0
scipy                 : 1.16.3
sqlalchemy            : 2.0.44
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.10.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.9
zstandard             : 0.25.0
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Window', 'Reduction Operations']",2025-11-23 22:05:29,2025-11-25 18:32:00,0,closed
63156,BUG: groupby median() doesn't exclude NaT for datetime64 columns,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame(
    data=[
        [1,'2025-11-18 10:00:00'],
        [1,'2025-11-18 11:00:00'],
        [1,'NaT'],
        [1,'NaT']
    ],
    columns=['group','time']
)
df['time'] = pd.to_datetime(df['time'])
```

### Issue Description

The documentation for DataFrameGroupBy.median says that missing values will be excluded from the calculation, which is generally true.  However, with a timestamp column (e.g. datetime64), I believe NaT is being treated as '0' (or some other non-null value).  In the example:

```
>>> df
   group                time
0      1 2025-11-18 10:00:00
1      1 2025-11-18 11:00:00
2      1                 NaT
3      1                 NaT
```

mean() works:

```
>>> df.groupby('group').mean()
                     time
group                    
1     2025-11-18 10:30:00
```

But median() does not:

```
>>> df.groupby('group').median()
                               time
group                              
1     1851-10-21 05:06:21.572612096
```

Note that the full Series median works; I only notice this behavior when using groupby:

```
>>> df['time'].median()
Timestamp('2025-11-18 10:30:00')
```


### Expected Behavior

NaT values in a datetime column should be treated like NaN values in a numeric column when computing the median.  That is, they should be excluded.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.11.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.12.38+deb12-amd64
Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.12.38-1~bpo12+1 (2025-07-27)
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.5
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 23.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Duplicate Report']",2025-11-19 18:51:21,2025-11-19 23:03:02,1,closed
63155,ENH: make sorting algorithm options to `value_counts` and record it in the docs,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

In the [dev documentation](https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.value_counts.html), it is not clear which sorting algorithm is used by `value_counts`, which is also fixed.

It would be helpful to make such an option and record it in the docs.

### Feature Description

We can follow the example of [`sort_values`](https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.sort_values.html), adding the parameter `kind`:

```py
import pandas as pd

s = pd.Series([1, 1, 1, 2, 2, 2]).value_counts(kind=""stable"")
```

### Alternative Solutions

We can just record in the docs that the algo is fixed to `quicksort`.

### Additional Context

Related to pandas-dev/pandas#63154 and numpy/numpy#30262. As a user I would like to know that the sorting result is deterministic and optionally stable.","['Enhancement', 'Algos', 'Sorting']",2025-11-19 16:58:46,2025-12-04 17:16:23,1,closed
63141,CLN:  s variable in _init_dict,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

series.py

### Documentation problem

The variable s in _init_dict is currently unclear for code readability.

### Suggested fix for documentation

Change s to series_obj","['Docs', 'Needs Triage', 'AI Slop']",2025-11-18 07:56:05,2025-11-18 22:31:32,1,closed
63140,"BUG: dateutil 2.9.x cannot load tzdata on systems without system zoneinfo (z/OS, AIX, some Unix variants)","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
pip install python-dateutil==2.9.0 tzdata
from dateutil.tz import gettz
print(gettz(""US/Pacific""))  # returns None on z/OS
```

### Issue Description

On platforms that do not provide a system IANA zoneinfo database (e.g., IBM z/OS, some AIX installations, minimal Unix builds), python-dateutil 2.9.x is unable to load any timezones. This breaks timezone-aware features in pandas, numpy, and many other libraries.

The issue appears to be a regression introduced when bundled tzdata was removed in 2.9.0.

### Expected Behavior

When the tzdata package is installed (which provides IANA tz files at tzdata/zoneinfo), dateutil should:

- detect tzdata
- automatically use its zoneinfo files
- return a valid tzinfo object

This behavior existed implicitly in dateutil 2.8.2, which bundled tzdata internally.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : None
python                : 3.13.3
python-bits           : 64
OS                    : OS/390
OS-release            : 30.00
Version               : 05
machine               : 3931
processor             : 
byteorder             : big
LC_ALL                : None
LANG                  : C
LOCALE                : None.None

pandas                : 2.3.3
numpy                 : 2.0.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : 3.1.3
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : 6.48.1
gcsfs                 : None
jinja2                : None
lxml.etree            : 6.0.1
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.44
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage']",2025-11-18 05:27:33,2025-11-18 08:42:27,0,closed
63138,ENH: Add DataFrame.to_toon() method for TOON serialization support,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Pandas is increasingly being used in workflows involving Large Language Models (LLMs), RAG pipelines, context-packing, and prompt engineering. Currently, pandas supports exporting data to formats such as JSON, Markdown, CSV, Parquet, and others, but there is no built-in way to export DataFrames to the TOON (Token-Oriented Object Notation) format.

TOON is a new open serialization format designed specifically for LLM-optimized structured data representation. It achieves significant token savings compared to JSON (often 30–60%) and is ideal for passing structured tabular data into LLM prompts.

The problem: pandas has no native way to convert a DataFrame into TOON format, even though DataFrames map naturally to TOON’s tabular representation. Adding `.to_toon()` would enable users to efficiently serialize DataFrames for LLM applications, improve cost efficiency, and reduce token usage.


### Feature Description

I propose adding a new method:

    DataFrame.to_toon(index=False, indent=2, columns=None, **kwargs)

Implementation outline:

1. A new IO function similar to `to_json`, `to_markdown`, and `to_csv`
2. Pandas normalizes the DataFrame to a Python dict or list-of-records structure
3. An external dependency (e.g., `pytoon`) performs the dict → TOON encoding
4. TOON’s tabular layout:
       fields: col1 col2 col3
       rows: N
       - value1 value2 value3

5. Follows pandas conventions:
       - optional `index=bool`
       - column selection via `columns`
       - `indent` for readable output
       - returns a string or writes to file

This design matches existing `.to_*` patterns and keeps TOON encoding logic outside core pandas.


### Alternative Solutions

A user can manually convert a DataFrame to a list of dicts and use a separate Python TOON encoding library, but this requires custom code for every project and loses the convenience and consistency of the pandas IO API.

There is currently no built-in pandas method or widely-used third-party library that provides seamless DataFrame → TOON serialization similar to `to_json()`.


### Additional Context

TOON Specification:
https://github.com/toon-format/spec

Reference implementation:
https://github.com/toon-format/toon

TOON (Token-Oriented Object Notation) is designed for compact, LLM-friendly serialization. It reduces token usage significantly when serializing large DataFrames, especially in uniform tabular structures.

Example DataFrame:

    df = pd.DataFrame({""name"": [""Ali"", ""Sara""], ""age"": [23, 21]})
    df.to_toon(index=False)

Possible output according to the TOON spec:

    fields: name age
    rows: 2
    - Ali 23
    - Sara 21

I am willing to contribute a PR with full implementation + tests if maintainers approve the feature.
","['Enhancement', 'Closing Candidate', 'IO Format Request']",2025-11-17 18:14:56,2025-11-18 14:22:03,4,closed
63128,BUG: undefined symbol when building Pandas with a custom python installation,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas
```

### Issue Description

When building pandas from source with a custom python installation (3.13 with debug symbols) I get an error when importing pandas.

```console
$ python -c 'import pandas'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
    import pandas
  File ""/home/user/pandas/pandas/__init__.py"", line 45, in <module>
    from pandas.core.api import (
    ...<61 lines>...
    )
  File ""/home/user/pandas/pandas/core/api.py"", line 46, in <module>
    from pandas.core.groupby import (
    ...<2 lines>...
    )
  File ""/home/user/pandas/pandas/core/groupby/__init__.py"", line 1, in <modul
e>
    from pandas.core.groupby.generic import (
    ...<3 lines>...
    )
  File ""/home/user/pandas/pandas/core/groupby/generic.py"", line 62, in <modul
e>
    from pandas.core.apply import (
    ...<4 lines>...
    )
  File ""/home/user/pandas/pandas/core/apply.py"", line 19, in <module>
    from pandas._libs.internals import BlockValuesRefs
ImportError: /home/user/pandas/build/cp313d/pandas/_libs/internals.cpython-313d-x86_64-linux-gnu.so: undefined symbol: __Pyx_PyUnstable_Object_IsUniqueReferencedTemporary
```

And indeed the symbol is undefined:

```console
$ nm build/cp313d/pandas/_libs/internals.cpython-313d-x86_64-linux-gnu.so | grep Unique
                 U __Pyx_PyUnstable_Object_IsUniqueReferencedTemporary
```

The build using my system python3.13 doesn't even have this symbol declared:
```console
$ nm build/cp313/pandas/_libs/internals.cpython-313-x86_64-linux-gnu.so | grep Unique
<empty>
```

### Expected Behavior

Can import pandas normally. Hence, either define the missing symbol or to don't declare it.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : fa5b90a079f6920a235ef6bb9c36ca0fafc765ed
python                : 3.13.9
python-bits           : 64
OS                    : Linux
OS-release            : 6.17.7-300.fc43.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Sun Nov  2 15:30:09 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : pt_BR.UTF-8
LOCALE                : pt_BR.UTF-8

pandas                : 3.0.0.dev0+2714.gfa5b90a079
numpy                 : 2.3.4
dateutil              : 2.9.0.post0
pip                   : 25.3
Cython                : 3.2.1
sphinx                : 8.2.3
IPython               : 9.7.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
bottleneck            : 1.6.0
fastparquet           : 2024.11.0
fsspec                : 2025.10.0
html5lib              : 1.1
hypothesis            : 6.148.0
gcsfs                 : 2025.10.0
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.7
numba                 : 0.62.1
numexpr               : 2.14.1
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.11
pymysql               : 1.4.6
pyarrow               : 22.0.0
pyiceberg             : 0.10.0
pyreadstat            : 1.3.2
pytest                : 9.0.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.10.0
scipy                 : 1.16.3
sqlalchemy            : 2.0.44
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.10.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.9
zstandard             : 0.25.0
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Build']",2025-11-16 15:37:05,2025-11-24 01:11:27,0,closed
63120,"Possible redundant assertion `issubclass(type(a), type(a))`","Since a class is considered a subclass of itself, the expression `issubclass(type(a), type(a))` is always `True`.  Therefore, the following three assert statements in `/pandas/tests/dtypes/test_dtypes.py` seem redundant and meaningless to me:

```
class TestDatetimeTZDtype(Base):
    def test_subclass(self):
        assert issubclass(type(a), type(a))

class TestPeriodDtype(Base):
    def test_subclass(self):
        assert issubclass(type(a), type(a))

class TestIntervalDtype(Base):
    def test_subclass(self):
        assert issubclass(type(a), type(a))
```

If there is some special purpose behind the assertion, could you please confirm? Thanks.","['Testing', 'Clean']",2025-11-15 07:29:39,2025-11-16 13:59:51,3,closed
63119,Redundant type checks and dead code requiring refactoring,"While reviewing `/pandas/tests/io/parser/test_na_values.py`, I found a redundant nested type check in `test_na_values_scalar(all_parsers, na_values, row_data)` with an unreachable branch:

```
    if parser.engine == ""pyarrow"" and isinstance(na_values, dict):
        if isinstance(na_values, dict):  # redundant because isinstance(na_values, dict) must be True
            err = ValueError
            msg = ""The pyarrow engine doesn't support passing a dict for na_values""
        else:  # unreachable
            err = TypeError
            msg = ""The 'pyarrow' engine requires all na_values to be strings""
        with pytest.raises(err, match=msg):
            parser.read_csv(StringIO(data), names=names, na_values=na_values)
        return
    elif parser.engine == ""pyarrow"":
        msg = ""The 'pyarrow' engine requires all na_values to be strings""
        with pytest.raises(TypeError, match=msg):
            parser.read_csv(StringIO(data), names=names, na_values=na_values)
        return
```

It seems that the inner implementation was not updated  when refactoring the outer code. I am not sure whether the intended code should be:

```
    if parser.engine == ""pyarrow"" and isinstance(na_values, dict):
        msg = ""The pyarrow engine doesn't support passing a dict for na_values""
        with pytest.raises(ValueError, match=msg):
            parser.read_csv(StringIO(data), names=names, na_values=na_values)
        return
    elif parser.engine == ""pyarrow"":
        msg = ""The 'pyarrow' engine requires all na_values to be strings""
        with pytest.raises(TypeError, match=msg):
            parser.read_csv(StringIO(data), names=names, na_values=na_values)
        return
```","['Testing', 'Clean']",2025-11-15 07:04:04,2025-11-18 03:12:57,3,closed
63092,ENH: Add Native Support for Reading CoNLL Files in Pandas,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could use pandas to directly read .conll files, which are standard formats in NLP for tasks like named entity recognition, part-of-speech tagging, and dependency parsing. Currently, I need to write custom parsers or use workarounds that are error-prone and not reusable across projects.

### Feature Description

Add a lightweight pandas.read_conll function built on existing pandas infrastructure:
\# Implementation would leverage existing read_csv capabilities
def read_conll(filepath_or_buffer, columns=None, group_by_sentence=False, 
               comment_char='#', **kwargs):
    """"""
    Read CoNLL format files into DataFrame.
    """"""
    Builds on pd.read_csv with CoNLL-specific preprocessing:
    - Tab-separated values with comment skipping
    - Sentence boundary detection via blank lines  
    - Optional sentence_id assignment
    - No external dependencies required
    """"""

### Alternative Solutions

1. Using read_csv with manual processing: Requires handling comments, blank lines, and sentence boundaries manually, which is fragile and error-prone.
df = pd.read_csv('data.conll', sep='\t', comment='#', skip_blank_lines=False)
2. Third-party libraries: Packages like conllu require additional dependencies and conversion steps to get DataFrames.
3. Custom parsers: Writing project-specific parsers is time-consuming and not reusable.

### Additional Context

Sample file:
Standard CoNLL-2003 format：
\# Document: example.txt
EU NNP B-NP B-ORG
rejects VBZ B-VP O
German JJ B-NP B-MISC
. . O O

The DT B-NP O
European NNP I-NP B-ORG
Commission NNP I-NP I-ORG
. . O O

Current pandas result (problematic):
Option 1: Loses sentence boundaries
df = pd.read_csv('data.conll', sep='\t', comment='#', names=['token', 'pos', 'chunk', 'ner'])
\# Result: All sentences merged, blank lines silently skipped
Option 2: Creates messy NaN rows
df = pd.read_csv('data.concll', sep='\t', comment='#', skip_blank_lines=False, 
                 names=['token', 'pos', 'chunk', 'ner'])
\# Result: Blank lines become NaN rows, requires manual sentence ID processing

Desired result with read_conll:
df = pd.read_conll('data.conll', group_by_sentence=True)
  token       pos chunk     ner  sentence_id
0     EU       NNP  B-NP   B-ORG            0
1 rejects       VBZ  B-VP       O            0
2  German        JJ  B-NP   B-MISC           0
3       .        .     O       O            0
4     The        DT  B-NP       O            1
5 European       NNP  I-NP   B-ORG           1
6 Commission     NNP  I-NP   I-ORG           1
7       .        .     O       O            1","['Enhancement', 'Needs Triage', 'Closing Candidate', 'IO Format Request']",2025-11-12 07:51:55,2025-11-25 19:43:33,5,closed
63089,BUG: read_csv results in SIGSEGV,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import io
import pandas as pd

pd.read_csv(io.StringIO(""""""h
4e492493924924""""""))
```

### Issue Description

Running the example results in SIGSEGV.

### Expected Behavior

Running the example should not cause SIGSEGV.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.12.10
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-87-generic
Version               : #88-Ubuntu SMP PREEMPT_DYNAMIC Sat Oct 11 09:28:41 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.3
Cython                : None
sphinx                : None
IPython               : 9.7.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 22.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'IO CSV', 'Segfault']",2025-11-12 05:14:05,2025-11-17 21:01:39,2,closed
63068,DOC: Improve now() by adding examples,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.now.html



### Documentation problem

Explain what are valid strings for this (IANA time zones presumably, not sure if anything else)

### Suggested fix for documentation

Sorry no time","['Docs', 'Timestamp']",2025-11-10 21:59:36,2025-11-29 13:04:00,2,closed
63061,BUG/ENH: `DataFrame.__eq__()` and `__ne__()` should return `bool`,"### [This comment below is a TLDR for this feature request](https://github.com/pandas-dev/pandas/issues/63061#issuecomment-3513513618)

See below for the original bug report.

---

### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd


def function_that_should_work_with_any_arg_type(obj, default_value=None):
    if obj != default_value:
        ...


function_that_should_work_with_any_arg_type(pd.DataFrame())
```

### Issue Description

### Background

I have close to zero experience with pandas, but I'm touching some input and output pandas objects (like `DataFrame`s) as part of a Django application - which uses [Sentry](https://sentry.io/) for monitoring. In one place, we're using [Django's cache framework](https://docs.djangoproject.com/en/5.2/topics/cache/#basic-usage) to cache [`GeoDataFrame` objects](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html) (which inherit from `DataFrame`).

Now, Sentry has a Django integration that hooks onto the caching framework, and it does the following check when calling [`cache.get()`](https://docs.djangoproject.com/en/5.2/topics/cache/#django.core.cache.cache.get):
https://github.com/getsentry/sentry-python/blob/b069aa24fdf3c52a9e8b75f4f83d5fee035c3234/sentry_sdk/integrations/django/caching.py#L86

In our case, `value` is a `GeoDataFrame`, and `default_value` is `None` - which creates the example scenario above.

<details>
<summary><i>Potentially relevant installed package versions</i></summary>

```
Django     : 4.2.23
geopandas  : 1.0.1
sentry-sdk : 2.43.0
```

</details>

### Issue

The code in the example above will raise `ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().` (because `obj != default_value` returns a `DataFrame` object).

Use cases similar to Sentry's Django cache hook seem perfectly reasonable and innocuous - in that doing a simple equality comparison between two arbitrary objects and checking the `bool` value of the result should always be a type safe operation - and so this definitely feels like a bug with pandas - hence why I chose the bug report issue template. However, from my understanding, this behavior is a deliberate design choice on your end (even if it feels like a slight misuse of Python's magic method system, to my naive mind 😅), and so you can also consider this a feature request - hence ""ENH"" in the issue title 🙂

Lastly, it could be relevant to mention that [the current implementation of `DataFrame.__eq__()` and `DataFrame.__ne__()` (through `OpsMixin`)](https://github.com/pandas-dev/pandas/blob/f4851e500a43125d505db64e548af0355227714b/pandas/core/arraylike.py#L40-L45) violate the conventional return type contract of `object.__eq__()` and `object.__ne__()`:
https://github.com/python/typeshed/blob/4239e94a3148fd3250e894f9b8e5a4ccbfe2f5a0/stdlib/builtins.pyi#L124-L125

### Expected Behavior

That `obj != default_value` (in the example above) returns `True` (i.e. a `bool`, not some `DataFrame`) and that `obj == default_value` returns `False`, and that `ValueError` is *not* raised.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.10.11
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26200
machine               : AMD64
processor             : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252
pandas                : 2.3.0
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : 5.4.0
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.11
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
```

</details>
","['Enhancement', 'Closing Candidate']",2025-11-10 14:01:56,2025-11-12 02:32:29,8,closed
63045,BUG: String categories are not quoted as expected,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
s = pd.Series([""apple"", ""banana"", ""cherry"", ""cherry""], dtype=""string"")
cat = pd.Categorical(s)
print(cat)
# Expected: Categories (3, string): ['apple', 'banana', 'cherry']
# Output: Categories (3, string): [apple, banana, cherry]
```

### Issue Description

When Categorical receives a pandas Series with dtype of “string”, representation of string elements in the list of unique categories will not include quotes. During my search against existing Issues, I think this relates to Issues #61890. The proposed fix in PR #61891 only checks dtype against “str”. Therefore, it still fails to check pandas Series with dtype of “string”. This PR fixes this problem by adding a check against “string”, like this:

```
- if self.categories.dtype == ""str"":
+ if self.categories.dtype == ""str"" or self.categories.dtype == ""string"":
```

If this fix is good to you, I can open a PR for your review. 

### Expected Behavior

String categories are expected to be quoted, but the current output is not.

Expected: Categories (3, string): ['apple', 'banana', 'cherry']
Output: Categories (3, string): [apple, banana, cherry]

### Installed Versions

<details>

commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.13.9
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 22.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Output-Formatting', 'Categorical']",2025-11-08 13:32:20,2025-11-13 12:04:12,2,closed
63027,ENH: Dataframe melt : the variable column should be typed as string,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

When I use a melt in my script, I always have to retype manually the ""variable"" column to string.

<img width=""347"" height=""193"" alt=""Image"" src=""https://github.com/user-attachments/assets/c28027ab-3b02-4f5a-a201-ecbbfe79e467"" />

### Feature Description

A column name is always a string so it would make sense to cast it directly to the good dtype by default, here a string.

### Alternative Solutions

The one I'm doing today : manually set the variable column to string

### Additional Context

_No response_","['Enhancement', 'Reshaping']",2025-11-07 14:55:59,2025-11-07 18:49:04,2,closed
63019,BUG: pd.json_normalize has inconsistent validation for non-string keys - accepts integers in some parameter combinations but rejects in others,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
data = [{""a"": 1, 12: ""meta_value"", ""nested"": [{""b"": 2}]}]
# This works - integer as meta  
pd.json_normalize(data, meta=[12])
# But this fails - integer as meta (with record_path)
pd.json_normalize(data, record_path=[""nested""], meta=[12])
```

### Issue Description

pd.json_normalize exhibits inconsistent behavior when using integer keys in the meta parameter:
meta=[12] (without record_path) - WORKS
 record_path=[""nested""], meta=[12] - FAILS with TypeError: sequence item 0: expected str instance, int found

### Expected Behavior

This should either be consistently supported or consistently rejected.

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Error Reporting', 'IO JSON']",2025-11-07 03:40:00,2025-11-27 10:22:39,5,closed
62998,BUG: query column if values in list fails in Debian,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df.head()[['Country','GDP']]
Country 	GDP
0 	Abkhazia 	NaN
1 	Afghanistan 	1.946902e+10
2 	Albania 	1.186387e+10
3 	Algeria 	1.590490e+11
4 	American Samoa 	NaN

un = ['Afghanistan',
 'Albania',
 'Algeria',
 'Andorra',
 'Angola',
 'Antigua and Barbuda', ... # etc. to Zimbabwe
]
df.query(f""Country in {un}"")
# 
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[4], line 1
----> 1 df.query(f'` in {un}')

File ~/anaconda3/envs/py13/lib/python3.13/site-packages/pandas/core/frame.py:4834, in DataFrame.query(self, expr, inplace, **kwargs)
   4832 kwargs[""level""] = kwargs.pop(""level"", 0) + 1
   4833 kwargs[""target""] = None
-> 4834 res = self.eval(expr, **kwargs)
   4836 try:
   4837     result = self.loc[res]

File ~/anaconda3/envs/py13/lib/python3.13/site-packages/pandas/core/frame.py:4954, in DataFrame.eval(self, expr, inplace, **kwargs)
   4952 kwargs[""level""] = kwargs.pop(""level"", 0) + 1
   4953 index_resolvers = self._get_index_resolvers()
-> 4954 column_resolvers = self._get_cleaned_column_resolvers()
   4955 resolvers = column_resolvers, index_resolvers
   4956 if ""target"" not in kwargs:

File ~/anaconda3/envs/py13/lib/python3.13/site-packages/pandas/core/generic.py:663, in NDFrame._get_cleaned_column_resolvers(self)
    659 if isinstance(self, ABCSeries):
    660     return {clean_column_name(self.name): self}
    662 return {
--> 663     clean_column_name(k): Series(
    664         v, copy=False, index=self.index, name=k, dtype=self.dtypes[k]
    665     ).__finalize__(self)
    666     for k, v in zip(self.columns, self._iter_column_arrays())
    667     if not isinstance(k, int)
    668 }

File ~/anaconda3/envs/py13/lib/python3.13/site-packages/pandas/core/series.py:496, in Series.__init__(self, data, index, dtype, name, copy, fastpath)
    493     index = ensure_index(index)
    495 if dtype is not None:
--> 496     dtype = self._validate_dtype(dtype)
    498 if data is None:
    499     index = index if index is not None else default_index(0)

File ~/anaconda3/envs/py13/lib/python3.13/site-packages/pandas/core/generic.py:519, in NDFrame._validate_dtype(cls, dtype)
    517 """"""validate the passed dtype""""""
    518 if dtype is not None:
--> 519     dtype = pandas_dtype(dtype)
    521     # a compound dtype
    522     if dtype.kind == ""V"":

File ~/anaconda3/envs/py13/lib/python3.13/site-packages/pandas/core/dtypes/common.py:1684, in pandas_dtype(dtype)
   1682     return npdtype
   1683 elif npdtype.kind == ""O"":
-> 1684     raise TypeError(f""dtype '{dtype}' not understood"")
   1686 return npdtype

TypeError: dtype '    object
    object
    object
    object
    object
    object
    object
    object
    object
dtype: object' not understood
```

### Issue Description

Use Pandas query to filter a pandas dataframe column of strings based on if they are present in a list of strings fails.

### Expected Behavior

Returns a dataframe with only the rows where the values in a column are present in a list.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.13.9
python-bits           : 64
OS                    : Linux
OS-release            : 6.12.48+deb13-amd64
Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.12.48-1 (2025-09-20)
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : 9.5.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.10.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.7
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.3
sqlalchemy            : 2.0.44
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.2
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None




</details>
","['Bug', 'Needs Triage']",2025-11-05 13:23:33,2025-11-06 22:00:13,5,closed
62981,BUG: `item_from_zerodim` converts objects of subclasses of `ndarray` to `float`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
###########
# main.py #
###########

# import astropy.units as u
import numpy as np
from pandas._libs.lib import item_from_zerodim

from new import item_from_zerodim_new

# Define a custom ndarray subclass
class TestArray(np.ndarray):
    def __new__(cls, input_array):
        return np.asarray(input_array).view(cls)

    def __array_finalize__(self, obj) -> None:
        self._is_test_array = True


# Define test data
val_0_dim = 1
val_1_dim = [1, 2, 3]

# 0-dim and 1-dim numpy arrays
arr_0_dim = np.array(val_0_dim)
arr_1_dim = np.array(val_1_dim)

# 0-dim and 1-dim TestArray arrays
test_arr_0_dim = TestArray(val_0_dim)
test_arr_1_dim = TestArray(val_1_dim)

# 0-dim and 1-dim astropy Quantity arrays
# q_0_dim: u.Quantity[u.physical.length] = u.Quantity(val_0_dim, u.m)
# q_1_dim: u.Quantity[u.physical.length] = u.Quantity(val_1_dim, u.m)

# Test each value
for val in [arr_0_dim, arr_1_dim, test_arr_0_dim, test_arr_1_dim]:
# for val in [arr_0_dim, arr_1_dim, test_arr_0_dim, test_arr_1_dim, q_0_dim, q_1_dim]:
    print(f""Testing value: {val=} with {val.ndim=}"")

    print(""\nShow val is instance of ndarray, TestArray or Quantity:"")
    print(f""{isinstance(val, np.ndarray)=}"")
    print(f""{isinstance(val, TestArray)=}"")
    # print(f""{isinstance(val, u.Quantity)=}"")

    print(""\nShow type of val is ndarray, TestArray or Quantity:"")
    print(f""{type(val) is np.ndarray=}"")
    print(f""{type(val) is TestArray=}"")
    # print(f""{type(val) is u.Quantity=}"")

    print(""\nUsing pandas item_from_zerodim:"")
    val_zero_dim_pandas = item_from_zerodim(val)
    print(f""{val_zero_dim_pandas=}"")
    print(f""{type(val_zero_dim_pandas)=}"")
    print(f""{getattr(val_zero_dim_pandas, ""_is_test_array"", None)=}"")
    # print(f""{getattr(val_zero_dim_pandas, ""unit"", None)=}"")

    print(""\nUsing new item_from_zerodim with cnp.PyArray_CheckExact check:"")
    val_zero_dim_new = item_from_zerodim_new(val)
    print(f""{val_zero_dim_new=}"")
    print(f""{type(val_zero_dim_new)=}"")
    print(f""{getattr(val_zero_dim_new, ""_is_test_array"", None)=}"")
    # print(f""{getattr(val_zero_dim_new, ""unit"", None)=}"")
    print(""\n\n\n"")



###########
# new.pyx #
###########

cimport numpy as cnp

def item_from_zerodim_new(val: object) -> object:
    """"""
    If the value is a zerodim ndarray (NOT subclass), return the item it contains.

    Parameters
    ----------
    val : object

    Returns
    -------
    object

    Examples
    --------
    >>> item_from_zerodim(1)
    1
    >>> item_from_zerodim('foobar')
    'foobar'
    >>> item_from_zerodim(np.array(1))
    1
    >>> item_from_zerodim(np.array([1]))
    array([1])
    """"""
    if cnp.PyArray_IsZeroDim(val) and cnp.PyArray_CheckExact(val):
        return cnp.PyArray_ToScalar(cnp.PyArray_DATA(val), val)
    return val



############
# setup.py #
############

from setuptools import setup
from Cython.Build import cythonize
import numpy as np

setup(
    ext_modules=cythonize(""new.pyx""),
    include_dirs=[np.get_include()]
)
```

### Issue Description

#### Issue Description
When using arithmetic operations of a pandas Series or DataFrame with scalar objects subclassed from `np.ndarray` (so `obj.ndim == 0`), the scalar objects are converted to `float` by the [`item_from_zerodim` function](https://github.com/pandas-dev/pandas/blob/77fdffd507f26b7032f99097a384394a89f656d3/pandas/_libs/lib.pyx#L302) as it is only checking if the value is of 0-dim ([see L325](https://github.com/pandas-dev/pandas/blob/77fdffd507f26b7032f99097a384394a89f656d3/pandas/_libs/lib.pyx#L325)). This is becomes an issue if custom attributes of the subclass are needed in the arithmetic operation.


#### Minimal Reproducible Example

For a minimal reproducible example I created the custom `TestArray` class (if astropy is in your dependency you can uncomment the corresponding lines). To execute the code, first compile the `new.pyx` file with `python setup.py build_ext --inplace`. Then executing the `main.py` file gives the following output:
<details>

```
Testing value: val=array(1) with val.ndim=0

Show val is instance of ndarray, TestArray or Quantity:
isinstance(val, np.ndarray)=True
isinstance(val, TestArray)=False
isinstance(val, u.Quantity)=False

Show type of val is ndarray, TestArray or Quantity:
type(val) is np.ndarray=True
type(val) is TestArray=False
type(val) is u.Quantity=False

Using pandas item_from_zerodim:
val_zero_dim_pandas=np.int64(1)
type(val_zero_dim_pandas)=<class 'numpy.int64'>
getattr(val_zero_dim_pandas, ""_is_test_array"", None)=None
getattr(val_zero_dim_pandas, ""unit"", None)=None

Using new item_from_zerodim with cnp.PyArray_CheckExact check:
val_zero_dim_new=np.int64(1)
type(val_zero_dim_new)=<class 'numpy.int64'>
getattr(val_zero_dim_new, ""_is_test_array"", None)=None
getattr(val_zero_dim_new, ""unit"", None)=None




Testing value: val=array([1, 2, 3]) with val.ndim=1

Show val is instance of ndarray, TestArray or Quantity:
isinstance(val, np.ndarray)=True
isinstance(val, TestArray)=False
isinstance(val, u.Quantity)=False

Show type of val is ndarray, TestArray or Quantity:
type(val) is np.ndarray=True
type(val) is TestArray=False
type(val) is u.Quantity=False

Using pandas item_from_zerodim:
val_zero_dim_pandas=array([1, 2, 3])
type(val_zero_dim_pandas)=<class 'numpy.ndarray'>
getattr(val_zero_dim_pandas, ""_is_test_array"", None)=None
getattr(val_zero_dim_pandas, ""unit"", None)=None

Using new item_from_zerodim with cnp.PyArray_CheckExact check:
val_zero_dim_new=array([1, 2, 3])
type(val_zero_dim_new)=<class 'numpy.ndarray'>
getattr(val_zero_dim_new, ""_is_test_array"", None)=None
getattr(val_zero_dim_new, ""unit"", None)=None




Testing value: val=TestArray(1) with val.ndim=0

Show val is instance of ndarray, TestArray or Quantity:
isinstance(val, np.ndarray)=True
isinstance(val, TestArray)=True
isinstance(val, u.Quantity)=False

Show type of val is ndarray, TestArray or Quantity:
type(val) is np.ndarray=False
type(val) is TestArray=True
type(val) is u.Quantity=False

Using pandas item_from_zerodim:
val_zero_dim_pandas=np.int64(1)
type(val_zero_dim_pandas)=<class 'numpy.int64'>
getattr(val_zero_dim_pandas, ""_is_test_array"", None)=None
getattr(val_zero_dim_pandas, ""unit"", None)=None

Using new item_from_zerodim with cnp.PyArray_CheckExact check:
val_zero_dim_new=TestArray(1)
type(val_zero_dim_new)=<class '__main__.TestArray'>
getattr(val_zero_dim_new, ""_is_test_array"", None)=True
getattr(val_zero_dim_new, ""unit"", None)=None




Testing value: val=TestArray([1, 2, 3]) with val.ndim=1

Show val is instance of ndarray, TestArray or Quantity:
isinstance(val, np.ndarray)=True
isinstance(val, TestArray)=True
isinstance(val, u.Quantity)=False

Show type of val is ndarray, TestArray or Quantity:
type(val) is np.ndarray=False
type(val) is TestArray=True
type(val) is u.Quantity=False

Using pandas item_from_zerodim:
val_zero_dim_pandas=TestArray([1, 2, 3])
type(val_zero_dim_pandas)=<class '__main__.TestArray'>
getattr(val_zero_dim_pandas, ""_is_test_array"", None)=True
getattr(val_zero_dim_pandas, ""unit"", None)=None

Using new item_from_zerodim with cnp.PyArray_CheckExact check:
val_zero_dim_new=TestArray([1, 2, 3])
type(val_zero_dim_new)=<class '__main__.TestArray'>
getattr(val_zero_dim_new, ""_is_test_array"", None)=True
getattr(val_zero_dim_new, ""unit"", None)=None




Testing value: val=<Quantity 1. m> with val.ndim=0

Show val is instance of ndarray, TestArray or Quantity:
isinstance(val, np.ndarray)=True
isinstance(val, TestArray)=False
isinstance(val, u.Quantity)=True

Show type of val is ndarray, TestArray or Quantity:
type(val) is np.ndarray=False
type(val) is TestArray=False
type(val) is u.Quantity=True

Using pandas item_from_zerodim:
val_zero_dim_pandas=np.float64(1.0)
type(val_zero_dim_pandas)=<class 'numpy.float64'>
getattr(val_zero_dim_pandas, ""_is_test_array"", None)=None
getattr(val_zero_dim_pandas, ""unit"", None)=None

Using new item_from_zerodim with cnp.PyArray_CheckExact check:
val_zero_dim_new=<Quantity 1. m>
type(val_zero_dim_new)=<class 'astropy.units.quantity.Quantity'>
getattr(val_zero_dim_new, ""_is_test_array"", None)=None
getattr(val_zero_dim_new, ""unit"", None)=Unit(""m"")




Testing value: val=<Quantity [1., 2., 3.] m> with val.ndim=1

Show val is instance of ndarray, TestArray or Quantity:
isinstance(val, np.ndarray)=True
isinstance(val, TestArray)=False
isinstance(val, u.Quantity)=True

Show type of val is ndarray, TestArray or Quantity:
type(val) is np.ndarray=False
type(val) is TestArray=False
type(val) is u.Quantity=True

Using pandas item_from_zerodim:
val_zero_dim_pandas=<Quantity [1., 2., 3.] m>
type(val_zero_dim_pandas)=<class 'astropy.units.quantity.Quantity'>
getattr(val_zero_dim_pandas, ""_is_test_array"", None)=None
getattr(val_zero_dim_pandas, ""unit"", None)=Unit(""m"")

Using new item_from_zerodim with cnp.PyArray_CheckExact check:
val_zero_dim_new=<Quantity [1., 2., 3.] m>
type(val_zero_dim_new)=<class 'astropy.units.quantity.Quantity'>
getattr(val_zero_dim_new, ""_is_test_array"", None)=None
getattr(val_zero_dim_new, ""unit"", None)=Unit(""m"")
```

</details>

It shows that the values of `TestArray` are converted to `float` for the `0-dim` case but not the `1-dim` case using the pandas `item_from_zerodim` function. However the newly created `item_from_zerodim_new` function, which adds `cnp.PyArray_CheckExact(val)` as an additional check, retains the types for both cases but still converts the `0-dim` `np.ndarray` case to a `float`.



#### Reservation
I currently only have tested the behavior of the new function with `np.ndarray` and the two subclasses. Not sure if there are other subclasses of `np.ndarray` (e.g. in pandas or other ExtensionDtype) that expect to be converted to `float` here and which would be broken by the new function.


#### Background:
I am currently developing an ExtensionDtype/ExtensionArray for astropy `Quantity ` objects (see [pandas-units-extension](https://github.com/Julian-Harbeck/pandas-units-extension)). The `Quantity` objects are a subclass of `np.ndarray`, but add an `unit` attribute for a physical dimension, e.g. `""m""` or `""cm""` for a length so that `u.Quantity(1, ""m"") + u.Quantity(10, ""cm"")` is equal to `u.Quantity(1.1, ""m"")` (1 meter plus 10 centimeter is 1.1 meter).

The `unit` attribute of the `Quantity` object is lost in arithmetic operations due to the aforementioned issue leading to a astropy `UnitConversionError` as it forbids to add two values of mismatching physical dimension. This cannot be ignored as in the example above the `10 cm` would be converted to the `float` value of `10.0` (in astropy terms is therefore dimensionless) and `1 m + 10 cm = 1.1 m` is not equal to `1 + 10 = 11`.

### Expected Behavior

I would expect that the `item_from_zerodim` only converts objects of type `np.ndarray` and dimension `0-dim` to `float`. Subclasses of `np.ndarray` lose required attributes at the moment. A possible fix is already discussed above.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.13.5
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.153.1-microsoft-standard-WSL2
Version               : #1 SMP Fri Mar 29 23:14:13 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.5
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.7.0
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.6
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.7.0
scipy                 : 1.16.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.24.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
",['Bug'],2025-11-04 16:13:48,2025-11-14 17:40:16,2,closed
62974,### Prerequisites,"### Prerequisites

- [x] I've searched the current open issues
- [x] I've updated to the latest version of Toolbox

### Toolbox version

0.18.0

### Environment

_No response_

### Client

_No response_

### Expected Behavior

nested JSON in spanner-list-tables output shouldn't be as string

### Current Behavior

<img width=""3366"" height=""1219"" alt=""Image"" src=""https://github.com/user-attachments/assets/c2c54c6e-902f-4899-902f-aa97c347dba1"" />

### Steps to reproduce?

1. Run spanner-list-tables tool in UI tool 


### Additional Details

_No response_

_Originally posted by @gRedHeadphone in https://github.com/googleapis/genai-toolbox/issues/1838_",[],2025-11-03 19:56:48,2025-11-03 19:57:28,0,closed
62963,BUG: EngFormatter accuracy wrong,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas.io.formats.format import EngFormatter

def test_eng_formatter():
    formatter4 = EngFormatter(use_eng_prefix=True, accuracy=4)
    formatter5 = EngFormatter(use_eng_prefix=True, accuracy=5)

    assert formatter4(12_345_678.90)   == ""123.5M""
    assert formatter5(12_345_678.90)   == ""123.46M""
    assert formatter4(0.0001234567890) == ""123.5u""
    assert formatter5(0.01234567890)   == ""12.346m""

test_eng_formatter()
```

### Issue Description

The accuracy argument of the **EngFormatter** is generating a different number of digits.

It is expected to obtain 5 digits **in total** with accuracy=5, but the argument seems to affect only the digits **behind the decimal point**. This is wrong. It is not Engineering Notation !





### Expected Behavior


_Example:_ 
with accuracy=4
- 12_345_678.90  returns  12.3457M , expected : 12.35M
- 123.4568u  returns 123.4568u, expected 123.5u

with accuracy=5
- 12_345_678.90 returns  12.34568M, expected 12.345M
- 0.01234567890 returns 12.34568m, expected 12.346m

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.9.21
python-bits           : 64
OS                    : Linux
OS-release            : 6.17.5-1.el9.elrepo.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Thu Oct 23 13:22:49 EDT 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8

pandas                : 2.3.3
numpy                 : 2.0.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 21.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : 4.6.5
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Output-Formatting']",2025-11-02 19:25:52,2025-11-03 23:51:08,6,closed
62949,BUG: concat warning when joining with empty and columns have different dtype,"The warning is due to https://github.com/pandas-dev/pandas/issues/39122

```python
pd.set_option(""infer_string"", True)
df = pd.DataFrame({""a"": ""x"", ""b"": [""y"", ""z""]}).set_index([""a"", ""b""])
df.columns = pd.Index([], dtype=""int"")
ser = pd.DataFrame({""b"": [""y"", ""z""], ""value"": [1, 2]}).set_index(""b"")[""value""]
df.join(ser, on=""b"")
# FutureWarning: The behavior of array concatenation with empty entries is deprecated...
```

In the above code, we hit

https://github.com/pandas-dev/pandas/blob/82fa27153e5b646ecdb78cfc6ecf3e1750443892/pandas/core/reshape/merge.py#L1134

with `left` having no columns but of `int64` dtype and `right` having columns with `str` dtype. Both before and after the deprecation we get `str` dtype for the columns. The call to `pandas.core.dtypes.concat.concat_compact` does result in `object` dtype, but then we wrap this with `Index._with_infer(...)` in `Index._concat` giving `str` again.

Should we be inferring dtype after the concat index is determined?

In merge, it seems to me we should skip calling `concat` with `axis=1` if `left` or `right` has empty columns but perhaps still call `concat` if both are empty.

cc @jbrockmendel @jorisvandenbossche ","['Bug', 'Reshaping', 'Needs Discussion', 'Warnings']",2025-11-01 12:22:51,2025-11-10 17:54:07,5,closed
62940,"ENH: Add an option to ""nunique"" to skip columns that contain unhashable values","### Feature Type

- [x] Adding new functionality to panda
- [ ] Changing existing functionality in pandas
- [ ] Removing existing functionality in pandas

### Problem Description

I wanted to get the `nunique` for each column in my df, but some columns contained unhashable values like lists, so I got `TypeError: unhashable type: 'list'`. It would be nice if `df.nunique()` could skip columns like that, putting `NaN` for them.

I got around the problem myself like this:
```python
def nunique_if_hashable(s: pd.Series) -> float:
    try:
        return s.nunique()
    except TypeError:
        return np.nan

df.apply(nunique_if_hashable)
```

With a result like this:
```
A    0.0
B    1.0
C    3.0
D    NaN
dtype: float64
```
Since `D` contains at least one list, and lists aren't hashable, it's skipped.

Setup:
```python
import numpy as np
import pandas as pd

df = pd.DataFrame({
    'A': [np.nan] * 4,
    'B': [1] * 4,
    'C': [5, 5, 6, 7],
    'D': [[], [], [], None]})
```

### Feature Description

I'm imagining a parameter like say `skip_unhashable: bool = False` that would do the equivalent of the above:

```
>>> df.nunique(skip_unhashable=True)
A      0
B      1
C      3
D    NaN
```

### Alternative Solutions

The helper function I wrote above isn't so bad. It's not crucial to put this functionality in Pandas, it would just be nice is all.

### Additional Context

I loaded this df from JSON.","['Enhancement', 'Needs Triage']",2025-10-31 17:41:14,2025-11-03 15:07:52,4,closed
62923,DOC: `tolerance` in `Index.reindex` can be a `Sequence` at runtime,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.Index.reindex.html

### Documentation problem

`tolerance` can be a `Sequence`, which is not too clear in the docs, saying `tolerance: int or float, optional`.
```py
In [56]: pd.Index([1, 0]).reindex([0, 1.1], method='nearest', tolerance=[0, .2])
Out[56]: (Index([0.0, 1.1], dtype='float64'), array([1, 0]))
```

This is found in https://github.com/pandas-dev/pandas-stubs/pull/1459/files#r2478806601.

### Suggested fix for documentation

Improve the description and make it consistent.","['Docs', 'good first issue', 'Index']",2025-10-30 20:52:36,2025-10-31 16:49:14,1,closed
62919,DEPR: verify_integrity in DataFrame.set_index,"xref #20110 

This just checks whether the index is unique.  I don't see why this is needed.","['Indexing', 'Deprecate']",2025-10-30 18:27:03,2025-11-07 19:43:04,2,closed
62915,BUG: index.union fails at DST boundary,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
index=pd.date_range('2025-10-25', '2025-10-26', freq='d', tz='Europe/Helsinki')
index2=pd.date_range('2025-10-25', '2025-10-28', freq='d', tz='Europe/Helsinki')
index.union(index2)
```

### Issue Description

index union fails when one index ends when day-light saving time ends and the other index contains timestamps after the transition.

The error comes from assert in  pandas/core/indexes/datetimelike.py"", line 703, in _fast_union
`
    assert dates._freq == self.freq  # type: ignore[union-attr]
`

After debugging the root cause is the following check when concatenating the indices
https://github.com/pandas-dev/pandas/blob/54c26ec4d247097928320a7a77ba03c627a91b19/pandas/core/arrays/datetimelike.py#L2386

When that check fails, like it does in the example, the frequency of the index is set  None resulting in failing assert in `_fast_union`

### Expected Behavior

The index union succeeds

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.13.5
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.4-300.fc42.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Fri Apr 25 15:43:38 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : 8.2.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : 6.136.9
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.5
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : 2.0.42
tables                : 3.10.2
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Timezones', 'good first issue', 'Needs Tests', 'Index']",2025-10-30 07:32:37,2025-12-01 18:49:59,11,closed
62911,BUG: Type stub deleted for function that is not even officially deprecated,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.to_pickle([], ""path"")

Type check the above code with e.g. mypy.
```

### Issue Description

mypy will say that there is no to_pickle function in pandas, even though the function is there. There was a discussion about how deprecating this function is a good idea in https://github.com/pandas-dev/pandas/issues/48402, but it was never deprecated. Even if it were deprecated, removing it from the type stubs completely would not be correct - it should be marked deprecated in the type stub. Removing the function entirely simply because a function was considered being deprecated doesn't seem good.

This came up for me while upgrading my python environment; the upgrade brought pandas-stubs from 2.3.0.250703 to 2.3.2.250926. We use this function dozens of times, and the code still works perfectly, but the mypy checks fail. I could sit down and spend the time to solve this problem as if this function really were going away shortly, but since the function is there, still part of the public API (i.e. part of __all__), still not deprecated, I'd prefer not to. Can the stub for to_pickle be added back to pandas-stubs for the next release?

### Expected Behavior

N/A

### Installed Versions

Not relevant
","['Bug', 'Needs Triage']",2025-10-29 21:54:33,2025-10-29 23:02:26,1,closed
62899,DOC: inconsistent default values in the initialiser for `RangeIndex`,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.RangeIndex.html

### Documentation problem

The default values differ from https://github.com/pandas-dev/pandas/blob/e11bf72296ed77c7de3183277f04f6693d19cb54/pandas/core/indexes/range.py#L154-L162

### Suggested fix for documentation

Either fix the documentation, saying that the default `None`s will be set to some other values, or fix the implementation.",['Docs'],2025-10-28 17:41:00,2025-10-29 18:37:25,1,closed
62898,Missing manylinux wheels for nightly build,"https://pypi.anaconda.org/scientific-python-nightly-wheels/simple/pandas is missing linux wheels for the most recent build. https://github.com/pandas-dev/pandas/actions/runs/18863344238/job/53826247322 seems to be the GH action run.

The logs show a compilation issue:

```
2025-10-28T03:42:50.2217984Z   [48/154] Compiling C object pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o
2025-10-28T03:42:50.2225643Z   FAILED: [code=1] pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o
2025-10-28T03:42:50.2239721Z   cc -Ipandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p -Ipandas/_libs/tslibs -I../pandas/_libs/tslibs -I../../tmp/pip-build-env-_9vmpzyj/overlay/lib/python3.11/site-packages/numpy/_core/include -I../pandas/_libs/include -I/opt/_internal/cpython-3.11.14/include/python3.11 -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -Wextra -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DCYTHON_USE_TYPE_SPECS=1 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o -MF pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o.d -o pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o -c pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c
2025-10-28T03:42:50.2251148Z   pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c: In function ‘__Pyx_modinit_shared_function_import_code’:
2025-10-28T03:42:50.2263205Z   pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c:5505:70: error: passing argument 3 of ‘__Pyx_ImportFunction_3_2_0b2’ from incompatible pointer type [-Wincompatible-pointer-types]
2025-10-28T03:42:50.2272463Z    5505 |       if (__Pyx_ImportFunction_3_2_0b2(__pyx_t_1, __pyx_import_name, *__pyx_import_pointer, __pyx_import_current_signature) < (0)) __PYX_ERR(0, 1, __pyx_L1_error)
2025-10-28T03:42:50.2280451Z         |                                                                      ^~~~~~~~~~~~~~~~~~~~~
2025-10-28T03:42:50.2288351Z         |                                                                      |
2025-10-28T03:42:50.2296218Z         |                                                                      void (*)(void)
2025-10-28T03:42:50.2305867Z   pandas/_libs/tslibs/base.cpython-311-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c:2315:89: note: expected ‘void (**)(void)’ but argument is of type ‘void (*)(void)’
2025-10-28T03:42:50.2323125Z    2315 | static int __Pyx_ImportFunction_3_2_0b2(PyObject *module, const char *funcname, void (**f)(void), const char *sig);
2025-10-28T03:42:50.2323956Z         |                                                                                 ~~~~~~~~^~~~~~~~
2025-10-28T03:42:50.6290017Z   [49/154] Compiling C object pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o
2025-10-28T03:42:50.6292458Z   FAILED: [code=1] pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o
2025-10-28T03:42:50.6303415Z   cc -Ipandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p -Ipandas/_libs/tslibs -I../pandas/_libs/tslibs -I../../tmp/pip-build-env-_9vmpzyj/overlay/lib/python3.11/site-packages/numpy/_core/include -I../pandas/_libs/include -I/opt/_internal/cpython-3.11.14/include/python3.11 -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -Wextra -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DCYTHON_USE_TYPE_SPECS=1 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o -MF pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o.d -o pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o -c pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/ccalendar.pyx.c
2025-10-28T03:42:50.6310789Z   pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/ccalendar.pyx.c: In function ‘__Pyx_modinit_shared_function_import_code’:
2025-10-28T03:42:50.6313337Z   pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/ccalendar.pyx.c:6421:70: error: passing argument 3 of ‘__Pyx_ImportFunction_3_2_0b2’ from incompatible pointer type [-Wincompatible-pointer-types]
2025-10-28T03:42:50.6346576Z    6421 |       if (__Pyx_ImportFunction_3_2_0b2(__pyx_t_1, __pyx_import_name, *__pyx_import_pointer, __pyx_import_current_signature) < (0)) __PYX_ERR(0, 1, __pyx_L1_error)
2025-10-28T03:42:50.6347703Z         |                                                                      ^~~~~~~~~~~~~~~~~~~~~
2025-10-28T03:42:50.6348452Z         |                                                                      |
2025-10-28T03:42:50.6349016Z         |                                                                      void (*)(void)
2025-10-28T03:42:50.6350740Z   pandas/_libs/tslibs/ccalendar.cpython-311-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/ccalendar.pyx.c:2617:89: note: expected ‘void (**)(void)’ but argument is of type ‘void (*)(void)’
2025-10-28T03:42:50.6352817Z    2617 | static int __Pyx_ImportFunction_3_2_0b2(PyObject *module, const char *funcname, void (**f)(void), const char *sig);
2025-10-28T03:42:50.6366249Z         |                                                                                 ~~~~~~~~^~~~~~~~
2025-10-28T03:42:51.0772306Z   [50/154] Compiling Cython source /project/pandas/_libs/window/indexers.pyx
2025-10-28T03:42:51.5021434Z   [51/154] Compiling Cython source /project/pandas/_libs/sparse.pyx
2025-10-28T03:42:51.7346835Z   [52/154] Compiling Cython source /project/pandas/_libs/window/aggregations.pyx
2025-10-28T03:42:54.0141462Z   [53/154] Compiling C object _cyutility.cpython-311-x86_64-linux-gnu.so.p/meson-generated_..__cyutility.c.o
2025-10-28T03:42:54.0143687Z   ninja: build stopped: subcommand failed.
2025-10-28T03:42:54.0666908Z   error: subprocess-exited-with-error
```


xref https://github.com/pandas-dev/pandas/issues/55903",['Build'],2025-10-28 17:17:57,2025-12-09 20:14:10,9,closed
62897,"BUG: `Index.join` works when `other` is `Series`, giving `Index`","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.Index([1, 2]).join(pd.Series([1, 2, 2]))  # Index([1, 2, 2], dtype='int64')
```

### Issue Description

`Index.join(Series)` is unspecified in the [docs](https://pandas.pydata.org/docs/reference/api/pandas.Index.join.html), but works at runtime.

### Expected Behavior

I would expect `Index.join(Series)` either fails, or gives `Series`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : f60bf958da887b232834e89af31a0630021b801d
python                : 3.11.13
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : Czech_Czechia.1252

pandas                : 3.0.0.dev0+2338.gf60bf958da.dirty
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.3
sphinx                : 8.2.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.5
bottleneck            : 1.5.0
fastparquet           : 2024.11.0
fsspec                : 2025.7.0
html5lib              : 1.1
hypothesis            : 6.138.3
gcsfs                 : 2025.7.0
jinja2                : 3.1.6
lxml.etree            : 6.0.1
matplotlib            : 3.10.5
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyiceberg             : 0.9.1
pyreadstat            : 1.3.1
pytest                : 8.4.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.7.0
scipy                 : 1.16.1
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.8.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
qtpy                  : None
pyqt5                 : None

</details>
","['Deprecate', 'Index']",2025-10-28 17:11:18,2025-11-04 19:35:08,5,closed
62881,"BUG: pd.read_csv crashes when CSV contains ""2e06918610327a"" in version 2.3.3","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
with open('test.csv', 'w') as f:
  f.write('id\n2e06918610327a\n')

import pandas as pd
df = pd.read_csv('test.csv', engine='c')
```

### Issue Description

The reproducible example will give a segfault.
This bug exists on the latest version 2.3.3, but does not exist on 2.3.2.
`engine='python'` also has this issue.

### Expected Behavior

Not crash can load the CSV correctly

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.10.13
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.0-33-generic
Version               : #33~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 19 17:02:30 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 8.37.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV', 'Closing Candidate']",2025-10-27 20:03:51,2025-10-27 21:30:19,1,closed
62878,BUG: Pyarrow numeric dtype fillna filled not null entry,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd


def test_show_double_pyarrow_has_issue_when_fillna():
    """"""
    Very strange behavior observed when using pyarrow 'double' dtype with pandas Series.
    When using `fillna(0.0)`, the second element which is supposed no op becomes is ""filled"".
    """"""
    for raw_ls in [
        [1, 2, 3, 4, 5, pd.NA],
        [1, 2, 3, 4, pd.NA, 6],
        [1, 2, pd.NA, 4, 5, 6],
        # [pd.NA, 2, 3, 4, 5, 6],       # this works fine when pd.NA is first
        # [1, 2, 3, 4, pd.NA],          # this works fine when there are five elements
        # [1, 2, 3, pd.NA, pd.NA, 6],   # this works fine when there are two pd.NA
    ]:
        def get_series(ls):
            return pd.Series(ls, dtype=""double[pyarrow]"")
        s = get_series(raw_ls)
        s_filled = s.fillna(0.0)
        zero_counts = (s_filled == 0.0).sum()
        assert zero_counts == 2
        assert raw_ls[1] != 0 and not(pd.isna(raw_ls[1])) and s_filled.iloc[1] == 0.0


def test_show_int_pyarrow_has_issue_when_fillna():
    """"""
    Similar to double dtype, very strange behavior observed when using pyarrow 'int64' dtype.
    """"""
    for raw_ls in [
        [1, 2, 3, 4, 5, pd.NA],
        [1, 2, 3, 4, pd.NA, 6],
        [1, 2, pd.NA, 4, 5, 6],
        # [pd.NA, 2, 3, 4, 5, 6],       # this works fine when pd.NA is first
        # [1, 2, 3, 4, pd.NA],          # this works fine when there are five elements
        # [1, 2, 3, pd.NA, pd.NA, 6],   # this works fine when there are two pd.NA
    ]:
        def get_series(ls):
            return pd.Series(ls, dtype=""int64[pyarrow]"")
        s = get_series(raw_ls)
        s_filled = s.fillna(0)
        zero_counts = (s_filled == 0).sum()
        assert zero_counts == 2
        assert raw_ls[1] != 0 and not(pd.isna(raw_ls[1])) and s_filled.iloc[1] == 0

test_show_double_pyarrow_has_issue_when_fillna()
test_show_int_pyarrow_has_issue_when_fillna()
```

### Issue Description

There are some issues when using fillna with pyarrow numeric series len >= 6. It will some row that is not supposed to be ""filled.""

### Expected Behavior

Only the rows with pd.NA should be filled.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.13.3
python-bits           : 64
OS                    : Windows
OS-release            : 11

pandas                : 2.3.3
numpy                 : 2.3.3
pyarrow               : 21.0.0


</details>
","['Bug', 'Missing-data', 'Upstream issue', 'Arrow']",2025-10-27 11:05:01,2025-10-28 10:17:01,3,closed
62849,BUG: bdate_range with cbh fails,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# Based on https://github.com/pandas-dev/pandas/issues/24555#issuecomment-450909512

START = pd.Timestamp(2009, 3, 13)
END1 = pd.Timestamp(2009, 3, 18)
END2 = pd.Timestamp(2009, 3, 19)

freq = 'CBH'
a = pd.bdate_range(START, END1, freq=freq, weekmask='Mon Wed Fri',
                   holidays=['2009-03-14'])  # <- raises

freq = 'cbh'
a = pd.bdate_range(START, END1, freq=freq, weekmask='Mon Wed Fri',
                   holidays=['2009-03-14'])  # <- still raises
```

### Issue Description

IIUC we deprecated the upper-cast ""h"" and so now instead of ""CBH"" we want ""cbh"".  But inside bdate_range we have a check for `isinstance(freq, str) and freq.startswith(""C"")`.  I suspect that needs to be updated to check for lowercase ""c""?  cc @natmokval ?

### Expected Behavior

N/A

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Frequency']",2025-10-25 21:54:48,2025-11-03 15:42:19,4,closed
62846,BUG: Resampler.interpolate downcast keyword deprecated?,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
NA
```

### Issue Description

The docstring says the keyword was ``.. deprecated:: 2.1.0``, and the method itself starts with ``assert downcast is lib.no_default  # just checking coverage``.  I suspect the keyword should have been removed entirely at some point?

### Expected Behavior

N/A

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Dtype Conversions', 'Deprecate', 'good first issue']",2025-10-25 21:17:32,2025-10-27 17:44:22,1,closed
62840,ENH: Make pd.to_datetime with format parameter more robust to dirty data,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

When using `pd.to_datetime` with a specific `format` parameter, the function strictly expects all data in the input series/column to adhere to that exact format. If even a single value deviates (e.g., due to dirty/corrupted data), the function raises an error (if `errors='raise'`, the default) or potentially converts all problematic values to `NaT` (if `errors='coerce'`). This often forces users to perform manual data cleaning or validation *before* calling `to_datetime` to handle these outliers, which can be cumbersome and interrupt the data processing workflow, especially when the user believes the *majority* of the data *should* follow the specified format.


### Feature Description

I propose modifying the internal behavior of `pd.to_datetime` when a `format` parameter is explicitly provided. The goal is to make the function more robust to a small proportion of dirty data that does not match the specified format, while still catching cases where the format is fundamentally incorrect for the data. This change would be internal and would *not* modify the `errors` parameter's existing behavior.

The suggested internal logic would be:

1.  **Check for Explicit Format:** If the `format` argument is provided (not `None` or `mixed`), proceed to the next steps. Otherwise, maintain the current behavior.
2.  **Initial Parsing Attempt:** Call the internal datetime parsing logic using the provided `format` and `errors='coerce'`. This produces an intermediate result series, let's call it `temp_result`.
3.  **Calculate Success Rate:** Calculate the proportion of non-NA/NaT values in `temp_result` compared to the total number of input values.
4.  **Apply Threshold Logic:**
    *   **If the success rate (proportion of non-NaT values in `temp_result`) is *greater than* a defined threshold (e.g., 50%):** Return `temp_result`. This means the specified `format` is considered appropriate for the majority of the data, and the few entries that failed to parse are marked as `NaT`.
    *   **If the success rate is *less than or equal to* the threshold:** Revert to the current behavior, which is equivalent to `errors='raise'`. An error should be raised, indicating that the data format does not match the specified format (or only a very small minority does). This ensures strictness when the format is likely incorrect.

**Pseudocode for the core logic:**

```python
def to_datetime(..., format=None, errors='raise', ...):
    # ... (existing validation and setup code) ...

    if format is not None and format != 'mixed':
        # --- New Robust Logic Starts Here ---
        # Step 1: Parse with format using errors='coerce'
        temp_result = _internal_parse_function(input_arg, format=format, errors='coerce')
        
        # Step 2: Calculate success rate
        total_values = len(temp_result)
        successful_parses = temp_result.notna().sum() # Count non-NaT values
        success_rate = successful_parses / total_values

        # Step 3: Apply threshold (e.g., 50%)
        THRESHOLD = 0.5 # This value might need discussion/configurability later
        if success_rate > THRESHOLD:
            # Most data matched the format, return with NaT for failures
            return temp_result
        else:
            # Too little data matched, raise error like errors='raise'
            # This would likely involve re-parsing with errors='raise' or
            # raising a specific error calculated from temp_result's NaT locations
            return _internal_parse_function(input_arg, format=format, errors='raise') # Or equivalent error logic
        # --- New Robust Logic Ends Here ---
    else:
        # ... (existing logic for format=None, format='mixed', etc.) ...
        return _internal_parse_function(input_arg, format=format, errors=errors)

```

This approach aims to make `to_datetime` more robust to small amounts of dirty data when a specific format is expected, reducing the need for pre-processing, while still catching cases where the format is fundamentally wrong.

### Describe alternatives you've considered

Currently, users must handle this scenario manually:
*   Pre-process the data to clean or identify/replace problematic entries before calling `pd.to_datetime`.
*   Call `pd.to_datetime` with `errors='coerce'`, check the number of `NaT` values generated, and decide if the result is acceptable or if further action is needed.
*   Use `pd.to_datetime` without a `format` (relying on the default flexible parser), which might be slower and less precise if the user knows the intended format.

The proposed solution would encapsulate this common pattern of ""try strict format, check success, decide action"" into the function itself, based *only* on the user providing an explicit `format`.

### Alternative Solutions

Currently, users must handle this scenario manually:
*   Pre-process the data to clean or identify/replace problematic entries before calling `pd.to_datetime`.
*   Call `pd.to_datetime` with `errors='coerce'`, check the number of `NaT` values generated, and decide if the result is acceptable or if further action is needed.
*   Use `pd.to_datetime` without a `format` (relying on the default flexible parser), which might be slower and less precise if the user knows the intended format.

The proposed solution would encapsulate this common pattern of ""try strict format, check success, decide action"" into the function itself, based *only* on the user providing an explicit `format`.

### Additional Context

This enhancement would primarily benefit data analysts and engineers working with potentially messy datasets where a datetime column is expected to follow a specific format, but a few entries might be incorrect. It streamlines the common workflow of parsing datetimes with an expected format while gracefully handling minor inconsistencies. The change is localized to the scenario where `format` is specified, minimizing the impact on other existing behaviors.

A potential future optimization could involve an initial pass to assess the overall ""date-likeness"" or format compliance of the data *before* attempting the main parsing logic. Such a pre-check could potentially offer even better performance or more precise error reporting in cases where the data is largely non-conforming. However, this initial implementation focuses on the simpler threshold-based approach described above, which provides immediate benefits with a more contained change. The pre-check idea could be explored in a follow-up enhancement if deemed necessary.","['Enhancement', 'Datetime', 'Closing Candidate']",2025-10-25 15:09:16,2025-11-26 20:43:56,5,closed
62838,BUG: ENH: Improve IntCastingNaNError message to suggest solutions for NaN in integer conversion,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

print(pd.__version__)

df = pd.DataFrame({
    ""roll"": [12, 14, np.nan],
    ""age"": [20, np.nan, 30]
})

print(""Original DataFrame:"")
print(df)
print(f""Data types: {df.dtypes}"")

print(""\nAttempting to convert 'roll' column to int64:"")
df['roll'] = df['roll'].astype('int64')
print(df)
```

### Issue Description

Throwing an unhelpful error when converting float columns with NaN values to integer.

Original DataFrame:
   roll   age
0  12.0  20.0
1  14.0   NaN
2   NaN  30.0
Data types: roll    float64
age     float64
dtype: object

Attempting to convert 'roll' column to int64:
    raise IntCastingNaNError(
        ""Cannot convert non-finite values (NA or inf) to integer""
    )
pandas.errors.IntCastingNaNError: Cannot convert non-finite values (NA or inf) to integer

### Expected Behavior

The error message should provide actionable guidance to help users understand how to handle NaN values when converting to integer types.

### Installed Versions

<details>

python                : 3.13.3
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26200
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_India.1252

pandas                : 2.2.3
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Enhancement', 'Dtype Conversions', 'Error Reporting', 'Needs Triage']",2025-10-25 12:59:18,2025-11-08 17:43:10,6,closed
62829,BUG: json_normalize doesn't handle nan well when max_level=n,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

d = {1: {""id"": 10, 'status': 'AVAL'},
     2: {""id"": 30, 'status': 'AVAL', 'items':  {""id"": 12, ""size"": 20}},
     3: {""id"": 50, 'status': 'AVAL', 'items':  {""id"": 13, ""size"": 30}}}


df = pd.DataFrame.from_dict(d, orient=""index"")

pd.json_normalize(df['items'].tolist(), max_level=0)
```

### Issue Description

json_normalize will raise an AttributeError of ""'float' object has no attribute 'values'"" when max_level is set as a number. But it works well when max_level is None. The following is the traceback:

Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8)
      1 d = {1: {""id"": 10, 'status': 'AVAL'},
      2      2: {""id"": 30, 'status': 'AVAL', 'items':  {""id"": 12, ""size"": 20}},
      3      3: {""id"": 50, 'status': 'AVAL', 'items':  {""id"": 13, ""size"": 30}}}
      6 df = pd.DataFrame.from_dict(d, orient=""index"")
----> [8](vscode-notebook-cell:?execution_count=6&line=8) pd.json_normalize(df['items'].tolist(), max_level=0)

File ~/Projects/newera/.venv/lib/python3.13/site-packages/pandas/io/json/_normalize.py:460, in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)
    457     return DataFrame(_simple_json_normalize(data, sep=sep))
    459 if record_path is None:
--> [460](https://file+.vscode-resource.vscode-cdn.net/Users/jwu/Projects/newera/notebooks/~/Projects/newera/.venv/lib/python3.13/site-packages/pandas/io/json/_normalize.py:460)     if any([isinstance(x, dict) for x in y.values()] for y in data):
    461         # naive normalization, this is idempotent for flat records
    462         # and potentially will inflate the data considerably for
    463         # deeply nested structures:
    464         #  {VeryLong: { b: 1,c:2}} -> {VeryLong.b:1 ,VeryLong.c:@}
    465         #
    466         # TODO: handle record value which are lists, at least error
    467         #       reasonably
    468         data = nested_to_record(data, sep=sep, max_level=max_level)
    469     return DataFrame(data)

### Expected Behavior

json_normalize shall return similarly to the result when max_level=None

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.13.8
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:29:54 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8122
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : C.UTF-8

pandas                : 2.3.3
numpy                 : 2.2.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.2
sphinx                : None
IPython               : 9.1.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.5
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : 2024.11.0
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Error Reporting', 'IO JSON']",2025-10-25 01:37:52,2025-11-04 18:03:19,1,closed
62820,CI: Pyodide checks failing with NotImplemented error,"After submitting my changes to my current PR (#62502 ), I noticed that one of the code checks (Pyodide) failed.
The pickle tests (`test_pickle`) try to read in a temporary pickle file containing a Categorical array (produced earlier in each test), but fail with NotImplementedError`.
After some investigation, I noticed that this error is raised in a Cython function that is passed a tuple with two entries but expects either one or three.

I also found another PR where this check fails for the same reason.

Here is an example of a failing test output:
```
__________________________________________ test_round_trip_current[cat-expected39-python_pickler-pandas_proto_5] ___________________________________________

typ = 'cat'
expected = [0, 1, 2, 3, 4, ..., 9995, 9996, 9997, 9998, 9999]
Length: 10000
Categories (10000, int64): [0, 1, 2, 3, ..., 9996, 9997, 9998, 9999]
pickle_writer = functools.partial(<function to_pickle at 0x7f92e2f2a520>, protocol=5), writer = <function python_pickler at 0x7f92db703ba0>
temp_file = PosixPath('/tmp/pytest-of-aija/pytest-0/test_round_trip_current_cat_ex29/7281f1e5-0c7b-4f7b-858a-12178d7dc0be')

    @pytest.mark.parametrize(
        ""pickle_writer"",
        [
            pytest.param(python_pickler, id=""python""),
            pytest.param(pd.to_pickle, id=""pandas_proto_default""),
            pytest.param(
                functools.partial(pd.to_pickle, protocol=pickle.HIGHEST_PROTOCOL),
                id=""pandas_proto_highest"",
            ),
            pytest.param(functools.partial(pd.to_pickle, protocol=4), id=""pandas_proto_4""),
            pytest.param(
                functools.partial(pd.to_pickle, protocol=5),
                id=""pandas_proto_5"",
            ),
        ],
    )
    @pytest.mark.parametrize(""writer"", [pd.to_pickle, python_pickler])
    @pytest.mark.parametrize(""typ, expected"", flatten(create_pickle_data()))
    def test_round_trip_current(typ, expected, pickle_writer, writer, temp_file):
        path = temp_file
        # test writing with each pickler
        pickle_writer(expected, path)

        # test reading with each unpickler
>       result = pd.read_pickle(path)
                 ^^^^^^^^^^^^^^^^^^^^

pandas/tests/io/test_pickle.py:174:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pandas/io/pickle.py:208: in read_pickle
    return pickle.load(handles.handle)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
pandas/core/arrays/categorical.py:1775: in __setstate__
    return super().__setstate__(state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
pandas/_libs/arrays.pyx:85: in pandas._libs.arrays.NDArrayBacked.__setstate__
    cpdef __setstate__(self, state):
```",[],2025-10-24 16:19:54,2025-10-27 20:51:58,3,closed
62810,BUG: False positive ChainedAssignmentError in 3.14 (but not 3.13),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({'group': ['A', 'A', 'B', 'B'],
                   'value': [10, 20, 30, 40]})
aggregated_df = df.groupby('group').agg(sum_value=('value', 'sum'))
aggregated_df['sum_value'] = aggregated_df['sum_value'] * 10
```

### Issue Description

In Python 3.13 the code above produces no diagnostics.
In Python 3.14 I get the dreaded
> FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!

Note that `agg` always produces a fresh `DataFrame`, so it is unclear how this could happen.

### Expected Behavior

no diagnostics, because `agg` always produces a fresh `DataFrame`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.14.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 85 Stepping 7, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : 9.6.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.9.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.7
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.2
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.2
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2025-10-23 20:31:48,2025-10-23 21:22:55,0,closed
62807,DOC: read_excel dtype_backend  default or no default?,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

Hello, 
I was reading read_excel documentation https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html


### Documentation problem

And something is not clear to me
At first we have 
`pandas.read_excel(io, sheet_name=0, *, header=0, names=None, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, parse_dates=False, date_parser=<no_default>, date_format=None, thousands=None, decimal='.', comment=None, skipfooter=0, storage_options=None, dtype_backend=<no_default>, engine_kwargs=None)`

So I understand there is no default value for dtype_backend

and a few lines below
```
dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’

    Back-end data type applied to the resultant [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) (still experimental). Behaviour is as follows:

        ""numpy_nullable"": returns nullable-dtype-backed [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) (default).
```

So here I understand the numpy_nullable is the default.

And finally, when reading the code https://github.com/pandas-dev/pandas/blob/v2.3.3/pandas/io/excel/_base.py#L451-L537

`dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default,`

### Suggested fix for documentation

Correct this part 
```
dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’

    Back-end data type applied to the resultant [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) (still experimental). Behaviour is as follows:

        ""numpy_nullable"": returns nullable-dtype-backed [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) (default).
```

To 
```
dtype_backend{‘numpy_nullable’, ‘pyarrow’}, no default

    Back-end data type applied to the resultant [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) (still experimental). Behaviour is as follows:

        ""numpy_nullable"": returns nullable-dtype-backed [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame).

```

It's better because it's true ;)
Best regards,

Simon","['Docs', 'IO Excel']",2025-10-23 09:22:22,2025-10-23 13:15:05,1,closed
62800,BUG: dtype and NA mask not preserved when applying numpy ufuncs to an ArrowDtype Series,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
import math

s = pd.Series(
[
      float(""-inf""),
      float(""inf""),
      float(""nan""),
      float(-234239487.4),
      float(-1.0),
      float(-0.000000001),
      float(0),
      float(0.000000001),
      float(0.9999999999),
      float(1.0),
      float(1.0000001),
      float(math.pi / 2),
      float(math.e),
      float(math.pi),
      float(234239487.4),
      float(1.23124 * (2**70)),
      pd.NA,
],
  dtype=pd.Float64Dtype(),
)
np.sin(s)
""""""
0
0	NaN
1	NaN
2	<NA>
3	-0.973067
4	-0.841471
5	-0.0
6	0.0
7	0.0
8	0.841471
9	0.841471
10	0.841471
11	1.0
12	0.410781
13	0.0
14	0.973067
15	0.948974
16	<NA>

dtype: Float64
"""""" 

s_pa = s.astype(""float64[pyarrow]"")
s_pa
""""""
	0
0	-inf
1	inf
2	<NA>
3	-234239487.4
4	-1.0
5	-0.0
6	0.0
7	0.0
8	1.0
9	1.0
10	1.0
11	1.570796
12	2.718282
13	3.141593
14	234239487.4
15	1453591627092105363456.0
16	<NA>
dtype: double[pyarrow]
""""""

np.sin(s_pa)
""""""
0
0	NaN
1	NaN
2	NaN
3	-9.730670e-01
4	-8.414710e-01
5	-1.000000e-09
6	0.000000e+00
7	1.000000e-09
8	8.414710e-01
9	8.414710e-01
10	8.414710e-01
11	1.000000e+00
12	4.107813e-01
13	1.224647e-16
14	9.730670e-01
15	9.489736e-01
16	NaN
dtype: float64
""""""
```

### Issue Description

When I apply a numpy ufunc to a pandas Series containing ArrowDtype(pa.float64()) data, the output type ends up being a numpy float64 and the original NA mask is lost.

### Expected Behavior

I would expect a ArrowDtype(pa.float64()) series to be returns and the original NA mask to be applied.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.12.12
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.105+
Version               : #1 SMP Thu Oct  2 10:42:05 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.1.2
Cython                : 3.0.12
sphinx                : 8.2.3
IPython               : 7.34.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.5
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.0
html5lib              : 1.1
hypothesis            : None
gcsfs                 : 2025.3.0
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.0
numba                 : None
numexpr               : 2.14.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.29.2
psycopg2              : 2.9.11
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.2
sqlalchemy            : 2.0.44
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.10.1
xlrd                  : 2.0.2
xlsxwriter            : None
zstandard             : 0.25.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Arrow', 'ufuncs']",2025-10-22 20:19:32,2025-10-29 23:03:30,4,closed
62787,BUG: Enabling Copy on Write with DataFrame.replace Raises Exception with np.nan as replacement value,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
pd.options.mode.copy_on_write = True
df = pd.DataFrame(data={'A' : [1, 2],'B' : ['b', 'i like pandas'],})
df[""Name""] = ""I Have a Name""
df['Name2'] = 'i like pandas'
df.replace(""i like pandas"", ""i like arrow"")
df.replace(np.nan, None) # works
replace_mappings = {
    pd.NA: None,
    pd.NaT: None,
    np.nan: None # Issue Type
}
df.replace(replace_mappings) # fails
```

### Issue Description

When using DataFrame.replace and trying to replace np.nan value when in a dictionary, it raises an error.  A standard replace(np.nan, None) will work though.  

The error raised:

```
builtins.ValueError: <weakref at 0x0000023B896B2840; to 'ExtensionBlock' at 0x0000023B896A26F0> is not in list
```


### Expected Behavior

The expected behavior is that the replace to not error out. 

### Installed Versions

<details>

This is reproducible in the v3 development release as of 10/22/25 and with the current public release.  I tested in multiple versions

</details>
","['Bug', 'Copy / view semantics']",2025-10-22 10:53:08,2025-10-26 17:44:16,8,closed
62778,BUG: groupby.<reduction>(numeric_only=) does not validate non-bool arguments,"```python
In [1]: import pandas as pd

In [2]: df = pd.DataFrame({""A"": range(5), ""B"": range(5)})

In [3]: df.groupby([""A""]).mean([""B""])
     B
A     
0  0.0
1  1.0
2  2.0
3  3.0
4  4.0
```

`[""B""]` is being passed to `numeric_only` which appears checked only to be truthy/falsy.

I would expect this to raise a `ValueError`","['Bug', 'Groupby', 'Error Reporting']",2025-10-21 21:13:18,2025-10-27 20:27:38,3,closed
62771,fillna for np.float64(np.nan),"I like `pd.Float64Dtype()` and other pandas datatypes, but one small glitch found in below minimal case:

```python
import pandas as pd

df = pd.DataFrame([(2, 1), (0, 0), (0, None)], columns=('a', 'b'))
df = df.astype(pd.Float64Dtype())

print(df['b'] / df['a'])
# 0     0.5
# 1     NaN
# 2    <NA>
# dtype: Float64

print((df['b'] / df['a']).fillna(0))
# 0    0.5
# 1    NaN
# 2    0.0
# dtype: Float64

print((df['b'] / df['a']).loc[1])
# np.float64(nan)
```

Is there any plan to make `fillna` adapt to `np.float64(np.nan)`? Or make sure `0 / 0` in `pd.Float64Dtype()` can return `pd.NA` rather than `np.float64(np.nan)`?
",['PDEP missing values'],2025-10-21 02:31:34,2025-10-24 03:49:53,3,closed
62768,DOC: MultiIndex.swaplevel default argument values could use an explanation,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.swaplevel.html#pandas.MultiIndex.swaplevel

### Documentation problem

The default values for `swaplevel()` are defined as `def swaplevel(self, i=-2, j=-1)`.  Level numbers should start at 0.  What the default does is refer to the last 2 levels.



### Suggested fix for documentation

I think it would be worthwhile to indicate that negative values are referring to the position of the level relative to the  end of the levels of the `MultiIndex`

Or add something that says that the default swaps the last 2 levels of the `MultiIndex`","['Docs', 'Index']",2025-10-20 21:21:56,2025-12-04 18:52:59,3,closed
62742,``validate``in ``merge`` should list out which rows are validating the condition,"```
import pandas as pd

df = pd.DataFrame({""a"": [1, 1, 3], ""b"": [4, 5, 6]})
df = df.merge(df, on=""a"", validate=""one_to_one"")
```

The error is very non-descriptive and doesn't tell you much about which rows are actually validating the condition


```
pandas.errors.MergeError: Merge keys are not unique in either left or right dataset; not a one-to-one merge
```

We should just add a suffix to this message that lists the values that are validating the condition. Probably cut off after a few values but mostly when I run into this it's one offending row that causes the error","['Reshaping', 'Error Reporting']",2025-10-18 22:53:52,2025-10-25 17:40:52,0,closed
62740,BUG: `read_csv` is inconsistent with large exponent,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import io

n_digits = [11, 12, 13]
header = ""h1,h2,h3""
data = "","".join(""10e"" + (""9"" * nd) for nd in n_digits)
buf = io.StringIO(header + ""\n"" + data)
engines = {""c"": [None, ""high"", ""round_trip""], ""python"": [None], ""pyarrow"": [None]}

print(""POSITIVE EXPONENT"")
print(""%-20s %-30s %-30s %-30s"" % (""type"", ""11 digits"", ""12 digits"", ""13 digits""))
for engine, float_precisions in engines.items():
    for float_precision in float_precisions:
        df = pd.read_csv(buf, engine=engine, float_precision=float_precision)
        pos, neg1, neg2 = df.iloc[0, :]
        print(""%-20s %-30s %-30s %-30s"" % (f""{engine}-{float_precision}"", pos, neg1, neg2))
        buf.seek(0)



data = "","".join(""10e-"" + (""9"" * nd) for nd in n_digits)
buf = io.StringIO(header + ""\n"" + data)

print(""\nNEGATIVE EXPONENT"")
print(""%-20s %-30s %-30s %-30s"" % (""type"", ""11 digits"", ""12 digits"", ""13 digits""))
for engine, float_precisions in engines.items():
    for float_precision in float_precisions:
        df = pd.read_csv(buf, engine=engine, float_precision=float_precision)
        pos, neg1, neg2 = df.iloc[0, :]
        print(""%-20s %-30s %-30s %-30s"" % (f""{engine}-{float_precision}"", pos, neg1, neg2))
        buf.seek(0)
```

### Issue Description

This issue is related to #62617 and #38794

I am raising this issue because the `c` and `python` engines have problems due to overflow when it's parsing floats, where it may segfault (see #62617), may assign `0.0` (incorrectly if the exponent is positive) or read as string.

The output of the example above is

```
POSITIVE EXPONENT
type                 11 digits                      12 digits                      13 digits
c-None               10e99999999999                 0.0                            10e9999999999999
c-high               10e99999999999                 0.0                            10e9999999999999
c-round_trip         10e99999999999                 10e999999999999                10e9999999999999
python-None          10e99999999999                 0.0                            10e9999999999999
pyarrow-None         inf                            inf                            inf

NEGATIVE EXPONENT
type                 11 digits                      12 digits                      13 digits
c-None               0.0                            10e-999999999999               0.0
c-high               0.0                            10e-999999999999               0.0
c-round_trip         0.0                            0.0                            0.0
python-None          0.0                            10e-999999999999               0.0
pyarrow-None         0.0                            0.0                            0.0
```

The pyarrow engine is the only one that is consistent.

### Expected Behavior

The `c` and `python` engines should either read the value as a string when an overflow occurs, or assign the correct float value (`inf`, `-inf`, `0.0`).

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : f7447cc05e285db78335a1c8cebf61a2e721939f
python                : 3.13.7
python-bits           : 64
OS                    : Linux
OS-release            : 6.16.12-200.fc42.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Sun Oct 12 16:31:16 UTC 2025
machine               : x86_64
processor             :
byteorder             : little
LC_ALL                : None
LANG                  : pt_BR.UTF-8
LOCALE                : pt_BR.UTF-8

pandas                : 3.0.0.dev0+2555.gf7447cc05e.dirty
numpy                 : 2.3.4
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.4
sphinx                : 8.2.3
IPython               : 9.6.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
bottleneck            : 1.6.0
fastparquet           : 2024.11.0
fsspec                : 2025.9.0
html5lib              : 1.1
hypothesis            : 6.141.1
gcsfs                 : 2025.9.0
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.7
numba                 : 0.62.1
numexpr               : 2.14.1
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.11
pymysql               : 1.4.6
pyarrow               : 21.0.0
pyiceberg             : 0.10.0
pyreadstat            : 1.3.1
pytest                : 8.4.2
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.9.0
scipy                 : 1.16.2
sqlalchemy            : 2.0.44
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.10.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.9
zstandard             : 0.25.0
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV']",2025-10-18 21:47:13,2025-10-21 19:51:44,0,closed
62739,BUG: reading space-delimited CSV file and skipping first row with double quote in it fails,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from pathlib import Path
# Bug is triggered when the first line has a double quote character
tbl = """"""\
""
a b
1 3
""""""
Path(""tbl.csv"").write_text(tbl)
pd.read_csv(""tbl.csv"", delimiter="" "", skiprows=1)
```

### Issue Description

This results in an exception:
```
Traceback (most recent call last):
  Cell In[1], line 9
    pd.read_csv(""tbl.csv"", delimiter="" "", skiprows=1)
  File ~/miniconda3-arm/envs/astropy-dev/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026 in read_csv
    return _read(filepath_or_buffer, kwds)
  File ~/miniconda3-arm/envs/astropy-dev/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620 in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File ~/miniconda3-arm/envs/astropy-dev/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620 in __init__
    self._engine = self._make_engine(f, self.engine)
  File ~/miniconda3-arm/envs/astropy-dev/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1898 in _make_engine
    return mapping[engine](f, **self.options)
  File ~/miniconda3-arm/envs/astropy-dev/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:93 in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File parsers.pyx:581 in pandas._libs.parsers.TextReader.__cinit__
EmptyDataError: No columns to parse from file
```

### Expected Behavior

This should skip the first line and return a dataframe:
```
>>> pd.read_csv(""tbl.csv"", delimiter="" "", skiprows=1)
   a  b
0  1  3
```


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.2
python-bits           : 64
OS                    : Darwin
OS-release            : 23.4.0
Version               : Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:49 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.3
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : 8.2.3
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : 1.1
hypothesis            : 6.125.3
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : 0.61.2
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.2.0
scipy                 : 1.15.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO CSV']",2025-10-18 17:52:57,2025-11-04 13:00:31,5,closed
62736,Create `.editorconfig`,"Pandas currently uses several formatters (e.g., `meson-fmt`, `clang-format`, `ruff format`) in its `.pre-commit-config.yaml` to enforce code style. To further improve developer experience, we could add [`.editorconfig`](https://editorconfig.org/). This helps text editors and IDEs apply formatting settings (e.g., spell language, text width, charset, etc.) that align with the project rules.","['CI', 'Needs Discussion']",2025-10-18 13:21:15,2025-10-28 18:55:05,19,closed
62733,ENH: `\centering` option for `DataFrame.to_latex()` method,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

When exporting data frames to Latex, I would like an option to center the table using LaTex  `\centering` command directly using `.to_latex()` method.

### Feature Description

New `.to_latex()` parameter, eg. `centering=`

### Alternative Solutions

Cycle through exported code and append `\centering` manually, e. g.
```
f = open(filename, 'w')
f.write(df.to_latex(...))
f.close()

table = ''
with open(filename, 'r') as f:
	while True:
		line = f.readline()
		if not line:
			break

		table += line
		if 'begin{table}' in line:
			table += '\\centering\n'

f.close()
f = open(filename, 'w')
f.write(table)
f.close()
```

### Additional Context

_No response_","['Enhancement', 'IO LaTeX']",2025-10-18 08:43:35,2025-10-26 06:52:42,2,closed
62725,BUG: `resample` with `asfreq` ignores `origin` if the dataframe has a fixed frequency,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import datetime
import pandas as pd

idx = [datetime.datetime(year=2025, month=10, day=17, hour=17, minute=ii, second=10, microsecond=0) for ii in range(15, 18, 1)]

# (17:15:10, 0), (17:16:10, 1), (17:17:10, 2)
df = pd.DataFrame({'value': range(len(idx))}, index=idx)

# Expected: (17:15:00, NaN), (17:16:00, NaN), (17:17:00, NaN)
# Result: (17:15:00, 0), (17:16:00, 1), (17:17:00, 2)
df_resample = df.resample('1min', origin='start_day').asfreq()
```

### Issue Description

`resample(..., origin='start_day')` groups the dataframe with respect to the first day at midnight.

Therefore, if resampling frequency is `1min` and each datetime index has a `second` value, expected result of `afreq()` would be `NaN` for all.

This expected behavior really happens when `second` value of  each datetime index is not the same:

```
idx = [datetime.datetime(year=2025, month=10, day=17, hour=17, minute=ii, second=10 if ii < 17 else 0, microsecond=0) for ii in range(15, 18, 1)]

# (17:15:10, 0), (17:16:10, 1), (17:17:00, 2)
df = pd.DataFrame({'value': range(len(idx))}, index=idx)

# Result: (17:15:00, NaN), (17:16:00, NaN), (17:17:00, 2)
df_resample = df.resample('1min', origin='start_day').asfreq()
```

However, if the `second` value of all datetime index is the same, resampling ignores the second value and just reindex the dataframe:
```

idx = [datetime.datetime(year=2025, month=10, day=17, hour=17, minute=ii, second=10, microsecond=0) for ii in range(15, 18, 1)]

# (17:15:10, 0), (17:16:10, 1), (17:17:10, 2)
df = pd.DataFrame({'value': range(len(idx))}, index=idx)

# Expected: (17:15:00, NaN), (17:16:00, NaN), (17:17:00, NaN)
# Produced: (17:15:00, 0), (17:16:00, 1), (17:17:00, 2)
df_resample = df.resample('1min', origin='start_day').asfreq()
```


### Expected Behavior

Expected result: `(17:15:00, NaN), (17:16:00, NaN), (17:17:00, NaN)`
Produced result: `(17:15:00, 0), (17:16:00, 1), (17:17:00, 2)`

### Installed Versions

<details>

commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.14.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : Korean_Korea.949

pandas                : 2.3.3
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Resample']",2025-10-17 11:16:32,2025-11-04 17:11:48,3,closed
62724,BUG: `UnboundLocalError` when calling `pd.option_context` with an invalid key on the `main` branch,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd


with pd.option_context(""invalid"", True):
     pass
```

### Issue Description

```python-traceback

Traceback (most recent call last):
  File ~/miniforge3/envs/dev/lib/python3.13/site-packages/pandas/_config/config.py:507 in option_context
    undo = tuple((pat, get_option(pat)) for pat, val in ops)
  File ~/miniforge3/envs/dev/lib/python3.13/site-packages/pandas/_config/config.py:507 in <genexpr>
    undo = tuple((pat, get_option(pat)) for pat, val in ops)
  File ~/miniforge3/envs/dev/lib/python3.13/site-packages/pandas/_config/config.py:185 in get_option
    key = _get_single_key(pat)
  File ~/miniforge3/envs/dev/lib/python3.13/site-packages/pandas/_config/config.py:129 in _get_single_key
    raise OptionError(f""No such keys(s): {pat!r}"")
OptionError: No such keys(s): 'invalid'

During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  Cell In[3], line 1
    with pd.option_context(""invalid"", True):
  File ~/miniforge3/envs/dev/lib/python3.13/contextlib.py:141 in __enter__
    return next(self.gen)
  File ~/miniforge3/envs/dev/lib/python3.13/site-packages/pandas/_config/config.py:512 in option_context
    for pat, val in undo:
UnboundLocalError: cannot access local variable 'undo' where it is not associated with a value
```

### Expected Behavior

In the latest stable release (2.3.3) one would get an `OptionError` instead:

```python-traceback
Traceback (most recent call last):
  Cell In[3], line 1
    with pd.option_context(""invalid"", True):
  File ~/miniforge3/envs/tmp/lib/python3.14/site-packages/pandas/_config/config.py:480 in __enter__
    self.undo = [(pat, _get_option(pat)) for pat, val in self.ops]
  File ~/miniforge3/envs/tmp/lib/python3.14/site-packages/pandas/_config/config.py:146 in _get_option
    key = _get_single_key(pat, silent)
  File ~/miniforge3/envs/tmp/lib/python3.14/site-packages/pandas/_config/config.py:132 in _get_single_key
    raise OptionError(f""No such keys(s): {repr(pat)}"")
OptionError: ""No such keys(s): 'invalid'""
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 531c0e3689fb1c4a1bd080263ed1869bc6fbda01
python                : 3.13.7
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:40 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8132
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2551.g531c0e3689
numpy                 : 2.3.3
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.4
sphinx                : None
IPython               : 9.6.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.6
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.2
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.25.0
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-10-17 08:54:44,2025-10-17 22:52:44,0,closed
62716,DOC: Where can I find the Pandas 3.0.0 Release timeline,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/index.html

### Documentation problem

Where can we find the release schedule for 3.0.0?  The dev doc states that is should have been released.  Is there an updated schedule? 

### Suggested fix for documentation

Provide a release schedule update. ","['Docs', 'Needs Triage']",2025-10-16 18:29:37,2025-10-16 20:48:05,2,closed
62712,BUG: `TimedeltaIndex` and `Index[timedelta]` behave differently upon `__rtruediv__`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from datetime import timedelta
from typing import reveal_type
import pandas as pd

timedeltaindex = pd.Index([pd.Timedelta(seconds=1)]) * [1]
reveal_type(timedeltaindex)  # TimedeltaIndex, contains pd.Timedelta
reveal_type(timedelta(seconds=1) / timedeltaindex)  # Index, no error

index_timedelta = pd.Index([1]) * [pd.Timedelta(seconds=1)]
reveal_type(index_timedelta)  # Index, contains datetime.timedelta
reveal_type(timedelta(seconds=1) / index_timedelta)  # error
```

### Issue Description

In pandas-dev/pandas#62524 I've shown that `Index[timedelta]` instead of `TimedeltaIndex` can be produced.

Here I found that these objects behave differently when `rtruediv`ed by another `timedelta`.

### Expected Behavior

Consistently giving `Index[float]`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.12.10
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : Czech_Czechia.1252

pandas                : 2.3.2
numpy                 : 2.3.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.6
numba                 : None
numexpr               : 2.13.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : 1.3.1
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.16.2
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.9.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.9
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Numeric Operations', 'Upstream issue']",2025-10-16 12:34:10,2025-10-23 21:29:30,7,closed
62711,BUG: pandera check fails with pandas `KeyError` on certain columns starting with version 2.3.3,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pandera.pandas as pa
import pandera.typing as pt


df = pd.DataFrame({
    'Status': ['Available', 'In_Use'],
    'Completed': ['true', 'false'],
    'UnaffectedData': [0, 3],
})

class PanderaTest(pa.DataFrameModel):
    Status: pt.Series[str] = pa.Field(
        isin=[
            ""Available"",
            ""In_Use"",
            ""Reserved"",
            ""Intended_For_Deactivation"",
            ""Expired"",
        ],
        nullable=False,
    )
    Completed: pt.Series[str] = pa.Field(
        isin=[""True"", ""true"", ""False"", ""false"", ""TRUE"", ""FALSE""], nullable=False
    )
    UnaffectedData: pt.Series[int] = pa.Field(nullable=False)

PanderaTest.validate(df)
```

### Traceback

<details>

```powershell
c:; cd 'c:\Users\<UID>\GitHub\<repo>'; & 'c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\python.exe' 'c:\Users\<UID>\.vscode\extensions\ms-python.debugpy-2025.14.1-win32-x64\bundled\libs\debugpy\launcher' '53595' '--' 'C:\Users\<UID>\GitHub\<repo>\.vscode\pandas-reprex.py' 
Traceback (most recent call last):
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\runpy.py"", line 198, in _run_module_as_main
    return _run_code(code, main_globals, None,
                     ""__main__"", mod_spec)
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\runpy.py"", line 88, in _run_code
    exec(code, run_globals)
    ~~~~^^^^^^^^^^^^^^^^^^^
  File ""c:\Users\<UID>\.vscode\extensions\ms-python.debugpy-2025.14.1-win32-x64\bundled\libs\debugpy\launcher/../..\debugpy\__main__.py"", line 71, in <module>
    cli.main()
    ~~~~~~~~^^
  File ""c:\Users\<UID>\.vscode\extensions\ms-python.debugpy-2025.14.1-win32-x64\bundled\libs\debugpy\launcher/../..\debugpy/..\debugpy\server\cli.py"", line 508, in main
    run()
    ~~~^^
  File ""c:\Users\<UID>\.vscode\extensions\ms-python.debugpy-2025.14.1-win32-x64\bundled\libs\debugpy\launcher/../..\debugpy/..\debugpy\server\cli.py"", line 358, in run_file
    runpy.run_path(target, run_name=""__main__"")
    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""c:\Users\<UID>\.vscode\extensions\ms-python.debugpy-2025.14.1-win32-x64\bundled\libs\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py"", line 310, in run_path
    return _run_module_code(code, init_globals, run_name, pkg_name=pkg_name, script_name=fname)
  File ""c:\Users\<UID>\.vscode\extensions\ms-python.debugpy-2025.14.1-win32-x64\bundled\libs\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py"", line 127, in _run_module_code
    _run_code(code, mod_globals, init_globals, mod_name, mod_spec, pkg_name, script_name)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""c:\Users\<UID>\.vscode\extensions\ms-python.debugpy-2025.14.1-win32-x64\bundled\libs\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py"", line 118, in _run_code
    exec(code, run_globals)
    ~~~~^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\<UID>\GitHub\<repo>\.vscode\pandera-reprex.py"", line 28, in <module>
    PanderaTest.validate(df)
    ~~~~~~~~~~~~~~~~~~~~^^^^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\pandas\model.py"", line 192, in validate
    cls.to_schema().validate(
    ~~~~~~~~~~~~~~~~~~~~~~~~^
        check_obj, head, tail, sample, random_state, lazy, inplace
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ),
    ^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\pandas\container.py"", line 117, in validate
    return self._validate(
           ~~~~~~~~~~~~~~^
        check_obj=check_obj,
        ^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
        inplace=inplace,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\pandas\container.py"", line 138, in _validate
    return self.get_backend(check_obj).validate(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        check_obj,
        ^^^^^^^^^^
    ...<6 lines>...
        inplace=inplace,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\container.py"", line 104, in validate
    error_handler = self.run_checks_and_handle_errors(
        error_handler,
    ...<8 lines>...
        random_state,
    )
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\container.py"", line 191, in run_checks_and_handle_errors
    error_handler.collect_error(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        validation_type(result.reason_code),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        result.original_exc,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\base\error_handler.py"", line 54, in collect_error
    raise schema_error from original_exc
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\container.py"", line 226, in run_schema_component_checks
    result = schema_component.validate(
        check_obj, lazy=lazy, inplace=True
    )
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\dataframe\components.py"", line 148, in validate
    return self.get_backend(check_obj).validate(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        check_obj,
        ^^^^^^^^^^
    ...<6 lines>...
        inplace=inplace,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\components.py"", line 141, in validate
    validated_column = validate_column(
        check_obj,
        column_name,
        return_check_obj=True,
    )
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\components.py"", line 101, in validate_column
    error_handler.collect_error(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        validation_type(err.reason_code), err.reason_code, err
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\base\error_handler.py"", line 54, in collect_error
    raise schema_error from original_exc
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\components.py"", line 77, in validate_column
    validated_check_obj = super(ColumnBackend, self).validate(
        check_obj,
    ...<6 lines>...
        inplace=inplace,
    )
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\array.py"", line 74, in validate
    error_handler = self.run_checks_and_handle_errors(
        error_handler,
    ...<5 lines>...
        random_state=random_state,
    )
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\array.py"", line 138, in run_checks_and_handle_errors
    error_handler.collect_error(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        validation_type(result.reason_code),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        original_exc=result.original_exc,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\base\error_handler.py"", line 54, in collect_error
    raise schema_error from original_exc
pandera.errors.SchemaError: Error while executing check function: KeyError(""<class 'pandas.core.series.Series'>"")
Traceback (most recent call last):
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\components.py"", line 240, in run_checks
    self.run_check(
    ~~~~~~~~~~~~~~^
        check_obj, schema, check, check_index, *check_args
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\base.py"", line 115, in run_check
    check_result: CheckResult = check(check_obj, *args)
                                ~~~~~^^^^^^^^^^^^^^^^^^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\checks.py"", line 230, in __call__
    return backend(check_obj, column)
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\checks.py"", line 349, in __call__
    check_output = self.apply(check_obj)
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\checks.py"", line 148, in apply
    return apply_fn(check_obj)
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\backends\pandas\checks.py"", line 156, in apply_field
    return self.check_fn(check_obj)
           ~~~~~~~~~~~~~^^^^^^^^^^^
  File ""c:\Users\<UID>\AppData\Local\miniconda3\envs\pandas_fail\Lib\site-packages\pandera\api\function_dispatch.py"", line 24, in __call__
    fn = self._function_registry[input_data_type]
         ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: <class 'pandas.core.series.Series'>
```

</details>


### Issue Description

I use `pandera` to check my `pandas` dataframe. Up to `pandas` version 2.3.2, checks passed as intended. Starting with version 2.3.3, I start encountering `KeyError: <class 'pandas.core.series.Series'>`.

I work with anaconda. The two venv reprex are:
```powershell
conda create -y --name pandas_pass python==3.14 pandas==2.3.2 pandera
```
```powershell
conda create -y --name pandas_fail python==3.14 pandas==2.3.3 pandera
```
The `pandera` version installed in the venv is the current 0.26.1, but I also tested 0.26.0, 0.25.0, and 0.24.0 and all work just fine with `pandas` <= 2.3.2.

### Expected Behavior

The `pandera` check is expected to pass (not throwing an exception) with correct data.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.14.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : de_DE.cp1252

pandas                : 2.3.3
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
",['Bug'],2025-10-16 09:11:01,2025-10-19 13:20:47,7,closed
62705,BUG: Pivoting empty DataFrame fails with period-like dtypes,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import datetime
import pandas as pd

values = [
    True,
    1,
    1.2,
    ""Hello"",
    [1, ""b""],
    {""key"": ""value""},
    datetime.datetime(2000, 1, 1),
    pd.Timestamp(2000, 1, 1),
    pd.offsets.MonthEnd(),
    pd.Timedelta(days=1),
    pd.Period(""2000-01-01""),
]

for value in values:
    series = pd.Series(value)

    frame = pd.DataFrame({""index"": [], ""columns"": [], ""values"": []})
    frame = frame.astype({""values"": series.dtype})

    print(f""Testing with {type(value)} ({frame['values'].dtype} dtype)..."", end="""")
    try:
        frame.pivot(index=""index"", columns=""columns"", values=""values"")
        print(""works."")
    except Exception:
        print(""fails!"")
        raise
```

### Issue Description

Pivoting an empty `DataFrame` fails if the column used for the values has a dtype derived from a `Period`.  the above code results in the following output:
```
Testing with <class 'bool'> (bool dtype)...works.
Testing with <class 'int'> (int64 dtype)...works.
Testing with <class 'float'> (float64 dtype)...works.
Testing with <class 'str'> (object dtype)...works.
Testing with <class 'list'> (object dtype)...works.
Testing with <class 'dict'> (object dtype)...works.
Testing with <class 'datetime.datetime'> (datetime64[ns] dtype)...works.
Testing with <class 'pandas._libs.tslibs.timestamps.Timestamp'> (datetime64[ns] dtype)...works.
Testing with <class 'pandas._libs.tslibs.offsets.MonthEnd'> (object dtype)...works.
Testing with <class 'pandas._libs.tslibs.timedeltas.Timedelta'> (timedelta64[ns] dtype)...works.
Testing with <class 'pandas._libs.tslibs.period.Period'> (period[D] dtype)...fails!
Traceback (most recent call last):
  File ""/home/ken/no_backup/pivot_example/pivot_example.py"", line 26, in <module>
    frame.pivot(index=""index"", columns=""columns"", values=""values"")
    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ken/no_backup/pivot_example/.venv/lib/python3.13/site-packages/pandas/core/frame.py"", line 9366, in pivot
    return pivot(self, index=index, columns=columns, values=values)
  File ""/home/ken/no_backup/pivot_example/.venv/lib/python3.13/site-packages/pandas/core/reshape/pivot.py"", line 570, in pivot
    result = indexed.unstack(columns_listlike)  # type: ignore[arg-type]
  File ""/home/ken/no_backup/pivot_example/.venv/lib/python3.13/site-packages/pandas/core/series.py"", line 4634, in unstack
    return unstack(self, level, fill_value, sort)
  File ""/home/ken/no_backup/pivot_example/.venv/lib/python3.13/site-packages/pandas/core/reshape/reshape.py"", line 520, in unstack
    return unstacker.get_result(
           ~~~~~~~~~~~~~~~~~~~~^
        obj._values, value_columns=None, fill_value=fill_value
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""/home/ken/no_backup/pivot_example/.venv/lib/python3.13/site-packages/pandas/core/reshape/reshape.py"", line 238, in get_result
    values, _ = self.get_new_values(values, fill_value)
                ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File ""/home/ken/no_backup/pivot_example/.venv/lib/python3.13/site-packages/pandas/core/reshape/reshape.py"", line 278, in get_new_values
    new_values = np.empty(result_shape, dtype=dtype)
TypeError: Cannot interpret 'period[D]' as a data type
```

If the `DataFrame` is not empty, no error is encountered.

### Expected Behavior

No error should occur.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.13.7
python-bits           : 64
OS                    : Linux
OS-release            : 6.17.1-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Mon, 06 Oct 2025 18:48:29 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Period', 'Needs Tests']",2025-10-15 21:58:05,2025-11-06 16:36:29,4,closed
62701,Ruff format fails on test_query_eval.py ,"Running `ruff format` or `ruff format --check` fails on the following code block in
[pandas/tests/frame/test_query_eval.py](https://github.com/pandas-dev/pandas/blob/066a4f7fe2b443920bcbb69de0fc12c9a73277b5/pandas/tests/frame/test_query_eval.py#L162)

`df = DataFrame(
    {
        ""A"": range(3),
        ""B"": range(3),
        ""C"": range(3)
    }
).rename(columns={""B"": ""A""})

res = df.query(""C == 1"", engine=engine, parser=parser)

expect = DataFrame(
    [[1, 1, 1]],
    columns=[""A"", ""A"", ""C""],
    index=[1]
)
`

steps to reproduce:
`git checkout main  & 
ruff format --check`

Expected behavior
`ruff format --check` should pass cleanly without reporting reformatting issues.

Actual Behavior:
`ruff format` reports that the file would be reformatted, causing formatting checks to fail in CI pipelines.
Example output:
Would reformat: pandas/tests/frame/test_query_eval.py
1 file would be reformatted, 1552 files already formatted

",[],2025-10-15 11:57:30,2025-10-15 17:26:44,2,closed
62694,PERF: rolling std/var x100 slower in current dev version,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

import pandas as pd
import numpy as np
import time
import platform

n = 2_000_000
window = 500
s = pd.Series(np.random.randn(n), name=""x"")

_ = s.rolling(window).std()


t0 = time.perf_counter()
_ = s.rolling(window).std()
t = time.perf_counter() - t0

print(f""Rolling std({window}) on {n:,} rows took {t:.4f} s"")

Running on current dev (installed by pip install git+https://github.com/pandas-dev/pandas.git
or  pip install --pre --extra-index https://pypi.anaconda.org/scientific-python-nightly-wheels/simple pandas)
I get 

> Rolling std(500) on 2,000,000 rows took 4.3563 s


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c4449f0e1ca05b612ced998f45b0afe08df07d41
python                : 3.14.0
python-bits           : 64
OS                    : Windows
OS-release            : 2022Server
Version               : 10.0.22631
machine               : AMD64
processor             : AMD64 Family 25 Model 24 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 3.0.0.dev0+2533.gc4449f0e1c
numpy                 : 2.3.3
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.10.6
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None



</details>


### Prior Performance

Running on pandas 2.3.3 I get 
> Rolling std(500) on 2,000,000 rows took 0.0377 s

Results on python 3.12 and 3.13 are same","['Performance', 'Window']",2025-10-14 14:18:46,2025-10-20 22:24:32,4,closed
62690,There may be a problem in the Dockerfile provided in the root directory.,"When I tried to use the Dockerfile provided in the root directory to build my local developing environment,  I encountered the following error:
```
[+] Building 21.7s (2/2) FINISHED                                                                                                          docker:default
 => [internal] load build definition from Dockerfile                                                                                                 0.0s
 => => transferring dockerfile: 957B                                                                                                                 0.0s
 => ERROR [internal] load metadata for docker.io/library/python:3.11.13                                                                             21.6s
------
 > [internal] load metadata for docker.io/library/python:3.11.13:
------
Dockerfile:1
--------------------
   1 | >>> FROM python:3.11.13
   2 |     WORKDIR /home/pandas
   3 |     
--------------------
ERROR: failed to build: failed to solve: python:3.11.13: failed to resolve source metadata for docker.io/library/python:3.11.13: failed to do request: Head ""https://registry-1.docker.io/v2/library/python/manifests/3.11.13"": dialing registry-1.docker.io:443 container via direct connection because disabled has no HTTPS proxy: connecting to registry-1.docker.io:443: dial tcp [2a03:2880:f127:283:face:b00c:0:25de]:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.
```

It shows that the base image used in the Dockerfile no longer exists, and I searched the Dockerhub and confirmed that it no longer exits.Now I changed the base image to python:3.11.14 as an alternative.

I think it is necessary to update this Dockerfile.","['Build', 'Closing Candidate']",2025-10-14 11:07:26,2025-10-30 01:25:21,6,closed
62679,"BUG: test_compression leaves ""pyarrow"" file behind","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example
Adding to conftest


```python

import os

@pytest.fixture(autouse=True)
def check_files():
    orig = os.listdir(os.getcwd())
    yield
    end = os.listdir(os.getcwd())
    assert orig == end
```

### Issue Description

pandas/tests/io/test_parquet.py::TestBasic::test_compression[pyarrow-None]

is the culprit, leaving behind a file just called ""pyarrow""

### Expected Behavior

N/A

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2025-10-13 18:12:03,2025-10-14 17:06:09,0,closed
62669,BUG: Calling SparseArray.cumsum leads to infinite recursion,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas.arrays import SparseArray
import numpy as np

# Create a simple SparseArray with integer values
# Integer arrays default to fill_value=0
sparse = SparseArray([1, 2, 3])

print(f""SparseArray: {sparse}"")
print(f""Fill value: {sparse.fill_value}"")
print(f""_null_fill_value: {sparse._null_fill_value}"")

# This should calculate cumulative sum [1, 3, 6]
# But it will cause RecursionError
try:
    result = sparse.cumsum()
    print(f""Cumsum result: {result}"")
except RecursionError as e:
    print(f""\nRecursionError occurred!"")
    print(f""Error: {e}"")
```

### Issue Description

The cumsum() method on SparseArray causes infinite recursion and crashes with RecursionError when the array has a non-null fill value, which is the default behavior for all integer SparseArrays (fill_value=0).

The offending code appears to be this:

https://github.com/pandas-dev/pandas/blob/1863adb252863b718ba29912922bf050ce0eaa3d/pandas/core/arrays/sparse/array.py#L1573-L1574

I discovered this bug as part of a project where we are use LLMs to search for bugs in popular open source repositories. Before filing this bug, we paid for three expert human reviewers to validate this bug, and I also manually validated the bug on my own machines. I am confident that the bug is real, I wrote and filed this report manually, and take responsibility for this bug report.

The LLM provided the following suggested fix that seems somewhat valid to me but I'm not familiar with this repo and so can't validate myself:

```diff
--- a/pandas/core/arrays/sparse/array.py
+++ b/pandas/core/arrays/sparse/array.py
@@ -1547,7 +1547,7 @@ class SparseArray(OpsMixin, PandasObject, ExtensionArray):
             raise ValueError(f""axis(={axis}) out of bounds"")

         if not self._null_fill_value:
-            return SparseArray(self.to_dense()).cumsum()
+            return SparseArray(self.to_dense().cumsum())

         return SparseArray(
             self.sp_values.cumsum(),
```

### Expected Behavior

It doesn't crash :)

Stacktrace provided below:

<details>
Traceback (most recent call last):
  File ""<python-input-5>"", line 1, in <module>
    result = sparse.cumsum()
  File ""/home/npc/pbt/agentic-pbt/envs/pandas_env/lib/python3.13/site-packages/pandas/core/arrays/sparse/array.py"", line 1550, in cumsum
    return SparseArray(self.to_dense()).cumsum()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File ""/home/npc/pbt/agentic-pbt/envs/pandas_env/lib/python3.13/site-packages/pandas/core/arrays/sparse/array.py"", line 1550, in cumsum
    return SparseArray(self.to_dense()).cumsum()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File ""/home/npc/pbt/agentic-pbt/envs/pandas_env/lib/python3.13/site-packages/pandas/core/arrays/sparse/array.py"", line 1550, in cumsum
    return SparseArray(self.to_dense()).cumsum()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  [Previous line repeated 981 more times]
  File ""/home/npc/pbt/agentic-pbt/envs/pandas_env/lib/python3.13/site-packages/pandas/core/arrays/sparse/array.py"", line 495, in __init__
    self._dtype = SparseDtype(sparse_values.dtype, fill_value)
                  ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/npc/pbt/agentic-pbt/envs/pandas_env/lib/python3.13/site-packages/pandas/core/dtypes/dtypes.py"", line 1689, in __init__
    self._check_fill_value()
    ~~~~~~~~~~~~~~~~~~~~~~^^
  File ""/home/npc/pbt/agentic-pbt/envs/pandas_env/lib/python3.13/site-packages/pandas/core/dtypes/dtypes.py"", line 1777, in _check_fill_value
    if not can_hold_element(dummy, val):
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File ""/home/npc/pbt/agentic-pbt/envs/pandas_env/lib/python3.13/site-packages/pandas/core/dtypes/cast.py"", line 1772, in can_hold_element
    np_can_hold_element(dtype, element)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File ""/home/npc/pbt/agentic-pbt/envs/pandas_env/lib/python3.13/site-packages/pandas/core/dtypes/cast.py"", line 1810, in np_can_hold_element
    info = np.iinfo(dtype)
RecursionError: maximum recursion depth exceeded
</detail>

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.13.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.0-1017-gcp
Version               : #18~24.04.1-Ubuntu SMP Tue Sep 23 17:51:44 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.3.2
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : 6.139.1
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Sparse', 'Transformations']",2025-10-12 18:04:18,2025-11-30 18:17:27,3,closed
62653,BUG: Series.str.replace stopped working with regex groups,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from re import sub

teststr = ""var.one[0]""
print(sub(r'\[(\d+)\]',r'(\1)',teststr))

s = pd.Series([""var.one[0]"", ""var.two[1]"", ""var.three[2]""]).convert_dtypes(dtype_backend=""pyarrow"")

t = s.str.replace(r'\[(\d+)\]',r'(\1)',regex=True)

print(t)
```

### Issue Description

The most recent pandas version produces the following error message:
```
var.one(0)
Traceback (most recent call last):
  File ""/workspaces/verbose-system/testfile.py"", line 9, in <module>
    t = s.str.replace(r'\[(\d+)\]',r'(\1)',regex=True)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspaces/verbose-system/.new/lib/python3.12/site-packages/pandas/core/strings/accessor.py"", line 140, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspaces/verbose-system/.new/lib/python3.12/site-packages/pandas/core/strings/accessor.py"", line 1580, in replace
    result = self._data.array._str_replace(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspaces/verbose-system/.new/lib/python3.12/site-packages/pandas/core/arrays/_arrow_string_mixins.py"", line 182, in _str_replace
    raise NotImplementedError(
NotImplementedError: replace is not supported with a re.Pattern, callable repl, case=False, flags!=0, or when the replacement string contains named group references (\g<...>, \d+)
```

I have trialed all possible combinations of `\1`, `\g<1>`, named groups, precompiled patterns, etc. However, this issue persists. I believe that this is related to: https://github.com/pandas-dev/pandas/issues/57636

### Expected Behavior

With pandas 2.3.0 the output of the script above was:
```
var.one(0)
0      var.one(0)
1      var.two(1)
2    var.three(2)
dtype: string[pyarrow]
```

### Installed Versions

```
INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.12.11
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-1030-azure
Version               : #35~22.04.1-Ubuntu SMP Mon May 26 18:08:30 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
```
","['Bug', 'Strings', 'Arrow']",2025-10-11 07:58:09,2025-10-27 17:36:41,2,closed
62646,"DOC: Index.where(..., other) other could be a Series/Index but not documented as such","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.Index.where.html#pandas.Index.where

### Documentation problem

Right now the docs for where for an Index mention that the `other` argument can be an array-like. As opposed to Series.where and DataFrame.where where other can be a Series or DataFrame.

At runtime this works totally fine with a Series or an Index: 
```python
import pandas as pd
import numpy as np

datetime_index = pd.DatetimeIndex(pd.date_range(start=""2025-01-01"", freq=""h"", periods=48))
mask = np.ones(48, dtype=bool)

condition = datetime_index.where(mask,datetime_index - pd.Timedelta(days=1))
```

### Suggested fix for documentation

Should we allow Index/Series type for `other`?

Happy to make the PR, we have added this in a PR in https://github.com/pandas-dev/pandas-stubs/pull/1420 so waiting for confirmation of the `pandas` team to move forward with it or not.
Thanks!",['Docs'],2025-10-10 19:58:47,2025-11-01 20:45:23,8,closed
62637,BUG: Python 3.14 no-gil version cannot install pandas on Windows,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
python3.14t -m pip install pandas
```

### Issue Description

```
H:\Desktop>python3.14t -m pip install pandas
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Collecting pandas
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/33/01/d40b85317f86cf08d853a4f495195c73815fdf205eef3993821720274518/pandas-2.3.3.tar.gz (4.5 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [2 lines of output]

      meson-python: error: Could not execute meson:
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

### Expected Behavior

Python 3.14t can install pandas normally.

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
",['Bug'],2025-10-09 09:47:10,2025-10-09 18:20:47,5,closed
62629,CI: New python-dev build causes errors in test-cases,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
https://github.com/pandas-dev/pandas/actions/runs/18354036003/job/52281282834
https://github.com/pandas-dev/pandas/actions/runs/18354390134/job/52282486675
https://github.com/pandas-dev/pandas/actions/runs/18352285478
```

### Issue Description

The same errors are cropping up in CI runs from today across multiple PRs.  

This is likely from a new python-dev build (3.14.0) and pydantic build (2.12) published yesterday.
For example on my PR, I have only changed the .rst and a test case since the previous commit, the errors have nothing to do with the test case I changed

I've also tried it on main and the errors existed there too

### Expected Behavior

Change tests to accomodate updates

### Installed Versions

NA
",['CI'],2025-10-08 19:28:10,2025-10-13 20:53:24,5,closed
62622,BUG: boolean indexing gets different results during multiple runnings,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
d = pd.Series(np.random.randn(1000001).astype(np.float32))
count = (d >= 0).sum()
mean = d.mean()

for i in range(1000):
    c = (d >= 0).sum()
    m = d.mean()
    if c != count:
        print(f'round {i}: not same', c, count, m, mean)
```

### Issue Description

when number of elements larger than 1000000, the bug appears randomly. However, when less than or equal to 1000000, no error was observed.

### Expected Behavior

no output

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.12.11
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 26 Model 36 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : Chinese (Simplified)_China.936

pandas                : 2.3.3
numpy                 : 2.0.1
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : 9.5.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.5
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.6
numba                 : None
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage']",2025-10-08 09:46:15,2025-10-08 09:48:51,0,closed
62617,BUG: incorrect string parsing leads to segfault,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import io
import pandas as pd
pd.read_csv(
    io.StringIO(""data1,81e3104049863b72,data3""),
    names=[""header1"", ""header2"", ""header3""]
)
```

### Issue Description

I tracked down a segfault to the provided snippet when parsing big raw CSVs. One of the values (here under ""header2"") is a hexadecimal 16-character string, and under some conditions, it cannot be parsed by the C engine, yielding a segfault. Pyarrow fails as well, but the Python engine succeeds.

I tested multiple variations of the issue (as illustrated by the examples below). My hypothesis is that the C engine tries to cast it as an integer written in scientific notation, but fails. I have not been able to pinpoint exactly the conditions in the time I spent debugging, and this is the only line that failed in the 35M lines that I parsed for this project.

The issue appears in the latest pandas version, but **not** in the main branch.

```python
import io

import pandas as pd

print(""works (only 0-9 after e)"")
pd.read_csv(
    io.StringIO(""data1,81e310404986372,data3""),
    names=[
        ""header1"",
        ""header2"",
        ""header3"",
    ],
)

print(""works (only 9 chars between e and b)"")
pd.read_csv(
    io.StringIO(""data1,81e310404986b72,data3""),
    names=[
        ""header1"",
        ""header2"",
        ""header3"",
    ],
)

print(""works (casted to string)"")
pd.read_csv(
    io.StringIO(""data1,81e3104049863b72,data3""),
    names=[
        ""header1"",
        ""header2"",
        ""header3"",
    ],
    dtype={""header2"": str},
)

print(""segfault"")
pd.read_csv(
    io.StringIO(""data1,81e3104049863b72,data3""),
    names=[
        ""header1"",
        ""header2"",
        ""header3"",
    ],
)
```
```
works (only 0-9 after e)
works (only 9 chars between e and b)
works (casted to string)
segfault
zsh: segmentation fault (core dumped)  python tests/test_segfault.py
```

### Expected Behavior

No segfault :)

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.12.10
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.0-33-generic
Version               : #33~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 19 17:02:30 UTC 2
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.6.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.9.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.6
numba                 : 0.62.1
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.9.0
scipy                 : 1.16.2
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : 2025.9.1
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV', 'Segfault']",2025-10-07 18:06:48,2025-10-21 19:51:43,9,closed
62611,BUG: Drop duplicates bug when using pyarrow (version 21.0.0) backend,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# run 'pip install pyarrow==21.0.0' before running this code to reproduce
import pandas as pd

# Having the empty string as well as a None value seems required to trigger the bug
df = pd.DataFrame(data={""A"": [""a"", ""b"", """", ""a"", None, ""b""]})
# If this column is added, drop duplicates even removes a wrong column
df[""B""] = ""b""
# Drop duplicates misses a duplicated column the first time its called with pyarrow backend
df = df.convert_dtypes(dtype_backend=""pyarrow"")

print(df)
print(""\n\n"")
# The first call doesn't drop all duplicates, and even drops the <NA> row if the column B is added
print(df.drop_duplicates())
print(""\n\n"")
# The second call drop the remaining duplicate
print(df.drop_duplicates().drop_duplicates())
```

### Issue Description

When calling ``df.drop_duplicates`` on a pandas Dataframe using the pyarrow backend, it incorrectly removes unique columns and keeps non unique columns. This only occurs with the pyarrow version 21.0.0, earlier versions seem to work without issue. 
I could only trigger this bug, having both another unique value and a None value in the column containing duplicate entries. 
Adding a second column with a fixed value (like ""B""), leads to ``drop_duplicates`` removing the row containing the <NA> value, even though the row is unique.

### Expected Behavior

I would expect ``drop_duplicates`` to only drop duplicate rows and keep unique rows when using a pyarrow backend (as it does correctly for the numpy nullable backend).

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.11.9
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 143 Stepping 8, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.3.3
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : 8.2.3
IPython               : 9.5.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.5
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.43
tables                : None
tabulate              : 0.9.0
xarray                : 2025.9.0
xlrd                  : None
xlsxwriter            : 3.2.9
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Windows', 'duplicated', 'Upstream issue', 'Arrow']",2025-10-07 13:52:04,2025-10-28 10:16:30,17,closed
62609,DOC: A small spelling mistake,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/development/contributing.html

### Documentation problem

Maybe the word ""though"" should be ""through""?An ""r"" is missing, maybe this is not a big issue, but I think it is better to fix it.

<img width=""926"" height=""83"" alt=""Image"" src=""https://github.com/user-attachments/assets/dfc221f8-2619-409b-9db6-7768297af1ee"" />

### Suggested fix for documentation

Please replace the wrong word ""though"" with ""through"".","['Docs', 'Needs Triage']",2025-10-07 09:09:55,2025-10-07 10:00:27,0,closed
62595,BUG: Multiplying a series of strings with a boolean different for arrow vs. python,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.Series([""a"", ""b"", ""c""], dtype=""string[python]"") * True  # works
pd.Series([""a"", ""b"", ""c""], dtype=""string"") * True  # fails
```

### Issue Description

Multiplying a string series by a boolean doesn't work with arrow based strings
Error reported:
```text
  File ""C:\Code\pandas_dev\pandas\pandas\core\ops\common.py"", line 70, in new_method
    return method(self, other)
           ^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\core\arraylike.py"", line 205, in __mul__
    return self._arith_method(other, operator.mul)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\core\series.py"", line 5924, in _arith_method
    return base.IndexOpsMixin._arith_method(self, other, op)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\core\base.py"", line 1485, in _arith_method
    result = ops.arithmetic_op(lvalues, rvalues, op)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\core\ops\array_ops.py"", line 272, in arithmetic_op
    res_values = op(left, right)
                 ^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\core\ops\common.py"", line 70, in new_method
    return method(self, other)
           ^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\core\arraylike.py"", line 205, in __mul__
    return self._arith_method(other, operator.mul)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\core\arrays\arrow\array.py"", line 1051, in _arith_method
    result = self._evaluate_op_method(other, op, ARROW_ARITHMETIC_FUNCS)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\core\arrays\arrow\array.py"", line 970, in _evaluate_op_method
    raise TypeError(""Can only string multiply by an integer."")
TypeError: Can only string multiply by an integer.
```


### Expected Behavior

No error

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : bb10b27dea9d9a2476de4c8122e0346689e1c9c3
python                : 3.11.13
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 3.0.0.dev0+2306.gbb10b27dea.dirty
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.3
sphinx                : 8.3.0
IPython               : 9.5.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.0
bottleneck            : 1.6.0
fastparquet           : 2024.11.0
fsspec                : 2025.9.0
html5lib              : 1.1
hypothesis            : 6.138.15
gcsfs                 : 2025.9.0
jinja2                : 3.1.6
lxml.etree            : 6.0.1
matplotlib            : 3.10.6
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyiceberg             : 0.9.1
pyreadstat            : 1.3.1
pytest                : 8.4.2
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.9.0
scipy                 : 1.16.1
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.9.0
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.24.0
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Numeric Operations', 'Strings']",2025-10-06 00:04:32,2025-10-16 00:25:44,9,closed
62590,"BUG: In main, `pd.Series(show_counts=False)` is broken","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
s=pd.Series([1])
s.info(show_counts=False)
```

### Issue Description

Above code fails with
```text
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Code\pandas_dev\pandas\pandas\core\series.py"", line 5187, in info
    return SeriesInfo(self, memory_usage).render(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\io\formats\info.py"", line 551, in render
    printer.to_buffer(buf)
  File ""C:\Code\pandas_dev\pandas\pandas\io\formats\info.py"", line 588, in to_buffer
    lines = table_builder.get_lines()
            ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Code\pandas_dev\pandas\pandas\io\formats\info.py"", line 1028, in get_lines
    self._fill_non_empty_info()
  File ""C:\Code\pandas_dev\pandas\pandas\io\formats\info.py"", line 1082, in _fill_non_empty_info
    self.add_body_lines()
  File ""C:\Code\pandas_dev\pandas\pandas\io\formats\info.py"", line 925, in add_body_lines
    [
  File ""C:\Code\pandas_dev\pandas\pandas\io\formats\info.py"", line 925, in <listcomp>
    [
ValueError: zip() argument 2 is shorter than argument 1
```

### Expected Behavior

No error

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : bb10b27dea9d9a2476de4c8122e0346689e1c9c3
python                : 3.11.13
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 3.0.0.dev0+2306.gbb10b27dea.dirty
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.3
sphinx                : 8.3.0
IPython               : 9.5.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.0
bottleneck            : 1.6.0
fastparquet           : 2024.11.0
fsspec                : 2025.9.0
html5lib              : 1.1
hypothesis            : 6.138.15
gcsfs                 : 2025.9.0
jinja2                : 3.1.6
lxml.etree            : 6.0.1
matplotlib            : 3.10.6
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyiceberg             : 0.9.1
pyreadstat            : 1.3.1
pytest                : 8.4.2
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.9.0
scipy                 : 1.16.1
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.9.0
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.24.0
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage', 'Describe/info/etc']",2025-10-05 14:10:57,2025-10-16 15:00:06,5,closed
62562,DOC: check_array_indexer also accepts an int or a slice,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.api.indexers.check_array_indexer.html

### Documentation problem

`check_array_indexer` also takes as input a single int or a slice as an argument for the `indexer`:
```python
>>> import pandas as pd
>>> arr = pd.array([1, 2])
>>> pd.api.indexers.check_array_indexer(arr, 1)
1
>>> pd.api.indexers.check_array_indexer(arr, slice(0,1,1))
slice(0, 1, 1)
```

Not clear if this is a doc issue, or if the code should be checking that those arguments are invalid.  I think they are valid since in the `decimal` extension array example, it is used in `__setitem__()` to check if the indexer is valid.


### Suggested fix for documentation

Specify that a single int or slice is valid.",['Docs'],2025-10-03 16:47:25,2025-10-05 19:41:30,1,closed
62561,Remove take action from CI,"The take action in github is well intentioned, but may be causing more harm than good. Often users comment ""take"" but progress immediately stalls, and new contributors can be discouraged from pursuing issues that are already assigned to users.

As an alternative, we can document better expectations for ""soft reserving"" an issue, much like sci-kit learn has done in https://github.com/scikit-learn/scikit-learn/pull/31568",['Admin'],2025-10-03 16:22:42,2025-10-07 18:04:20,4,closed
62545,BUG: Some existing tests are now failing for latest `numexpr` version `2.13.`. Addition and Multiplication of Booleans no longer issues the expected warning,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# reproducing involves running this test with numexpr versions 2.13.0 where it passes
# and version 2.13.1 where it fails due to the warning not being issued. There are 5 failing tests in total 

def test_add_list_to_masked_array_boolean(self, request):
    # GH#22962
    warning = (
        UserWarning
        if request.node.callspec.id == ""numexpr"" and NUMEXPR_INSTALLED
        else None
    )
    ser = Series([True, None, False], dtype=""boolean"")
    msg = ""operator is not supported by numexpr for the bool dtype""
    with tm.assert_produces_warning(warning, match=msg):
        result = ser + [True, None, True]
    expected = Series([True, None, True], dtype=""boolean"")
    tm.assert_series_equal(result, expected)
```

### Issue Description

As of `numexpr` version `2.13.1`, we have failing tests in 
```
pandas/tests/frame/test_arithmetic.py
pandas/tests/series/test_arithmetic.py
pandas/tests/test_expressions.py
```


All are related to warnings no longer being emitted when adding and multiplying bools. 

### Expected Behavior

```
    def test_add_list_to_masked_array_boolean(self, request):
        ser = Series([True, None, False], dtype=""boolean"")
        with tm.assert_produces_warning(None):
            result = ser + [True, None, True]
        expected = Series([True, None, True], dtype=""boolean"")
        tm.assert_series_equal(result, expected)

test_add_list_to_masked_array_boolean()
```

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Numeric Operations', 'Dependencies']",2025-10-02 08:49:29,2025-10-03 00:43:49,2,closed
62534,ENH: Add null percent in output of describe(),"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I would like to add `null percent` to output of describe()

### Feature Description

df[col].describe()
```output
count     15003
unique    13971
null_percent 0.5 (50% rows that are null/NA)
top        test
freq         11
Name: title, dtype: object
```

### Alternative Solutions

_

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-10-01 16:39:22,2025-10-01 17:15:02,1,closed
62531,BUG: Parsing a CSV file with string that looks like a number,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
#!/usr/bin/env python3
import pandas as pd
import sys

# The problematic git hash that causes pandas to crash
PROBLEMATIC_HASH = ""2396e87607016390692dc45951f2ba1878ffb39e""

# A working git hash for comparison  
WORKING_HASH = ""7354ea14bf673b25106c5edc37bb92118e4ebf63""

def create_test_csv(hash_value, filename):
    """"""Create a minimal CSV with just the problematic hash""""""
    content = f""""""GitHash
{hash_value}""""""
    
    with open(filename, 'w') as f:
        f.write(content)

def test_hash(hash_value, description):
    """"""Test reading a CSV with the given hash""""""
    filename = f""test_{description}.csv""
    create_test_csv(hash_value, filename)
    
    print(f""\n=== Testing {description} hash: {hash_value} ==="")
    try:
        df = pd.read_csv(filename)
        print(f""SUCCESS: Read {len(df)} rows"")
        return True
    except Exception as e:
        print(f""EXCEPTION: {e}"")
        return False
    # Note: If this causes a segfault, we won't reach this return statement

def main():
    print(""Pandas CSV Crash Minimal Reproducer"")
    print(""="" * 50)
    print(f""pandas version: {pd.__version__}"")
    print(f""python version: {sys.version.split()[0]}"")
    print()
    
    # Test the working hash first
    working = test_hash(WORKING_HASH, ""working"")
    
    # Test the problematic hash
    # WARNING: This will likely crash the Python interpreter
    print(f""\nWARNING: The next test will likely crash the interpreter!"")
    problematic = test_hash(PROBLEMATIC_HASH, ""problematic"")
    
    print(f""\nIf you see this message, the crash was avoided and this was a bad repo"")

if __name__ == '__main__':
    main()


When I run this on the main branch I get:


Pandas CSV Crash Minimal Reproducer
==================================================
pandas version: 3.0.0.dev0+2466.g5cc3240965
python version: 3.12.3


=== Testing working hash: 7354ea14bf673b25106c5edc37bb92118e4ebf63 ===
SUCCESS: Read 1 rows

WARNING: The next test will likely crash the interpreter!

=== Testing problematic hash: 2396e87607016390692dc45951f2ba1878ffb39e ===
[1]    619586 segmentation fault (core dumped)  python minimal_reproducer.py
```

### Issue Description

`2396e87607016390692dc45951f2ba1878ffb39e` looks like it causes the crash. I think in the code we get to `floatify` somewhere via `maybe_convert_numeric` and then bad things happen in the C 😄 

### Expected Behavior

It shouldn't crash and fall back to treating the value as a string.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 5cc32409652002b4de442cfa6a065553aa17c3c9
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.87.2-microsoft-standard-WSL2
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 3.0.0.dev0+2466.g5cc3240965
numpy                 : 2.4.0.dev0+git20251001.8fecead
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV']",2025-10-01 09:52:35,2025-10-02 20:01:27,3,closed
62524,BUG: Inconsistent runtime typing for Index[num] * list[timestamp],"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from typing import reveal_type
import pandas as pd
from datetime import timedelta

reveal_type(pd.Index([1]) * timedelta(days=1))
reveal_type(pd.Index([1]) * [timedelta(days=1)])
reveal_type(pd.Index([1]) * pd.Timedelta(1, ""D""))
reveal_type(pd.Index([1]) * [pd.Timedelta(1, ""D"")])
```

### Issue Description

Result is inconsistent
```text
Runtime type is 'TimedeltaIndex'
Runtime type is 'Index'
Runtime type is 'TimedeltaIndex'
Runtime type is 'Index'
```

### Expected Behavior

Results should all be `TimedeltaIndex

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.12.10
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : Czech_Czechia.1252

pandas                : 2.3.2
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.2
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.6
numba                 : None
numexpr               : 2.13.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : 1.3.1
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.16.2
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.9.0
xlrd                  : 2.0.2
xlsxwriter            : 3.2.9
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Numeric Operations', 'Index', 'Closing Candidate']",2025-09-30 20:52:40,2025-11-26 19:23:50,7,closed
62522,BUG: ArrowEA._cast_pointwise_result([Decimal(NaN)]),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from decimal import Decimal

ser = pd.Series([], dtype=""float64[pyarrow]"")
arr = ser.array
item = Decimal(""NaN"")

>>> arr._cast_pointwise_result([item]).dtype
null[pyarrow]
```

### Issue Description

This goes through `pa.array([item], from_pandas=True)` so may be an upstream issue.  

### Expected Behavior

Ideally this should give back a decimal dtype.


### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Arrow']",2025-09-30 19:47:21,2025-10-16 21:14:00,0,closed
62520,BUG: Regression in Series.pow with all-NA `double[pyarrow]` values in pandas 3.x,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pyarrow as pa

s = pd.Series([None, None], dtype=pd.ArrowDtype(pa.float64()))
s.pow(2)
```

### Issue Description

The snippet above succeeds with pandas 2.x, but raises a `ArrowNotImplementedError: Function 'replace_with_mask' has no kernel matching input types (double, null, double)` with pandas 3.x

```pytb
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
Cell In[1], line 6
      2 import pyarrow as pa
      5 s = pd.Series([None, None], dtype=pd.ArrowDtype(pa.float64()))
----> 6 s.pow(2)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/series.py:6591, in Series.pow(self, other, level, fill_value, axis)
   6589 @Appender(ops.make_flex_doc(""pow"", ""series""))
   6590 def pow(self, other, level=None, fill_value=None, axis: Axis = 0) -> Series:
-> 6591     return self._flex_method(
   6592         other, operator.pow, level=level, fill_value=fill_value, axis=axis
   6593     )

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/series.py:6050, in Series._flex_method(self, other, op, level, fill_value, axis)
   6047         return op(self, fill_value)
   6048     self = self.fillna(fill_value)
-> 6050 return op(self, other)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/ops/common.py:70, in _unpack_zerodim_and_defer.<locals>.new_method(self, other)
     66         return NotImplemented
     68 other = item_from_zerodim(other)
---> 70 return method(self, other)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/arraylike.py:245, in OpsMixin.__pow__(self, other)
    243 @unpack_zerodim_and_defer(""__pow__"")
    244 def __pow__(self, other):
--> 245     return self._arith_method(other, operator.pow)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/series.py:5924, in Series._arith_method(self, other, op)
   5922 def _arith_method(self, other, op):
   5923     self, other = self._align_for_op(other)
-> 5924     return base.IndexOpsMixin._arith_method(self, other, op)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/base.py:1485, in IndexOpsMixin._arith_method(self, other, op)
   1482     rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)
   1484 with np.errstate(all=""ignore""):
-> 1485     result = ops.arithmetic_op(lvalues, rvalues, op)
   1487 return self._construct_result(result, name=res_name, other=other)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:272, in arithmetic_op(left, right, op)
    259 # NB: We assume that extract_array and ensure_wrapped_if_datetimelike
    260 #  have already been called on `left` and `right`,
    261 #  and `maybe_prepare_scalar_for_op` has already been called on `right`
    262 # We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy
    263 # casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)
    265 if (
    266     should_extension_dispatch(left, right)
    267     or isinstance(right, (Timedelta, BaseOffset, Timestamp))
   (...)    270     # Timedelta/Timestamp and other custom scalars are included in the check
    271     # because numexpr will fail on it, see GH#31457
--> 272     res_values = op(left, right)
    273 else:
    274     # TODO we should handle EAs consistently and move this check before the if/else
    275     # (https://github.com/pandas-dev/pandas/issues/41165)
    276     # error: Argument 2 to ""_bool_arith_check"" has incompatible type
    277     # ""Union[ExtensionArray, ndarray[Any, Any]]""; expected ""ndarray[Any, Any]""
    278     _bool_arith_check(op, left, right)  # type: ignore[arg-type]

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/ops/common.py:70, in _unpack_zerodim_and_defer.<locals>.new_method(self, other)
     66         return NotImplemented
     68 other = item_from_zerodim(other)
---> 70 return method(self, other)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/arraylike.py:245, in OpsMixin.__pow__(self, other)
    243 @unpack_zerodim_and_defer(""__pow__"")
    244 def __pow__(self, other):
--> 245     return self._arith_method(other, operator.pow)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py:1055, in ArrowExtensionArray._arith_method(self, other, op)
   1053     parr = result._pa_array
   1054     mask = pc.is_nan(parr).to_numpy()
-> 1055     arr = pc.replace_with_mask(parr, mask, pa.scalar(None, type=parr.type))
   1056     result = type(self)(arr)
   1057 return result

File ~/gh/dask/.venv/lib/python3.12/site-packages/pyarrow/compute.py:252, in _make_generic_wrapper.<locals>.wrapper(memory_pool, *args)
    250 if args and isinstance(args[0], Expression):
    251     return Expression._call(func_name, list(args))
--> 252 return func.call(args, None, memory_pool)

File ~/gh/dask/.venv/lib/python3.12/site-packages/pyarrow/_compute.pyx:399, in pyarrow._compute.Function.call()

File ~/gh/dask/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:155, in pyarrow.lib.pyarrow_internal_check_status()

File ~/gh/dask/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:92, in pyarrow.lib.check_status()

ArrowNotImplementedError: Function 'replace_with_mask' has no kernel matching input types (double, null, double)
```

### Expected Behavior

The pandas 2.x output:

```
0    <NA>
1    <NA>
dtype: double[pyarrow]
```

### Installed Versions

<details>

In [2]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : d8159471b1e6e9327274a1b98aca2b88c9b5e0b5
python                : 3.12.8
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:40 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6041
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2463.gd8159471b1
numpy                 : 2.4.0.dev0+git20250808.622f874
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : 2025.3.2
html5lib              : None
hypothesis            : 6.136.1
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyiceberg             : None
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : None
s3fs                  : 2025.7.0
scipy                 : None
sqlalchemy            : 2.0.41
tables                : None
tabulate              : None
xarray                : 2025.9.0
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Regression', 'Numeric Operations', 'good first issue', 'Arrow']",2025-09-30 18:38:50,2025-10-05 19:56:03,7,closed
62518,"BUG: Regression in `DataFrame.__setitem__` with DataFrame, Categorical  level of MultiIndex in pandas 3.x","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

meta = pd.DataFrame(columns=pd.MultiIndex.from_arrays([['a', 'a', 'z', 'z'], pd.Categorical([1, 2, 1, 2])]), dtype=object)
meta['z'] = meta['z'].astype('int64')
```

### Issue Description

The above script runs fine with pandas==2.3.3

```
❯ uv run --python=3.13 --with pandas python debug.py  # no errors
```

But with pandas nightly, it errors

```
❯ uv run --python=3.13 --extra-index-url=https://pypi.anaconda.org/scientific-python-nightly-wheels/simple --prerelease=allow --with ""pandas>=3.0.0.dev0"" python debug.py
```

with

```pytb
Traceback (most recent call last):
  File ""/Users/toaugspurger/gh/debug.py"", line 4, in <module>
    meta['z'] = meta['z'].astype('int64')
    ~~~~^^^^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/UTaM244H3ORq6tmIZVRZk/lib/python3.13/site-packages/pandas/core/frame.py"", line 4322, in __setitem__
    self._set_item_frame_value(key, value)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/UTaM244H3ORq6tmIZVRZk/lib/python3.13/site-packages/pandas/core/frame.py"", line 4457, in _set_item_frame_value
    and not cols_droplevel.any()
            ~~~~~~~~~~~~~~~~~~^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/UTaM244H3ORq6tmIZVRZk/lib/python3.13/site-packages/pandas/core/indexes/base.py"", line 7285, in any
    return vals._reduce(""any"")
           ~~~~~~~~~~~~^^^^^^^
  File ""/Users/toaugspurger/.cache/uv/archive-v0/UTaM244H3ORq6tmIZVRZk/lib/python3.13/site-packages/pandas/core/arrays/categorical.py"", line 2425, in _reduce
    result = super()._reduce(name, skipna=skipna, keepdims=keepdims, **kwargs)
  File ""/Users/toaugspurger/.cache/uv/archive-v0/UTaM244H3ORq6tmIZVRZk/lib/python3.13/site-packages/pandas/core/arrays/base.py"", line 2225, in _reduce
    raise TypeError(
    ...<2 lines>...
    )
TypeError: 'Categorical' with dtype category does not support operation 'any'

```

### Expected Behavior

Ehh I don't know, this is some odd code. In https://github.com/dask/dask/blob/3fb02e610cf2baf3ee570888e5e49159ee60c0e5/dask/dataframe/dask_expr/_reductions.py#L694-L701, dask is attempting to create an empty dataframe with certain dtypes. I can try to update that to workaround this if needed.

### Installed Versions

<details>

3.0.0.dev0+2463.gd8159471b1

</details>
","['Bug', 'Regression', 'MultiIndex']",2025-09-30 16:51:26,2025-10-02 16:34:52,3,closed
62517,"BUG: `DataFrame.add(Series, axis=0)` mis-broadcasts with numpy 2.3 on wide DataFrames","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from numpy.testing import assert_array_equal
import numpy as np
import pandas as pd

N = 1500
M = 1000
df = pd.DataFrame(np.ones((N, M)))
s = pd.Series(np.linspace(1, N, N))
res = df.add(s, axis=0)

for i in range(M):
    assert_array_equal(res.iloc[:, i], np.arange(2, N+2), err_msg=f'column {i} failed: {res.iloc[:, i]}')
```

### Issue Description

Arithmetic (e.g. `df.add(s, axis=0)` or `df.div(s, axis=0)`) between a `pd.DataFrame` and a row-aligned `pd.Series` using `axis=0` appears to mis-broadcast when numpy==2.3.x and when the frame has many columns (high `M` in my example). 

Early columns are correct (the first assertions in my example typically work fine); later columns contain incorrect values (mostly zeros for me in my example, but on the way to a minimal example I've had all kinds of garbage). 

Note:
* The same code works fine on numpy==2.2.x.
* Small values like `M=10` work fine.
* Bug seems to only happen for `N > M`.

### Expected Behavior

* The `res` dataframe from the example should be a dataframe with 1000 exactly matching columns, i.e. each column should contain the range from 2 to 1501.

* All assertions should pass.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 9c8bc3e55188c8aff37207a74f1dd144980b8874
python                : 3.12.1
python-bits           : 64
OS                    : Linux
OS-release            : 6.12.44-3-MANJARO
Version               : #1 SMP PREEMPT_DYNAMIC Mon, 01 Sep 2025 23:07:10 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.3.3
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.4
sphinx                : 8.2.3
IPython               : 9.5.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.0
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.6
numba                 : None
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.2
sqlalchemy            : None
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.7.1
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Numeric Operations', 'Needs Info']",2025-09-30 16:41:16,2025-09-30 18:31:53,2,closed
62513,FutureWarning: DataFrameGroupBy.apply does not have a clear solution.,"IDK how pandas will classify this issue.

```python
df = df.sort_values(by=['oeisID', 'n', 'boundary'], ascending=[True, True, False])
def addColumnsGrowing(groupBy: pandas.DataFrame) -> pandas.DataFrame:
    groupBy['bucketsGrowing'] = groupBy['buckets'].diff().gt(0).fillna(True)
    groupBy['arcCodesGrowing'] = groupBy['arcCodes'].diff().gt(0).fillna(True)
    return groupBy
df = df.groupby(['oeisID', 'n'], group_keys=False).apply(addColumnsGrowing)
```

> C:\Users\hunte\AppData\Local\Temp\ipykernel_25960\1489258679.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  df = df.groupby(['oeisID', 'n'], group_keys=False).apply(addColumnsGrowing)


""... pass `include_groups=False`"" to what method? 

""... or explicitly select the grouping columns."" Is that a method, a parameter, or a topic in the documentation?

```python
df = df.sort_values(by=['oeisID', 'n', 'boundary'], ascending=[True, True, False])
def addColumnsGrowing(groupBy: pandas.DataFrame) -> pandas.DataFrame:
    groupBy['bucketsGrowing'] = groupBy['buckets'].diff().gt(0).fillna(True)
    groupBy['arcCodesGrowing'] = groupBy['arcCodes'].diff().gt(0).fillna(True)
    return groupBy
df = df.groupby(['oeisID', 'n'], group_keys=False, include_groups=False).apply(addColumnsGrowing)
```

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[6], line 6
      4     groupBy['arcCodesGrowing'] = groupBy['arcCodes'].diff().gt(0).fillna(True)
      5     return groupBy
----> 6 df = df.groupby(['oeisID', 'n'], group_keys=False, include_groups=False).apply(addColumnsGrowing)

TypeError: DataFrame.groupby() got an unexpected keyword argument 'include_groups'
```
Not `groupby`.

```python
df = df.sort_values(by=['oeisID', 'n', 'boundary'], ascending=[True, True, False])
def addColumnsGrowing(groupBy: pandas.DataFrame) -> pandas.DataFrame:
    groupBy['bucketsGrowing'] = groupBy['buckets'].diff().gt(0).fillna(True)
    groupBy['arcCodesGrowing'] = groupBy['arcCodes'].diff().gt(0).fillna(True)
    return groupBy
df = df.groupby(['oeisID', 'n'], group_keys=False).apply(addColumnsGrowing, include_groups=False)
```

`apply` can't be right because I am getting diagnostic errors.

```json
[{
	""resource"": ""/c:/apps/mapFolding/mapFolding/reference/matrixMeandersAnalysis/buckets.ipynb"",
	""owner"": ""pylance12"",
	""code"": {
		""value"": ""reportCallIssue"",
		""target"": {
			""$mid"": 1,
			""path"": ""/microsoft/pylance-release/blob/main/docs/diagnostics/reportCallIssue.md"",
			""scheme"": ""https"",
			""authority"": ""github.com""
		}
	},
	""severity"": 8,
	""message"": ""No parameter named \""include_groups\"""",
	""source"": ""Pylance"",
	""startLineNumber"": 6,
	""startColumn"": 77,
	""endLineNumber"": 6,
	""endColumn"": 91,
	""origin"": ""extHost1""
}]
```

But if I run the cell, it seems to work, and I don't get a `FutureWarning`.

# Partial solution

As I have tried to express many times to the Python community: the original linter was called a ""spell checker."" The same CI/CD concepts _and tools_ can easily be applied to the words-that-are-not-code, which would have many benefits, including fewer ""Issues"" from confused users.","['Groupby', 'Apply']",2025-09-30 00:24:45,2025-10-01 01:28:07,8,closed
62497,BUG: Merge_asof fails for unknow reason,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from io import StringIO

alloc_txt = """"""User_Id  Date  Allocated_Points  alloc_flag_day
1      466 2025-02-08                560            True
1      469 2025-02-08                560            True""""""

red_txt = """"""User_Id  Date  Deducted_Points  red_flag_day
1         466 2025-02-08              560          True
2         728 2025-02-02              750          True
3         728 2025-02-09              540          True
4         829 2025-04-03              800          True
5         869 2025-04-22             1400          True""""""
# Read as if reading a CSV/TSV file
alloc_df = pd.read_csv(StringIO(alloc_txt), delim_whitespace=True)
red_df = pd.read_csv(StringIO(red_txt), delim_whitespace=True)



alloc_df['User_Id'] = alloc_df['User_Id'].astype(int)
red_df['User_Id'] = red_df['User_Id'].astype(int)

# Ensure date is datetime and remove any possible NaT
alloc_df = alloc_df[alloc_df['Date'].notnull()]
red_df = red_df[red_df['Date'].notnull()]
alloc_df['Date'] = pd.to_datetime(alloc_df['Date'])
red_df['Date'] = pd.to_datetime(red_df['Date'])
try:
    one_result = pd.merge_asof(
        alloc_df,
        red_df,
        by='User_Id',
        left_on='Date',
        right_on='Date',
        direction='forward',
        tolerance=pd.Timedelta(days=3),
        suffixes=('_alloc', '_red')
    )
    print(""Success!"", one_result)
except Exception as ex:
    print(""Fails for first row alone:"", ex)
```

### Issue Description

I am trying to find the users who have redeemed some points within 3 days of allocation. This is a sample data.
Not sure why this fails.

### Expected Behavior

Ideally it should return the users who have entries in red_df within 3 days of alloc_df

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.12.9
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.3.2
numpy                 : 2.2.5
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.40
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : 2.4.1
pyqt5                 : None
</details>
","['Docs', 'Reshaping']",2025-09-29 07:56:24,2025-10-27 20:51:13,7,closed
62492,BUG: `pd.read_html()` is broken with beautifulsoup4 4.14.0,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
DF = pd.DataFrame({""a"": [1, 2, 3], ""b"": [0.0, 0.0, 0.0]})
DF.to_html(""foo.html"")
pd.read_html(""foo.html"", flavor=[""bs4""])
```

### Issue Description

The above code works fine with `beautifulsoup4` version 4.13.5.
It breaks with `beautiifulsoup4` version 4.14.0 that was released on 9/27/25

So we either need to pin `beautifulsoup4` or fix the bug.


### Expected Behavior

No bug with pandas 2.3.2

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.11.13
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.3.2
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.14.0
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : 3.10.6
numba                 : None
numexpr               : 2.13.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : 1.3.1
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.16.2
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.9.0
xlrd                  : 2.0.2
xlsxwriter            : 3.2.9
zstandard             : 0.24.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO HTML', 'Needs Tests']",2025-09-29 00:42:28,2025-09-30 21:09:34,12,closed
62490,BUG: rolling().first() and rolling().last() don't work,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

#According to documentation, this should happen
s = pd.Series(range(5))
s.rolling(3).last()
0         NaN
```

### Issue Description

What actually happens:

(Pdb) !s
0    0
1    1
2    2
3    3
4    4
dtype: int64
(Pdb) !s.rolling(3).last()
*** AttributeError: 'Rolling' object has no attribute 'last'
(Pdb) !s.rolling(3).first()
*** AttributeError: 'Rolling' object has no attribute 'first'
(Pdb) 

(Pdb) pd.__version__
'2.2.1'

### Expected Behavior

https://pandas.pydata.org/docs/dev/reference/api/pandas.core.window.rolling.Rolling.last.html

>>> s = pd.Series(range(5))
>>> s.rolling(3).last()
0         NaN
1         NaN
2         2.0
3         3.0
4         4.0
dtype: float64

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Info', 'Window']",2025-09-28 21:04:59,2025-09-28 22:08:06,3,closed
62488,PEP8 WIP,optimization to PEP8,[],2025-09-28 18:04:51,2025-09-28 19:22:38,0,closed
62472,BUG: FloatingArray.__setitem__ accepts FloatingArray as key,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

arr = pd.array([0, pd.NA, 1], dtype=""Float64"")
key = arr[[1]]
arr[key] = np.nan  # <- should raise, instead is no-op
```

### Issue Description

This is an invalid key, should raise.

### Expected Behavior

N/A

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2025-09-27 01:07:21,2025-09-27 01:08:27,0,closed
62467,BUG: flex op with ExtensionArray and fill_value,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
left = pd.Series([pd.Timestamp(2025, 8, 20)] * 2)

right = left._values

left.sub(right, fill_value=left[0])  # <- raises
```

### Issue Description

In Series._flex_method we have a check for `elif isinstance(other, (np.ndarray, list, tuple)):` that should include `ExtensionArray`

### Expected Behavior

N/A

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Numeric Operations']",2025-09-26 18:13:46,2025-10-27 17:52:57,1,closed
62462,BUG: cannot use index to set values in `.iloc` for `Int64[pyarrow]` Series,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [1]: import pandas as pd

In [2]: s = pd.Series([1,2,3], dtype='Int64[pyarrow]')

In [3]: s.iloc[pd.Index([0, 1])] = pd.Series([7, 8], dtype='Int64[pyarrow]')

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[3], line 1
----> 1 s.iloc[pd.Series([0, 1])] = pd.Series([7, 8], dtype='Int64[pyarrow]')

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:943, in _LocationIndexer.__setitem__(self, key, value)
    938 self._has_valid_setitem_indexer(key)
    940 iloc: _iLocIndexer = (
    941     cast(""_iLocIndexer"", self) if self.name == ""iloc"" else self.obj.iloc
    942 )
--> 943 iloc._setitem_with_indexer(indexer, value, self.name)

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:1969, in _iLocIndexer._setitem_with_indexer(self, indexer, value, name)
   1967     self._setitem_with_indexer_split_path(indexer, value, name)
   1968 else:
-> 1969     self._setitem_single_block(indexer, value, name)

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:2233, in _iLocIndexer._setitem_single_block(self, indexer, value, name)
   2230     value = self._align_frame(indexer, value)._values
   2232 # actually do the set
-> 2233 self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:603, in BaseBlockManager.setitem(self, indexer, value)
    599     # No need to split if we either set all columns or on a single block
    600     # manager
    601     self = self.copy()
--> 603 return self.apply(""setitem"", indexer=indexer, value=value)

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:441, in BaseBlockManager.apply(self, f, align_keys, **kwargs)
    439         applied = b.apply(f, **kwargs)
    440     else:
--> 441         applied = getattr(b, f)(**kwargs)
    442     result_blocks = extend_blocks(applied, result_blocks)
    444 out = type(self).from_blocks(result_blocks, self.axes)

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:1659, in EABackedBlock.setitem(self, indexer, value)
   1656 check_setitem_lengths(indexer, value, values)
   1658 try:
-> 1659     values[indexer] = value
   1660 except (ValueError, TypeError):
   1661     if isinstance(self.dtype, IntervalDtype):
   1662         # see TestSetitemFloatIntervalWithIntIntervalValues

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py:2184, in ArrowExtensionArray.__setitem__(self, key, value)
   2181     key = key[0]
   2183 key = check_array_indexer(self, key)
-> 2184 value = self._maybe_convert_setitem_value(value)
   2186 if com.is_null_slice(key):
   2187     # fast path (GH50248)
   2188     data = self._if_else(True, value, self._pa_array)

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py:2408, in ArrowExtensionArray._maybe_convert_setitem_value(self, value)
   2406 """"""Maybe convert value to be pyarrow compatible.""""""
   2407 try:
-> 2408     value = self._box_pa(value, self._pa_array.type)
   2409 except pa.ArrowTypeError as err:
   2410     msg = f""Invalid value '{value!s}' for dtype '{self.dtype}'""

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py:514, in ArrowExtensionArray._box_pa(cls, value, pa_type)
    512 if isinstance(value, pa.Scalar) or not is_list_like(value):
    513     return cls._box_pa_scalar(value, pa_type)
--> 514 return cls._box_pa_array(value, pa_type)

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py:663, in ArrowExtensionArray._box_pa_array(cls, value, pa_type, copy)
    660     mask = is_pdna_or_none(arr_value)  # type: ignore[assignment]
    662 try:
--> 663     pa_array = pa.array(value, type=pa_type, mask=mask)
    664 except (pa.ArrowInvalid, pa.ArrowTypeError):
    665     # GH50430: let pyarrow infer type, then cast
    666     pa_array = pa.array(value, mask=mask)

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pyarrow/array.pxi:312, in pyarrow.lib.array()

File ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pyarrow/array.pxi:115, in pyarrow.lib._handle_arrow_array_protocol()

ValueError: Cannot specify a mask or a size when passing an object that is converted with the __arrow_array__ protocol.
```

### Issue Description

The above raises **on the latest nightly release**, whereas it used to work until yesterday's (it also works with the latest stable release)

This was spotted in Narwhals where it broke the nightly CI

### Expected Behavior

```python
In [4]: s
Out[4]:
0   7
1   8
2   3
dtype: int64[pyarrow]
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : e4ca40511c13cf845058066ce174d4e233840d92
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.87.2-microsoft-standard-WSL2
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 3.0.0.dev0+2449.ge4ca40511c
numpy                 : 2.4.0.dev0+git20250823.9b16cb3
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.1.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : 2025.9.0
html5lib              : None
hypothesis            : 6.138.14
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyiceberg             : None
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Indexing', 'Regression', 'Arrow']",2025-09-26 11:27:57,2025-10-05 20:38:45,2,closed
62443,BUG: Dataframe.aggregate has inconsistent behaviour for empty dataframe,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""A"": [""A""], ""B"": [""B""]})
# This works
df[""C""] = df.aggregate(""."".join, axis=""columns"")

empty_df = pd.DataFrame({""A"": [], ""B"": []})
# This raises an error as the return value is a dataframe not a series
empty_df[""C""] = empty_df.aggregate(""."".join, axis=""columns"")
```

### Issue Description

When using ``df.aggregate`` the return value is inconsistent/ incorrect for an empty dataframe. In this case, the same empty dataframe is returned instead of a single aggregated column, which fails if the result is supposed to be applied to a single column. 

### Expected Behavior

I would expect ``df.aggregate`` to return an empty Series for an empty dataframe as it would be more consistent and easier to use. Also the documentation of df.aggregate states:

> Returns:
> scalar, Series or DataFrame
> The return can be:
> scalar : when Series.agg is called with single function
> Series : when DataFrame.agg is called with a single function
> DataFrame : when DataFrame.agg is called with several functions

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.11.9
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 143 Stepping 8, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.3.2
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : 8.2.3
IPython               : 9.5.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.5
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.2
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.43
tables                : None
tabulate              : 0.9.0
xarray                : 2025.9.0
xlrd                  : None
xlsxwriter            : 3.2.9
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
","['Bug', 'Apply']",2025-09-25 06:56:41,2025-09-26 11:28:31,2,closed
62442,DEPR: passing `ax` to plotting methods,"IIUC (a big if), we allow users to plot multiple objects on the same figure by passing an `ax` argument to plotting methods.  I think this is part/all of the reason why we need all the statefulness in #54485.

What if we deprecate that argument, so all `obj.plot(...)` calls get a new `ax` object?  Would that break any use cases we really care about?  Would it allow us to simplify the code enough to make it actually maintainable?","['Visualization', 'Deprecate']",2025-09-25 01:42:40,2025-09-27 15:58:21,6,closed
62429,BUG: Dockerfile doesn't install pandas directly,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
N/A
```

### Issue Description

The Dockerfile doesn't contain any direct installation instruction of Pandas. The [CI logs](https://github.com/pandas-dev/pandas/actions/runs/17962963025/job/51089957269#step:4:841) shows that it is installed as a dependency of `fastparquet`.

> Collecting pandas>=1.5.0 (from fastparquet>=2024.11.0->-r /tmp/requirements-dev.txt (line 22))

### Expected Behavior

Pandas should be explicitly installed from PyPi during the Docker build, or it should be built from source. The latter seems to be the preferred option, as the objective of the Dockerfile is to [contribute to pandas](https://pandas.pydata.org/docs/development/contributing_environment.html#option-3-using-docker).

**Edit**: Another option is to extend the documentation, adding instructions to build it from source.

### Installed Versions

N/A
","['Build', 'Docs']",2025-09-24 11:29:04,2025-11-07 17:53:31,6,closed
62382,BUG: Automatic transformation from int to bool with `read_excel`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pathlib import Path

import pandas as pd

# Save this into a Excel sheet, adjust path if needed
#    1   True      c
#    a      b   True
# True      1      1
filex = Path(__file__).parent / ""intbool.xlsx""

# Read with Excel, getting:
#    0     1     2
# 0  1  True     c
# 1  a     b  True
# 2  1  True  True
dfx = pd.read_excel(filex, header=None, dtype=str)
print(dfx)
```

### Issue Description

Weird interaction between Python & Excel. When both integers and booleans of the same value are present on the same column (1/True or 0/False), `read_excel` will cast all these to the value that appears first in each series.

* Behaviour tested with Openpyxl & Calamine. 
* I expected that setting `dtype=str` would fix the issue, but it had no effect
* `read_csv` doesn't have this issue. The data would be read as strings
* writing Excel file from dataframe of strings will format excel content as string, thus preventing the issue to happen

I tracked the issue down to `sanitize_objects` coded in Cython. I believe the issue is from this piece of code:
* Iterations over the content of the column. `memo` stores the values already known.
* When current value `val` is in `memo`, reuse it.
* As `1 == True` is true, the first value of 1/True found in the series is used for all the upcoming 1/True. Similar behaviour with 0/False

 https://github.com/pandas-dev/pandas/blob/e87248e1a5d6d78a138039f2856a3aec6b9fef54/pandas/_libs/parsers.pyx#L2143-L2144

### Expected Behavior

I may not have the full picture, but i guess
* current behaviour is fine when there are no strings in the series
* when strings are found in the series, read everything as strings ? Not sure of this one
* setting `dtype=str` in `read_excel` should read the series as containing only `str`s, thus preventing this conversion

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.12.2
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United Kingdom.1252

pandas                : 2.3.2
numpy                 : 1.26.2
pytz                  : 2023.3.post1
dateutil              : 2.8.2
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.0.0
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : 2024.6.0
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Duplicate Report', 'IO Excel', 'Needs Triage']",2025-09-19 13:55:38,2025-09-19 14:38:18,1,closed
62377,ENH: add value_counts_with_normalization,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish i could write one function which will create view for normalizied data and non-normalized data while calliing thi function.

### Feature Description

i see this function in this way
```
def value_counts_with_normalization(self, subset=None,sort=True, ascending=False, dropna=True,percentage = False):
    ----
    non_normalized_df = self.value_counts(subset = subset
                 ,sort = True,ascending=ascending
                ,dropna = dropna, normalization = False).reset_index()
   
    normalized_df = self.value_counts(subset = subset,sort = True,ascending=ascending,dropna = dropna, normalization = False).reset_index()
    multiplier = 100 if percentage else 1
    normalized_df['proportion'] = normalized_df['proportion']*multiplier
return non_normalized_df.merge(normalized_df , on = subset)
```



### Alternative Solutions

I couln't find alternative solution

### Additional Context

Example of output 

<img width=""1622"" height=""846"" alt=""Image"" src=""https://github.com/user-attachments/assets/d8badfda-0dbf-449c-a6c8-67f481cbb957"" />","['Enhancement', 'API Design', 'Algos', 'Closing Candidate']",2025-09-18 20:45:00,2025-09-19 09:44:05,3,closed
62370,BUG: None in dtype=bool Series does not behave like 3-valued logic,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Default bool dtype
arr1 = pd.Series([True, None, False])
arr2 = pd.Series([False, True, None])

print(""OR:\n"", arr1 | arr2)
print(""AND:\n"", arr1 & arr2)
```

### Issue Description

The output recieved is :
OR:
0     True
1    False
2    False

AND:
0     True
1    False
2    False


### Expected Behavior

Expected output:
OR:
0     True
1     True
2     < NA >

AND:
0     True
1    False
2     < NA >

If we were to follow the kleene's 3 value logic/ principle

### Installed Versions

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : 7f670c17cd815a14167734abe5f3ad6e2b15d94d
python                : 3.11.13
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.139-0601139-generic
Version               : #202505202314 SMP PREEMPT_DYNAMIC Tue May 20 23:54:01 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.0.dev0+3321.g7f670c17cd
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.3
sphinx                : 8.1.3
IPython               : 9.5.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.5
bottleneck            : 1.5.0
fastparquet           : 2024.11.0
fsspec                : 2025.9.0
html5lib              : 1.1
hypothesis            : 6.138.14
gcsfs                 : 2025.9.0
jinja2                : 3.1.6
lxml.etree            : 6.0.1
matplotlib            : 3.10.6
numba                 : 0.61.2
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 21.0.0
pyiceberg             : 0.9.1
pyreadstat            : 1.3.1
pytest                : 8.4.2
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.9.0
scipy                 : 1.16.1
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.9.0
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.24.0
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Duplicate Report', 'Numeric Operations', 'Closing Candidate']",2025-09-18 06:49:57,2025-09-19 16:30:09,11,closed
62354,BUG: Imported floats from Stata do not round correctly,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas
statafile = pandas.read_stata(statafilepath)
statafile[""webphone""]
statafile[""webphone""].round(2).astype(float)
round(statafile[""webphone""],2).astype(float)
```

### Issue Description

When rounding numbers that have been brought in from stata, rounding does not work as expected. The identical number imported from a postgres table does not have the same issue. You can see the issue in the image below:

<img width=""643"" height=""325"" alt=""Image"" src=""https://github.com/user-attachments/assets/6b14bd1e-4a64-42bc-a5b7-83cd032f4b51"" />

### Expected Behavior

Rounding a float should round it.

### Installed Versions

<details>
>>> pandas.show_versions() 

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.12.1
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_Canada.1252

pandas                : 2.3.0
numpy                 : 2.3.0
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 23.2.1
Cython                : None
sphinx                : None
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.41
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO Stata', 'Needs Triage']",2025-09-16 22:05:24,2025-09-17 21:08:26,6,closed
62353,ENH: arithmetic between DatetimeArray / TimedeltaArray and list,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish Pandas could implement arithmetic operations for DatetimeArrays and Python native lists.

### Feature Description

I wish the following code could run.

```py
from datetime import datetime
import pandas as pd

arr = pd.to_datetime([""2020-01-01"", ""2020-01-02""]).array
assert isinstance(arr, pd.arrays.DatetimeArray)

arr - [datetime(2019, 12, 31), datetime(2020, 1, 1)]  # TypeError: unsupported operand type(s) for -: 'DatetimeArray' and 'list'
```

### Alternative Solutions

```py
from datetime import datetime
import pandas as pd

arr = pd.to_datetime([""2020-01-01"", ""2020-01-02""]).array
assert isinstance(arr, pd.arrays.DatetimeArray)

arr - pd.to_datetime([datetime(2019, 12, 31), datetime(2020, 1, 1)]).array
```

### Additional Context

_No response_

### Updates

- 22.09.2025: Parallel problem also exists for TimedeltaArray","['Enhancement', 'Numeric Operations', 'Needs Triage']",2025-09-16 20:03:53,2025-11-26 19:23:50,13,closed
62344,BUG: cast_pointwise_result with BooleanDtype and all-NA values,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
arr = pd.array([True, False])

values = [pd.NA]

result = arr._cast_pointwise_result(values)

>>> result
array([<NA>], dtype=object)
```

### Issue Description

We'd expect this to retain BooleanDtype.

### Expected Behavior

NA

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Dtype Conversions', 'NA - MaskedArrays']",2025-09-15 15:12:00,2025-09-22 14:17:52,1,closed
62342,BUG: inconsistent comparison results when using PyArrow backend.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
for do_parq_roundtrip in [False, True]:
    for do_fillna in [False, True]:
        df = df_comparison.copy().reset_index(drop=True)

        print(""="" * 100)
        parq_str = ""with"" if do_parq_roundtrip else ""without""
        fillna_str = ""with"" if do_fillna else ""without""
        print(f""{parq_str} parquet roundtrip, {fillna_str} fillna"")
        print(""="" * 100)

        # Do parquet roundtrip
        if do_parq_roundtrip:
            df.to_parquet(""df.parquet"", index=True)
            df = pd.read_parquet(""df.parquet"")

        print(""Column datatypes:"")
        print(df.dtypes)
        print()

        # Detect mismatch
        if do_fillna:
            is_mismatch = df.col_a.fillna(2) != df.col_b.fillna(2)
        else:
            is_mismatch = df.col_a != df.col_b

        print(""Mismatch detected @:"")
        print(np.argwhere(is_mismatch.fillna(False)))
        print()

        df[""is_mismatch""] = is_mismatch

        # Print rows where there is a detected mismatch
        print(""Detected mismatch rows:"")
        print(df[df.is_mismatch])
        print(""="" * 100)
        print()
```

### Issue Description

I get inconsistent results when comparing two `int64[pyarrow]` columns, depending on whether I use `fillna` or first store the dataframe in parquet and read it again. 

The input data is the result of merging two dataframes resulting from `df = pd.read_sql(f""select * from some.Table"", some_connection, dtype_backend=""pyarrow"")`. I've redacted the results by selecting only certain columns and renaming them. The dataframe contains two columns (`col_a` and `col_b`) that are compared. These columns are of dtype `int64[pyarrow]` and contain either 0, 1, or NA. There is a third column `ManualIndex` which I've added to make sure nothing cheeky is happening with the index, but which is probably useless.

I've attached a .parquet and .feather export of the data. But keep in mind storing and then re-loading the data apparently has an effect on the output of the comparison, so loading this data and running the script probably gives a different output then the one I've posted below.

[df.zip](https://github.com/user-attachments/files/22336312/df.zip)

The output of the script gives me the following results:
```
====================================================================================================
without parquet roundtrip, without fillna
====================================================================================================
Column datatypes:
ManualIndex             int64
col_a          int64[pyarrow]
col_b          int64[pyarrow]
dtype: object

Mismatch detected @:
[[252518]
 [252519]]

Detected mismatch rows:
        ManualIndex  col_a  col_b is_mismatch
252518       252518      1   <NA>        <NA>
252519       252519      1   <NA>        <NA>
====================================================================================================

====================================================================================================
without parquet roundtrip, with fillna
====================================================================================================
Column datatypes:
ManualIndex             int64
col_a          int64[pyarrow]
col_b          int64[pyarrow]
dtype: object

Mismatch detected @:
[[252512]
 [252513]
 [252518]
 [252519]]

Detected mismatch rows:
        ManualIndex  col_a  col_b is_mismatch
252512       252512      1      1        True
252513       252513      1      1        True
252518       252518      1   <NA>        True
252519       252519      1   <NA>        True
====================================================================================================

====================================================================================================
with parquet roundtrip, without fillna
====================================================================================================
Column datatypes:
ManualIndex             int64
col_a          int64[pyarrow]
col_b          int64[pyarrow]
dtype: object

Mismatch detected @:
[]

Detected mismatch rows:
Empty DataFrame
Columns: [ManualIndex, col_a, col_b, is_mismatch]
Index: []
====================================================================================================

====================================================================================================
with parquet roundtrip, with fillna
====================================================================================================
Column datatypes:
ManualIndex             int64
col_a          int64[pyarrow]
col_b          int64[pyarrow]
dtype: object

Mismatch detected @:
[[252505]
 [252506]]

Detected mismatch rows:
        ManualIndex  col_a  col_b is_mismatch
252505       252505      1      1        True
252506       252506      1      1        True
====================================================================================================

```

As you can see the results are different for each combination of settings.

### Expected Behavior


My expectations:
- I expect the roundtrip to a parquet file and back to have no bearing on the comparison at all.
- I expect `is_mismatch` to be `True` only at `ManualIndex` `252518` and `252519`  when using `fillna(2)` before comparing.
- I expect `is_mismatch` to be `NA` at `ManualIndex` `252518` and `252519` when not using `fillna` before comparing.
- I expect `is_mismatch` to be `False` everywhere else.
- I expect that when `is_mismatch` is `NA` that it does not get found by `np.argwhere` or result in returned rows when using it to index into a dataframe. 

I don't understand how the rows where `col_a` and `col_b` are both `1` can ever result in them not being equal according to the comparison. When I filter on these rows and do the comparison manually suddenly the comparison evalues to them being equal.

In short I am very confused, am I doing something wrong here?


### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.13.4
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 106 Stepping 6, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_Netherlands.1252

pandas                : 2.3.2
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.2.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.42
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Arrow']",2025-09-15 12:21:25,2025-09-15 22:00:57,1,closed
62336,DOC: pandas.DataFrame.to_hdf missing data,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html

### Documentation problem

1. Section ""Returns"" is missing.
2. Section ""Raises"" is missing.

Said differently, in the HTML source, ""Parameters"" is a defined term, but ""Returns"" and ""Raises"" are not in the HTML source.

```html
  <dl class=""py method"">
    <dt class=""sig sig-object py"" id=""pandas.DataFrame.to_hdf"">
    ...
    <dd>
    ... 
      <dt class=""field-odd"">Parameters
...
```

### Suggested fix for documentation

The data for those sections probably exists, but it isn't being published by the automated process. I tried to figure out where the rst file lives, but I gave up after half an hour.","['Docs', 'Needs Triage']",2025-09-14 02:11:38,2025-09-14 23:10:57,5,closed
62330,RLS: 2.3.3,"2.3.3 milestone: https://github.com/pandas-dev/pandas/milestone/122

We have some additional fixes merged to 2.3.x, including a regression that was introduced in 2.3.2, so I think it would be good to do another 2.3.x release soon (somewhere next week?)

cc @pandas-dev/pandas-core",['Release'],2025-09-12 21:02:24,2025-09-30 18:17:38,6,closed
62314,DOC: Clarify parentheses vs. brackets,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/getting_started/intro_tutorials/index.html

### Documentation problem

I teach Python and pandas to new programmers, and one of the things my students consistently get tripped up on is when to use parentheses vs. square brackets, and why. (I'm a pandas contributor, and _I_ don't always have a good explanation about why something in pandas uses one vs. another.)

### Suggested fix for documentation

I wrote up the following:

https://python-public-policy.afeld.me/en/columbia/brackets.html

pandas documentation assumes a baseline level of Python knowledge (though doesn't say that explicitly…?), so probably doesn't make sense to copy that page wholesale. I propose adapting that page to a new page/section in the [Getting Started tutorials](https://pandas.pydata.org/docs/dev/getting_started/intro_tutorials/index.html).

---

Looking for a 👍 from another maintainer, then happy to review a pull request from whomever is interested in picking this up!","['Docs', 'good first issue']",2025-09-11 05:37:12,2025-09-19 16:44:12,4,closed
62310,BUG: Addition/subtracting `pd.Timedelta` constructed from hour offset gives wrong results,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from datetime import datetime, timedelta, timezone

import pandas as pd

dt = datetime(2025, 9, 10, 23, 0, 0, tzinfo=timezone.utc)

print(dt - pd.Timedelta(hours=1))  # 2025-09-10 22:00:00+00:00
print(dt - pd.Timedelta(pd.tseries.offsets.Hour(1)))  # 2025-09-10 23:00:00+00:00


exp_result = datetime(2025, 9, 10, 22, 0, 0, tzinfo=timezone.utc)

assert dt - timedelta(hours=1) == exp_result  # works
assert dt - pd.Timedelta(hours=1) == exp_result  # works
assert pd.Timedelta(pd.tseries.offsets.Hour(1)) == timedelta(hours=1)  # works
assert dt - pd.Timedelta(pd.tseries.offsets.Hour(1)) == exp_result  # assertion error
```

### Issue Description

Adding/subtracting pd.Timedelta constructed from an hour offset gives the wrong result.
It works as expected when constructing the pd.Timedelta object using e.g. `pd.Timedelta(hours=1)` instead of `pd.Timedelta(pd.tseries.offsets.Hour(1))`.

I initially noticed this constructing a pd.Timedelta from an datetimeindex freq, e.g. `pd.Timedelta(df.index.freq)`

### Expected Behavior

Adding/subtracting pd.Timedelta should give the same results whether the timedelta is constructed from an hour offset or using `hours={int}`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.0-29-generic
Version               : #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Aug 14 16:52:50 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.3.2
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Numeric Operations', 'Timedelta', 'Frequency']",2025-09-10 10:01:31,2025-09-12 16:13:34,18,closed
62307,BUG: `to_datetime` inconsistant between Series & DataFrame with timestamps,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""datecol"": [1, 2]})

# Passes with Series
pd.to_datetime(df[""datecol""])
# Fails with DataFrame
pd.to_datetime(df)
```

### Issue Description

Converting timestamps to datetime works with Series, but fails with DataFrame

Error with DataFrame:
```
Traceback (most recent call last):
  File ""C:\Users\...\main.py"", line 8, in <module>
    pd.to_datetime(df)
  File ""C:\Users\...\.venv\Lib\site-packages\pandas\core\tools\datetimes.py"", line 1075, in to_datetime
    result = _assemble_from_unit_mappings(arg, errors, utc)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\...\.venv\Lib\site-packages\pandas\core\tools\datetimes.py"", line 1191, in _assemble_from_unit_mappings
    raise ValueError(
ValueError: to assemble mappings requires at least that [year, month, day] be specified: [day,month,year] is missing
```

### Expected Behavior

`to_datetime` should have consistent behaviour between Series & DataFrame.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.12.10
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 2.3.2
numpy                 : 2.3.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime']",2025-09-10 08:39:54,2025-09-16 21:18:44,4,closed
62306,ENH: add support for .ceil() and .floor() for series/frame,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Currently ceil and floor is achieved by codes like:
```python
df = df.mask(df > 0, 0)
```
this can be slow in terms of performance compared to native methods like `np.ceil` and `np.floor`

### Feature Description

Add new `.ceil` and `.floor` method, possibly using `np.ceil` and `np.floor`

### Alternative Solutions

```python
df = pd.DataFrame(np.ceil(df.values, 0), index=df.index, columns=df.columns)
```

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-09-10 02:40:05,2025-09-10 02:41:29,0,closed
62287,ENH: Support `method` and `tolerance` kwargs in `Index.slice_indexer`,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could pass `method` and `tolerance` kwargs to `pandas.Index.slice_indexer`, as I can to `pandas.Index.get_indexer`.

### Feature Description

Add new parameters to `pandas.Index.slice_indexer`, so that it looks like

```python
class Index:
    def slice_indexer(start=None, end=None, step=None, method=None, tolerance=None):
        """"""
        Compute the slice indexer for input labels and step.

        Index needs to be ordered and unique.

        Parameters
        ----------
        start : label, default None
            If None, defaults to the beginning.
        end : label, default None
            If None, defaults to the end.
        step : int, default None
        method : {None, ‘pad’/’ffill’, ‘backfill’/’bfill’, ‘nearest’}, optional
            - default: exact matches only.
            - pad / ffill: find the PREVIOUS index value if no exact match.
            - backfill / bfill: use NEXT index value if no exact match
            - nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value.
        tolerance : optional 
            Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance.

            Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type.

        Returns
        -------
        slice
```

### Alternative Solutions

Currently I've effectively written a custom version of `.slice_indexer` that calls `.get_indexer` on the start and end bounds, but I'm worried that doing this downstream will miss edge cases compared to solving it upstream in pandas.

### Additional Context

This would be nice for use within the internals of xarray's `.sel()` methods. See https://github.com/pydata/xarray/issues/10710.","['Enhancement', 'Indexing', 'Needs Info']",2025-09-07 15:02:37,2025-09-16 21:42:16,4,closed
62277,BUG: DatetimeIndex.get_indexer(timestamp[pyarrow]),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
dti = pd.date_range(""2016-01-01"", periods=3)
arr = dti.astype(""timestamp[ns][pyarrow]"")

>>> dti.get_indexer(arr)
array([-1, -1, -1])
```

### Issue Description

i suspect the problem is in is_comparable_dtype.

Note this is the underlying issue behind #61231

### Expected Behavior

>>> dti.get_indexer(dti)
array([0, 1, 2])


### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Datetime', 'Arrow']",2025-09-06 16:04:20,2025-09-08 20:53:04,0,closed
62268,ENH: Make ExtensionDtype.numpy_dtype part of the interface?,"`pandas.ArrowDtype` and nullable types define `numpy_dtype` to express the analogous NumPy type. Should `ExtensionDtype.numpy_dtype` be part of the interface as well, defaulting to `np.dtype(object)`?","['Enhancement', 'Needs Discussion', 'ExtensionArray']",2025-09-06 00:35:31,2025-09-10 00:47:02,3,closed
62264,BUG: pd.json_normalize error when non-string keys are used in arguments,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pytest

sample_json = [
    {
        ""customer_id"": 1,
        12: 20,
        13: [{""a"": [1, 2,3]}, {""a"": [4, 5]}],
        14: [{1000: [1, 2,3]}, {1000: [4, 5]}],
        ""name"": ""Alice"",
        ""purchases"": [
            {
                ""purchase_id"": 301,
                ""date"": ""2024-06-01"",
                ""waffles"": [
                    {""waffle_id"": ""W1"", ""type"": ""Belgian"", ""flavor"": ""Vanilla"", ""quantity"": 2},
                    {""waffle_id"": ""W2"", ""type"": ""Chocolate"", ""flavor"": ""Chocolate"", ""quantity"": 1}
                ]
            },
            {
                ""purchase_id"": 302,
                ""date"": ""2024-06-10"",
                ""waffles"": [
                    {""waffle_id"": ""W3"", ""type"": ""Strawberry"", ""flavor"": ""Strawberry"", ""quantity"": 3}
                ]
            }
        ]
    }
]

@pytest.mark.parametrize(
    ""args"",
    [
        pytest.param({}, id=""no args (base case)""),
        pytest.param( dict(record_path=[13]), id=""int as record path""),
        pytest.param( dict(record_path=[13, ""a""]), id=""int and str as record path""),
        pytest.param( dict(record_path=[14, 1_000]), id=""int + int as record path""),
        pytest.param(dict(meta=[12]), id=""int as meta field""),
        pytest.param(dict(meta=[12, 13]), id=""int + int as meta field""),
        pytest.param(dict(meta=[12, ""name""]), id=""int + string as meta fields""),
        pytest.param(dict(meta=[""name"", 12]), id=""int + string as meta fields (2)""),

        pytest.param(dict(record_path=[""purchases""], meta=[12]), id=""int as meta & record path""),
        pytest.param(dict(record_path=[""purchases""], meta=[12, 13]), id=""int +int as meta & record path""),
        pytest.param(dict(record_path=[""purchases""], meta=[12, ""name""]), id=""int + string as meta fields & record path""),
        pytest.param(dict(record_path=[""purchases""], meta=[""name"", 12]), id=""int + string as meta fields & record path(2)""),

        pytest.param(

    ]
)
def test_normalize_df(args: dict, data: dict = sample_json):
    """"""""
    Load from nested object into a dataframe
    """"""
    df = pd.json_normalize(data, **args)
```

### Issue Description

Specifying an integer as a `meta` field while specifying a `record_path` results in TypeError.
```
        meta_vals: DefaultDict = defaultdict(list)
>       meta_keys = [sep.join(val) for val in _meta]
                     ^^^^^^^^^^^^^
E       TypeError: sequence item 0: expected str instance, int found
```

Details:
* This error only happens when the `record_path` argument is provided.
* No error with ints as keys in `record_path`
* Yes error when a single int is passed as `meta` arg, or when multiple fields are passes (and one of them is an int) 

### Expected Behavior

`pd.json_normalize` should use integers as keys in the `meta` argument.

### Installed Versions

<details>
>>> import pandas as pd
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.12.7
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:40 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8132
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.3.2
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.4.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'IO JSON']",2025-09-05 16:07:34,2025-09-10 01:30:03,4,closed
62260,BUG: Incorrect Future warning using a logical operation between two pyarrow boolean series,"### Reproducible Example

```python
import pandas as pd

index_2000 = pd.date_range(""2000-01-01"", periods=2, freq=""YS"")
index_2001 = pd.date_range(""2001-01-01"", periods=1, freq=""YS"")

df_2000 = pd.Series(index=index_2000, data=[True, False], dtype=""bool[pyarrow]"")

df_2001 = pd.Series(index=index_2001, data=[True], dtype=""bool[pyarrow]"")
union = df_2000 & df_2001
print(union)
```

### Issue Description

When using an operation like ``&`` or ``|`` between two boolean series with a different index and the pyarrow backend, I get the warning:
>FutureWarning: Operation between non boolean Series with different indexes will no longer return a boolean result in a future version. Cast both Series to object type to maintain the prior behavior.

However my series are of the dtype ``bool[pyarrow]``, so I would expect this to be ""boolean Series"" and not see a warning. 

I checked this on the main branch of pandas 3.0, and the result is indeed not a boolean series anymore, as I get 
```
2000-01-01     <NA>
2001-01-01    False
Freq: YS-JAN, dtype: bool[pyarrow]
```
returned.
If the two Series are of type ``bool`` (using numpy nullable backend), I don't get the warning and for pandas 3.0 I get 

```
2000-01-01    False
2001-01-01    False
Freq: YS-JAN, dtype: bool
```

### Expected Behavior

I would expect to get the same behaviour for Series of type ``bool[pyarrow]`` as I get for type ``bool``. 
So no warning about the input not being boolean series and for pandas 3.0 the same output as for numpy nullable boolean series.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.11.9
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 143 Stepping 8, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.3.2
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : 8.2.3
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.0
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.41
tables                : None
tabulate              : 0.9.0
xarray                : 2025.4.0
xlrd                  : None
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Numeric Operations', 'Arrow']",2025-09-05 09:59:20,2025-09-23 02:02:50,15,closed
62259,ERR: improve error message when specifying a previously-deprecated frequency alias,"For example, in `pd.date_range`, we deprecated some frequency aliases. With pandas 2.3:

```
>>> pd.date_range(""2012-01-01"", periods=3, freq='H')
FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.
DatetimeIndex(['2012-01-01 00:00:00', '2012-01-01 01:00:00',
               '2012-01-01 02:00:00'],
              dtype='datetime64[ns]', freq='h')
```

and now with main (pandas 3.0) the deprecation is enforeced and we raise an error:

```
>>> pd.date_range(""2012-01-01"", periods=3, freq='H')
... 
ValueError: Invalid frequency: H, failed to parse with error message: ValueError(""Invalid frequency: H, failed to parse with error message: KeyError('H')"")
```

However, I think that is not the best error message .. 
First, it's already a bit confusing that it contains the message twice. 
But secondly, I think it would also be nice to still keep a mapping of those aliases around, so that when parsing fails and the input is an old alias, we can provide a more informative error message (something like ""invalid frequency 'H'. Did you mean 'h'?"")","['Error Reporting', 'Frequency']",2025-09-05 09:25:46,2025-10-05 20:12:23,2,closed
62244,PERF: `DataFrame.unstack()` and `DataFrame.pivot_table()` upcasting take up more memory than needed,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

In this example, a `bool` dtype series will be ""unstacked"" by `DataFrame.unstack()` as `object` given that `bool` dtype does not accept `NaN` values. A cast to `boolean` instead would be preferable to save memory.

Additionally, `DataFrame.pivot_table()` results in a `float64` dtype which is different from `DataFrame.unstack()` for the same operation.


```python
import pandas as pd

df = pd.DataFrame(
    {
        ""level_0"": [""foo"", ""toto""],
        ""level_1"": [""A"", ""B""],
        ""values"": [True, False],
    }
)

multiindex_df = df.set_index([""level_0"", ""level_1""])

# Unstack the first level of the index
unstacked_df = multiindex_df.unstack(""level_0"")
pivoted_df = df.pivot_table(index=""level_1"", columns=""level_0"")

# unstacked_df - object dtypes
#-------------
#         values       
# level_0    foo   toto
# level_1              
# A         True    NaN
# B          NaN  False

# pivoted_df - float dtypes
#-----------
#         values     
# level_0    foo toto
# level_1            
# A          1.0  NaN
# B          NaN  0.0

# unstack results in a object dtype while pivot_table results in a float dtype.
print(f""unstacked_df takes up {unstacked_df.memory_usage(deep=True).sum()} bytes and has dtypes: \n {unstacked_df.dtypes} \n"")
print(f""pivoted_df takes up {pivoted_df.memory_usage(deep=True).sum()} bytes and has dtypes: \n {pivoted_df.dtypes}"")

# unstacked_df takes up 252 bytes and has dtypes: 
#          level_0
# values  foo        object
#         toto       object
# dtype: object 

# pivoted_df takes up 148 bytes and has dtypes: 
#          level_0
# values  foo        float64
#         toto       float64
# dtype: object

```

If we cast the original `""value""` column to `boolean`, this dtype will be kept with `unstack` but not with `pivot_table`:

```python
multiindex_df = multiindex_df.astype(""boolean"")
unstacked_df = multiindex_df.unstack(""level_0"")
print(f""unstacked_df takes up {unstacked_df.memory_usage(deep=True).sum()} bytes and has dtypes: \n {unstacked_df.dtypes} \n"")

# unstacked_df takes up 124 bytes and has dtypes: 
#          level_0
# values  foo        boolean
#         toto       boolean
# dtype: object 


df[""values""] = df[""values""].astype(""boolean"")
pivoted_df = df.pivot_table(index=""level_1"", columns=""level_0"")
print(f""pivoted_df takes up {pivoted_df.memory_usage(deep=True).sum()} bytes and has dtypes: \n {pivoted_df.dtypes}"")

# pivoted_df takes up 152 bytes and has dtypes: 
#          level_0
# values  foo        Float64
#         toto       Float64
# dtype: object
```

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : b9da66298eff52aa7f6b81f2415153b34149288a
python                : 3.11.8
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : AMD64 Family 25 Model 80 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : fr_FR.cp1252

pandas                : 3.0.0.dev0+2352.gb9da66298e
numpy                 : 2.2.4
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : 9.1.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None


</details>


### Prior Performance

_No response_","['Performance', 'Nullable by default']",2025-09-03 08:43:52,2025-09-10 01:40:57,10,closed
62243,Where are the docs for idiots like me?,"```python
c:\apps\mapFolding\mapFolding\algorithms\mmPandas.py:48: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df[""col""][row_indexer] = value

Use `df.loc[row_indexer, ""col""] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  dataframeCurveLocations['analyzed'] = 0
```

Ok, `df.loc[row_indexer, ""col""] = values`. I can do that. I don't have a row indexer, so it's easy: `dataframeCurveLocations.loc['analyzed'] = 0`.

```python
...
TypeError: Cannot perform 'and_' with a dtyped [float64] array and scalar of type [bool]
```

I don't have float or scalar bool in the module, so I got the wrong cow.

I'll read the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy) in the error message.

> ### Compare these two access methods:

> ```python
> dfmi['one']['second']
> Out[384]: 
> 0    b
> 1    f
> 2    j
> 3    n
> Name: second, dtype: object
> ```
> 
> ```python
> dfmi.loc[:, ('one', 'second')]
> Out[385]: 
> 0    b
> 1    f
> 2    j
> 3    n
> Name: (one, second), dtype: object 
> ```

I don't know what to compare between the two. They look the same to me.

> These both yield the same results...

So they are the same? 

> ## Why does assignment fail when using chained indexing?

I don't even know what chained indexing means. I thought the topic was ""Returning a view versus a copy."" In `dataframeCurveLocations['analyzed'] = 0`, how does 0 have a view or a copy?

<img width=""862"" height=""224"" alt=""Image"" src=""https://github.com/user-attachments/assets/8098536e-7375-4b78-8499-3b1182681b15"" />

Nah, man. If I were in the mood to be abused and told that if I really needed help, I would have done things the right way, I would call one of my parents: I don't need StackOverflow for that.

<img src=""https://stackexchange.com/users/flair/5539048.png"" width=""208"" height=""58"" alt=""profile for hunterhogan on Stack Exchange, a network of free, community-driven Q&amp;A sites"" title=""profile for hunterhogan on Stack Exchange, a network of free, community-driven Q&amp;A sites"" />

So, where are the stupid-people docs? After three days of trying, I don't understand. Copilot in VS Code doesn't understand: Claude Sonnet 4, GPT-5 (Preview), MCP context7, weblinks to pandas.pydata.org. I don't get it. 

I don't want help fixing this warning. I have tons of problems, so I need the dumbed-down docs.",[],2025-09-03 05:09:40,2025-09-03 18:47:30,1,closed
62240,BUG: str.match and str.contains behave inconsistently on compiled regexps (regression in 2.3.2 relative to 2.3.1),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import re
import pandas as pd

rex = re.compile(""foo"",flags=re.IGNORECASE)
l = [""Foo"",""foo"",""Bar"",""_Foo_"",""_foo_""]
s = pd.Series(l, index=l)
pd.DataFrame({
    ""Python Match"":[bool(rex.match(x)) for x in l],
    ""Python Search"":[bool(rex.search(x)) for x in l],
    ""Match Flat"":s.str.match(rex),
    ""Match Case"":s.str.match(rex,case=False),
    ""Contains Flat"":s.str.contains(rex),
    # ""Contains Case"":s.str.contains(rex,case=False),
}, index=l)
```

### Issue Description

(0) if you uncomment the last line (`Contains Case`), you get an error
> ValueError: cannot process flags argument with a compiled pattern

this looks like a bug in its own right, but I am not filing a separate issue because I think both bugs reside in the same code and a single patch will fix both.

(1) the code supplied returns
```
       Python Match  Python Search  Match Flat  Match Case  Contains Flat
Foo            True           True       False        True           True
foo            True           True        True        True           True
Bar           False          False       False       False          False
_Foo_         False           True       False       False           True
_foo_         False           True       False       False           True
```
Since I already passed `re.IGNORECASE` to `re.compile`, I expected that `Python Match`, `Match Flat` and `Match Case` to be identical.
This is not the case.
Note that `Contains Flat` and `Python Search` _are_ identical (good!)

### Expected Behavior

I expected the columns `Python Match`, `Match Flat` and `Match Case` to be identical, because the _compiled_ regexp should override the `case` argument.

If you will argue that the `case` argument to `str.match`  overrides the `flags` argument to `re.compile`,
then the behavior of `str.contains` is inconsistent because there the default `case` does *not* override the `flags` argument to `re.compile`.

Thus, the default `case` behavior of `str.contains` and `str.match` are inconsistent with each other.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.13.5
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 85 Stepping 7, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.3.2
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.7.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.1
matplotlib            : 3.10.5
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.7.0
scipy                 : 1.16.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.2
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Strings', 'Needs Discussion', 'API - Consistency']",2025-09-02 18:42:16,2025-11-18 22:27:42,8,closed
62236,ENH: High-performance implementation function for appending rows to a dataframe,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I receive real-time financial data using a dataframe. While pandas 1.x allows appending rows via df.loc[new_index]=..., pandas 2 does not support this operation. Therefore, I  implemented a function to  enables row appending, and this function is highly efficient.



### Feature Description

append row to dataframe 

### Alternative Solutions


```python
def df_append_row(df: 'pd.DataFrame', row_index: 'pd.Timestamp|pd.NaT|int|None' = None, extend_size=1_000, *, ffill: bool = False) -> None:
    mgr = df._mgr
    df_len = len(df)
    
    for blk in mgr.blocks:
        # blk.values is data array
        arr: 'np.ndarray' = blk.values
        arr_shape = arr.shape
        cached_array: 'np.ndarray|None' = getattr(blk, ""mx_cache_d"", None)
        if cached_array is None or cached_array.shape[-1] <= arr_shape[-1]:
            new_shape = list(arr_shape)
            new_shape[-1] += extend_size
            cached_array = np.empty(new_shape, arr.dtype)
            cached_array[..., :arr_shape[-1]] = arr
            if arr.dtype.kind == 'f':  # if dtype is np.floatN: set to np.nan
                cached_array[..., arr_shape[-1]:] = np.nan
            setattr(blk, ""mx_cache_d"", cached_array)
        blk.values = cached_array[..., :arr_shape[-1] + 1]
        if ffill:
            blk.values[-1] = blk.values[-2]

    index = mgr.axes[1]
    if isinstance(index, pd.DatetimeIndex):
        cached_index: 'pd.DatetimeIndex' = getattr(mgr, ""mx_cache_i"", None)
        if cached_index is None or len(cached_index) <= df_len:
            cached_array = np.zeros(df_len + extend_size, dtype='datetime64[ns]')  
            cached_array[:df_len] = index._data._ndarray
            cached_index = pd.DatetimeIndex(data=cached_array, copy=False, name=index.name)  
            setattr(mgr, ""mx_cache_i"", cached_index)  
        # index._data._ndarray=cached_index[:len(index)+1] # _ndarray not changed
        # setattr(index._data,""_ndarray"",cached_index[:len(index)+1])

        new_index = cached_index[:df_len + 1]  # new_index = pd.DatetimeIndex(cached_index[:df_len + 1], copy=False, name=index.name)
        if row_index is None:
            row_index = _NaT_i64  
        elif isinstance(row_index, pd.Timestamp):
            row_index = row_index.to_datetime64().view(np.int64)
        new_index._data._ndarray[-1] = row_index
        mgr.axes[1] = new_index
    elif isinstance(index, pd.RangeIndex):  #
        r: 'range' = index._range
        index._range = range(r.start, r.stop + r.step, r.step)
        _ = getattr(index, ""_cache"", {}).pop(""_data"", None) 
    else:
        raise NotImplemented(f""df_append_row:The type of index ({type(index)}) is not supported. [DatetimeIndex/RangeIndex]"")

    df._reset_cache()
    df._clear_item_cache()
```

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-09-02 12:45:17,2025-09-02 16:27:27,2,closed
62233,BUG:,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
update all sheet of this file as per ""Example Data"" sheet and share with me.
```

### Issue Description

[](url)

### Expected Behavior

[](url)

### Installed Versions

[pivot-table-report-filter.xlsx](https://github.com/user-attachments/files/22091242/pivot-table-report-filter.xlsx)

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2025-09-02 09:26:05,2025-09-02 16:27:58,1,closed
62228,ENH: Add Support for Reading .conll Files in Pandas,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could use Pandas to directly read .conll files, a common format in NLP for tasks like named entity recognition and POS tagging, without writing custom parsers.

### Feature Description

Add a `pandas.read_conll` function to parse `.conll` files into a DataFrame. It should handle tab-separated data, blank lines for sentence boundaries, and optional comments (e.g., lines starting with #).

Proposed API. 
```
import pandas as pd
df = pd.read_conll('data.conll', columns=['token', 'pos', 'chunk', 'ner'], group_by_sentence=False)
```

Key Features
- Parse tab-separated .conll files into a DataFrame.
- Support custom column names.
- Optionally group by sentence with a sentence_id column.
- Skip comments and handle blank lines.
- Support common formats (e.g., CoNLL-2003, CoNLL-U).



### Alternative Solutions

- Use pandas.read_csv with sep='\t' and manually process blank lines/comments, but this is prone to errors.
- Use the conllu package to parse CoNLL-U files, but it requires additional dependencies and conversion to DataFrames.
- Write custom parsers, which is time consuming and not reusable.

### Additional Context

_No response_","['Enhancement', 'IO Data', 'IO Format Request']",2025-08-31 17:30:43,2025-09-01 12:36:54,1,closed
62213,BUG: Using unstacking with `sort=False` after grouping with `sort=False` incorrectly shuffles data,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

multi_index = pd.MultiIndex.from_tuples([
    (""A"", ""1""),
    (""A"", ""2""),
    (""B"", ""1""),
    (""B"", ""2""),
    (""C"", ""1""),
    (""C"", ""2""),
    (""A"", ""3""),
    (""B"", ""3""),
    (""C"", ""3""),
], names=['level_1', 'level_2'])

values = [f""{l1} {l2}"" for l1, l2 in multi_index]
series = pd.Series(values, index=multi_index, name='Example')

series.groupby(['level_1', 'level_2'], sort=False).sum().unstack('level_2', sort=False)
```

### Issue Description

The code above generates this output:
```
| level_1   | 1   | 2   | 3   |
|:----------|:----|:----|:----|
| A         | A 1 | A 2 | B 1 |
| B         | B 2 | C 1 | C 2 |
| C         | A 3 | B 3 | C 3 |
```
where the values are not placed correctly in their respective columns and indices. For example, the value `B 1` can only belong to the row `B` and column `1`, but here it appears in `A3`.

### Expected Behavior

The issue disappears if any of the `sort` flags is set to `True`. Namely, replacing the last line with any of these 3 lines returns the expected output:

* `series.groupby(['level_1', 'level_2'], sort=True).sum().unstack('level_2', sort=False)`.
* `series.groupby(['level_1', 'level_2'], sort=True).sum().unstack('level_2', sort=True)`.
* `series.groupby(['level_1', 'level_2'], sort=False).sum().unstack('level_2', sort=True)`.

The issue only seems to appear when both `groupby` and `unstack` have `sort=False`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.11.11
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:51 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_US.UTF-8

pandas                : 2.3.2
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.5
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.43
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.24.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Reshaping', 'Needs Info']",2025-08-29 07:58:26,2025-09-01 12:26:18,3,closed
62206,ENH: keep the type option for keys in groupby,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Sometimes, I have in my data fully null object columns. When I apply a groupby, for no acceptable reason, it's transformed into float. It makes no sense to change to groupby key types, especially when I didn't ask for it.
I edited : it appears with object, not string.
### Feature Description

A new parameter to groupby function : keep_key_types (or whatever the name) that force the groupby to keep the exact types.

### Alternative Solutions

N/A

### Additional Context

_No response_","['Bug', 'Groupby', 'Missing-data', 'Dtype Conversions', 'Duplicate Report']",2025-08-28 11:02:31,2025-09-01 12:46:55,9,closed
62205,BUG: `record_prefix` ignored when `record_path` is empty,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

_s = pd.Series([    {""k"": f""{i}"", ""m"": ""q""} for i in range(5)
])

pd.json_normalize(_s,record_prefix=""T"")
```

### Issue Description

The `record_prefix` is completely ignored and the resulting columns are note renamed.

```
   k  m
0  0  q
1  1  q
2  2  q
3  3  q
4  4  q
```

The issue is in [L517-L527](https://github.com/pandas-dev/pandas/blob/main/pandas/io/json/_normalize.py#L517-L527) where a short cut is taken for speed.
One option to fix this would be to pass the `record_prefix` as `prefix` to `nested_to_record`.

This is hidden in the documentation example by using a positional argument for the record path

```
    >>> data = {""A"": [1, 2]}
    >>> pd.json_normalize(data, ""A"", record_prefix=""Prefix."")
        Prefix.0
    0          1
    1          2
```


### Expected Behavior

The prefix should be applied (or an explicit warning be given)

```
  T.k T.m
0   0   q
1   1   q
2   2   q
3   3   q
4   4   q
```

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.13.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.0-28-generic
Version               : #28-Ubuntu SMP PREEMPT_DYNAMIC Wed Jul 23 12:05:14 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.1
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : 2024.11.0
fsspec                : 2025.5.1
html5lib              : 1.1
hypothesis            : 6.135.20
gcsfs                 : 2025.5.1
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.5
numba                 : None
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.29.1
psycopg2              : None
pymysql               : 1.4.6
pyarrow               : 21.0.0
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2025.5.1
scipy                 : 1.16.1
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.6.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : 2.4.3
pyqt5                 : None

</details>
","['Bug', 'IO JSON']",2025-08-28 09:37:49,2025-09-15 17:30:02,6,closed
62204,BUG: memory leak in to_json when converting DateTime values,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
while True:
    dr = pd.date_range(start='1/1/2019', end='1/2/2019', freq='s', tz='UTC')
    df = pd.DataFrame({'col1': [12.34]}, index=dr)
    df.reset_index().to_json(orient='values', date_format='iso')
```

### Issue Description

There appears to be a memory leak in Pandas `to_json()` when converting DateTime values.  When running the reproducer, the system's memory use will continuously increase.

### Expected Behavior

Memory use should be stable.

### Installed Versions

<details>
__
INSTALLED VERSIONS
------------------
commit                : 4665c10899bc413b639194f6fb8665a5c70f7db5
python                : 3.11.13
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.0-1012-aws
Version               : #12~24.04.1-Ubuntu SMP Fri Aug 15 00:16:05 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.2
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO JSON']",2025-08-27 19:09:25,2025-08-28 17:45:52,4,closed
62202,Bug report,A bug has been reported. Details to be provided by the user.,[],2025-08-27 16:50:41,2025-08-27 17:12:59,1,closed
62196,"BUG: divmod(pd.NA, s) returns single Series instead of tuple","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
print(divmod(pd.NA, pd.Series([1,2,3])))
print(divmod(pd.NA, pd.Series([1,2,3], dtype='Int64')))
```

### Issue Description

The output for the first one above is
```
(0    <NA>
1    <NA>
2    <NA>
dtype: object, 0    <NA>
1    <NA>
2    <NA>
dtype: object)
```
which looks correct

The output of the second one is
```
0    <NA>
1    <NA>
2    <NA>
dtype: Int64
```
which doesn't look correct, i'd have expected a tuple

### Expected Behavior

```
(0    <NA>
1    <NA>
2    <NA>
dtype: Int64, 0    <NA>
1    <NA>
2    <NA>
dtype: Int64)
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 57742901a0851b293e02b33454f65935acd0bb2e
python                : 3.11.12
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.87.2-microsoft-standard-WSL2
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2310.g57742901a0.dirty
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.3
sphinx                : 8.2.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
bottleneck            : 1.5.0
fastparquet           : 2024.11.0
fsspec                : 2025.7.0
html5lib              : 1.1
hypothesis            : 6.138.0
gcsfs                 : 2025.7.0
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.5
numba                 : 0.61.2
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 21.0.0
pyiceberg             : 0.9.1
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.7.0
scipy                 : 1.16.1
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.7.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Numeric Operations']",2025-08-26 16:16:19,2025-09-03 18:30:34,2,closed
62195,release wheel of v1.5.3 for python 3.12,"There exists no wheel for pandas 1.5.3 for python 3.12. This combination of pandas and python version is used by the Databricks Runtime [16.4](https://docs.databricks.com/aws/en/release-notes/runtime/16.4lts) (current LTS). When testing code meant for this runtime in a CI/CD environment, pandas needs to be built from source or cached somewhere. Building pandas from source regularly is cumbersome, as is setting up a custom cache for this. Is it possible to release a wheel of that version for python 3.12?",[],2025-08-26 15:28:10,2025-08-26 16:46:24,1,closed
62185,"DOC: NaT - ""alias of NaT""; NA - ""alias of <NA>""","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.NaT.html

### Documentation problem

> pandas.NaT
>
> alias of NaT

It's an alias of itself???

### Suggested fix for documentation

I'm not sure yet; I'll need to check the source code.","['Docs', 'Missing-data', 'good first issue']",2025-08-25 15:39:12,2025-09-30 19:27:33,8,closed
62184,ENH:  Add DataFrame versioning extension for Git-like version control,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could use pandas to **track changes and maintain version history of DataFrames during data analysis workflows with Git-like semantics**.

Currently, when working with DataFrames in data science projects, there's no built-in way to:
- Save snapshots of DataFrame states before risky transformations
- Roll back to previous versions when experiments fail  
- Create experimental branches for testing different feature engineering approaches
- Track what changes were made with commit messages and timestamps
- Compare different versions to see exactly what changed between transformations
- Export and import complete analysis history for reproducibility

This leads to verbose manual backup code (`df_backup = df.copy()`), lost work when transformations go wrong, and difficulty collaborating on data analysis where team members need to understand the evolution of datasets.


### Feature Description

I propose adding a **DataFrame versioning extension** that provides Git-like version control capabilities directly integrated into pandas DataFrames through the accessor pattern.

**Core API using pandas accessor:**
```python
import pandas as pd

# Create DataFrame and enable versioning
df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'salary': [50000, 60000, 70000]
})

# Enable versioning with optional delta storage for efficiency
df.enable_versioning(use_delta_storage=True)

# Make changes and commit with messages
df['salary'] = df['salary'] * 1.1
df.commit(""Increased salaries by 10%"")

# Oops, that was wrong - rollback!
df.rollback()  # DataFrame reverts to previous state

# View complete history with timestamps and messages
print(df.version.history())
```

**Advanced branching and merging:**
```python
# Create experimental branch
df.version.create_branch(""experiment"")
df.version.switch_branch(""experiment"")

# Make experimental changes
df['bonus'] = df['salary'] * 0.1
df.commit(""Added experimental bonus calculation"")

# Switch back to main and merge if satisfied
df.version.switch_branch(""main"")
df.version.merge_branch(""experiment"")
```

**Diff visualization and comparison:**
```python
# See what changed between versions
diff = df.version.diff_with(version=0)
diff.show()  # Formatted console output
diff.to_html(""analysis_changes.html"")  # Generate HTML report

# Get summary of changes
print(diff.summary())
```

**Export/Import for reproducibility:**
```python
# Export complete history for backup/sharing
df.version.export_history(""analysis_v1.pkl"")
df.version.export_history(""analysis_v1.json"")  # Multiple formats
df.version.export_history(""analysis_v1.pkl.gz"")  # With compression

# Import history later or on different machine
df.version.import_history(""analysis_v1.pkl"")

# Check storage efficiency
stats = df.version.get_storage_stats()
print(f""Space saved with delta storage: {stats['space_saved_percentage']:.1f}%"")
```

**Implementation details:**
- Uses `@register_dataframe_accessor(""version"")` for clean integration
- Efficient delta storage system (saves 20-80% memory vs full snapshots)
- Support for both accessor style (`df.version.commit()`) and direct methods (`df.commit()`)
- Thread-safe version management with configurable limits
- Multiple serialization formats (pickle, JSON, dill) with compression support
- Comprehensive error handling and validation


### Alternative Solutions

**Current workarounds:**
1. **Manual DataFrame copying**: 
   ```python
   df_v1 = df.copy()  # Memory intensive, no metadata
   df_v2 = df.copy()  # Becomes unmanageable quickly
   ```

2. **External file-based versioning**:
   ```python
   df.to_pickle(f'data_v{version}.pkl')  # Manual, error-prone
   ```

3. **Custom tracking dictionaries**:
   ```python
   versions = {'v1': df.copy(), 'v2': df.copy()}  # Memory explosion
   ```

**3rd party packages:**
- **DVC (Data Version Control)**: File-based, requires external commands, not integrated with DataFrame operations
- **MLflow**: Focused on ML experiments, not interactive data analysis workflows
- **Pachyderm**: Heavyweight, container-based, overkill for DataFrame versioning
- **Git LFS**: Requires external files, complex setup, no DataFrame-native operations

None provide the seamless, DataFrame-native experience with efficient memory usage that this extension would offer.


### Additional Context

**Complete working implementation available:**
I have developed a fully functional prototype demonstrating this capability with the following features:

- **Comprehensive test suite**: 67+ tests covering all functionality
- **Performance optimizations**: Delta storage reduces memory usage by 20-80%
- **Production-ready code**: Error handling, type hints, documentation
- **Multiple storage backends**: Full snapshots or efficient delta-only storage
- **Export formats**: Pickle, JSON, Dill with optional compression
- **Professional documentation**: Complete API reference and examples

**Repository structure:**
```
pandas-versioning/
├── pandas_versioning/
│   ├── core/
│   │   ├── pandas_extension.py      # Main accessor implementation
│   │   ├── version_manager.py       # Core versioning logic
│   │   ├── delta_storage.py         # Efficient delta storage
│   │   ├── diff_engine.py          # Change visualization
│   │   └── serializer.py           # Export/import functionality
├── tests/                          # Comprehensive test suite
├── examples/                       # Usage examples
└── docs/                          # Documentation
```

**Performance benchmarks** (from testing):
- Delta storage: 65% memory reduction on typical datasets
- Commit operations: <50ms for DataFrames up to 100K rows  
- Branch operations: O(1) time complexity
- Export/import: Supports datasets up to several GB

**Real-world use cases proven:**
- **Data Science Workflows**: Safe experimentation with feature engineering
- **Collaborative Analysis**: Team members can track and understand changes
- **Production Pipelines**: Automated rollback when data validation fails
- **Educational**: Clear demonstration of data transformation steps

**Integration approach:**
The implementation uses pandas' official extension mechanism and follows all pandas conventions:
- Follows pandas accessor patterns
- Maintains DataFrame immutability where appropriate  
- Compatible with pandas 1.3+ and Python 3.8+
- No breaking changes to existing pandas functionality
- Clean, documented API following pandas style

This feature would significantly enhance pandas' capabilities for interactive data analysis while maintaining full backward compatibility and following established pandas design patterns.","['Enhancement', 'Needs Triage']",2025-08-25 15:11:55,2025-08-25 17:54:51,1,closed
62167,1911902583,"> Nice, thanks! This is  https://github.com/pandas-dev/pandas/pull/57003 indeed. I looked for an open PR but didn't find it somehow ... 

 _Originally posted by @lesteve in [#57082](https://github.com/pandas-dev/pandas/issues/57082#issuecomment-)_",[],2025-08-21 23:00:13,2025-08-22 00:25:09,3,closed
62161,DOC: date_range/timedelta_range confusing 3 out of 4 arguments note,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.date_range.html#pandas.date_range
https://pandas.pydata.org/docs/dev/reference/api/pandas.timedelta_range.html

### Documentation problem

We are trying to improve the type hinting for those two functions in `pandas-stubs` repo.
Currently the note in both those functions mentions that we should specify 3 out of 4 arguments among `start`, `end`, `period`, `freq`.
However it seems like the usage (even in the examples) one can specify only 2 of the arguments like start and end. After looking at the code, it seems like `freq` even if none gets converted to `D`.

### Suggested fix for documentation

I believe we should specify two out of three `start`, `end`, `periods` and never all four together. Rephrasing from three out of four but if freq is omitted then ... would help clarify the goal.

Here would be a possible version:

```
Of the four parameters start, end, periods, and freq, between two and three must be specified. If freq is omitted,
 the resulting TimedeltaIndex will have periods linearly spaced elements between start and end (closed on both
 sides) using a day increment.
```","['Docs', 'Datetime', 'Timedelta']",2025-08-20 23:57:56,2025-09-10 03:44:09,3,closed
62157,BUG/API: comparison datetime64-vs-dates,"```python
ser = pd.Series([""2016-01-01""], dtype=""date32[pyarrow]"")
ser2 = ser.astype(""timestamp[ns][pyarrow]"")
ser3 = ser.astype(""datetime64[ns]"")

assert (ser3 != ser[0]).all()
assert (ser3 != ser).all()

assert (ser == ser3).all()  # <- uh-oh! inconsistent with reversed operation
assert (ser == ser2).all()  # inconsistent with the non-pyarrow behavior
assert (ser2 == ser).all()
```


Long ago we made a decision that datetime64 arrays (and Timestamp) were _not_ comparable to date objects, matching the stdlib behavior.  We should have that behavior for pyarrow dtypes too.
","['Bug', 'Datetime', 'Numeric Operations', 'datetime.date']",2025-08-20 22:02:13,2025-09-24 00:50:02,1,closed
62150,BUG: Inconsistent behaviour when merging reset-ed MultiIndex dataframe,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df = pd.DataFrame(data={(""column_1"", """"): [1,1], (""column_2"", """"): [2,2]})
df.index = pd.MultiIndex.from_arrays([[1,1], [""metadata_1"", ""metadata_2""]], names=[""index"", ""metadata""])

df2 = pd.DataFrame(data=[1,1], index=[1,1]).rename_axis(""index"", axis=0)
df2.columns = pd.MultiIndex.from_product([[""new_data""], [""""]])

df.reset_index().merge(df2.reset_index(), on=""index"")
# IndexError: Requested axis not found in manager

df.reset_index().merge(df2.reset_index(), on=[(""index"", """")])
# IndexError: Requested axis not found in manager

df.reset_index().columns
# MultiIndex([(   'index', ''),
#             ('metadata', ''),
#             ('column_1', ''),
#             ('column_2', '')],
#            )

df2.reset_index().columns
# MultiIndex([(   'index', ''),
#             ('new_data', '')],
#            )

## We have a workaround if we force df2 to start as a multi-index, then it works
df2 = pd.DataFrame(data=[1,1], index=[1,1]).rename_axis(""index"", axis=0)
df2.columns = pd.MultiIndex.from_product([[""new_data""], [""""]])
df2.index = pd.MultiIndex.from_arrays(
        [df2.index, [""dummy""] * len(df2)],
        names=[""index"", ""dummy_index""],
    )

# Merge works
df.reset_index().merge(df2.reset_index(), on=[(""index"", """")])

# Extra empty multi-index column
df2.reset_index().columns
# MultiIndex([(   'index', ''),
#            ('dummy_index', ''),
#            ('new_data', '')],
#           )
```

### Issue Description

I have a dataframe (main) with multi-indexed index and columns. I would like to do a merge with a second dataframe (side) that is single-index and single column.

I coerced (side) to give it the same levels in the columns, and do a pd.merge on (main).reset_index and side.reset_index.

The example works if you make (side) also a multi-index index.

Additionally, the first example above is fine on 2.2.3, but is broken on 2.3.1



### Expected Behavior

Usual merge behaviour. Not sure why the initial dataframe must both have multi-index indexes when we use .reset_index() on both and merge over a single column

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.12.11
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.93+
Version               : #1 SMP PREEMPT_DYNAMIC Fri Jun 27 09:03:39 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.1
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.36.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.2
html5lib              : None
hypothesis            : None
gcsfs                 : 2025.3.2
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.5
numba                 : 0.61.2
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : 2.0.42
tables                : None
tabulate              : 0.9.0
xarray                : 2025.1.2
xlrd                  : 2.0.1
xlsxwriter            : 3.2.3
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Reshaping', 'MultiIndex', 'Needs Tests']",2025-08-20 01:55:34,2025-12-05 17:43:28,3,closed
62148,RLS: 2.3.2,"2.3.2 milestone: https://github.com/pandas-dev/pandas/milestone/121

I could do a 2.3.2 release tomorrow, just with what is available right now (and we can always do a 2.3.3 release later if needed)

cc @pandas-dev/pandas-core ",['Release'],2025-08-19 20:40:14,2025-08-22 16:12:21,2,closed
62140,BUG: type error in `df.attrs.update()`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]})
df.attrs.update(name=""dummy"", foo=""bar"") # type error here
```

### Issue Description

`ty 0.0.1-alpha.18 (d697cc092 2025-08-14)` raises a type error:

> Argument to bound method `update` is incorrect: Expected `Mapping[str, Any]`, found `dict[Hashable, Any]`tyinvalid-argument-type


### Expected Behavior

changing to

```diff
- df.attrs.update(name=""dummy"", foo=""bar"")
+ df.attrs.update({""name"": ""dummy"", ""foo"": ""bar""})
```

resolves the type error. both work at run time and i'd expect both to type check fine

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.13.0
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:40 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6041
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.1
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.5
numba                 : None
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.4.0
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.1
sqlalchemy            : None
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.24.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Typing', 'metadata']",2025-08-18 09:31:00,2025-08-18 21:25:40,1,closed
62134,BUG: groupby.apply() drops _metadata from subclassed DataFrame,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas._testing as tm
import numpy as np

subdf = tm.SubclassedDataFrame(
    {""X"": [1, 1, 2, 2, 3], ""Y"": np.arange(0, 5), ""Z"": np.arange(10, 15)}
)
subdf.testattr = ""test""

# Calculate groupby-sum in two ways: one preserves metadata, one does not
expected = subdf.groupby(""X"").sum()
result = subdf.groupby(""X"").apply(np.sum, axis=0, include_groups=False)

# Both dataframes have equivalent content
tm.assert_frame_equal(result, expected)

print(expected.testattr)    # prints ""test""
print(result.testattr)      # raises AttributeError
```

### Issue Description

When extending the `pandas.DataFrame` by subclassing, most operations preserve the `_metadata` attributes. This is not the case for `groupby.apply()`, after which the `_metadata` fields are dropped. I think this is unintended behavior, because an equivalent call using a built-in groupby method (such as `groupby.mean()`) does preserve the fields. 

### Expected Behavior

```python
import pandas._testing as tm
import numpy as np

subdf = tm.SubclassedDataFrame(
    {""X"": [1, 1, 2, 2, 3], ""Y"": np.arange(0, 5), ""Z"": np.arange(10, 15)}
)
subdf.testattr = ""test""
result = subdf.groupby(""X"").apply(np.sum, axis=0, include_groups=False)

assert result.testattr == ""test""   # attribute should be preserved after groupby-apply
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.12.11
python-bits           : 64
OS                    : Darwin
OS-release            : 24.6.0
Version               : Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:51 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.1
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1
Cython                : 3.1.3
sphinx                : 8.1.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : 2024.11.0
fsspec                : 2025.7.0
html5lib              : 1.1
hypothesis            : 6.138.2
gcsfs                 : 2025.7.0
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.5
numba                 : 0.61.2
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 21.0.0
pyreadstat            : 1.3.1
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2025.7.0
scipy                 : 1.16.1
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.8.0
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Groupby', 'Apply', 'metadata', 'Subclassing']",2025-08-17 15:03:32,2025-09-22 17:29:28,1,closed
62121,BUG: Addition of __set_module__ breaks PyCharm PyDev debugger functionalities,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import sys

import pandas as pd

# Envo. must have ""pydev-pycharm"", install with:
# pip install pydevd-pycharm
from _pydevd_bundle.pydevd_user_type_renderers import UserTypeRenderer, parse_set_type_renderers_message
from _pydevd_bundle.pydevd_user_type_renderers_utils import try_get_type_renderer_for_var

# Payload extrated from PyCharm's PyDev runtime. Contains the payload provided by the IDE's Java side,
# built from a sample customizing. It contains two ""Custom Views"", one for builtins.object (working fine)
# and one for pandas.DataFrame (not working).
SAMPLE_TR_CONFIG_MESSAGE = """"""RENDERERS	9	builtins.object	builtins.object	object	C:/Program Files/JetBrains/PyCharm 2025.2.0.1/plugins/python-ce/helpers/typeshed/stdlib/builtins.pyi	1	0	""DBG:""+ type(self).__name__	1	0	9	pandas.DataFrame	pandas.DataFrame	pandas.core.frame.DataFrame	<PYTHON_PATH>/Lib/site-packages/pandas-stubs/core/frame.pyi	0	0	""DBG:""+ type(self).__name__	1	0""""""

def main() -> None:
    pass
    renderers = parse_set_type_renderers_message(SAMPLE_TR_CONFIG_MESSAGE)
    df = pd.DataFrame({""dmmy"": map(float,range(8))})
    obj = object()

    object_type_renderer: UserTypeRenderer = try_get_type_renderer_for_var(obj, renderers)
    dataframe_type_renderer: UserTypeRenderer = try_get_type_renderer_for_var(df, renderers)

    print(f'Type Renderer for builtins.object | {object_type_renderer}')
    print(f'Type Renderer for pandas.DataFrame | {dataframe_type_renderer}')

    # Should Output
    """"""
    Type Renderer for builtins.object | <_pydevd_bundle.pydevd_user_type_renderers.UserTypeRenderer object at 0x0000048DDD0B2950>
    Type Renderer for pandas.DataFrame | None
    """"""


if __name__ in {""__main__"", ""__mp_main__""}:
    sys.exit(main())
```

### Issue Description

As described at the referenced comment at ""ENH: 55178"": 

The addition of the decorator set_module for DataFrames and Series, as far as i managed to verify, seems to be causing issues in PyCharm's Type Renderers functionality. When a Custom Data View is created at the IDE, we need to inform the target type, and the settings engine will validate the input against stubs and save the full qualified name it gets from them! Since no such abreviation is in place at the stubs, it will record ""pandas.core.frame.DataFrame"" as the qualified name.

Then when JetBrains's PyDev fork, that is base for PyCharm debugger, search for Type Renderers customized for the DataFrame, it compares the abreviated ""qualname"" from runtime (basically f'{type(obj)module}.{type(obj)name}' that will make for ""andas.DataFrame"") with the one he recorded from the customizing based on the stubs (that will be ""pandas.core.frame.DataFrame"").

There's also some hard-codes for pandas.core.frame.DataFrame and pandas.core.frame.Series at a ""Table Provider"" routine, used to determine PyCharm's ""Data View"" tool engine for pandas's DataFrames and Series, but that i believe should be addressed over there?

Thanks!

### Expected Behavior

Considering the reproducible provided, ""pydevd-pycharm"" should provide a ""UserTypeRenderer"" object from the call performed to ""try_get_type_renderer_for_var"", since it was configured correctly at PyCharm's Type Rederers settings.

### Installed Versions

<details>

None

</details>

For some reason ""show_versions"" return None. I identified this issue while experimenting with pandas release provided at the [scientific nightly conda channel](https://anaconda.org/scientific-python-nightly-wheels/), where we have a pandas 3.0.0 release.",[],2025-08-15 09:36:01,2025-09-10 12:43:55,3,closed
62119,PERF: Importing pandas_parser lib takes 50MB of memory,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

Hello,
I was doing some memory profiling of an application that employs, among other libraries, Pandas. I have noticed it was using more than 50MB of memory just from imports, so I dug up and found that this line
`import pandas._libs.pandas_parser`
is the culprit.

Looking at the imported lib files they seem pretty small, so I wonder, what would be causing this memory blowup?
I have added some files for reproducibility. Machine and Python details below.

[test_pandas_import_mem.py](https://github.com/user-attachments/files/21792026/test_pandas_import_mem.py)

[mem_logs.txt](https://github.com/user-attachments/files/21792018/mem_logs.txt)

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.12.11
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United Kingdom.1252

pandas                : 2.3.1
numpy                 : 2.0.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : 3.1.2
sphinx                : None
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : 2.0.41
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.2
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>


### Prior Performance

_No response_","['Performance', 'Needs Triage']",2025-08-15 09:14:07,2025-08-16 14:15:53,5,closed
62107,`Documentation Improvement: Clarify setup steps and troubleshooting in CONTRIBUTING.md`,"While setting up Pandas for local development, I noticed the docs assume familiarity with certain tools and skip details that could benefit new contributors.

Observations:

The prerequisites section doesn’t specify the recommended Python version.

Instructions lack an example `pip install -e` . for editable installs.

A short “Troubleshooting” section for common issues (e.g., missing `Cython`, build failures) could prevent confusion.

Proposed Solution:

Update `CONTRIBUTING.md` to include:

Recommended Python version(s)

Example for editable install (`pip install -e .`)

Basic troubleshooting tips for errors during setup

Making these additions will streamline onboarding and reduce repetitive setup questions.

I’d be happy to create a PR for this, if it sounds helpful.","['Docs', 'Needs Triage']",2025-08-14 17:30:30,2025-08-16 14:27:22,1,closed
62106,Documentation Improvement: Clarify steps for setting up local development environment in CONTRIBUTING.md,"While setting up Pandas for local development, I noticed that the current documentation assumes prior familiarity with certain tools and skips a few details that might be useful for new contributors.

Observations:

The prerequisites section does not explicitly mention the recommended Python version.

The instructions could include an example pip install -e . command for editable installs.

A short troubleshooting section for common errors (e.g., missing cython dependency) could save time for first-time contributors.

Proposed Solution:

Update CONTRIBUTING.md to include:

Recommended Python version(s)

Example editable install command

Quick troubleshooting tips for common build/install issues

This would make onboarding smoother for new contributors and reduce repetitive setup questions.

I’d be happy to work on a PR for this if it sounds helpful.",[],2025-08-14 17:25:28,2025-08-14 19:51:50,2,closed
62100,BUG: `tz_localize(None)` incorrectly converts timestamps when using `pyarrow` dtypes,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

tz = ""Australia/Sydney""
start = pd.Timestamp.now(tz).normalize()
index = pd.date_range(start=start, periods=24, freq=""h"", tz=tz)

rng = np.random.default_rng(42)
values = rng.random(len(index))
df = pd.DataFrame({""value"": values}, index=index)
df.index.name = ""value_timestamp""
df = df.reset_index()
pa_df = df.convert_dtypes(dtype_backend='pyarrow')

expected = df['value_timestamp'].dt.tz_localize(None)
actual = pa_df['value_timestamp'].dt.tz_localize(None)
assert actual.equals(expected)
```

### Issue Description

`tz_localize(None)` should strip the timezone information and otherwise leave the timestamps unchanged. This is what happens with the normal dtype backend. With `pyarrow` dtypes the timestamps are incorrectly shifted by the timestamp offset.


### Expected Behavior

The `pyarrow` dtypes don't shift the timestamps and give the same result as the normal dtypes.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.11
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.0-1010-aws
Version               : #10~24.04.1-Ubuntu SMP Fri Jul 18 20:44:30 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : 8.2.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.7.0
html5lib              : 1.1
hypothesis            : 6.137.3
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.5
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.7.0
scipy                 : 1.16.1
sqlalchemy            : 2.0.37
tables                : None
tabulate              : 0.9.0
xarray                : 2025.7.1
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
```
</details>
","['Bug', 'Needs Triage']",2025-08-13 01:34:44,2025-08-14 03:04:20,3,closed
62094,"BUG: In main, TimedeltaIndex.shift() requires freq in the index, but it may not be available because it was computed","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
ind = pd.date_range(""1/1/2021"", ""1/5/2021"") - pd.Timestamp(""1/3/2019"")
ind.shift(1)
```

### Issue Description

This only occurs on main.  NOT a current bug in pandas. 

Gives error:
```text
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Code\pandas_dev\pandas\pandas\core\indexes\datetimelike.py"", line 512, in shift
    raise NullFrequencyError(""Cannot shift with no freq"")
pandas.errors.NullFrequencyError: Cannot shift with no freq
```
The above code works fine with pandas 2.3.


### Expected Behavior

No error.  A user doing a calculation that produces a `TimedeltaIndex` can't be expected to set the `freq` of the index.

I think was introduced by @jbrockmendel in #61985 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : bb10b27dea9d9a2476de4c8122e0346689e1c9c3
python                : 3.11.13
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 3.0.0.dev0+2306.gbb10b27dea.dirty
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : 3.1.2
sphinx                : 8.2.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
bottleneck            : 1.5.0
fastparquet           : 2024.11.0
fsspec                : 2025.7.0
html5lib              : 1.1
hypothesis            : 6.137.3
gcsfs                 : 2025.7.0
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.5
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyiceberg             : 0.9.1
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.7.0
scipy                 : 1.16.1
sqlalchemy            : 2.0.43
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.7.1
xlrd                  : 2.0.1
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Timedelta', 'Blocker', 'good first issue']",2025-08-12 15:52:45,2025-09-10 18:34:33,15,closed
62093,BUILD: Nightly wheel building failed for some platforms,"### Installation check

- [x] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Github Actions ubuntu-latest

### Installation Method

pip install

### pandas Version

nightly wheel (see below)

### Python Version

3.12

### Installation Logs

<details>

```
  python -m pip install \
  --index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple/ \
  --trusted-host pypi.anaconda.org \
  --no-deps --pre --upgrade \
  matplotlib \
  numpy \
  pandas \
  scipy;

...

Collecting pandas
  Downloading https://pypi.anaconda.org/scientific-python-nightly-wheels/simple/pandas/3.0.0.dev0%2B2306.gbb10b27dea/pandas-3.0.0.dev0%2B2306.gbb10b27dea.tar.gz (4.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 84.0 MB/s  0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'error'
  error: subprocess-exited-with-error
  
  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [3 lines of output]
      Looking in indexes: https://pypi.anaconda.org/scientific-python-nightly-wheels/simple/
      ERROR: Could not find a version that satisfies the requirement meson-python>=0.13.1 (from versions: none)
      ERROR: No matching distribution found for meson-python>=0.13.1
      [end of output]
```

</details>

~I noticed that the installation guide says to use `--extra-index` but I had found in the past that that installed an incorrect combination of packages. I can try that, but I'm surprised this has started failing only just today. I'm wondering if someone maybe deleted `meson-python` from the `pypi.anaconda.org/scientific-python-nightly-wheels` location?~

Edit: See comments below. My CI tried to install the nightly build but there was no wheel for my platform so it installed from source and failed because `meson-python` was not available. My workaround was to add `--only-binary "":all:""` to my pip install command. The nightly builds could maybe be restarted?",['Build'],2025-08-12 11:36:04,2025-08-15 18:06:48,3,closed
62084,"BUG: Pandas to_datetime parsing error with ""May"" month","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
dates = ['May 12, 2025', 'May 13, 2025', 'May 14, 2025', 'May 15, 2025', 'May 16, 2025', 'May 17, 2025', 'May 18, 2025', 'May 19, 2025', 'May 20, 2025', 'May 21, 2025', 'May 22, 2025', 'May 23, 2025', 'May 24, 2025', 'May 25, 2025', 'May 26, 2025', 'May 27, 2025', 'May 28, 2025', 'May 29, 2025', 'May 30, 2025', 'May 31, 2025', 'Jun 1, 2025', 'Jun 2, 2025', 'Jun 3, 2025', 'Jun 4, 2025', 'Jun 5, 2025', 'Jun 6, 2025', 'Jun 7, 2025', 'Jun 8, 2025', 'Jun 9, 2025', 'Jun 10, 2025', 'Jun 11, 2025', 'Jun 12, 2025', 'Jun 13, 2025', 'Jun 14, 2025', 'Jun 15, 2025', 'Jun 16, 2025', 'Jun 17, 2025', 'Jun 18, 2025', 'Jun 19, 2025', 'Jun 20, 2025', 'Jun 21, 2025', 'Jun 22, 2025', 'Jun 23, 2025', 'Jun 24, 2025', 'Jun 25, 2025', 'Jun 26, 2025', 'Jun 27, 2025', 'Jun 28, 2025', 'Jun 29, 2025', 'Jun 30, 2025', 'Jul 1, 2025', 'Jul 2, 2025', 'Jul 3, 2025', 'Jul 4, 2025', 'Jul 5, 2025', 'Jul 6, 2025', 'Jul 7, 2025', 'Jul 8, 2025', 'Jul 9, 2025', 'Jul 10, 2025', 'Jul 11, 2025', 'Jul 12, 2025', 'Jul 13, 2025', 'Jul 14, 2025', 'Jul 15, 2025', 'Jul 16, 2025', 'Jul 17, 2025', 'Jul 18, 2025', 'Jul 19, 2025', 'Jul 20, 2025', 'Jul 21, 2025', 'Jul 22, 2025', 'Jul 23, 2025', 'Jul 24, 2025', 'Jul 25, 2025', 'Jul 26, 2025', 'Jul 27, 2025', 'Jul 28, 2025', 'Jul 29, 2025', 'Jul 30, 2025', 'Jul 31, 2025', 'Aug 1, 2025', 'Aug 2, 2025', 'Aug 3, 2025', 'Aug 4, 2025', 'Aug 5, 2025', 'Aug 6, 2025', 'Aug 7, 2025', 'Aug 8, 2025', 'Aug 9, 2025', 'Aug 10, 2025']

# Trying to parse these dates will raises an error, as it assumes that month followss ""%B"" format.
pd_dates = pd.to_datetime(dates)
```

### Issue Description

I use pd.to_datetime function to automaatically convert a list of String dates to timestamps, no matter the format. In my application, I assume that the provided date format is unknown. However, there is an issue with pandas to_datetime when:

1. The format is ""%b %d, %Y""
2. The first month in the list is ""May""

In this case, the pandas parses the first date and it assumes that all months follow %B format. However, ""May"" month could also be described by %b format. To address this issue I have to use ""mixed"" format to parse each date individually, but it is slower. 

### Expected Behavior

The expected behavior is to parse the dates. For instance, if you replace May in the first row with Aug, the parsing of the entire list is successful

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : fd3f57170aa1af588ba877e8e28c158a20a4886d
python                : 3.11.7.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.87.2-microsoft-standard-WSL2
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.0
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.8.2
setuptools            : 69.0.3
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.1.0
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.21.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.2.0
gcsfs                 : None
matplotlib            : 3.8.2
numba                 : 0.59.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 15.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : 2.0.41
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'Duplicate Report', 'Needs Triage']",2025-08-10 17:22:39,2025-08-11 05:41:27,2,closed
62064,ENH: Persistent and Interoperable Accessors for NDFrame and Index (sliding window POC),"### Feature Type

- [x] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

### Please regard

_Took some free time this month for a run of the mill architecture assessment over pandas Python sources, this is delivery.
For health related reasons, I use LLMs redactions to prevent severe prolix on public comms. I'll mark personal remarks in italics.
Engage at will, worry free._

### Summary

This issue proposes an enhancement to the internal architecture of accessors in pandas, aiming to support **persistent, interoperable accessors** that can coexist with and respond to their host `NDFrame` or `Index` objects in a lifecycle-aware and low-copy fashion.

### Motivation

Accessors in pandas today (e.g., `.str`, `.dt`, `.cat`, `.attrs`) are **ephemeral**, created on-demand for every access. While this is lightweight and efficient for simple use cases, it limits their utility in richer modeling contexts, especially for:

- Persisting contextual or intermediate state;
- Synchronizing behavior after host transformations (e.g., `.copy()`, slicing, chaining);
- Acting as first-class modeling layers inside pandas pipelines.

These limitations are especially relevant in **data engineering and ML pipelines**, where pandas often loses its centrality once feature computation becomes non-trivial — forcing users to fall back to NumPy, custom classes, or external orchestration layers.

_Also beware of safe excess over implementaion of promissing concepts around the topic. Obstructive inclinations drives punctual desertion, often followed by antithetical responses, the well known crux of enhencement frameworks design. Structure that is expressive and interoperable, supports continuity, comprehension, and reuse, granting high reach under few familiar instrument sets should principle approach.__

### Feature Description

### Proposal

Introduce a model for **persistent accessors**, which:

- Are instantiated once per host instance and survive as long as the host exists;
_- Lifetime enroll either to the high level Pandas Object, the underlying data suplly, or be a composite of both legs_
- Can respond to critical transformations on the host (e.g., copy, assignment, indexing);
- Are managed via hooks, weak references, or catalog mechanisms;
- Remain interoperable with existing pandas workflows;
- Allow extension via a structured API for third parties.

_Advise tripartit roadmap, issuing persistence and minimal lifetime only hooks first, follows immersion for carefull goldilock hooks placement for full managed event propagation design. Finally, design and edify provision infrastructure, availing accessors with outlined read access to source data entities._

This does **not** require a full architectural rewrite, and can initially be scoped to enable a formal accessor lifecycle with opt-in semantics and soft hooks.

### Alternative Solutions

### Proof of Concept (POC)

I'm currently prototyping an accessor extension under a sliding window use case. The idea is to offer **vertical rolling windows** of fixed size, supporting:

- Multiple successive rolls along axis 0 (depth stacking);
- Minimal memory footprint (zero-copy across views);
- ""Scoped local views"" for stateless metric computation, visual flattening, and full-dimensional window analysis;
- A lightweight dispatcher for `apply`-like routines, with hyperparameter support and batch-level threading (GIL-free environments only).

**Key implementation details include:**

- Accessors are indexed by a catalog based on `weakref.WeakKeyDictionary`, using `attrs` as the sole strong reference;
- A root hook object (UUID-backed immutable `set` subclass) is stored in `.attrs`, which carries the accessor identity;
- Copy operations trigger deep hooks via custom `__deepcopy__`, enabling propagation of the accessor along with host duplication;
- When the host NDFrame is garbage collected, the accessor is finalized via `weakref.finalize`;
- NDArray-level hooks are being tested to track mathematical and logical transformations on the underlying data;
- Implementation is being tested in a dev environment with no-GIL support, built using scientific nightly wheels via Mamba.

This approach is entirely backward-compatible and demonstrates how a structured accessor lifecycle could empower pandas to participate in more advanced modeling scenarios — without becoming a full modeling framework itself.

### Additional Context

### Use Case Implications

Persistent, interoperable accessors would allow:

- Declarative pipelines inside pandas (e.g., `.fe`, `.validate`, `.track`, etc.);
- Advanced feature engineering without leaving NDFrame;
- Safer data transformation propagation (accessor-aware copies and views);
- Reuse and encapsulation of logic across modeling contexts.
_- Architectural equidistance with _

It could also help pandas reclaim some conceptual territory currently dominated by _ honestly interface-wise dead ringer doppelganger marvels__ (e.g., Polars, Modin, cuDF), by moving beyond isomorphic syntax and toward architectural expression.","['Enhancement', 'Needs Triage']",2025-08-07 18:01:32,2025-08-08 15:57:13,3,closed
62063,BUG: is_scalar(Enum) gives False,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from enum import Enum, auto
from pandas.api.types import is_scalar

class Thing(Enum):
    one = auto()
    two = auto()

print(is_scalar(Thing.one))  # False
```

### Issue Description

`pandas` does not take `Enum`s as a Scalar.

Inspired by https://github.com/pandas-dev/pandas-stubs/issues/1288#issuecomment-3157127431.

### Expected Behavior

gives True

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.13.5
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : Czech_Czechia.1252

pandas                : 2.3.1
numpy                 : 2.2.6
pytz                  : 2023.4
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.7.0
html5lib              : None
hypothesis            : 6.136.5
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.16.1
sqlalchemy            : 2.0.41
tables                : None
tabulate              : None
xarray                : 2025.7.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Docs', 'Numeric Operations']",2025-08-07 16:02:45,2025-10-16 20:14:31,17,closed
62061,DOC: There is an unreadable dark text on the dark background in the getting started page,"The text under the ""Intro to pandas"" section is unreadable due to being displayed in a dark color over the dark background.

</br>

<img width=""1512"" height=""853"" alt=""Image"" src=""https://github.com/user-attachments/assets/38020b97-44cb-4293-ab71-40bcb28e4f51"" />

</br>
</br>

The issue is valid for the `2.3` and `2.2` versions.


- [2.3](https://pandas.pydata.org/pandas-docs/version/2.3/getting_started/index.html#intro-to-pandas)
- [2.2](https://pandas.pydata.org/pandas-docs/version/2.2/getting_started/index.html#intro-to-pandas)",[],2025-08-06 15:24:26,2025-08-06 15:30:58,1,closed
62059,BUG: Inconsistent behavior of `MultiIndex.union` depending on duplicates and names,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

index1_without_name = pd.Index([1, 2])
index1_with_name = pd.Index([1, 2], name=""x"")
index2_without_duplicates = pd.Index([2, 3])
index2_with_duplicates = pd.Index([2, 3, 2])

multi_index1_without_name = pd.MultiIndex.from_tuples([(1, ""a""), (2, ""b"")])
multi_index1_with_name = pd.MultiIndex.from_tuples([(1, ""a""), (2, ""b"")], names=[""x"", ""y""])
multi_index2_without_duplicates = pd.MultiIndex.from_tuples([(2, ""b""), (3, ""c"")])
multi_index2_with_duplicates = pd.MultiIndex.from_tuples([(2, ""b""), (3, ""c""), (2, ""b"")])

# These work
print(index1_without_name.union(index2_without_duplicates))
print(index1_without_name.union(index2_with_duplicates))
print(index1_with_name.union(index2_without_duplicates))
print(index1_with_name.union(index2_with_duplicates))
print(multi_index1_without_name.union(multi_index2_without_duplicates))
print(multi_index1_without_name.union(multi_index2_with_duplicates))
print(multi_index1_with_name.union(multi_index2_without_duplicates))

# This one raises
print(multi_index1_with_name.union(multi_index2_with_duplicates))
```

### Issue Description

For 2 `MultiIndex` instances `i1` and `i2`, `i1.union(i2)` behaves inconsistently depending on whether `i1` has names and whether `i2` has duplicates:

* If `i1` has no names or `i2` has no duplicates then `i1.union(i2)` works as expected
* If `i1` has names and `i2` has duplicates then `i1.union(i2)` raises `ValueError: cannot join with no overlapping index names`

In addition, if `i1` and `i2` are plain `Index` instances, then the case that is problematic for `MultiIndex` (names and duplicates) works as expected.

### Expected Behavior

I expect no exception to be raised. The result should contain the duplicate values of the second `MultiIndex` as duplicates, just as in the other cases for consistency (although personally this did surprise me, but that's a different topic).

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.12.7
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.0-27-generic
Version               : #27~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 22 17:38:49 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.1
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 18.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : 3.10.1
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'MultiIndex', 'setops']",2025-08-06 13:35:07,2025-10-18 18:04:45,3,closed
62057,DOC: DataFrame.to_feather() does not accept *all* file-like objects,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html

### Documentation problem

Since #35408, the method docs say:

> path str, path object, file-like object
> String, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function.

And indeed it does work with a BytesIO buffer or an open *file*:

```python
with open('/tmp/foo', 'wb') as handle:
    df.to_feather(handle)
```

But not with other file-like objects, such as an `AsyncWriter` from `hdfs.InsecureClient.write()`:

```python
with self.client.write(self._path(name)) as writer:
    df.to_feather(writer)

Traceback (most recent call last):
  File ""/home/chris/ram-system/.venv/lib/python3.10/site-packages/pyarrow/feather.py"", line 186, in write_feather
    _feather.write_feather(table, dest, compression=compression,
AttributeError: 'AsyncWriter' object has no attribute 'closed'
ValueError: I/O operation on closed file
```

I note that it's not actually supposed to work: [pyarrow.feather.write_feather](https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html) says:

> dest[str](https://docs.python.org/3/library/stdtypes.html#str)
> Local destination path.

Which says nothing about file-like objects being acceptable. It does seem to have some special cases for handling buffers specifically, but this is undocumented and could change at any time.

I think that `write_feather` insists on checking the `closed` attribute of the passed handle, which this one doesn't have. It seems to work if I poke such an attribute onto the object, but it could easily stop working.

Also I know about [hdfs.ext.dataframe.write_dataframe](https://hdfscli.readthedocs.io/en/latest/api.html#module-hdfs.ext.kerberos) for this particular use case, but it only supports Avro which is not a great file format for DataFrames, and there are likely to be other *file-like* objects that people might try to pass to `to_feather()`.

Similarly, [read_feather](https://pandas.pydata.org/docs/reference/api/pandas.read_feather.html) claims to accept:

> pathstr, path object, or file-like object
> String, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function.

But `read()` is not enough:

```
  File ""pyarrow/_feather.pyx"", line 79, in pyarrow._feather.FeatherReader.__cinit__
  File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 89, in pyarrow.lib.check_status
io.UnsupportedOperation: seek
```

### Suggested fix for documentation

I think it's better to describe these functions as officially taking only strings (URLs and paths) and mmap objects. File-like objects currently work but this is not guaranteed.","['Docs', 'IO Parquet', 'Closing Candidate']",2025-08-06 12:44:29,2025-08-06 21:58:34,4,closed
62056,ENH: pd.DataFrame.describe(): rename top to mode,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

In pd.DataFrame.describe(), the most frequent value is termed 'top'. 

> The top is the most common value.

But there exists a statistical term 'mode' (https://en.wikipedia.org/wiki/Mode_(statistics)) depicting the same. 
To reduce disambiguity I propose to rename _top_ to _mode_, both in the docs as well as in the print-out of the function. 

### Feature Description

I guess it would start here (replacing top with mode): 

```
def describe_categorical_1d(
    data: Series,
    percentiles_ignored: Sequence[float],
) -> Series:
    """"""Describe series containing categorical data.

    Parameters
    ----------
    data : Series
        Series to be described.
    percentiles_ignored : list-like of numbers
        Ignored, but in place to unify interface.
    """"""
    names = [""count"", ""unique"", ""mode"", ""freq""]
    objcounts = data.value_counts()
    count_unique = len(objcounts[objcounts != 0])
    if count_unique > 0:
        mode, freq = objcounts.index[0], objcounts.iloc[0]
        dtype = None
    else:
        # If the DataFrame is empty, set 'mode' and 'freq' to None
        # to maintain output shape consistency
        mode, freq = np.nan, np.nan
        dtype = ""object""

    result = [data.count(), count_unique, mode, freq]

    from pandas import Series

    return Series(result, index=names, name=data.name, dtype=dtype)
```

### Alternative Solutions

Leave as it is. 

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-08-06 08:09:24,2025-08-06 15:32:21,1,closed
62052,BUG: Pandas mean function fails on type pd.NA with axis=1 but not axis=0,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame({'a': [1.0, pd.NA, 3.0], 'b': [4.0, 5.0, pd.NA]}, dtype='Float64')
df.mean(axis=0, skipna=False) # works
df.mean(axis=1, skipna=False) # fails
```

### Issue Description

the treatment of type pd.NA is inconsistent across rows versus down columns. 

### Expected Behavior

the mean across rows should resolve to type pd.NA when at least one of the values in any column is of type pd.NA

### Installed Versions

pandas           : 1.5.3
numpy            : 1.24.4","['Bug', 'Needs Triage']",2025-08-05 23:13:56,2025-08-05 23:16:41,0,closed
62048,API: Series[Float64] == False,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
ser = pd.Series([0], dtype=""Float64"")
>>> ser == False
0    True
dtype: boolean
```

### Issue Description

NA

### Expected Behavior

I would expect this to be stricter in type-safety.  The lack of strictness necessitates special-casing in mask_missing (called from Block.replace).

Note that these also compare as equal for numpy float64 and float64[pyarrow]

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2025-08-05 15:35:11,2025-08-06 02:34:49,2,closed
62047,BUG: failing when groupby on data containing bytes,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd                                                                                                                                                                          
pd.Series(np.array([b""""])).groupby(level=0).last()
```

### Issue Description

when calling `groupby` on a frame or series containing bytes, an exception is raised:
`AttributeError: 'numpy.dtypes.BytesDType' object has no attribute 'construct_array_type'`


### Expected Behavior

Normal groupby behaviour

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.13.5
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-425.3.1.el8.x86_64
Version               : #1 SMP Fri Sep 30 11:45:06 EDT 2022
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.1
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Duplicate Report', 'Error Reporting', 'Constructors']",2025-08-05 15:23:14,2025-08-06 01:51:55,1,closed
62036,BUG: rank with object dtype and small values,"```python
# Based on test_rank_ea_small_values
import pandas as pd

ser = pd.Series(
    [5.4954145e29, -9.791984e-21, 9.3715776e-26, pd.NA, 1.8790257e-28],
    dtype=""Float64"",
)
ser2 = ser.astype(object)

>>> ser.rank(method=""min"")
0    4.0
1    1.0
2    3.0
3    NaN
4    2.0
dtype: float64

>>> ser2.rank(method=""min"")
0    4.0
1    1.0
2    1.0
3    NaN
4    1.0
dtype: float64
```

I'd expect 1) the values to match and 2) to get NA rather than NaN at least for the Float64 case.

Update: if we convert to float64[pyarrow] first we do get NA back and a uint64[pyarrow] dtype.","['Bug', 'Missing-data', 'NA - MaskedArrays', 'Transformations']",2025-08-03 17:18:34,2025-09-12 21:17:34,4,closed
62010,"DOC: Series and DataFrame.reindex accepts Timedelta as tolerance, which is not documented","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html
- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html

### Documentation problem

The following code snippet works:
```py
import pandas as pd

sr = pd.Series([1, 2], pd.to_datetime([""2023-01-01"", ""2023-01-02""]))
sr.reindex(index=pd.to_datetime([""2023-01-02"", ""2023-01-03""]), method=""ffill"", tolerance=pd.Timedelta(""1D""))

df = sr.to_frame()
df.reindex(index=pd.to_datetime([""2023-01-02"", ""2023-01-03""]), method=""ffill"", tolerance=pd.Timedelta(""1D""))
```
but in the documentation, `tolerance` being `Timedelta` is undefined.

### Suggested fix for documentation

Append `Timedelta` in the documentation for `tolerance`.","['Docs', 'Needs Triage']",2025-07-31 09:23:06,2025-08-05 07:47:38,2,closed
62004,"BUG: `IntervalArray.overlaps()` documents that it accepts another `IntervalArray`, but it is not implemented","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
data = [(0, 1), (1, 3), (2, 4)]
intervals = pd.arrays.IntervalArray.from_tuples(data)
intervals.overlaps(intervals)
```

### Issue Description

When running the above, pandas reports:
```text
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arrays\interval.py"", line 1406, in overlaps
    raise NotImplementedError
NotImplementedError
```

### Expected Behavior

Either we don't document this functionality, or we implement it (ideally the latter!!)


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.10.14
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.3.1
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.41
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2025.6.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Docs', 'Interval']",2025-07-30 21:28:09,2025-09-07 16:46:52,7,closed
61998,DOC: documenting pandas.MultIndex.argsort,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

No docs yet for `pd.MultiIndex.argsort`.
Only for `pd.Index`: https://pandas.pydata.org/docs/dev/reference/api/pandas.Index.argsort.html

### Documentation problem

Currently `pd.MultiIndex.argsort` is not documented, only `pd.Index` is.
Considering that the function signature is different (MultiIndex version has a `na_position` argument between `*args` and `**kwargs`.

Is this something we intend to document in the future or is it something that is not recommended for use by the users?

Thanks!

### Suggested fix for documentation

Adding docs seems the best option forward.","['Docs', 'MultiIndex', 'Sorting']",2025-07-29 23:49:59,2025-10-05 21:23:23,3,closed
61994,PERF: `pandas.DataFrame.stack` with `future_stack=True`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

```
import numpy as np
df = pd.DataFrame(np.random.randn(100, 100))
%timeit df.stack(future_stack=False)
%timeit df.stack(future_stack=True)
```

```
242 μs ± 40.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
25.6 ms ± 4.75 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.11.13
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-553.36.1.el8_10.x86_64
Version               : #1 SMP Wed Jan 22 03:07:54 EST 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.1
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.7.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.7.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.41
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>


### Prior Performance

_No response_","['Performance', 'Needs Triage']",2025-07-29 15:20:05,2025-07-29 16:43:23,1,closed
61993,BUG: Inconsistent `datetime` dtype based on how the dataframe gets initialized,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
(Pdb) pd.DataFrame({""0"": [datetime.fromtimestamp(1568888888, tz=pytz.utc)]}).dtypes
0    datetime64[ns, UTC]
dtype: object
(Pdb) pd.DataFrame({""0"": datetime.fromtimestamp(1568888888, tz=pytz.utc)}, index=[0]).dtypes
0    datetime64[us, UTC]
dtype: object
(Pdb)
```

### Issue Description

When creating a Pandas DataFrame with a timezone-aware datetime object (e.g., datetime.datetime with tzinfo=pytz.UTC), the inferred datetime64 precision differs depending on whether the datetime is passed as a scalar or inside a list. This leads to inconsistent and potentially unexpected behavior

### Expected Behavior

Both DataFrame initializations should infer the same datetime dtype (datetime64[ns, UTC]), ideally following Pandas’ default precision of nanoseconds.

### Installed Versions

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.13.5
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-47-generic
Version               : #47-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 27 22:03:50 UTC 2024
machine               : aarch64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.3.1
numpy                 : 2.3.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : 8.2.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
>>> 

</details>
","['Bug', 'Datetime', 'Needs Info']",2025-07-29 14:24:21,2025-07-31 15:04:51,5,closed
61992,"DOC: Point out difference in usage of ""str"" dtype in constructor and astype member","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

This concerns the 3.0 migration guide: https://pandas.pydata.org/docs/user_guide/migration-3-strings.html)

### Documentation problem

The string migration [guide](https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#the-missing-value-sentinel-is-now-always-nan) suggests using `""str""` in place of `""object""` to write compatible code. The example only showcases this suggestion for the Series constructor, where it indeed works as intended (Pandas 2.3.0):
```python
>>> import pandas as pd
>>> pd.Series([""a"", None, np.nan, pd.NA], dtype=""str"").array 
 <NumpyExtensionArray>
 ['a', None, nan, <NA>]
 Length: 4, dtype: object
```

However, the semantics of using `""str""` are different if the series has already been initialized with an `""object""` dtype and the user calls `astype(""str"")` on it:

```python
>>> series = pd.Series([""a"", None, np.nan, pd.NA])
>>> series.array
<NumpyExtensionArray>
['a', None, nan, <NA>]
Length: 4, dtype: object
>>> series.astype(""str"").array
<NumpyExtensionArray>
['a', 'None', 'nan', '<NA>']
Length: 4, dtype: object
```

Note that all values have been cast to strings. In fact, this behavior appears to be the behavior of passing the literal `str` as the data type that is mentioned later in the bug-fix [section](https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#astype-str-preserving-missing-values).



### Suggested fix for documentation

I believe this subtle difference should be pointed out in the migration guide. Ideally, a suggestion should be made on how one may write 3.0-compatible code using `astype`. In my case, the current Pandas 2 code is casting a categorical column (with string categories) into an object column, but I'd like to write code such that this operation becomes a string column in Pandas 3.","['Docs', 'Strings']",2025-07-29 09:24:33,2025-08-20 02:40:09,3,closed
61991,"BUG: Python Package fails to load for some users, but not others.","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# Code
import pandas as pd

df = pd.DataFrame({""Name"":[""Braund""]})
```

### Issue Description

# Venv
The venv is owned by root:root with 755 permissions.
Pandas version 2.3.1 (but also happens with 2.2.3)

# Command
/opt/.venv/bin/python /home/user.name/python_scripts/sketches.py

# Traceback Message 
Traceback (most recent call last):
  File ""/home/user.name/python_scripts/sketches.py"", line 7, in <module>
    df = pandas.DataFrame(
AttributeError: module 'pandas' has no attribute 'DataFrame'

Note: In fact, regardless of the method used, it seems to always output the same error message. I have used <user.name> to work with other packages in the same environment without any problem. However, if I use root user, then all the scripts I've tried with pandas work as expected.

### Expected Behavior

No error message, and creation of a data frame.

### Installed Versions


 -> Replace this line with the output of pd.show_versions()

Using root privileges, 

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-142-generic
Version               : #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8
 
pandas                : 2.3.1
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 22.0.2
Cython                : None
sphinx                : 8.1.3
IPython               : 8.35.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.2
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.3.2
matplotlib            : 3.10.1
numba                 : 0.61.2
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.40
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None","['Bug', 'Needs Info']",2025-07-29 08:57:34,2025-07-30 08:19:21,3,closed
61986,DOC: Improve docstrings in utility functions in pandas/core/common.py (lines 176–210),"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

pandas/core/common.py (lines 176–210)


### Documentation problem

Several internal utility functionshave unclear or missing docstrings.

- `not_none` returns a generator, unlike others nearby which return booleans (not documented).
- Functions like `any_not_none`, `all_none`, and `any_none` lack parameter descriptions, return types
- `any_not_none` duplicates the logic of `any_none` but does not explain the inversion 


### Suggested fix for documentation

To imrpove the docstrings for the following utility functions in `pandas/core/common.py`:

- Add return type clarification to `not_none` to explain that it returns a generator, unlike others in the section.
- For `any_not_none`, `all_none`, and similar functions, add full docstring structure with:
  - Parameters section
  - Returns section  np
- Optional: refactor duplicated logic between `any_not_none` and `any_none`.
",['Docs'],2025-07-29 00:36:13,2025-07-29 01:33:53,1,closed
61976,"BUG: infer_dtype returns ""mixed"" for complex and pd.NA mix","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
from pandas.api.types import infer_dtype

print(infer_dtype([1 + 1j, np.nan]))
# complex
print(infer_dtype([1 + 1j, pd.NA]))
# mixed
```

### Issue Description

`infer_dtype` on complex arrays with NA does not produce consistent results.
Similar to #61621, which has been fixed for the case of float type. I will submit a PR.

### Expected Behavior

Should return `complex`.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 49ca01ba9023b677f2b2d1c42e99f45595258b74
python                : 3.10.16
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.16.3-microsoft-standard-WSL2
Version               : #1 SMP Fri Apr 2 22:23:49 UTC 2021
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1580.g68d9dcab5b.dirty
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : 3.0.11
sphinx                : 8.1.3
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2024.12.0
html5lib              : 1.1
hypothesis            : 6.124.7
gcsfs                 : 2024.12.0
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 19.0.0
pyiceberg             : None
pyreadstat            : 1.2.8
pytest                : 8.3.4
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : 1.0.10
s3fs                  : 2024.12.0
scipy                 : 1.15.1
sqlalchemy            : 2.0.37
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Missing-data', 'Dtype Conversions']",2025-07-27 20:14:57,2025-07-28 16:13:42,1,closed
61975,BUG: 'Sphinx parallel build error' when building docs locally prevents index.html creation,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
python make.py html
```

### Issue Description

The following error prevents doc/build/html/index.html from getting created. Here are the steps to reproduce.

1. Pulled down from upstream/original Pandas
2. Created an environment using Option 2 (pip) from https://pandas.pydata.org/docs/dev/development/contributing_environment.html
3. Followed steps for building the documentation locally from https://pandas.pydata.org/docs/dev/development/contributing_documentation.html 
4. After running python make.py html, received a 'Sphinx parallel build error', 'Runtime unexpected exception' error in file `/doc/source/getting_started/comparison/comparison_with_sas.rst` line 135`

Screenshot of error

<img width=""2100"" height=""1500"" alt=""Image"" src=""https://github.com/user-attachments/assets/6ad9a45c-b4c5-47b5-aef2-f64917f01f44"" />

Machine:
2020 Macbook Pro 1.4 GHz Quad-Core Intel Core i5

### Expected Behavior

I expected the docs to be built and for doc/build/html/index.html to be created.

### Installed Versions

<details>

pandas 3.0.0.dev0+2267.ge4a03b6e47

INSTALLED VERSIONS
------------------
commit                : e4a03b6e47a8ef9cd045902916289cbc976d3d33
python                : 3.12.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.1.0
Version               : Darwin Kernel Version 23.1.0: Mon Oct  9 21:27:27 PDT 2023; root:xnu-10002.41.9~6/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2267.ge4a03b6e47
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 23.2.1
Cython                : 3.1.2
sphinx                : 8.1.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
bottleneck            : 1.5.0
fastparquet           : 2024.11.0
fsspec                : 2025.7.0
html5lib              : 1.1
hypothesis            : 6.136.4
gcsfs                 : 2025.7.0
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 21.0.0
pyiceberg             : 0.9.1
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.7.0
scipy                 : 1.16.0
sqlalchemy            : 2.0.41
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.7.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
qtpy                  : None
pyqt5                 : None

</details>
","['Docs', 'Needs Info', 'Closing Candidate']",2025-07-27 18:12:58,2025-08-05 16:05:55,6,closed
61968,DOC: code coverage app provided in documentation is invalid,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/development/contributing_codebase.html

### Documentation problem

The link to the pandas-coverage-app links to an empty page on heroku.

### Suggested fix for documentation

Either fix the doc to not mention the documentation coverage tool, rework the tool to be within the code base, or update the heroku link to contain the tool",['Docs'],2025-07-26 20:18:56,2025-07-27 15:44:53,1,closed
61955,DOC: shift argument in `Series.shift()` is not used,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html#pandas.Series.shift

### Documentation problem

The documentation for `Series.shift()` documents the parameter `suffix`, but it has no effect in the code. 



### Suggested fix for documentation

I think the `suffix` argument should be removed from the docs.
","['Docs', 'Transformations']",2025-07-25 22:14:34,2025-08-18 15:50:36,2,closed
61952,BUG: Using `Series.str.fullmatch()` and `Series.str.match()` with a compiled regex fails with arrow strings,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import re
DATA = [""applep"", ""bananap"", ""Cherryp"", ""DATEp"", ""eGGpLANTp"", ""123p"", ""23.45p""]
s=pd.Series(DATA)
s.str.fullmatch(re.compile(r""applep""))
s.str.match(re.compile(r""applep""))
sa=pd.Series(DATA, dtype=""string[pyarrow]"")
sa.str.fullmatch(re.compile(r""applep""))
sa.str.match(re.compile(r""applep""))
```

### Issue Description

with pyarrow strings, the last line fails with:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Condadirs\envs\pandasstubs311\Lib\site-packages\pandas\core\strings\accessor.py"", line 140, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Condadirs\envs\pandasstubs311\Lib\site-packages\pandas\core\strings\accessor.py"", line 1429, in fullmatch
    result = self._data.array._str_fullmatch(pat, case=case, flags=flags, na=na)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Condadirs\envs\pandasstubs311\Lib\site-packages\pandas\core\arrays\_arrow_string_mixins.py"", line 320, in _str_fullmatch
    if not pat.endswith(""$"") or pat.endswith(""\\$""):
           ^^^^^^^^^^^^
AttributeError: 're.Pattern' object has no attribute 'endswith'
>>> sa.str.match(re.compile(r""applep""))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Condadirs\envs\pandasstubs311\Lib\site-packages\pandas\core\strings\accessor.py"", line 140, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Condadirs\envs\pandasstubs311\Lib\site-packages\pandas\core\strings\accessor.py"", line 1388, in match
    result = self._data.array._str_match(pat, case=case, flags=flags, na=na)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Condadirs\envs\pandasstubs311\Lib\site-packages\pandas\core\arrays\_arrow_string_mixins.py"", line 309, in _str_match
    if not pat.startswith(""^""):
           ^^^^^^^^^^^^^^
AttributeError: 're.Pattern' object has no attribute 'startswith'
```


### Expected Behavior

No exception

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.11.9
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.3.1
numpy                 : 2.3.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : 2.0.41
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2025.6.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Strings', 'Arrow']",2025-07-25 17:43:38,2025-08-14 06:20:28,2,closed
61948,BUG: Replacement fails after NA value with PyArrow-backed strings,"This does not occur on the main branch, only 2.3.x. I plan to run a git-bisect later today.

```python
pd.set_option(""infer_string"", True)
ser = pd.Series([""a"", np.nan, ""a"", ""a""])
print(ser.replace({""a"": ""b""}))
# 0      b
# 1    NaN
# 2      a
# 3      b
# dtype: str
```","['Bug', 'replace', 'Arrow']",2025-07-25 14:28:33,2025-07-25 18:17:20,1,closed
61944,DOC: Standardize noncompliant docstrings in pandas/io/html.py (flake8-docstrings),"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/io/html.py


### Documentation problem

Several functions in `pandas/io/html.py` have docstrings that violate [PEP 257](https://peps.python.org/pep-0257/) and pandas documentation guidelines. Flagged violations include:

- `D400`: Docstring summary should end with a period  
- `D205`: Docstring summary should be followed by a blank line  
- `D401`: First line of docstring should be in imperative mood  

These inconsistencies reduce clarity and hinder automated validation.



### Suggested fix for documentation

Standardize docstring formatting based on `flake8-docstrings` and `pydocstyle` feedback to meet pandas’ documentation standards. This includes:

- Adding missing punctuation and spacing  
- Rewriting summaries for clarity and imperative voice  
- Ensuring consistent style across the module  

This fix is scoped to documentation and does not impact functionality.


Suggested labels: doc, refactor, good first issue","['Docs', 'IO HTML']",2025-07-25 02:59:21,2025-09-09 00:35:19,8,closed
61942,BUG: Using `Series.str.contains()` with a compiled regex and arrow strings fails,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> import pandas as pd
>>> import re
>>> DATA = [""applep"", ""bananap"", ""Cherryp"", ""DATEp"", ""eGGpLANTp"", ""123p"", ""23.45p""]
>>> s = pd.Series(DATA)
>>> s.str.contains(re.compile(r""a""), regex=True)
0     True
1     True
2    False
3    False
4    False
5    False
6    False
dtype: bool
>>> pd.options.future.infer_string=True
>>> s = pd.Series(DATA)
>>> s.dtype
str
>>> s.str.contains(re.compile(r""a""), regex=True)
```

### Issue Description

The last line fails with:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\strings\accessor.py"", line 140, in wrapper
    return func(self, *args, **kwargs)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\strings\accessor.py"", line 1346, in contains
    result = self._data.array._str_contains(pat, case, flags, na, regex)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arrays\string_arrow.py"", line 359, in _str_contains
    return ArrowStringArrayMixin._str_contains(self, pat, case, flags, na, regex)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arrays\_arrow_string_mixins.py"", line 299, in _str_contains
    result = pa_contains(self._pa_array, pat, ignore_case=not case)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pyarrow\compute.py"", line 265, in wrapper
    options = _handle_options(func_name, options_class, options,
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pyarrow\compute.py"", line 228, in _handle_options
    return options_class(*args, **kwargs)
  File ""pyarrow\\_compute.pyx"", line 1121, in pyarrow._compute.MatchSubstringOptions.__init__
  File ""pyarrow\\_compute.pyx"", line 1104, in pyarrow._compute._MatchSubstringOptions._set_options
  File ""<stringsource>"", line 15, in string.from_py.__pyx_convert_string_from_py_6libcpp_6string_std__in_string
TypeError: expected bytes, re.Pattern found
```


### Expected Behavior

No failure.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.10.14
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.3.1
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.41
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2025.6.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Strings', 'Arrow']",2025-07-24 22:44:11,2025-08-14 08:27:11,0,closed
61940,BUG: Using `/` operator with a `Path` and `Series` of string dtype fails,"### Reproducible Example

```python
import pandas as pd
from pathlib import Path
pd.options.future.infer_string = True  # Only needed with 2.3.1
folder = Path.cwd()
files = pd.Series([""a.png"", ""b.png""])
folder / files[0]   # This works
folder / files  # This raises an exception
```

### Issue Description

The `/` operator with `Path` works fine with 2.3.1 with strings being object dtype, but not with arrow strings.  The last statement produces this stacktrace:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\ops\common.py"", line 76, in new_method
    return method(self, other)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arraylike.py"", line 214, in __rtruediv__
    return self._arith_method(other, roperator.rtruediv)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\series.py"", line 6146, in _arith_method
    return base.IndexOpsMixin._arith_method(self, other, op)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\base.py"", line 1391, in _arith_method
    result = ops.arithmetic_op(lvalues, rvalues, op)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\ops\array_ops.py"", line 273, in arithmetic_op
    res_values = op(left, right)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\roperator.py"", line 27, in rtruediv
    return right / left
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\ops\common.py"", line 76, in new_method
    return method(self, other)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arraylike.py"", line 214, in __rtruediv__
    return self._arith_method(other, roperator.rtruediv)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arrays\arrow\array.py"", line 836, in _arith_method
    return self._evaluate_op_method(other, op, ARROW_ARITHMETIC_FUNCS)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arrays\arrow\array.py"", line 768, in _evaluate_op_method
    other = self._box_pa(other)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arrays\arrow\array.py"", line 407, in _box_pa
    return cls._box_pa_scalar(value, pa_type)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arrays\string_arrow.py"", line 154, in _box_pa_scalar
    pa_scalar = super()._box_pa_scalar(value, pa_type)
  File ""C:\Condadirs\envs\pandasstubs\lib\site-packages\pandas\core\arrays\arrow\array.py"", line 443, in _box_pa_scalar
    pa_scalar = pa.scalar(value, type=pa_type, from_pandas=True)
  File ""pyarrow\\scalar.pxi"", line 1670, in pyarrow.lib.scalar
  File ""pyarrow\\error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow\\error.pxi"", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Could not convert WindowsPath('c:/Code/pandas-stubs') with type WindowsPath: did not recognize Python value type when inferring an Arrow data type
```

While the error says something about Windows, a similar error occurs in Linux.



### Expected Behavior

No exception thrown

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.10.14
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.3.1
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 21.0.0
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.41
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2025.6.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Enhancement', 'Strings']",2025-07-24 22:02:26,2025-09-02 16:36:16,8,closed
61939,DOC: Docstring for BooleanDType inconsistent with the rest of repo,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/core/arrays/boolean.py

### Documentation problem

The docstring for the class BooleanDType has very little information about the functionality of the class.
 """"""
    Extension dtype for boolean data.

    .. warning::

       BooleanDtype is considered experimental. The implementation and
       parts of the API may change without warning.

    Attributes
    ----------
    None

    Methods
    -------
    None

    See Also
    --------
    StringDtype : Extension dtype for string data.

    Examples
    --------
    >>> pd.BooleanDtype()
    BooleanDtype
    """"""
What pieces of the docstring are included are sparse, and there is no parameters section included. In order to understand anything that the class is used for, I have to read through the code, which defeats the purpose of having the docstring. I read through the other class in the file and its docstring is far more developed and had a multi sentence summary of the classes uses, a parameters section and examples that used example data to convey a use case.

### Suggested fix for documentation

I suggest that somebody find what parameters are being used and add a section for them. Also, that a more specific example section be added and that the summary section be extended to include more description of the class's function.

I would be happy to start remedying these problems.","['Docs', 'NA - MaskedArrays']",2025-07-24 21:27:51,2025-11-04 12:51:45,10,closed
61935,"BUG: `assert_index_equal(CategoricalIndex, CategoricalIndex, check_categorical=True, exact=False)` raises TypeError instead of AssertionError","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [1]: import pandas as pd

In [2]: p_left = pd.Index([1, 2, 3], name=""a"", dtype=""category"")

In [3]: p_right = pd.Index([1, 2, 6], name=""a"", dtype=""category"")

In [4]: pd.testing.assert_index_equal(p_left, p_right, check_categorical=True, exact=False)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[10], line 1
----> 1 pd.testing.assert_index_equal(p_left, p_right, check_categorical=True, exact=False)

    [... skipping hidden 1 frame]

File ~/pandas/core/ops/common.py:70, in _unpack_zerodim_and_defer.<locals>.new_method(self, other)
     66         return NotImplemented
     68 other = item_from_zerodim(other)
---> 70 return method(self, other)

File ~/pandas/core/arrays/categorical.py:143, in _cat_compare_op.<locals>.func(self, other)
    141 msg = ""Categoricals can only be compared if 'categories' are the same.""
    142 if not self._categories_match_up_to_permutation(other):
--> 143     raise TypeError(msg)
    145 if not self.ordered and not self.categories.equals(other.categories):
    146     # both unordered and different order
    147     other_codes = recode_for_categories(
    148         other.codes, other.categories, self.categories, copy=False
    149     )

TypeError: Categoricals can only be compared if 'categories' are the same.
```

### Issue Description

pandas asserters should always raise an `AssertionError`.

### Expected Behavior

I would expect this to raise an `AssertionError`.

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Testing']",2025-07-24 17:48:56,2025-10-03 17:17:39,0,closed
61932,"BUG: Unexpected Code Segment Executed, Causing Logical Error","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# 示例数据
df = pd.DataFrame({
    'product_code': ['X', 'X', 'X', 'Y', 'Y'],
    'units': ['P', 'P', 'Q', 'Q', 'Q']
})

df2 = df.head(2)
df2 = df2.sort_values('product_code', ascending=False)\
            .groupby(['product_code',
                      'unit_name'])\
            .first().reset_index(drop=True)
print(df2)
```

### Issue Description

When there are only 2 lines of data in df, this code will run successfully, even if the fields do not exist. We have not found this situation in other rows so far

### Expected Behavior

It exists in version 2.2.2 of pandas。

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Groupby']",2025-07-24 09:04:20,2025-07-24 20:04:05,2,closed
61927,DOC: Add tzdata to dependencies section in README,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

[https://github.com/pandas-dev/pandas/blob/main/README.md](https://github.com/pandas-dev/pandas/blob/main/README.md)

### Documentation problem

tzdata is listed as a required dependency in the [installation documentation](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#dependencies), but is not listed with the other required dependencies in the README.

### Suggested fix for documentation

Add tzdata to the list of dependencies in the README so that the README matches the most current and accurate documentation. I intend to work on this issue.",['Docs'],2025-07-23 03:11:10,2025-07-25 16:37:25,1,closed
61926,BUG: Merge fails on pyarrow datetime columns,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Create a datetime index
t = pd.date_range(""2025-07-06"", periods=3, freq=""h"")

# Left dataframe: one row per timestamp
df1 = pd.DataFrame({""time"": t, ""val1"": [1, 2, 3]})

# Right dataframe: two rows per timestamp (duplicates)
df2 = pd.DataFrame({""time"": t.repeat(2), ""val2"": [10, 20, 30, 40, 50, 60]})

# This works
print(pd.merge(df1, df2, on=""time"", how=""left""))

# This fails
print(
    pd.merge(
        df1.convert_dtypes(dtype_backend=""pyarrow""),
        df2.convert_dtypes(dtype_backend=""pyarrow""),
        on=""time"",  # pyarrow datetime column causes error
        how=""left"",
    )
)
```

### Issue Description

Error message:
`ValueError: Length mismatch: Expected axis has 6 elements, new values have 3 elements`

### Expected Behavior

The merge should succeed and return 6 rows, like it does when not using `dtype_backend=""pyarrow""`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.12.11
python-bits           : 64
OS                    : Darwin
OS-release            : 24.5.0
Version               : Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:29 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6030
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : C.UTF-8

pandas                : 2.3.1
numpy                 : 2.3.1
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'Reshaping', 'good first issue', 'Arrow']",2025-07-23 02:57:47,2025-10-06 16:22:59,14,closed
61922,"BUG: `date_range` gives different output ends for fixed `end` and varying `start` when `freq=""B""`","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import datetime as dt

import pandas as pd

end = dt.datetime(2025, 7, 26, 6)  # Saturday early morning
for d in range(18, 22):  # Friday, Saturday, Sunday & Monday
    start = dt.datetime(2025, 7, d, 10)
    print(f""start={start}: {pd.date_range(start, end, freq=""B"")}\n"")
```

### Issue Description

Running `date_range` with varying `start` and fixed `end` gives outputs differing final entries when `freq=""B""`. Specifically, in the above example, the first and last iteration are missing an entry for `2025-07-25 10:00:00`.

### Expected Behavior

The last three iterations in the above code should produce the same output, and the first iteration should differ from the other three only in that it additionally includes `2025-07-18 10:00:00` as a first entry. Instead the output is the following:

```
start=2025-07-18 10:00:00: DatetimeIndex(['2025-07-18 10:00:00', '2025-07-21 10:00:00',
               '2025-07-22 10:00:00', '2025-07-23 10:00:00',
               '2025-07-24 10:00:00'],
              dtype='datetime64[ns]', freq='B')

start=2025-07-19 10:00:00: DatetimeIndex(['2025-07-21 10:00:00', '2025-07-22 10:00:00',
               '2025-07-23 10:00:00', '2025-07-24 10:00:00',
               '2025-07-25 10:00:00'],
              dtype='datetime64[ns]', freq='B')

start=2025-07-20 10:00:00: DatetimeIndex(['2025-07-21 10:00:00', '2025-07-22 10:00:00',
               '2025-07-23 10:00:00', '2025-07-24 10:00:00',
               '2025-07-25 10:00:00'],
              dtype='datetime64[ns]', freq='B')

start=2025-07-21 10:00:00: DatetimeIndex(['2025-07-21 10:00:00', '2025-07-22 10:00:00',
               '2025-07-23 10:00:00', '2025-07-24 10:00:00'],
              dtype='datetime64[ns]', freq='B')
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-553.36.1.el8_10.x86_64
Version               : #1 SMP Wed Jan 22 03:07:54 EST 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 9.1.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.2
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.3.2
matplotlib            : 3.10.1
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.3.2
scipy                 : 1.15.2
sqlalchemy            : 2.0.40
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime']",2025-07-22 10:30:51,2025-07-22 20:27:57,2,closed
61918,QST: Global future flag,"### Research

- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

N/A

### Question about pandas

I remember talking about it during the last contributor meeting, but have unfortunately forgotten the specifics. 

I know there is currently a 'future_stack' flag for future features to be activated in the future for the stack function. During the discussion, there was a consensus to use a global flag instead of function-specific variables to implement new functionalities to existing functions that would warrant a deprecation warning first. 

Is that implemented yet? I can't seem to find anything currently in the code but could've sworn we talked about one existing.",['Usage Question'],2025-07-21 20:55:32,2025-07-21 21:27:25,1,closed
61917,BUG: `IntervalIndex.unique()` only contains the first interval if all interval borders are negative,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

print(pd.__version__)

idx_pos = pd.IntervalIndex.from_tuples([(3, 4), (3, 4), (2, 3), (2, 3), (1, 2), (1, 2)])

print(idx_pos.unique())
assert idx_pos.unique().shape == (3,)  # succeeds

idx_neg = pd.IntervalIndex.from_tuples([(-4, -3), (-4, -3), (-3, -2), (-3, -2), (-2, -1), (-2, -1)])

print(idx_neg.unique())
assert idx_neg.unique().shape == (3,), f""Actual shape: {idx_neg.unique().shape}""
```

### Issue Description

Output with current main:

```
3.0.0.dev0+2250.g13f7b8b7e3
IntervalIndex([(3, 4], (2, 3], (1, 2]], dtype='interval[int64, right]')
IntervalIndex([(-4, -3]], dtype='interval[int64, right]')
Traceback (most recent call last):
  File ""/home/jmu3si/tmp/pd-demo.py"", line 12, in <module>
    assert idx_neg.unique().shape == (3,), f""Actual shape: {idx_neg.unique().shape}""
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Actual shape: (1,)
```

Only the interval `(-4, 3]` appears in the uniqued index.

A couple of other observations:

* The same result occurs with `closed=""left""`
* Intervals that are not fully negative, e.g. `(-2, 0]` also appear in the uniqued index
* This does not seem to be a regression. I reproduced it all the way back to pandas-1.4.3

### Expected Behavior

Expect correct unique index for `index_neg` to be 

`IntervalIndex([(-4, -3], (-3, -2], (-2, -1]], dtype='interval[int64, right]')` as it correctly did with the positive interval index.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 13f7b8b7e3dc6695c4e4b00afd0cccbd754210bd
python                : 3.13.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-60-generic
Version               : #63~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 22 19:00:15 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8

pandas                : 3.0.0.dev0+2250.g13f7b8b7e3
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : 3.1.2
sphinx                : 8.2.3
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
bottleneck            : 1.5.0
fastparquet           : 2024.11.0
fsspec                : 2025.7.0
html5lib              : 1.1
hypothesis            : 6.136.1
gcsfs                 : 2025.7.0
jinja2                : 3.1.6
lxml.etree            : 6.0.0
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 21.0.0
pyiceberg             : 0.9.1
pyreadstat            : 1.3.0
pytest                : 8.4.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.7.0
scipy                 : 1.16.0
sqlalchemy            : 2.0.41
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.7.1
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Algos', 'Interval']",2025-07-21 16:29:54,2025-11-25 21:10:03,4,closed
61916,"String dtype: backwards compatibility of selecting ""object"" vs ""str"" columns in `select_dtypes`","We provide the `DataFrame.select_dtypes()` method to easily subset columns based on data types (groups). See https://pandas.pydata.org/pandas-docs/version/2.3/user_guide/basics.html#selecting-columns-based-on-dtype

At the moment, as documented, the select string columns you must use the `object` dtype:

```python
>>> pd.options.future.infer_string = False
>>> df = pd.DataFrame(
...     {
...         ""string"": list(""abc""),
...         ""int64"": list(range(1, 4)),
...     }
... )
>>> df.dtypes
string    object
int64      int64
dtype: object
>>> df.select_dtypes(include=[object])
  string
0      a
1      b
2      c
```

On current main, with the string dtype enabled, the above dataframe now has a `str` column, and so selecting `object` dtype columns gives an empty result. One can use `str` instead:

```python
>>> pd.options.future.infer_string = True
>>> df = pd.DataFrame(
...     {
...         ""string"": list(""abc""),
...         ""int64"": list(range(1, 4)),
...     }
... )
>>> df.dtypes
string      str
int64     int64
dtype: object
>>> df.select_dtypes(include=[object])
Empty DataFrame
Columns: []
Index: [0, 1, 2]
>>> df.select_dtypes(include=[str])
  string
0      a
1      b
2      c
```

On the one hand, that is an ""obvious"" behaviour change as a consequence of the column now having a different dtype. But on the other hand, this will also break all code currently using `select_dtypes` to select string columns (and potentially silently, since it just no longer select them).

**How to write compatible code?**

One can select both object and string dtypes, so you select those columns in both older and newer pandas. One gotcha is that `df.select_dtypes(include=[str])` is not allowed in pandas<=2.3 (_""string dtypes are not allowed, use 'object' instead""_), and has to use ""string"" instead of ""str"" (although the default dtype is `str` ..). This will select opt-in nullable string columns as well, but so also the new default str dtype:

```python
# this gives the same result in both infer_string=True or False
>>> df.select_dtypes(include=[object, ""string""])
  string
0      a
1      b
2      c
```

TODO: this should be added to the migration guide in https://pandas.pydata.org/docs/dev/user_guide/migration-3-strings.html#the-dtype-is-no-longer-object-dtype (update -> https://github.com/pandas-dev/pandas/pull/62403)

**Can we make this upgrade experience smoother?**

Given that this will essentially break every use case of `select_dtypes` that involves selecting string columns (and given the fact this is a method, so we are more flexible compared to `ser.dtype == object`), I am wondering if we should provide some better upgrading behaviour. Some options:

- For now let `select_dtypes(include=[object])` keep selecting string columns as well, for backwards compatibility (and we can (later) add a warning we will stop doing that in the future)
- When a user does  `select_dtypes(include=[object])` in pandas 3.0, and we see that there are `str` columns, raise a warning mentioning to the user they likely want to do `include=[str]` instead.

For both cases, it gets annoying if you _actually_ want to select `object` columns, because then you have a (false positive) warning that you can't really do anything about (except ignoring/suppressing)

And in any case, we should probably still add a warning to pandas 2.3 about this when the string mode is enabled (for if we do a 2.3.2 release)",['Strings'],2025-07-21 13:24:05,2025-10-19 20:10:40,3,closed
61915,BUG: Cannot interpret string dtype as a valid data type,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

df = pd.DataFrame([[1,2], [3,4]], columns=[""first"", ""second""])

np.dtype(df.columns.dtype)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[10], line 1
----> 1 np.dtype(df.columns.dtype)

TypeError: Cannot interpret '<StringDtype(storage='python', na_value=nan)>' as a data type
```

### Issue Description

Hi, this issue was caught in scikit-learn's CI (for instance [here](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78378&view=logs&jobId=dfe99b15-50db-5d7b-b1e9-4105c42527cf&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf&t=ef785ae2-496b-5b02-9f0e-07a6c3ab3081)) a couple of days ago and only involves the dev version of pandas. It looks like there was a recent change in pandas string dtypes that make them not recognized as numpy dtypes.

ping @jorisvandenbossche, I saw that you merged several PRs about pandas strings last week :)

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 930c8a479d3e4644cb71de34770271f49f4862ff
python                : 3.13.5
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-60-generic
Version               : #63~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 22 19:00:15 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : fr_FR.UTF-8
LOCALE                : fr_FR.UTF-8

pandas                : 3.0.0.dev0+2249.g930c8a479d
numpy                 : 2.3.1
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : 3.1.2
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pytz                  : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
qtpy                  : None
pyqt5                 : None


</details>
",['Strings'],2025-07-21 08:31:47,2025-07-26 09:20:33,3,closed
61906,ENH: Make attributes saved by default,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Hi,

When doing

```python
df.attrs['array'] = array

df.to_parquet('file.parquet')
```

I see that I am not saving the array, could this be implemented?

Cheers

### Feature Description

The code above would safe `array` and it would load it when loading the parquet file into a dataframe.

### Alternative Solutions

I guess doing it myself separately with some helper function

### Additional Context

_No response_","['Enhancement', 'IO Parquet', 'metadata', 'Needs Triage']",2025-07-19 10:03:08,2025-07-21 03:02:18,3,closed
61903,DOC: Clarify to_numeric behavior for numeric dtypes,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.to_numeric.html#pandas-to-numeric

### Documentation problem

The docstring for the `to_numeric` function needs to be improved for clarity and accuracy. The current documentation states, ""The default return dtype is float64 or int64,"" which can be misleading. This statement doesn't account for cases where the input data is already of a numeric ExtensionDtype (e.g., Int32, Float32, or Arrow dtypes where `_is_numeric` is `True`). In these instances, `to_numeric` correctly preserves the original dtype rather than converting it, making the current documentation incomplete.

### Suggested fix for documentation

1. If the input is already of a numeric dtype, its dtype is preserved.
2. The conversion to a default float64 or int64 dtype primarily applies to non-numeric inputs.","['Docs', 'Downcasting']",2025-07-19 06:39:17,2025-07-21 20:53:50,2,closed
61898,BUG: `AttributeError` in `pd.eval()` when calling attribute after binary operation,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd


series1 = pd.Series([1,2,3,4,5])
series2 = pd.Series([2,3,5,1,2])

pd.eval(
    ""(a / b).cumsum()"",
    local_dict={""a"": series1, ""b"": series2}
)
```

### Issue Description

```
AttributeError: 'BinOp' object has no attribute 'value'
```
raised.

### Expected Behavior

```
0    0.500000
1    1.166667
2    1.766667
3    5.766667
4    8.266667
dtype: float64
```

should yield this result.



### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.11.10
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.3.1
numpy                 : 2.3.1
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-07-18 16:08:05,2025-07-18 16:13:34,2,closed
61890,Output formatting: the repr of the Categorical categories (quoted or unquoted strings?),"Because of the new string dtype, we also implicitly changes the representation of the unique categories in the Categorical dtype repr (aside the `object` -> `str` change for the dtype):

```python
>>> pd.options.future.infer_string = False
>>> pd.Categorical(list(""abca""))
['a', 'b', 'c', 'a']
Categories (3, object): ['a', 'b', 'c']
>>> pd.options.future.infer_string = True
>>> pd.Categorical(list(""abca""))
['a', 'b', 'c', 'a']
Categories (3, str): [a, b, c]
```

So the actual array values are always quotes, but the list of unique categories in the dtype repr goes from `['a', 'b', 'c']` to `[a, b, c]`.

Brock already fixed a bunch of xfails in the tests because of this in https://github.com/pandas-dev/pandas/pull/61727. And we also run into this issue for the failing doctests (https://github.com/pandas-dev/pandas/issues/61886).

@jbrockmendel mentioned there:

> It isn't 100% obvious that the new repr for Categoricals is an improvement, but it's non-crazy. 

With which I agree, also no strong opinion either way.

But before we also go fixing doctests, let's confirm that we are OK with this change. Because if we don't have a strong opinion that it is an improvement, we could also leave it how it was originally (and avoiding _some_ breakage because of this for downstream projects or users (eg who also have doctests))

","['Output-Formatting', 'Categorical']",2025-07-17 07:41:48,2025-08-13 19:57:02,7,closed
61889,BUG: make to_json with JSON Table Schema work correctly with string dtype,"(noticed because of some doctest failures cfr https://github.com/pandas-dev/pandas/issues/61886)

Currently, for the strings as object dtype, it seems that we assume that object dtype are actually strings, and encode that as such in the schema part of the JSON Table Schema output:

```python
>>> pd.Series([""a"", ""b"", None], dtype=object).to_json(orient=""table"", index=False)
'{""schema"":{""fields"":[{""name"":""values"",""type"":""string""}],""pandas_version"":""1.4.0""},""data"":[{""values"":""a""},{""values"":""b""},{""values"":null}]}'
```

But for the now-default string dtype, this is still seen as some custom extension dtype:

```python
>>> pd.Series([""a"", ""b"", None], dtype=""str"").to_json(orient=""table"", index=False)
'{""schema"":{""fields"":[{""name"":""values"",""type"":""any"",""extDtype"":""str""}],""pandas_version"":""1.4.0""},""data"":[{""values"":""a""},{""values"":""b""},{""values"":null}]}'
```

(note the `""type"":""string""` vs `""type"":""any"",""extDtype"":""str""`)

Given that the Table Schema spec has a ""string"" type, let's also use that when exporting our string dtype.

","['Bug', 'IO JSON', 'Strings']",2025-07-17 07:30:55,2025-07-26 11:27:38,4,closed
61888,ENH: Images embedded in cells. The DISPIMG function of WPS,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Hi!
I found that there is an issue with the WPS image. The software allows images to be directly embedded into cells, and the format is similar to `=DISPIMG (""ID5BA4F81A0D674C7AA8849A79AC5645C8"", 1)`. 

<img width=""691"" height=""431"" alt=""Image"" src=""https://github.com/user-attachments/assets/e32caa36-9729-44ca-8a46-477aec421e79"" />

Therefore, it cannot be accessed through **worksheets. _images**

If we unzip Excel, we can find all the images under _xl/media_, and the image indexes are in _xl/-rels/cellimages.xml.rels_ and _xl/ellimages.xml_

This is a unique feature of WPS, at least I haven't found it in Office.


I found a similar [implementation](https://github.com/wangguanquan/eec/issues/363)

### Feature Description

This is my code, which will decompress Excel, read the file, and return an Id to address mapping

```pthon
def wps_embed_images(file_path, save_path) -> dict:
    img_map = {}

    with zipfile.ZipFile(file_path, ""r"") as zip_ref:
        zip_ref.extractall(save_path)

    id2target = {}
    rels = os.path.join(save_path, ""xl"", ""_rels"", ""cellimages.xml.rels"")
    tree = ET.parse(rels)
    root = tree.getroot()
    for child in root:
        id2target[child.attrib.get(""Id"")] = os.path.join(save_path, ""xl"", child.attrib.get(""Target""))

    namespaces = {
        'etc': 'http://www.wps.cn/officeDocument/2017/etCustomData',
        'xdr': 'http://schemas.openxmlformats.org/drawingml/2006/spreadsheetDrawing',
        'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',
        'r': 'http://schemas.openxmlformats.org/officeDocument/2006/relationships'
    }

    cellimages = os.path.join(save_path, ""xl"", ""cellimages.xml"")
    tree = ET.parse(cellimages)
    root = tree.getroot()
    for cell_image in root.findall('etc:cellImage', namespaces):
        c_nv_pr = cell_image.find('.//xdr:cNvPr', namespaces)
        image_name = c_nv_pr.get('name') if c_nv_pr is not None else None

        blip = cell_image.find('.//a:blip', namespaces)
        embed_id = blip.get(f'{{{namespaces[""r""]}}}embed') if blip is not None else None

        if image_name and embed_id:
            img_map[image_name] = id2target[embed_id]

    return img_map
```

### Alternative Solutions

We leave it as it is and I continue using the solution shown above.

### Additional Context

_No response_","['Enhancement', 'Needs Info', 'Needs Triage', 'Closing Candidate']",2025-07-17 07:26:02,2025-08-05 16:28:29,11,closed
61886,DOC: fix doctests for repr changes with the new string dtype,"Now the string dtype is turned on by default (https://github.com/pandas-dev/pandas/pull/61722), we also have to fix the doctests to match the new behaviour (the doctests are currently, temporarily, allowed to fail to avoid red CI until this issue is fixed).

The failures can be seen in the current doctests logs, for example at https://github.com/pandas-dev/pandas/actions/runs/16332737970/job/46138722939#step:6:23

There are two main groups of failures:
- `dtype: object` that needs to become `dtype: str` in Series output (or object->str in some other reprs, and a few None->NaN changes)
- The representation of Categorical no longer using quoted values

I would propose to first start with the first bullet point (we should maybe reconsider if the categorical repr change is actually what we want -> https://github.com/pandas-dev/pandas/issues/61890), and the failing files are:

- [x] `pandas/core/base.py` (https://github.com/pandas-dev/pandas/pull/61905)
- [x] `pandas/core/generic.py`
- [x] `pandas/core/strings/accessor.py`
- [x] `pandas/core/arrays/datetimelike.py`, `pandas/core/arrays/datetimes.py` and `pandas/core/indexes/datetimelike.py`

Let's do one PR per bullet point here.

You can run the doctest and verify changes with for example:

```
pytest --doctest-modules pandas/core/base.py
```

Example PR for some fixes in other files: https://github.com/pandas-dev/pandas/pull/61887
","['Docs', 'Strings']",2025-07-17 07:06:43,2025-07-26 17:14:22,9,closed
61873,BUG:float_precision type hints differ in release version from github and docs pandas==2.3.1,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
python -m venv venv
source ./venv/scripts/activate
python -m pip install pandas==2.3.1


Open a modern ide like pycharm and type


pd.read_csv(path, float_precision='round_trip')

and you will see type check erroring because the code is different.
```

### Issue Description

This is probably a bug in distribution.

I currently have installed on my windows system pandas 2.3.1. When I open 
```
.venv/Lib/site-packages/pandas/io/parsers/readers.py
```
I see the following line in 3 different definitions for read_csv:

```
    float_precision: Literal[""high"", ""legacy""] | None = None,
```

However, the documentation specifies a third option, 'round_trip', and so does the code here on github

https://github.com/pandas-dev/pandas/blob/1d153bb1a4c6549958a20e04508967e2ed45159f/pandas/io/parsers/readers.py#L141

I don't understand how this line is different in a pip installed latest version, but not on github.com. This code was fixed back at the beginning of 2024, 18+ months ago.

https://github.com/pandas-dev/pandas/commit/37d7db4a1a1f6928a1541eaab05f51318d1d3344

Why does it not appear in pip installable distributions?

### Expected Behavior

I expect the line

    float_precision: Literal[""high"", ""legacy""] | None = None,

in pandas/io/parsers/readers.py to read

    float_precision: Literal[""high"", ""legacy"", ""round_trip""] | None = ...,



### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.13.2
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252
pandas                : 2.3.1
numpy                 : 2.2.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.2.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO CSV', 'Typing', 'Closing Candidate']",2025-07-16 13:01:08,2025-07-22 12:26:03,1,closed
61866,BUG:  Operations not implemented for non-1D ExtensionArrays ,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# mypy: ignore-errors
import pandas as pd
import numpy as np
import pandas._testing as tm

df = pd.DataFrame(np.arange(50).reshape(10, 5)).notna().values

# -> works
NP_array = pd.array([i for i in range(10)], dtype=tm.SIGNED_INT_NUMPY_DTYPES[0]).reshape(10,1) #dtype: NumpyExtensionArray

# -> doesnt work (NotImplemented)
EA_array = pd.array([i for i in range(10)], dtype=tm.SIGNED_INT_EA_DTYPES[0]).reshape(10,1)    #dtype: IntExtensionArray

print(df * NP_array)
# NotImplementedError: can only perform ops with 1-d structures
print(df * EA_array)
```

### Issue Description

I was working on creating test cases for ExtensionArrays following comments on PR #61828 when I realized that I could not use the '&' operation on EAs like I could with NP arrays. 

After a bit of digging around, it appears they both call self._logical_method, but whereas NP returns NotImplemented and continues operation, EA raises an error.

If someone wants to take a look while I work on something else, they are more than welcome to, otherwise I can work out a fix when I come back to it.

I have found that to be the case for both '*' and '&' so it's probably something deeper there

### Expected Behavior

- Operators should function the same for both Numpy arrays and other ExtensionArrays

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 8a1d5a06f9fb3c232249e3ed301932053efb06d8
python                : 3.10.17
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.0-29-generic
Version               : #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2177.g8a1d5a06f9
numpy                 : 2.2.5
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 8.36.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.3.2
html5lib              : 1.1
hypothesis            : 6.131.15
gcsfs                 : 2025.3.2
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 20.0.0
pyiceberg             : None
pyreadstat            : 1.2.8
pytest                : 8.3.5
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.3.2
scipy                 : 1.15.2
sqlalchemy            : 2.0.40
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2025.4.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.3
zstandard             : 0.23.0
qtpy                  : None
pyqt5                 : None
None

</details>
","['Bug', 'Numeric Operations', 'NA - MaskedArrays']",2025-07-15 19:55:29,2025-08-14 22:39:02,2,closed
61863,BUG: describe(include=..) fails with unrelated error if provided data types are not present,"Example with current main:

```
>>> df = pd.DataFrame({""a"": [1, 2, 3]})
>>> df.describe(include=[""datetime""])
...
ValueError: No objects to concatenate
```

I assume the error comes from under the hood trying to concatenate the results of calculating the describe results for each of the incluced dtype groups, and in this case for datetime there is no content, so nothing to concatenate. 

But we shouldn't propagate that error message to the user, I think. Either we should provide a better error message about none of the included dtypes being present, or just return an empty DataFrame.","['Bug', 'Error Reporting']",2025-07-15 14:09:59,2025-10-05 22:22:27,3,closed
61861,BUG: pd.eval raises AttributeError: 'BinOp' object has no attribute 'value',"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

x = pd.DataFrame(np.empty((3, 4)))
y = pd.DataFrame(np.empty((3, 4)))

pd.eval(""(x * y).sum()"")
```

### Issue Description

The above code raises this error:

<img width=""1384"" height=""203"" alt=""Image"" src=""https://github.com/user-attachments/assets/38af2da0-9437-4638-8215-fe8a0f699fd3"" />

Related issue: #61175

### Expected Behavior

.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.12.11
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-122-generic
Version               : #132-Ubuntu SMP Thu Aug 29 13:45:52 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.0
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : 8.2.3
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : 6.135.0
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.0
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.41
tables                : 3.10.2
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage']",2025-07-15 08:32:29,2025-07-15 08:47:03,1,closed
61860,"ENH: New method ""ends"" as a combination of “head” and ""tail""","### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I often work with time series and want to see at a glance where and how they begin and end.  

### Feature Description

That's why I registered an ""ends"" accessor, which provides me with both ends in one call as a combination of ""head"" and ""tail"". It's really simple, but very usefull to me:

```
class _EndsAccessor:
    def __init__(self, pandas_obj):
        self._obj = pandas_obj

    def __call__(self, n=2):
        return pd.concat([self._obj.head(n), self._obj.tail(n)])

@pd.api.extensions.register_dataframe_accessor(""ends"")
class EndsAccessorDataframe(_EndsAccessor):
    pass


@pd.api.extensions.register_series_accessor(""ends"")
class EndsAccessorSeries(_EndsAccessor):
    pass

```

### Alternative Solutions

We leave it as it is and I continue using the solution shown above.

### Additional Context

_No response_","['Enhancement', 'Needs Triage', 'Closing Candidate']",2025-07-15 07:43:12,2025-08-05 16:28:53,3,closed
61848,DOC: Series.__init__ doc incorrectly says dtype is ignored if data is a Series,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.html

### Documentation problem

```
    dtype : str, numpy.dtype, or ExtensionDtype, optional
        Data type for the output Series. If not specified, this will be
        inferred from `data`.
        See the :ref:`user guide <basics.dtypes>` for more usages.
        If ``data`` is Series then is ignored.
```
The last line here is incorrect. specifying a dtype will override the default behavior. See this example

```
>>> import pandas as pd
>>> ser = pd.Series([1,2,3])
>>> ser
0    1
1    2
2    3
dtype: int64
>>> pd.Series(ser, dtype=float)
0    1.0
1    2.0
2    3.0
dtype: float64
```

### Suggested fix for documentation

Just remove that line","['Docs', 'Series']",2025-07-14 02:00:43,2025-07-14 15:48:26,0,closed
61841,"BUG: .rolling().mean() returns all NaNs on re-execution, despite .copy() use","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import yfinance as yf
import pandas as pd

# Step 1: Getting the data
ticker = ""AAPL""

data = yf.download(ticker, start=""2020-01-01"", end=""2025-07-13"", auto_adjust=True, progress=False)

# Step 3: Reduce to ""Close"" and copy
data = data[[""Close""]].copy() # Note the use of .copy()

# Step 3: Calculate rolling averages
shortWindow = 20

# NOTE: Re-running the following line (in e.g. a Jupyter notebook cell) results in a column full of NaNs.
data[f""SMA{shortWindow}""] = data[""Close""].rolling(window=shortWindow).mean()
```

### Issue Description

When running a simple rolling mean assignment on a copied DataFrame, the operation works as expected on first execution, but subsequent executions result in columns full of NaNs, even though `.copy()` was used explicitly to break view ties.

This behavior suggests that `.rolling()` or the assignment mechanism is not fully stateless or clean between executions, which violates expectations around `.copy()` providing safe memory isolation.

### Expected Behavior

After using `.copy()` on the sliced DataFrame—`data = data[['Close']].copy()`—I expect the object to be fully decoupled from its original state, and for repeated assignments to `data['SMA20'] = data['Close'].rolling(20).mean()` to behave identically and reliably, regardless of how many times the line is executed.

Instead, the observed behaviour is as follows:

- On the first execution, `.rolling().mean()` works correctly.
- On any subsequent execution, the assigned columns become full of NaNs.
- This occurs even though `.copy()` was used on the entire DataFrame.

### Confirmed Environment Behavior

This issue was tested in:

- ✅ **Pandas 1.5.3** — No issue: repeated `.rolling().mean()` assignments behave as expected.
- ❌ **Pandas 2.3.1** (Replit script & JupyterLab) — Repeated assignment results in columns filled with NaNs.

This confirms the issue is a **regression introduced in Pandas 2.x**. The behavior is reproducible across multiple environments and interfaces.

### 🔄 Update: Bug Persists Even with `.copy()` on the Input Series

I believe I have further confirmed that the issue is **not due to view/copy ambiguity** of the input data.

I tested the following pattern, using `.copy()` explicitly on the input Series before applying `.rolling()`, as shown below:

```python
# Create a clean copy of the 'Close' column
closeData = data[""Close""].copy()

# Assign rolling mean result to new column
data[f""SMA{shortWindow}""] = closeData.rolling(window=shortWindow).mean()
```

However, **re-running this assignment line a second time still results in the `SMA20` column being filled with NaNs**. This happens **even though `closeData` is a deep copy**, isolated from any previous DataFrame state.

This suggests that:

* The issue is **not caused by shared views** or copy/reference issues in the input.
* The bug may instead be related to **reassigning the same target column** (`SMA20`) multiple times with a rolling result — possibly due to internal caching, memory reuse, or stale index alignment in Pandas’ internal `BlockManager`.

The bug continues to occur in both:

* ✅ Replit script execution (not as a notebook)
* ✅ JupyterLab environment

This further supports the conclusion that the bug is **environment-independent**, and likely a **regression in core Pandas 2.x logic** for repeated rolling assignments.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.11.3
python-bits           : 64
OS                    : Darwin
OS-release            : 24.0.0
Version               : Darwin Kernel Version 24.0.0: Mon Aug 12 20:51:54 PDT 2024; root:xnu-11215.1.10~2/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.3.1
numpy                 : 2.3.1
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 22.3.1
Cython                : None
sphinx                : None
IPython               : 9.4.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Indexing', 'MultiIndex', 'Window']",2025-07-12 23:36:26,2025-09-24 17:00:02,6,closed
61837,BUG: read_csv() on_bad_lines callable does not raise ParserWarning when index_col is set,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> import pandas as pd
>>> def line_fixer(line):     
...     return [1, 2, 3, 4, 5]
... 
>>> df = pd.read_csv('test.csv', engine='python', on_bad_lines=line_fixer)
<stdin>:1: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.
>>> df = pd.read_csv('test.csv', engine='python', on_bad_lines=line_fixer, index_col=0)
>>>
```

### Issue Description

### test.csv, with extra column (""E"") in row 3
```
id,field_1,field_2
101,A,B
102,C,D,E
103,F,G
```
Callable `line_fixer` returns a list with 5 elements, which is more elements than expected.

Documentation for the read_csv() on_bad_lines callable states: 
> If the function returns a new list of strings with more elements than expected, a ParserWarning will be emitted while dropping extra elements.

This behavior is correctly seen when index_col=None (the default), but not when index_col is set.

### Expected Behavior

A ParserWarning should be raised regardless of the index_col parameter.  In either case, data (elements 4 and 5, in this example) are being lost, but this is done silently when index_col is set.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.10.4
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.3.1
numpy                 : 2.2.5
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : 3.2.3
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO CSV']",2025-07-12 03:01:11,2025-09-29 20:40:08,2,closed
61835,DOC: README.md link for issues specified for Docs and good first issue doesn't reference properly,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas?tab=readme-ov-file#contributing-to-pandas

### Documentation problem

In the README.md, the links for 'Docs' and 'good first issue' doesn't reference to the appropriate labels.

### Suggested fix for documentation

Change the links so they reference the proper labels.","['Docs', 'Needs Triage']",2025-07-12 02:55:35,2025-07-12 18:49:36,1,closed
61831,BUG: Intersection of Pandas Index Object is not working properly,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
indA=pd.Index([1,3,5,7,9])
indB=pd.Index([2,3,5,7,11])

indA & indB
```

### Issue Description

the output i am getting is : 
Index([0, 3, 5, 7, 9], dtype='int64')



### Expected Behavior


but after intersection, i should get the output: 
Index([3, 5, 7], dtype='int64')

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2025-07-11 09:57:14,2025-07-11 15:41:22,1,closed
61829,ENH: Add a function like PYQT signal,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I hope this function can use to keep two or more dataframe same like PyQt View and Model (if I revise model view will change )

### Feature Description

from typing import Callable


class Index():
    def __init__(self,column = -1,row = -1):
        self.column = column
        self.row = row
    def check(self,reviseRange):
        """"""if self.column,self.index in range return True""""""
        return True
class dataframe:
    def __init__(self):
        self.handlers = {Index:Callable}#index,function
    def _trigger(self,reviseRange):
        """"""use @ to adapt iloc loc __setitem__ """"""
        for i,f in self.handlers.items():
            if i.check():
                f(reviseRange)
        


### Alternative Solutions

pyqt singal

### Additional Context

_No response_","['Enhancement', 'Needs Triage', 'Closing Candidate']",2025-07-11 01:25:12,2025-08-05 16:19:10,4,closed
61824,BUG: `mask` in `test_mask_stringdtype` would always return the same result regardless of `cond`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# test_mask_stringdtype
obj = pd.DataFrame(
    {""A"": [""foo"", ""bar"", ""baz"", pd.NA]},
    index=[""id1"", ""id2"", ""id3"", ""id4""],
    dtype=pd.StringDtype(),
)
filtered_obj = pd.DataFrame(
    {""A"": [""this"", ""that""]}, index=[""id2"", ""id3""], dtype=pd.StringDtype()
)
expected = pd.DataFrame(
    {""A"": [pd.NA, ""this"", ""that"", pd.NA]},
    index=[""id1"", ""id2"", ""id3"", ""id4""],
    dtype=pd.StringDtype(),
)

filter_ser = pd.Series([False, True, True, False])
obj.mask(filter_ser, filtered_obj)
#         A
# id1  <NA>
# id2  this
# id3  that
# id4  <NA>

filter_ser = pd.Series([True, False, False, True])
obj.mask(filter_ser, filtered_obj)
#         A
# id1  <NA>
# id2  this
# id3  that
# id4  <NA>

filter_ser = pd.Series([False, False, False, False])
obj.mask(filter_ser, filtered_obj)
#         A
# id1  <NA>
# id2  this
# id3  that
# id4  <NA>

filter_ser = pd.Series([True, True, True, True])
obj.mask(filter_ser, filtered_obj)
#         A
# id1  <NA>
# id2  this
# id3  that
# id4  <NA>
```

### Issue Description

Found during #60772 .
I suppose the purpose of this test is to check if `mask` works as expected with `pd.StringDtype()` (See #40824 ), but the test seems to return the same result regardless of `cond` since it fails to align in `_where`.

If we want to check if `mask` replaces with `other` only where `cond` is `True` and let `cond` propagate where `cond` is `False`, I think `filter_ser` should have `index` so that `mask` can recognize the corresponding `other` value.

### Expected Behavior

```python
filter_ser = pd.Series([False, True, True, False], index=[""id1"", ""id2"", ""id3"", ""id4""])
obj.mask(filter_ser, filtered_obj)
#         A
# id1   foo
# id2  this
# id3  that
# id4  <NA>
```

### Installed Versions

<details>

commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.12.7
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 25 Model 80 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : Korean_Korea.949

pandas                : 2.3.1
numpy                 : 2.3.1
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Testing']",2025-07-10 13:32:51,2025-07-11 16:35:47,1,closed
61823,BUG: drop doesn't recognise MultiIndexes,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
foo = pd.DataFrame({'a': [1, 2, 3], 'b': ['foo', 'foo', 'bar']})
foo = pd.concat([foo], keys=['foo'], axis=1)
foo.drop(index='b', level=1, axis=1)
```

### Issue Description

When drop is called, an AssertionError is raised `AssertionError: axis must be a MultiIndex`
On inspection of the dataframe, the columns are a MultiIndex

### Expected Behavior

Drop should not raise an incorrect AssertionError

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.10.8
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.87.2-microsoft-standard-WSL2
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.1
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 8.37.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : 6.129.3
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.1.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>

Also tested in a clean 3.13.5 environment:

<details>

INSTALLED VERSIONS
------------------
commit                : c888af6d0bb674932007623c0867e1fbd4bdc2c6
python                : 3.13.5
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.87.2-microsoft-standard-WSL2
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.3.1
numpy                 : 2.3.1
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
",['Bug'],2025-07-10 09:39:27,2025-07-18 02:18:17,7,closed
61811,DOC: Lacking information on error type raised by pd.to_numeric,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html

### Documentation problem

There is no ""Raises"" section that describes *which* errors are raised when setting the argument ""errors"" to ""raise"". It is not immediately clear if a conversion error will cause a TypeError or ValueError, or both depending on how conversion failed. 

This would be useful when doing as recommended to ""Catch exceptions explicitly instead."", and writing a `try: except:` with specific errors caught to avoid an overly generic error-catch which is bad practice etc. etc.

### Suggested fix for documentation

Add a ""Raises:"" section or include specific error names instead of the generic ""Raises an exception"".

See for example: https://numpy.org/doc/2.1/reference/generated/numpy.array.html

> For False it raises a ValueError if a copy cannot be avoided. Default: True.","['Docs', 'Needs Triage']",2025-07-08 08:46:42,2025-07-16 16:29:48,1,closed
61791,DOC: Improve text color in dark mode for tutorial navigation buttons,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/getting_started/index.html#intro-to-pandas

### Documentation problem

In dark mode, the text within the tutorial navigation boxes under ""Intro to pandas"" has low contrast against the background, making it difficult to read.

<img width=""741"" height=""578"" alt=""Image"" src=""https://github.com/user-attachments/assets/2d941542-05b3-40f9-b235-769bfda89c31"" />


### Suggested fix for documentation

Lighten the font color to `#CED6DD` which matches the color of other texts in dark mode.","['Docs', 'Duplicate Report', 'Web', 'Needs Triage']",2025-07-07 04:28:05,2025-07-07 13:15:53,3,closed
61788,"BUG: read_excel() converts the string ""None"" in an Excel file to ""NaN""","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. (I am working on compiling and testing this.)


### Reproducible Example

```python
import pandas as pd

excel_file = ""string_list.xlsx""

df_openpyxl = pd.read_excel(excel_file, engine=""openpyxl"")
df_calamine = pd.read_excel(excel_file, engine=""calamine"")

print(""openpyxl engine"")
print(""==============="")
print(df_openpyxl)

print(""calamine engine"")
print(""==============="")
print(df_calamine)
```

### Issue Description

The attached excel file `string_list.xlsx` contains the following data:

```
  Header
0  Alone
1   Bone
2   None
3   Cone
4   Done
```
It looks like this:

<img width=""612"" height=""452"" alt=""Image"" src=""https://github.com/user-attachments/assets/f1633bf0-e09a-469b-beb4-784d77a9d5cc"" />

When read with `read_excel()` using either the `openpyxl` or `calamine` engine it converts the string cell ""None"" to a `NaN`. The output from the above program is:

```
openpyxl engine
===============
  Header
0  Alone
1   Bone
2    NaN
3   Cone
4   Done
calamine engine
===============
  Header
0  Alone
1   Bone
2    NaN
3   Cone
4   Done
```

Note that ""None"" has changed to `NaN`.

Sample file: 

[string_list.xlsx](https://github.com/user-attachments/files/21082151/string_list.xlsx)

I checked `openpyxl`, `calamine` and `python-calamine` outside of Pandas and they each print the expected string ""None"".

### Expected Behavior

The string ""None"" from an Excel file shouldn't be interpreted as Python `None` and/or converted to `NaN`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.11.1
python-bits           : 64
OS                    : Darwin
OS-release            : 24.5.0
Version               : Darwin Kernel Version 24.5.0: Tue Apr 22 19:53:26 PDT 2025; root:xnu-11417.121.6~2/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.0
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : 3.2.5
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO Excel', 'Closing Candidate']",2025-07-05 22:56:52,2025-07-08 22:06:33,6,closed
61784,ENH: Add Coefficient of Variation to DataFrame.describe(),"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

The `DataFrame.describe()` method includes standard deviation (`std`), but its significance is hard to interpret without context, as it depends on the data’s scale. The coefficient of variation (CV = `std / mean * 100`) provides a relative measure of variability, making it easier to assess if `std` is ""big.""

### Feature Description

Add CV as a row in `DataFrame.describe()` output for numeric columns, optionally enabled via `df.describe(include_cv=True)`.

## Example
```python
import pandas as pd
data = {'A': [10, 12, 14, 15, 13], 'B': [1000, 1100, 900, 950, 1050]}
df = pd.DataFrame(data)
desc = df.describe()
desc.loc['CV (%)'] = (df.std() / df.mean() * 100)
print(desc)
```

**Output**:
```
               A            B
count   5.000000     5.000000
mean   12.800000  1000.000000
std     1.923538    79.056942
min    10.000000   900.000000
25%    12.000000   950.000000
50%    13.000000  1000.000000
75%    14.000000  1050.000000
max    15.000000  1100.000000
CV (%) 15.027641     7.905694
```

## Benefits
- **Interpretability**: CV shows relative variability, aiding comparison across columns.
- **Usability**: Simplifies exploratory data analysis.
- **Relevance**: Widely used in fields like finance and biology.

### Alternative Solutions

Users can compute CV manually, but this is less convenient.

### Additional Context

_No response_","['Enhancement', 'Closing Candidate']",2025-07-05 19:51:00,2025-07-07 21:26:48,4,closed
61783,PERF: Unnecessary string interning in read_csv?,"Going through parsers.pyx, particularly _string_box_utf8, I'm trying to figure out what the point of the hashtable checks are:

```
        k = kh_get_strbox(table, word)

        # in the hash table
        if k != table.n_buckets:
            # this increments the refcount, but need to test
            pyval = <object>table.vals[k]
        else:
            # box it. new ref?
            pyval = PyUnicode_Decode(word, strlen(word), ""utf-8"", encoding_errors)

            k = kh_put_strbox(table, word, &ret)
            table.vals[k] = <PyObject *>pyval

        result[i] = pyval
```

This was introduced in 2012 a9db003.  I don't see a clear reason why this isn't just

```
result[i] = PyUnicode_Decode(word, strlen(word), ""utf-8"", encoding_errors)
```

<s>My best guess is that it involves string interning.  Prior to py37, only small strings were interned.  Now most strings up to 4096 I think are interned.  Under the old system, the hashtable could prevent a ton of memory allocation, but that may no longer be the case.</s> No, that doesn't apply to runtime-created strings. So that may be the reason why, but if so it is still a valid one.

Does anyone have a longer memory than me on this?
","['Performance', 'Needs Triage']",2025-07-05 16:26:49,2025-07-08 14:52:42,2,closed
61780,BUG: tz_localize(None) with Arrow timestamp,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# based on test_dt_tz_localize_none
import pandas as pd
import pyarrow as pa

ts = pd.Timestamp(""2023-01-02 3:00:00"")

ser = pd.Series(
    [ts, None],
    dtype=pd.ArrowDtype(pa.timestamp(""ns"", tz=""US/Pacific"")),
)
res = ser.dt.tz_localize(None)

assert res[0] == ser[0].tz_localize(None)  # <- nope!
```

### Issue Description

The pyarrow tz_localize AFAICT is equivalent to `.tz_convert(""UTC"").tz_localize(None)`

### Expected Behavior

Equivalent to pointwise operation, matching the non-pyarrow tz_localize

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Datetime', 'Timezones', 'Arrow']",2025-07-04 15:08:39,2025-08-11 16:29:29,1,closed
61778,"BUG?: creating Categorical from pandas Index/Series with ""object"" dtype infers string","When creating a pandas Series/Index/DataFrame, I think we generally differentiate between passing a pandas object with `object` dtype and a numpy array with `object` dtype:

```
>>> pd.options.future.infer_string = True
>>> pd.Index(pd.Series([""foo"", ""bar"", ""baz""], dtype=""object""))
Index(['foo', 'bar', 'baz'], dtype='object')
>>> pd.Index(np.array([""foo"", ""bar"", ""baz""], dtype=""object""))
Index(['foo', 'bar', 'baz'], dtype='str')
```

So for pandas objects, we preserve the dtype, for numpy arrays of object dtype, we essentially treat that as a sequence of python objects where we infer the dtype (@jbrockmendel that's also your understanding?)

But for categorical that doesn't seem to happen:

```
>>> pd.options.future.infer_string = True
>>> pd.Categorical(pd.Series([""foo"", ""bar"", ""baz""], dtype=""object""))
['foo', 'bar', 'baz']
Categories (3, str): [bar, baz, foo]   # <--- categories inferred as str
```

So we want to preserver the dtype for the categories here as well?","['Dtype Conversions', 'Categorical']",2025-07-04 10:15:04,2025-10-24 17:13:26,4,closed
61775,API/BUG: different constructor behavior for numpy vs pyarrow dt64tzs,"```python
import pandas as pd

dtype1 = ""datetime64[ns, US/Eastern]""
dtype2 = ""timestamp[ns, US/Eastern][pyarrow]""

ts = pd.Timestamp(""2025-07-03 18:10"")

>>> pd.Series([ts], dtype=dtype1)[0]
Timestamp('2025-07-03 18:10:00-0400', tz='US/Eastern')

>>> pd.Series([ts], dtype=dtype2)[0]
Timestamp('2025-07-03 14:10:00-0400', tz='US/Eastern')
```

Long ago we decided that when passing tznaive datetimes and specifying a tzaware dtype, we treat the input as a wall-time.  It looks like the pyarrow path (which I'm pretty sure just ends up calling `pa.array([ts], type=...)`) treats it as a UTC time.

cc @jorisvandenbossche ","['Bug', 'API - Consistency', 'Arrow']",2025-07-04 01:16:30,2025-07-07 16:54:31,1,closed
61763,BUG: StringDtype objects from pandas <2.3.0 cannot be reliably unpickled in 2.3.0.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
### Using pandas 2.2.3
import pandas as pd

pd.DataFrame([['a', 'b'], ['c', 'd']]).astype('string').to_pickle('G:/temp/test2.pkl')
```

```python
### Using pandas 2.3.0

import pandas as pd

df = pd.read_pickle('G:/temp/test2.pkl') # looks ok

df.dtypes # raises AttributeError: 'StringDtype' object has no attribute '_na_value'

df[0] + df[1] # also raises AttributeError
```

### Issue Description

The code in a StringDtype object in 2.3 refers to an internal _na_value representation that appears not to have existed prior to 2.3.0. Pickled objects containing StringDtype columns pickled in earlier versions, including 2.2.3, may initially appear to unpickle successfully. However, listing the dtypes or even implicitly checking the dtypes by doing an operation, raises an AttributeError.

### Expected Behavior

The documentation at read_pickle indicates backward compatibility to version 0.20.3, so a pickle from 2.2.3 should be readable and usable in 2.3.0.

A current workaround is something like this, to wrap the object in a freshly created 2.3.0-compatible dtype:

```
def unpickle_wrap(fn):
   df = pd.read_pickle(fn)
   for col, dtype in df.dtypes.items():
       if pd.api.types.is_string_dtype(dtype):
           df[col] = df[col].astype(object).astype('string')
   return df
```

### Installed Versions

<details>

In [55]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.11.12
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.3.0
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.3
numba                 : 0.61.2+0.g1e70d8ceb.dirty
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.5.1
scipy                 : 1.15.2
sqlalchemy            : 2.0.41
tables                : None
tabulate              : 0.9.0
xarray                : 2025.6.1
xlrd                  : None
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>

(Edit: fixed example to make copy-pastable, and confirmed on main)","['Bug', 'Strings', 'IO Pickle']",2025-07-02 21:35:45,2025-07-07 07:41:23,1,closed
61753,BUG: Segmentation fault when misusing `VariableWindowIndexer.get_window_bounds`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas.core.indexers.objects import VariableWindowIndexer

variable_window_indexer = VariableWindowIndexer()
variable_window_indexer.get_window_bounds(1)
```

### Issue Description

Hi,

For a research paper, we carried out a large-scale benchmark of [Pynguin](https://www.pynguin.eu/), an Automatic Unit Test Generation Tool for Python, to test its new feature that can find Python interpreter crashes. In this benchmark, we found a potential bug in pandas, and we are making this issue to report it.

### Expected Behavior

In our opinion, pandas should not produce a segmentation fault when calling a public function. However, we don't know whether this function is part of pandas' public API so we just wanted to at least warn you that this behaviour exists, so that you can take the action that suits you best.

### Installed Versions

commit                : 0ab10aa1417f19ecf265ff9383b1aa851b02736b
python                : 3.10.16
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.11-300.fc42.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Tue Jun 10 16:24:16 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2192.g0ab10aa141
numpy                 : 2.2.6
dateutil              : 2.9.0.post0
pip                   : 23.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : N/A
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None","['Bug', 'Window', 'Segfault', 'Needs Triage']",2025-07-01 12:09:36,2025-07-02 16:46:47,2,closed
61746,CLN: references/tests for item_cache,"test_to_dict_of_blocks_item_cache is about _item_cache invalidation, but IIRC we got rid of that cache a while back.  grepping for ""item_cache"" i see a bunch of comments that are no longer accurate and tests that are no longer testing anything.  These can be updated/removed.",['Clean'],2025-06-30 17:44:51,2025-07-07 16:29:17,1,closed
61740,CI Failures due to new scipy and new numpydoc,"The job `Downstream Compat` is failing in CI because `statsmodels` 0.14.4 is incompatible with `scipy` 1.16.0.  The latter was released on June 22, so that's why we have recent failures.

Should we lock down the `scipy` version to 1.15.3  in `ci/deps/actions-311-downstream_compat.yaml` ?

The job `Docstring validation, typing, and other manual pre-commit hooks` is failing because `numpydoc` 1.9 was released on June 24.  Should we pin `numpydoc` to 1.8?
",['CI'],2025-06-29 18:17:31,2025-08-13 18:09:34,3,closed
61734,Removal of members from pandas-triage team,"If your Github handle is in the list below, we intend to remove you from the `pandas-triage` team due to lack of activity in the `pandas` repository since the beginning of 2024.

If you have any objection to such removal, please make a note in this issue by July 31, 2025.  Otherwise, there is no need to respond.

@paulreece
@ParfaitG
@ssche
@AlexKirko
@Moisan
@debnathshoham
@CloseChoice
@DriesSchaumont
@realead
@erfannariman
@alexhlim
@ivanovmg
@afeld
@ShaharNaveh
@charlesdong1991
@dsaxton
@arw2019
@AnnaDaglis
@moink
@smithto1
@jnothman
@martindurant
@fujiaxiang
@hasB4K
@fjetter
@cdknox",['Admin'],2025-06-29 00:42:58,2025-09-23 17:24:15,11,closed
61733,DOC: Index.infer_objects is missing from docs,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.Index.html

### Documentation problem

While `infer_objects()` is listed as a method for `pandas.Index`, the link to the actual method documentation is missing.


### Suggested fix for documentation

Probably have to add `Index.infer_objects` into the conversion section of `pandas/doc/source/reference/indexing.rst`","['Docs', 'good first issue', 'Index']",2025-06-28 18:12:20,2025-06-30 17:20:12,0,closed
61731,ENH: Type support for variables in `DataFrame.query()`,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Now using variables inside `df.query(""col > @my_var"")` doesn’t produce strong typing:
IDEs don’t catch type mismatches (e.g. `my_var` is a string but `col` is numeric).

### Feature Description

Add type support in `pandas-stubs` so that functions like `query()`:

- Accept variables bound via `@`
- Validate that their types align with the DataFrame column dtype
- Offer **autocomplete** in IDEs

Example:
```python
from typing import TypedDict

class Record(TypedDict):
    a: int
    b: str

df: DataFrame[Record] = ...
my_var: int = 5
filtered = df.query(""a > @my_var"") 

other_var: str = ""foo""
df.query(""a > @other_var"")  
# Should flag type mismatch in IDE/type-checker

### Alternative Solutions

```python
# Validate variable type before calling query
from typing import assert_type

my_var = 5
assert_type(my_var, int)  # Mypy will enforce this
df.query(""a > @my_var"")
```
OR
```python
# Type-safe alternative using boolean indexing
df[df[""a""] > my_var]  # Fully type-checkable, no strings

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-06-28 14:29:23,2025-06-29 05:00:09,0,closed
61730,BUG: `read_csv()` : inconsistent dtype and content parsing.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
""field1"" ,""field2"" ,""field3"" ,""field4"" ,""field5""      ,""field6"" ,""field7""
""1""      ,      14 ,       6 ,      21 ,""euia""        ,    0.54 ,    1
""2""      ,      30 ,       5 ,      26 ,""euia""        ,    0.82 ,    1
""2""      ,       1 ,       0 ,       0 ,""eua""         ,    0    ,    0
""3""      ,      27 ,       7 ,      17 ,""euia""        ,    1    ,    1
""4""      ,      14 ,       0 ,       9 ,""euia""        ,    0.64 ,    0.92
""4""      ,      10 ,       0 ,       0 ,""eua""         ,    0    ,    0
""9""      ,      17 ,       1 ,       6 ,""euia""        ,    0.65 ,    0.58
""10""     ,      27 ,       4 ,      13 ,""eu""          ,    1    ,     
""10""     ,         ,       0 ,       0 ,""euia""        ,    0    ,     
""12""     ,      14 ,       1 ,      13 ,""uia""         ,    1    ,    0.75
""12""     ,       5 ,       1 ,       4 ,""ui   eiuaea"" ,    1    ,    1
""13""     ,      22 ,       3 ,       7 ,"" euia""       ,    0.89 ,    1
""6""      ,      22 ,       3 ,       5 ,""euia""        ,    0.84 ,    0.79
""7""      ,      23 ,       5 ,       4 ,""uia""         ,    0.78 ,    1
""8""      ,      26 ,      11 ,       2 ,""euia""        ,    1.12 ,    1.30
""5""      ,      28 ,       3 ,       3 ,""euia""        ,    0.72 ,    0.68



import pandas as pd


pd.set_option('display.max_columns', 1000)
pd.set_option('display.max_rows', 1000)
pd.set_option('display.width', 1000)
pd.set_option(""display.max_colwidth"", None)

df = pd.read_csv(""exemple.csv"")
# df = pd.read_csv(""exemple.csv"", quoting=1)  # change nothing
list(df.columns)
df.dtypes
list(df[""field5      ""])

df = pd.read_csv(""exemple.csv"", sep=r""\s*,\s*"", engine=""python"")
list(df.columns)
df.dtypes
list(df[""field5""])

df = pd.read_csv(""exemple.csv"", quoting=2)
list(df.columns)
df.dtypes
list(df[""field5      ""])

df = pd.read_csv(""exemple.csv"", quoting=3)
list(df.columns)
df.dtypes
list(df['""field5""      '])

df = pd.read_csv(""exemple.csv"", quoting=2, dtype={""field1 "": ""object"",
                                                  ""field2 "": ""Int32"",  # fail
                                                  ""field3 "": ""int"",
                                                  ""field4 "": ""int"",
                                                  ""field5      "": ""object"",
                                                  ""field6 "": ""float"",
                                                  ""field7"": ""float""  # fail
                                                  })
```

### Issue Description

Hello,

I tried to parse a file like the exemple given, and I spent an afternoon just on this. Nothing looks logical to me. So I am sorry, I will make one ticket for everything, cause it would be to long to make one for each problem. Fill free to divide it in several task.

Expected colums dtypes look quite easy to guess to me : the user used quotemarks on `field1` to force a string type. Fields 2-4 are expected to be integers. It could be almost understandable if `field2` was converted to a float because np.int dtype doesn’t manage NA values. But Pandas has a integer type which does. So there is no reason. `Field5` should be string containing text between quotemarks. Field 6 and 7 are expected to be float. Let see what happen

First try : `df = pd.read_csv(""exemple.csv"")`

* Columns names quotemarks are removed, but trailing space are keeped. That’s quite surprising as there is no logic : Or you consider quotemarks are text delimiters and should be removed, but in this case, why to keep characters outside the delimiters ? Or you consider a everything is part of the string and in this case you must keep everything.
* dtypes are problematic:
    - `field1` have been implicitly converted to `int64`. The user explicitly asked for a `str`. The convention “what is between quotemarks is a string” is common to R, C++ and Python and wide spread. Why to not respect it
    - `field2` is converted to a string. Missing values are a common case to handle. I would understand a conversion to float, or an error raised. But why a conversion to a string ?
    - `field5` have the same problem than column names.
    - `field7` is converted to a string. Here it is not understandable at all as np.float handle NA values.
    - Other field are correct. Which is also a little surprising. So initials and trailing spaces pose problem in string fields and empty fields, but not in number field ?


Case : `df = pd.read_csv(""exemple.csv"", sep=r""\s*,\s*"", engine=""python"")`

Here init and trailing spaces are removed, but not quotemarks. This ticket is probably already opened somewhere. Field types are ok, excepted for `field2`, which should be `Int32`.


Case : `df = pd.read_csv(""exemple.csv"", quoting=2)`

Here I tried to explicitly tel the methods that quotemarks means string. Nonetheless it doesn’t work. But integer field are now floats. Excepted for `field2` and `field7` which are… strings !


Case : `df = pd.read_csv(""exemple.csv"", quoting=3)`

Here, the parsing of column names and string fields is wrong, but at least logical. It just keep everything.
Fields containing NA values are still converted to string.


Case : `df = pd.read_csv(""exemple.csv"", quoting=2, dtype={""field1 "": ""object"",
                                                  ""field2 "": ""Int32"",  # fail
                                                  ""field3 "": ""int"",
                                                  ""field4 "": ""int"",
                                                  ""field5      "": ""object"",
                                                  ""field6 "": ""float"",
                                                  ""field7"": ""float""  # fail
                                                  })`

Raise errors and doesn’t handle fields names correctly.

### Expected Behavior

No implicit conversion. Never.

For string field : I understand I may have to tweak the `quoting` and `quotechar` parameters, but once done, everything between quotemark should be string, not int or float, and white spaces outside should be ignored.

For float fields containing NA values : should be float field with NA values.

For int field containing NA values : ideally should be parsed as pandas `IntXX` which handle NA values. At minimum as a np.float. But never a string.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.13.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.12.34-1-MANJARO
Version               : #1 SMP PREEMPT_DYNAMIC Thu, 19 Jun 2025 15:49:06 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : fr_FR.UTF-8
LOCALE                : fr_FR.UTF-8

pandas                : 2.3.0
numpy                 : 2.3.1
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO CSV']",2025-06-28 10:24:36,2025-07-19 15:50:19,3,closed
61728,BUG: AttributeError in pandas.core.algorithms.diff when passing non-numeric types,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.Series([1, 2, 3]).diff(""hello"")


Raises:

AttributeError: 'str' object has no attribute 'is_integer'
```

### Issue Description

When passing non-numeric types (like strings, None, or other objects) to the `diff` function in `pandas/core/algorithms.py`, it raises an `AttributeError` instead of the expected `ValueError`. This affects any code that uses `Series.diff()`, `DataFrame.diff()`, or calls the `diff` function directly.

The issue occurs because the validation logic tries to call `n.is_integer()` on non-float objects that don't have this method, resulting in an `AttributeError`.

### Expected Behavior

```python
import pandas as pd
pd.Series([1, 2, 3]).diff(""hello"")
```

Should raise:
```
ValueError: periods must be an integer
```

### Installed Versions

<details>
python                  : 3.12.10

OS                         : Linux
OS-release            : 4.18.0-553.56.1.el8_10.x86_64
</details>
","['Bug', 'Needs Triage']",2025-06-28 08:40:10,2025-06-30 17:32:41,1,closed
61721,DOC: https://pandas.pydata.org/pandas-docs/version/2.3 does not work,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/version/2.3

### Documentation problem

https://pandas.pydata.org/pandas-docs/version/2.3 does not work, though https://pandas.pydata.org/pandas-docs/version/2.2 does; only https://pandas.pydata.org/pandas-docs/version/2.3.0 is available.

### Suggested fix for documentation

use https://pandas.pydata.org/pandas-docs/version/2.3 to be a floating release for the latest 2.3.x","['Docs', 'Web']",2025-06-27 12:29:39,2025-07-08 07:47:11,4,closed
61720,BUG: CI docstring-validation fails with AttributeError in numpydoc validate.py,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.

### Reproducible Example

The failure is visible in GitHub Actions.

**Steps**
1. Push any branch that is up-to-date with `pandas-dev/main`, **or** open a fresh PR.
2. Observe workflow **Code Checks / Docstring validation, typing, and other manual pre-commit hooks**.
3. The job stops in step *Run docstring validation* with the traceback below.

**Example failing runs (public logs)** 
- https://github.com/pandas-dev/pandas/actions/runs/15921431436 ← PR #61718  
- https://github.com/pandas-dev/pandas/actions/runs/15886481522 ← another PR on latest main

```text
File "".../site-packages/numpydoc/validate.py"", line 234, in name
    return ""."".join([self.obj.__module__, self.obj.__name__])
AttributeError: 'getset_descriptor' object has no attribute '__module__'. Did you mean: '__reduce__'?
```

### Issue Description

* The *docstring-validation* job crashes before any pandas code is executed, so all current PRs fail.
* The stack trace originates inside **numpydoc/validate.py**; no pandas files are involved.

### Expected Behavior

The *docstring-validation* step should complete without errors, allowing the entire CI workflow to finish green.

### Installed Versions

<details>

* python : 3.11.13  (conda-forge)
* pandas : source checkout of current `main` (not installed / build failed locally)
* numpydoc: 1.8.0
* os : Ubuntu-22.04 (GitHub Actions runner)

</details>
","['Bug', 'CI']",2025-06-27 10:00:20,2025-06-27 23:40:55,2,closed
61715,BUG/API: floordiv by zero in Int64Dtype,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
ser = pd.Series([0, 1])
ser2 = ser.astype(""Int64"")

>>> ser // 0
0    NaN
1    inf
dtype: float64

>>> ser2 // 0
0    0
1    0
dtype: Int64

# with int64[pyarrow] this just raises pyarrow.lib.ArrowInvalid: divide by zero
```

### Issue Description

We patch the results of floordiv in dispatch_fill_zeros, but don't do this for the masked dtypes, and the pyarrow one raises.

### Expected Behavior

Ideally these would be consistent across backends.

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2025-06-26 22:26:46,2025-06-27 01:50:40,2,closed
61713,BUG: Inconsitent behaviour for different backends due to nullable bool values,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
all(pd.Series([None, 3,5], dtype=float) > 3)
all(pd.Series([None, 3,5], dtype='float[pyarrow]') > 3)
```

### Issue Description

Do to the pyarrow nullable bool type, there is a TypeError and the behaviour is inconsistent:

```python
all(pd.Series([None, 3,5], dtype=float) > 3)
```
Out[10]: False`


```python 
all(pd.Series([None, 3,5], dtype='float[pyarrow]') > 3)
```
Traceback (most recent call last):
  File ""C:\Users\Schleehauf\PycharmProjects\viodata\viotools\.venv\Lib\site-packages\IPython\core\interactiveshell.py"", line 3672, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-9-43ea68ea33b1>"", line 1, in <module>
    all(pd.Series([None, 3,5], dtype='float[pyarrow]') > 3)
  File ""pandas/_libs/missing.pyx"", line 392, in pandas._libs.missing.NAType.__bool__
TypeError: boolean value of NA is ambiguous

### Expected Behavior

Be consitant, fill the bool NA value with False for the next xxx releases and maybe add a DeprecationWarning

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.12.10
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : de_DE.cp1252
pandas                : 2.3.0
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.3
numba                 : 0.61.2
numexpr               : 2.11.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2025.5.1
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.2
xlsxwriter            : 3.2.5
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'PDEP missing values']",2025-06-26 14:25:31,2025-07-19 17:00:06,1,closed
61691,Proposal: Add pd.check(df) utility function for quick dataset diagnostics,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

While working with pandas DataFrames during exploratory data analysis (EDA), analysts frequently perform the same manual steps to understand their dataset:

- Count null and non-null values
- Check unique value counts
- Estimate missing percentages

These operations are often repeated multiple times, especially after data cleaning, filtering, or merging. Currently, users rely on combinations like:
```
df.isnull().sum()
df.nunique()
df.notnull().sum()
```
There is no single built-in pandas utility that offers this all-in-one diagnostic view.

### Feature Description

Add a utility function pd.check(df) that returns a concise column-wise summary of a DataFrame’s structure, including:

- Unique values per column
- Non-null value counts
- Missing value counts
- Missing percentages (rounded to 2 decimals by default)

This function is designed to streamline early-stage exploratory data analysis by combining multiple common pandas operations into one, reusable utility.

Suggested API:
`def check(df: pd.DataFrame, round_digits: int = 2) -> pd.DataFrame:
    ...
`
- Optional round_digits parameter to control percentage precision
- Returns a pandas DataFrame
- No side effects (no printing)
- Aligns well with other utility functions like pd.describe()

### Alternative Solutions

There are existing pandas functions like:

- `df.info()` – shows non-null counts and data types
- `df.describe() `– provides statistical summaries (only for numeric data)
- `df.isnull().sum()` – shows missing values per column
- `df.nunique() `– shows unique counts

However, none of these provide a combined summary in a single DataFrame format. Users must manually combine several operations, which can be repetitive and error-prone.

Third-party options:

**pandas-profiling** and **sweetviz** offer full data profiling, but they are heavy-weight, generate HTML reports, and not ideal for lightweight inspection or script-based pipelines.

My package [pandas_eda_check](https://pypi.org/project/pandas-eda-check/) implements this specific summary cleanly and could be a minimal addition to pandas.

### Additional Context

Why in pandas?

- Aligns with pandas’ mission of being a one-stop shop for tabular data operations
- Adds convenience and consistency to common EDA workflows
- Minimal overhead and easy to implement
- Could serve as a precursor to a more comprehensive eda submodule in the future

Reference Implementation

I've implemented this in an open-source utility here:
🔗 https://github.com/CS-Ponkoj/pandas_eda_check

PyPI: https://pypi.org/project/pandas-eda-check/

Open to Feedback

I’d love to hear from the maintainers and community about:

- Whether this function aligns with pandas’ philosophy
- Suggestions to improve API or return format
- If accepted, I’m happy to submit a PR with tests and docs

Thanks for your time and consideration.

Ponkoj Shill
PhD Candidate, ML Engineer
Email: [csponkoj@gmail.com](mailto:csponkoj@gmail.com)","['Enhancement', 'Needs Triage']",2025-06-23 07:49:28,2025-08-27 18:26:48,2,closed
61690,DOC: The Slack invite link to join the Pandas Dev community is broken,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/development/community.html#community-slack

### Documentation problem

This issue was raised before #61298 but I do not have permission to reopen the issue. On clicking the link, one lands on [this page](https://pandas-dev-community.slack.com/join/shared_invite/zt-2blg6u9k3-K6_XvMRDZWeH7Id274UeIg#/shared-invite/error)

![Image](https://github.com/user-attachments/assets/6eb48c88-0794-451e-b6ed-368b24dda377)

### Suggested fix for documentation

Someone from the Slack admin team needs to update the link to the documentation. ","['Docs', 'Needs Triage', 'Community']",2025-06-23 04:24:46,2025-06-25 11:23:03,3,closed
61687,BUG: DataFrame.mul() corrupts data by setting values to zero,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
import sys


# Create DataFrame with datetime index and multiple columns
# This reproduces the bug with ~6 years of hourly data (2033-2038)
np.random.seed(42)
date_range = pd.date_range('2033-01-01', '2038-12-31 23:00:00', freq='H')
n_cols = 40
data = np.random.rand(len(date_range), n_cols) * 0.1  # Values between 0 and 0.1

df = pd.DataFrame(data, index=date_range, columns=range(n_cols))

# Create a Series of ones with the same index
ones_series = pd.Series(1.0, index=df.index)

print(f""DataFrame shape: {df.shape}"")
print(f""Memory usage (MB): {df.memory_usage(deep=True).sum() / 1024**2:.2f}"")
print(f""Original data sample (should be > 0):"")
print(df.iloc[32:37, 23])  # Show some sample values

# Perform the multiplication that causes corruption
print(""\nPerforming multiplication..."")
result = df.mul(ones_series, axis=0)

# Check for corruption
print(f""After multiplication (should be identical):"")
print(result.iloc[32:37, 23])

# Verify corruption
are_equal = df.equals(result)
print(f""\nDataFrames equal: {are_equal}"")

if not are_equal:
    # Count corrupted values
    diff_mask = df.values != result.values
    n_corrupted = diff_mask.sum()
    print(f""CORRUPTION DETECTED: {n_corrupted} values corrupted!"")
    
    # Show corruption details
    corrupted_rows, corrupted_cols = np.where(diff_mask)
    if len(corrupted_rows) > 0:
        print(f""\nCorruption sample:"")
        for i in range(min(5, len(corrupted_rows))):
            row, col = corrupted_rows[i], corrupted_cols[i]
            original = df.iloc[row, col]
            corrupted = result.iloc[row, col]
            date = df.index[row]
            print(f""  {date}, Col {col}: {original:.4f} -> {corrupted:.4f}"")
    
    # Verify that corrupted values are zeros
    corrupted_values = result.values[diff_mask]
    all_zeros = np.all(corrupted_values == 0.0)
    print(f""\nAre all corrupted values zero? {all_zeros}"")
    
    # Show which columns are affected
    unique_affected_cols = np.unique(corrupted_cols)
    print(f""Number of affected columns: {len(unique_affected_cols)}"")
    print(f""Affected columns: {unique_affected_cols}"")

# Demonstrate that numpy approach works correctly
print(f""\nTesting numpy workaround..."")
numpy_result = pd.DataFrame(
    df.to_numpy() * ones_series.to_numpy()[:, None],
    index=df.index,
    columns=df.columns
)

numpy_works = df.equals(numpy_result)
print(f""Numpy approach works correctly: {numpy_works}"")
```

### Issue Description

The DataFrame.mul() method is corrupting data by setting non-zero values to zero when multiplying a DataFrame with datetime index by a Series of ones. This occurs only under specific conditions related to DataFrame size and affects data integrity.

### Expected Behavior

When multiplying a DataFrame by a Series of ones using df.mul(ones_series, axis=0), all original values should be preserved (multiplied by 1.0).

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.11
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.14393
machine               : AMD64
processor             : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.3.0
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : 8.1.3
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : None
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.37
tables                : 3.10.2
tabulate              : None
xarray                : 2025.1.1
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : 2.4.3
pyqt5                 : None

</details>
","['Bug', 'Numeric Operations', 'Needs Info', 'Closing Candidate']",2025-06-22 10:03:03,2025-07-23 09:12:16,6,closed
61675,BUG: DataFrame.join(other) raises InvalidIndexError if column index is CategoricalIndex,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

cat_data = pd.Categorical([15, 16, 17, 18], categories=pd.Series(list(range(3, 24)), dtype=""Int64""), ordered=True)
df1 = pd.DataFrame({""hr"": cat_data, ""values1"": ""a b c d"".split()}).set_index(""hr"")
df2 = pd.DataFrame({""hr"": cat_data, ""values2"": ""xyzzy foo bar ..."".split()}).set_index(""hr"")
df1.columns = pd.CategoricalIndex([4], dtype=cat_data.dtype, name=""other_hr"")
df2.columns = pd.CategoricalIndex([3], dtype=cat_data.dtype, name=""other_hr"")

print(pd.__version__)

# Clunky, but works
df_joined_1 = df1.reset_index(level=""hr"").merge(df2.reset_index(level=""hr""), on=""hr"").set_index(""hr"")

# Works on 1.4.4 and nightly (3.0.0.dev0+2177.g8a1d5a06f9), not 2.2.3 or 2.3.0
df_joined_2 = df1.join(df2)

# returns True... assuming we got this far
df_joined_1.equals(df_joined_2)
```

### Issue Description

`join`ing two DataFrames which have a `CategoricalIndex` as columns (for example, due to having pivoted on a categorical column) results in an InvalidIndexError (see below) on Pandas versions 2.2.3 and 2.3.0. The same code works with 1.4.4 (from which I am trying to migrate to 2.x) and nightly.

While the issue does not manifest in nightly, I am still reporting it in the hopes of getting it fixed in future 2.x releases.

### Expected Behavior

Code executes successfully and the last statement returns `True`

### Installed Versions

Pandas 1.4.4 (works)

<details>
INSTALLED VERSIONS
------------------
commit           : ca60aab7340d9989d9428e11a51467658190bb6b
python           : 3.10.16.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 24.5.0
Version          : Darwin Kernel Version 24.5.0: Tue Apr 22 19:53:27 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6041
machine          : arm64
processor        : arm
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.4.4
numpy            : 1.23.5
pytz             : 2025.2
dateutil         : 2.9.0.post0
setuptools       : 80.7.1
pip              : 25.1.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 8.36.0
pandas_datareader: None
bs4              : None
bottleneck       : None
brotli           : None
fastparquet      : None
fsspec           : None
gcsfs            : None
markupsafe       : None
matplotlib       : 3.10.3
numba            : None
numexpr          : None
odfpy            : None
openpyxl         : 3.1.2
pandas_gbq       : None
pyarrow          : 8.0.0
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : None
snappy           : None
sqlalchemy       : 2.0.41
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
zstandard        : None
</details>

Pandas 2.2.3 (does not work)

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Darwin
OS-release            : 24.5.0
Version               : Darwin Kernel Version 24.5.0: Tue Apr 22 19:53:27 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6041
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 9.0.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.39
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
</details>

Pandas 2.3.0 (does not work)

<details>
INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.12.9
python-bits           : 64
OS                    : Darwin
OS-release            : 24.5.0
Version               : Darwin Kernel Version 24.5.0: Tue Apr 22 19:53:27 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6041
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.0
numpy                 : 2.3.0
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>

Pandas nightly (works)

<details>
INSTALLED VERSIONS
------------------
commit                : 8a1d5a06f9fb3c232249e3ed301932053efb06d8
python                : 3.12.9
python-bits           : 64
OS                    : Darwin
OS-release            : 24.5.0
Version               : Darwin Kernel Version 24.5.0: Tue Apr 22 19:53:27 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6041
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2177.g8a1d5a06f9
numpy                 : 2.4.0.dev0+git20250617.32f4afa
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyiceberg             : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>

### Error traceback

On Pandas 2.3.3:

```
Traceback (most recent call last):
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/indexes/base.py"", line 3812, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""pandas/_libs/index.pyx"", line 167, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 175, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index_class_helper.pxi"", line 86, in pandas._libs.index.MaskedInt64Engine._check_type
KeyError: slice(None, None, None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/frame.py"", line 10764, in join
    return merge(
           ^^^^^^
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/reshape/merge.py"", line 184, in merge
    return op.get_result(copy=copy)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/reshape/merge.py"", line 888, in get_result
    result = self._reindex_and_concat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/reshape/merge.py"", line 837, in _reindex_and_concat
    left = self.left[:]
           ~~~~~~~~~^^^
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/frame.py"", line 4080, in __getitem__
    and key in self.columns
        ^^^^^^^^^^^^^^^^^^^
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/indexes/category.py"", line 368, in __contains__
    return contains(self, key, container=self._engine)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/arrays/categorical.py"", line 230, in contains
    loc = cat.categories.get_loc(key)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/voipiti/.pyenv/versions/pd23-312/lib/python3.12/site-packages/pandas/core/indexes/base.py"", line 3818, in get_loc
    raise InvalidIndexError(key)
pandas.errors.InvalidIndexError: slice(None, None, None)
```","['Bug', 'Reshaping', 'Regression', 'Categorical']",2025-06-18 07:59:14,2025-11-01 20:37:39,9,closed
61669,ENH: Switch to trusted publishing for package upload to PyPI in CI,"### Feature Type

- [ ] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I would like to audit the `pandas` wheel easily.

### Feature Description

Trusted publishing (with attestations) means I can know for certain that what I download from PyPI is the same artefact which was generated in GitHub CI, meaning that what I see in GitHub is the same as what is installed - handy for auditing (rather than having to manually review all of the installed files on each release).

See [the Python packaging documentation](https://packaging.python.org/en/latest/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/#configuring-trusted-publishing), [the PyPI documentation](https://docs.pypi.org/trusted-publishers/), and [the official pypi-publish GitHub action documentation](https://github.com/pypa/gh-action-pypi-publish?tab=readme-ov-file#trusted-publishing) on trusted publishing - you'll need to configure an environment in PyPI and GitHub.

### Alternative Solutions

Manually review all of the installed files on each release

### Additional Context

_No response_","['Enhancement', 'Build', 'CI']",2025-06-16 12:56:12,2025-09-21 07:27:07,1,closed
61665,ENH: Support for Orthodox Easter,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

The [pandas.tseries.offsets.Easter](https://github.com/pandas-dev/pandas/blob/c067bcd701e6cb4125e869a2802ef867d8395800/pandas/_libs/tslibs/offsets.pyx#L4511) class currently calculates the date of Western Easter only. However, it does not support the calculation of Orthodox Easter.

This limitation makes it more difficult to work with holidays that are relative to Orthodox Easter, such as Orthodox Good Friday and Orthodox Easter Monday.


### Feature Description

With a small and fully backwards-compatible change to the [pandas.tseries.offsets.Easter](https://github.com/pandas-dev/pandas/blob/c067bcd701e6cb4125e869a2802ef867d8395800/pandas/_libs/tslibs/offsets.pyx#L4511) class, support for Orthodox Easter (and Julian Easter) can be added by introducing an optional `method` parameter to the `Easter` constructor.

This `method` parameter specifies the method to use for calculating easter and would then be passed to [dateutil.easter](https://dateutil.readthedocs.io/en/stable/easter.html), which is used internally by the Easter class.

Usage example:
```python
from dateutil.easter import EASTER_ORTHODOX

OrthodoxGoodFriday = Holiday(""Good Friday"", month=1, day=1, offset=[Easter(method=EASTER_ORTHODOX), Day(-2)])
OrthodoxEasterMonday = Holiday(""Easter Monday"", month=1, day=1, offset=[Easter(method=EASTER_ORTHODOX), Day(1)])
```

This is similar to how the [GoodFriday](https://github.com/pandas-dev/pandas/blob/c067bcd701e6cb4125e869a2802ef867d8395800/pandas/tseries/holiday.py#L609) and [EasterMonday](https://github.com/pandas-dev/pandas/blob/c067bcd701e6cb4125e869a2802ef867d8395800/pandas/tseries/holiday.py#L611)  holidays for Western Easter are implemented in the `pandas.tseries.holiday` module.

### Alternative Solutions

An alternative solution, without modifying the Easter class as suggested, is to use the observance parameter.

```python
def calculate_orthodox_good_friday(dt):
    offset = easter(dt.year, method=EASTER_ORTHODOX) - timedelta(days=2) - dt.date()
    return dt + offset

OrthodoxGoodFriday = Holiday(
    ""Good Friday"",
    month=1,
    day=1,
    observance=calculate_orthodox_good_friday)
```
","['Enhancement', 'Needs Triage']",2025-06-16 05:17:43,2025-06-16 19:39:28,0,closed
61663,BUG: Incorrect guess_datetime_format response,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

print(pd.tseries.api.guess_datetime_format('2025-06-15T21:25:00.000000Z'))
print(pd.tseries.api.guess_datetime_format('2025-06-15T20:24:00.000000Z'))
print(pd.tseries.api.guess_datetime_format('2025-06-15T20:25:00.000000Z'))

# %Y-%m-%dT%H:%M:%S.%f%z
# %Y-%m-%dT%H:%M:%S.%f%z
# None
```

### Issue Description

I'm receiving a strange `None` from `guess_datetime_format` for a very particular string combination. I can change the hours and minutes separately and it works fine, but when I set the time to exactly `20:25` it produces a None result.

### Expected Behavior

It should produce the same `'%Y-%m-%dT%H:%M:%S.%f%z'` format as the other two examples.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 6.15.1-061501-generic
Version               : #202506041425 SMP PREEMPT_DYNAMIC Wed Jun  4 18:01:32 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : C.UTF-8
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.3.0
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 0.29.37
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'Duplicate Report', 'Needs Triage']",2025-06-15 20:46:06,2025-06-16 18:53:51,5,closed
61662,DOC: Improve documentation for DataFrame.__setitem__ and .loc assignment from Series,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

pandas.DataFrame.__setitem__
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.__setitem__.html

pandas.core.indexing.IndexingMixin.loc
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html

User Guide: Indexing and Selecting Data
https://pandas.pydata.org/docs/user_guide/indexing.html


### Documentation problem

*Documentation Enhancement**

    The following behavior is not clearly explained in the documentation:

    ```python
    import pandas as pd

    df = pd.DataFrame({'a': [1, 2, 3]})
    df['b'] = pd.Series({1: 'b'})
    print(df)

    # Output:
    #    a    b
    # 0  1  NaN
    # 1  2    b
    # 2  3  NaN
    ```

    - The Series is **reindexed** to match the DataFrame index.
    - Values are inserted **by index label**, not by position.
    - Missing labels yield **NaN**, and the order is adjusted accordingly.

    This behavior is:
    - Not explained in the `__setitem__` documentation (which is missing entirely).
    - Only mentioned vaguely in `.loc` docs, with no example.
    - Absent from the ""Indexing and Selecting Data"" user guide when assigning Series with unordered or partial index.

### Suggested fix for documentation

1. **Add docstring for `DataFrame.__setitem__`** with clear explanation that:
       > When assigning a Series, pandas aligns on index. Values in the Series that don't match an index label will result in `NaN`.

    2. **Update `.loc` documentation**:
       Include a note that when assigning a Series to `.loc[row_labels, col]`, pandas aligns the Series by index and **not by order**.

    3. **Add example in the User Guide** under:
       [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)
       > Assigning a Series with unordered/missing index keys to a DataFrame column.

    **Suggested example:**

    ```python
    df = pd.DataFrame({'a': [1, 2, 3]})
    s = pd.Series({2: 'zero', 1: 'one', 0: 'two'})
    df['d'] = s

    # Output:
    #    a     d
    # 0  1   two
    # 1  2   one
    # 2  3  zero
    ```

    ### 📈 Why this is better:

    The current documentation is incomplete and vague about how Series alignment works in assignments. This fix:

    - Makes `__setitem__` behavior explicit and discoverable.
    - Improves `.loc` docs with better clarity and practical context.
    - Adds real-world examples to the user guide to reduce silent bugs and confusion.

    These improvements help all users—especially beginners—understand how pandas handles Series assignment internally.","['Docs', 'Needs Triage']",2025-06-15 15:06:12,2025-08-01 15:31:05,1,closed
61645,BUG: `Series.std` and `Series.var` give incorrect results for complex values.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

arr = np.array([-1j, 0j, 1j], dtype=complex)
s = pd.Series(arr, dtype=complex)

print(arr.std(ddof=0))  # 0.816496580927726
print(s.std(ddof=0))  # nan
print(arr.var(ddof=0))  # 0.666
print(s.var(ddof=0))  # -0.666
```

### Issue Description

1. The results diverge from numpy.
2. pandas yields nonsensical results like negative floats.

### Expected Behavior

For complex variables, `std` and `var` should give non-negative floating results. Recall that $` σ ≔ \sqrt{𝔼|x-μ|^2 } `$. Often, authors that only use real-valued variables leave out the absolute value, which I guess is what happened here.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.13.4
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.0-26-generic
Version               : #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.0
numpy                 : 2.3.0
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : 8.2.3
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : 6.135.9
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.0
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-06-13 09:27:37,2025-06-16 17:13:27,0,closed
61643,BUG: replace value failed,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

start_time = '2025-06-06'
end_time = '2025-06-09'

sig1 = pd.read_parquet('data1.par')
sig1 = sig1[(sig1.tradeDate >= start_time) & (sig1.tradeDate <= end_time)]
sig1 = sig1.pivot(index='tradeDate', columns='ticker', values='signal_value').fillna(0)

sig2 = pd.read_parquet('data2.par')
sig2 = sig2[(sig2.tradeDate >= start_time) & (sig2.tradeDate <= end_time)]
sig2 = sig2.pivot(index='tradeDate', columns='ticker', values='signal_value').fillna(0)

sig = sig1 + sig2

filt = pd.read_feather('filter.fea').set_index('tradeDate')
filt.index = pd.to_datetime(filt.index)
filt = filt.reindex(sig.index, columns=sig.columns)

# method 1: make a copy then filter
s1 = sig.copy()
s1.values[:] = np.where(filt == 1, s1, np.nan)
print(s1.count(axis=1))

# method 2: directly filter
sig.values[:] = np.where(filt == 1, sig, np.nan)
print(sig.count(axis=1))
```

### Issue Description

Why not work：

    sig.values[:] = np.where(filt == 1, sig, np.nan)

If using a copy, the sentence above works:

    s1 = sig.copy()
    s1.values[:] = np.where(filt == 1, s1, np.nan)

### Expected Behavior

Both methods should work.

### Installed Versions

[data.zip](https://github.com/user-attachments/files/20718931/data.zip)

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.7.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 24.5.0
Version               : Darwin Kernel Version 24.5.0: Tue Apr 22 19:48:46 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8103
machine               : arm64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 75.1.0
pip                   : 24.2
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.1
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.27.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.6.1
scipy                 : 1.13.1
sqlalchemy            : 2.0.34
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2023.6.0
xlrd                  : None
zstandard             : 0.23.0
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Needs Info', 'replace']",2025-06-13 03:08:09,2025-08-05 17:10:35,5,closed
61636,BUG: Groupby aggregate coersion of outputs inconsistency for pyarrow dtypes,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from pyarrow import string

df = pd.DataFrame([
    [0,""X"",""A""],
    [1,""X"",""A""],
    [2,""X"",""A""],
    [3,""X"",""B""],
    [4,""X"",""B""],
    [5,""X"",""B""],], columns = [""a"",""b"",""c""]).astype({""a"":int,
    ""b"":str,""c"":pd.ArrowDtype(string())})

df.set_index(""b"").groupby(""a"").agg(lambda df: df.to_dict())
```

### Issue Description

When applying groupby aggregate on a column with type defined using `pd.ArrowDtype()` the pandas tries to cast the output into the original type, which can raise an error (e.g. `pyarrow.lib.ArrowNotImplementedError: Unsupported cast from struct<location_abbreviation: string> to utf8 using function cast_string` for the example provided).


For example, if `string[pyarrow]` is used, then this behaviour doesn't occur:

```python
import pandas as pd


df = pd.DataFrame([
    [0,""X"",""A""],
    [1,""X"",""A""],
    [2,""X"",""A""],
    [3,""X"",""B""],
    [4,""X"",""B""],
    [5,""X"",""B""],], columns = [""a"",""b"",""c""]).astype({""a"":int,
    ""b"":str,""c"":""string[pyarrow]""})

df.set_index(""b"").groupby(""a"").agg(lambda df: df.to_dict())

```

Or if the user-defined function also has `*args` or `**kwargs`, this coercion is not applied:
```python
import pandas as pd


df = pd.DataFrame([
    [0,""X"",""A""],
    [1,""X"",""A""],
    [2,""X"",""A""],
    [3,""X"",""B""],
    [4,""X"",""B""],
    [5,""X"",""B""],], columns = [""a"",""b"",""c""]).astype({""a"":int,
    ""b"":str,""c"":pd.ArrowDtype(string()})

df.set_index(""b"").groupby(""a"").agg(lambda df, _: df.to_dict(), [])
```
both returns:

|   a | c          |
|----:|:-----------|
|   0 | {'X': 'A'} |
|   1 | {'X': 'A'} |
|   2 | {'X': 'A'} |
|   3 | {'X': 'B'} |
|   4 | {'X': 'B'} |
|   5 | {'X': 'B'} |


### Expected Behavior

I would expect the code from example to return:
|   a | c          |
|----:|:-----------|
|   0 | {'X': 'A'} |
|   1 | {'X': 'A'} |
|   2 | {'X': 'A'} |
|   3 | {'X': 'B'} |
|   4 | {'X': 'B'} |
|   5 | {'X': 'B'} |


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.11.6
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.223-211.872.amzn2.x86_64
Version               : #1 SMP Mon Jul 29 19:52:29 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.0
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : 6.135.0
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : 7.4.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'Arrow']",2025-06-12 11:37:40,2025-07-29 16:13:54,2,closed
61631,DOC: Description of pandas_datetime_exec function,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/_libs/src/datetime/pd_datetime.c

### Documentation problem

The file pd_datetime.c has missing documentation on line 195 for the function static int pandas_datetime_exec(PyObject *Py_UNUSED(module)). We need to add documentation for what the role of this function is.

### Suggested fix for documentation

The suggested fix is to add documentation for the function that has been defined on line 195.

The function initializes and exposes a custom datetime C-API from the Pandas library by creating a PyCapsule that stores function pointers, which can be accessed later by other C code (or Cython code) that imports the capsule.","['Docs', 'Needs Triage']",2025-06-11 04:32:00,2025-06-30 18:12:43,2,closed
61627,"BUG: the behavior of DataFrameGroupBy.apply(..., include_groups=True) breaks post-mortem debugging","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

def f(df):
    df[""group""]
    raise TypeError(""a very subtle bug"")

pd.DataFrame({""group"": [""a"", ""a"", ""b"", ""b""], ""data"": [0, 1, 2, 3]}).groupby(""group"").apply(f)
```

### Issue Description

The argument in the title and the corresponding behavior is described like this:

```
When True, will attempt to apply func to the groupings in the case that they are columns of the DataFrame.
If this raises a TypeError, the result will be computed with the groupings excluded. When False,
the groupings will be excluded when applying func.
```

https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html

I think the described behavior is problematic and it renders close to impossible to use post-mortem debugging for the `TypeError(""a very subtle bug"")`. pandas should not swallow `TypeError` hoping that developers will figure it out in a pile of logs.

### Expected Behavior

There should be no ""attempts"" from the docs and pandas should not catch and swallow any exceptions from the payload.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.13.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.9-300.fc42.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Thu May 29 14:27:53 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.0
numpy                 : 2.3.0
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Groupby', 'Apply']",2025-06-10 20:00:05,2025-07-31 02:07:16,2,closed
61626,DOC: Pandas contributor take limit,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/development/contributing.html#id2

### Documentation problem

I don't believe it is documented anywhere but I think there is a two assigned task limit for issues. Currently, whenever I type ""take"" the bot doesn't auto assign me a task.

I think this should be documented as I have one completed issue which is still waiting for a PR review, and another which needs further discussion during a meeting. It's not like I'm just randomly taking tasks. Other people could run into something similar. A maintainer should verify that there is a limit before we edit the documentation first though.

Example:
https://github.com/pandas-dev/pandas/issues/61583#issuecomment-2960369848
https://github.com/pandas-dev/pandas/issues/61511#issuecomment-2932438936

### Suggested fix for documentation

Just add that there is a limit to the number of issues you can concurrently take and to contact a maintainer if you run into issues and need more (up to maintainer discretion on the last part)",['Docs'],2025-06-10 19:38:27,2025-06-22 11:21:17,6,closed
61622,BUG: CoW - eq not implemented for <class 'pandas.core.internals.blocks.ExtensionBlock'>,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.options.mode.copy_on_write = True

idx = pd.Index(['a', 'b', 'c'], dtype=""string[pyarrow]"")
pd.Series(idx).replace({""z"": ""b"", ""a"": ""d""})
```

### Issue Description

The above code raises the following issue:
`NotImplementedError: eq not implemented for <class 'pandas.core.internals.blocks.ExtensionBlock'>`

### Expected Behavior

The code should run without raising any error as it does without the CoW clause, shouldn't it?

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.12.5
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_United Kingdom.1252

pandas                : 2.3.0
numpy                 : 2.1.3
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : 2.0.35
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'replace', 'Copy / view semantics']",2025-06-10 11:01:53,2025-07-28 16:24:08,2,closed
61621,BUG: infer_dtype result for float with embedded pd.NA,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas.api.types import infer_dtype
assert infer_dtype(pd.Series([1.,2.,.3,pd.NA], dtype=object)) ==  infer_dtype(pd.Series([1.,2.,.3,np.nan], dtype=object))
```

### Issue Description

Dear pandas-folks,

This was checked for pandas V 2.3.0 and 2.2.X

When using pandas' `infer_dtype` on an object array consisting out of floats with embedded `pd.NA`, the result will be `mixed-integer-float` tough `skipna` is `True` as a default.

The same test for embedded `np.nan` returns `floating`.

```python
    >>> from pandas.api.types import infer_dtype
    >>> infer_dtype(pd.Series([1,2,3,pd.NA], dtype=object))
    'integer'
    >>> infer_dtype(pd.Series([1,2,3,np.nan], dtype=object))
    'integer'
    >>> infer_dtype(pd.Series([1.,2.,.3,pd.NA], dtype=object))
    'mixed-integer-float' v <<< should be `floating`
    >>> infer_dtype(pd.Series([1.,2.,.3,np.nan], dtype=object))
    'floating'
    >>> infer_dtype(pd.Series(['1.0', np.nan],dtype=object))
    'string'
    >>> infer_dtype(pd.Series(['1.0', pd.NA],dtype=object))
    'string'
```

In case of other types, like integer or strings, the function does not produce a false / different output w.r.t. the na-type.


Context, I am maintaining a small project which assures integers in columns to stay integers - a common known issue. I you know of a well established extension for this purpose, feel free to point me towards it. 

### Expected Behavior

`>>> infer_dtype(pd.Series([1.,2.,.3,pd.NA], dtype=object))` should return `floating`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.13.3
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-553.51.1.el8_10.x86_64
Version               : #1 SMP Fri Apr 25 00:55:37 EDT 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.0
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.2.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Missing-data', 'Dtype Conversions']",2025-06-10 09:15:31,2025-07-11 19:08:58,5,closed
61616,CI: New NumPy release breaking Numba in our CI,"```
pandas/tests/groupby/aggregate/test_numba.py:19: in <module>
    numba = pytest.importorskip(""numba"")
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   pytest.PytestDeprecationWarning: 
E   Module 'numba' was found, but when imported by pytest it raised:
E       ImportError('Numba needs NumPy 2.2 or less. Got NumPy 2.3.')
E   In pytest 9.1 this warning will become an error by default.
E   You can fix the underlying problem, or alternatively overwrite this behavior and silence this warning by passing exc_type=ImportError explicitly.
E   See https://docs.pytest.org/en/stable/deprecations.html#pytest-importorskip-default-behavior-regarding-importerror
```
Source: https://github.com/pandas-dev/pandas/actions/runs/15541539524/job/43753280349#step:9:76

Somehow related: https://github.com/numba/numba/issues/10105

@pandas-dev/pandas-core I guess it's not the first time this happens, since seems that Numba has been raising `ImportError` instead of pining the dependencies for at least one more version. Doesn't seem like we should be pinning NumPy ourselves. Any idea how this was fixed before if it already happened, or what should we do to fix the CI?","['CI', 'Dependencies']",2025-06-09 19:28:13,2025-10-13 20:52:48,3,closed
61610,BUG: `explode()` converts timestamps at millisecond resolution in DatetimeIndex to nanosecond resolution,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
test = pd.Series([pd.date_range(""2020-01-01T00:00:00Z"", ""2020-01-01T02:00:00Z"", freq=""1h"", unit=""ms"")])
test.explode().dtype
```

### Issue Description

The docs for `pd.date_range` state that the `unit` keyword argument is the resolution of timestamps in the returned DatetimeIndex, which is true---and counter to the usage of `unit` elsewhere, e.g. in `pd.to_datetime`. Regardless of this discrepancy, `explode` does not respect the millisecond resolution of timestamps in a DatetimeIndex, converting them to nanosecond resolution in the returned Series or DataFrame.

### Expected Behavior

dtypes should not be changed by `explode`.

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.11.13
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-139-generic
Version               : #149~20.04.1-Ubuntu SMP Wed Apr 16 08:29:56 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.3.0
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : 8.2.3
IPython               : 9.3.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.4.0
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.5.1
scipy                 : 1.15.3
sqlalchemy            : 2.0.41
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
","['Bug', 'Needs Triage']",2025-06-08 18:02:53,2025-06-11 22:40:10,2,closed
61602,BUG: Writing UUIDs fail,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> df = pd.DataFrame({'id': [uuid.uuid4(), uuid.uuid4(), uuid.uuid4()]})
>>> df
                                     id
0  6f6303cd-516d-4a27-9165-bb703f9e2240
1  c250ba7f-31db-47de-b02b-54296ac6a4df
2  c523257a-51ab-4160-957b-619ce55c78f9
>>> df.to_parquet('sample_pandas_pa.parquet', engine='pyarrow')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".venv/lib/python3.12/site-packages/pandas/util/_decorators.py"", line 333, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/pandas/core/frame.py"", line 3113, in to_parquet
    return to_parquet(
           ^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/pandas/io/parquet.py"", line 480, in to_parquet
    impl.write(
  File "".venv/lib/python3.12/site-packages/pandas/io/parquet.py"", line 190, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow/table.pxi"", line 4793, in pyarrow.lib.Table.from_pandas
  File "".venv/lib/python3.12/site-packages/pyarrow/pandas_compat.py"", line 639, in dataframe_to_arrays
    arrays = [convert_column(c, f)
              ^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/pyarrow/pandas_compat.py"", line 626, in convert_column
    raise e
  File "".venv/lib/python3.12/site-packages/pyarrow/pandas_compat.py"", line 620, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow/array.pxi"", line 365, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 90, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: (""Could not convert UUID('6f6303cd-516d-4a27-9165-bb703f9e2240') with type UUID: did not recognize Python value type when inferring an Arrow data type"", 'Conversion failed for column id with type object')
```

### Issue Description

Writing UUIDs fail. pyarrow supports writing UUIDs

### Expected Behavior

Writing UUIDs pass

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-57-generic
Version               : #59~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Mar 19 17:07:41 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO Parquet', 'Needs Triage', 'Upstream issue', 'Arrow']",2025-06-07 21:30:22,2025-07-31 01:43:59,3,closed
61598,BUG: Dangerous inconsistency: `~` operator changes behavior based on context outside a target.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame({
   ...:     'A': [1, 9, 6, 2, 7],
   ...:     'B': [6, 1, 3, 6, 3],
   ...:     'C': [2, 8, 4, 4, 4]
   ...: }, index=list('abcde'))
df.apply(lambda x: ~((x['B'] > 3) & (x['C'] < 8)), axis=1)
df['vals'] = df.apply(lambda x: ~((x['B'] > 3) & (x['C'] < 8)), axis=1)
df.apply(lambda x: ~((x['B'] > 3) & (x['C'] < 8)), axis=1)
```

### Issue Description

This ia reprot about `~` opetarotr in pandas dataframe.

Here is the example on python=3.10.12, pandas=2.2.3.

```
python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.34.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import pandas as pd

In [2]: df = pd.DataFrame({
   ...:     'A': [1, 9, 6, 2, 7],
   ...:     'B': [6, 1, 3, 6, 3],
   ...:     'C': [2, 8, 4, 4, 4]
   ...: }, index=list('abcde'))

In [3]: df
Out[3]:
   A  B  C
a  1  6  2
b  9  1  8
c  6  3  4
d  2  6  4
e  7  3  4

In [3]: df
Out[3]:
   A  B  C
a  1  6  2
b  9  1  8
c  6  3  4
d  2  6  4
e  7  3  4

In [4]: df.apply(lambda x: ~((x['B'] > 3) & (x['C'] < 8)), axis=1)
Out[4]:
a    False
b     True
c     True
d    False
e     True
dtype: bool

In [5]: df['vals'] = df.apply(lambda x: ~((x['B'] > 3) & (x['C'] < 8)), axis=1)

In [6]: df
Out[6]:
   A  B  C   vals
a  1  6  2  False
b  9  1  8   True
c  6  3  4   True
d  2  6  4  False
e  7  3  4   True

In [7]: df.apply(lambda x: ~((x['B'] > 3) & (x['C'] < 8)), axis=1)
Out[7]:
a   -2
b   -1
c   -1
d   -2
e   -1
dtype: int64
```

In the above example, the same `df.apply(lambda x: ~((x['B'] > 3) & (x['C'] < 8)), axis=1)` is executed in step 4, 5, and 7.
However, the result of step 7 is ridiculous.
In spite of `~`, `not` operator returns a correct answer.
It seems that `~` operator in pandas dataframe quite dangerous and unreliable.

In the environment of python 3.13.3, panads=2.2.3, **only for the step 7**, python returns warning that `<ipython-input-7-7d5677ff0f59>:1: DeprecationWarning: Bitwise inversion '~' on bool is deprecated and will be removed in Python 3.16. This returns the bitwise inversion of the underlying int object and is usually not what you expect from negating a bool. Use the 'not' operator for boolean negation or ~int(x) if you really want the bitwise inversion of the underlying int.`.
However, I think this is a warning by python (not by pandas) from a different point of view.



### Expected Behavior

The result of step 7 is same as step 4, 5.

### Installed Versions

python = 3.10.12
pandas = 2.2.3

</details>
",['Usage Question'],2025-06-07 13:43:53,2025-06-13 08:35:04,4,closed
61596,VOTE: Voting issue for PDEP-15: Reject adding PyArrow as a required dependency,"### Locked issue

- [x] I locked this voting issue so that only voting members are able to cast their votes or comment on this issue.


### PDEP number and title

PDEP-15: Reject PDEP-10

### Pull request with discussion

https://github.com/pandas-dev/pandas/pull/58623

### Rendered PDEP for easy reading

https://github.com/pandas-dev/pandas/blob/c159851cc0762625f9e51f9d9bd1d18011b79aa7/web/pandas/pdeps/0015-do-not-require-pyarrow.md

### Discussion participants

5 voting members (active maintainers) participated in the discussion

### Voting will close in 15 days.

2025-06-22

### Vote

Cast your vote in a comment below.
* +1: approve.
* 0: abstain.
    * Reason: A one sentence reason is required.
* -1: disapprove
    * Reason: A one sentence reason is required.
A disapprove vote requires prior participation in the linked discussion PR.

@pandas-dev/pandas-core
",['Vote'],2025-06-07 09:02:40,2025-07-17 10:40:25,21,closed
61590,RLS: 2.3.1,"Placeholder issue _if_ we decide to release 2.3.1. At the time of writing this issue, it's expected that pandas 3.0 would be the next version https://github.com/pandas-dev/pandas/issues/57064

Notable tasks for 2.3.1:

- [x] Re-enable Python 3.9 support (https://github.com/pandas-dev/pandas/issues/61563, https://github.com/pandas-dev/pandas/issues/61579)
    - [x] Revert https://github.com/pandas-dev/pandas/pull/60792
    - [x] Merge https://github.com/pandas-dev/pandas/pull/61569 (without hardcoding version)
- [x] Re-enable musl-aarch64 wheels
    - [x] Merge https://github.com/pandas-dev/pandas/pull/61569 (without hardcoding version)
   ",['Release'],2025-06-06 17:01:06,2025-07-28 16:12:16,13,closed
61588,CI: Test pandas with numpy 1.26,"See #60154 

We should add the build and fix the existing errors","['Testing', 'CI', 'Dependencies', 'good first issue']",2025-06-06 14:17:38,2025-07-08 15:44:44,5,closed
61587,WEB: Add table of content for the Ecosystem page,"We did it for the PDEP pages here: #58791

I don't think it should be difficult to also add a table of contents for the ecosystem page, which is quite large and not so easy to find things","['good first issue', 'Web']",2025-06-06 13:56:03,2025-06-16 09:09:07,4,closed
61583,"BUG: StataWriter returns ascii error when length of string is < 2045, but encoded length is > 2045","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({'doubleByteCol': ['§'*1500]})
df.to_stata('temp.dta', version=118)

len_encoded = df['doubleByteCol'].str.encode('utf-8').str.len()     # _encode_strings() count = 3000 -> no byte encoding because assumed will become strL (stata.py:2694)
len_typlist = df['doubleByteCol'].str.len()                         # _dtype_to_stata_type() = 1500 -> typ 1500 (stata.py:2193)
len_typlist < 2045      # True -> Tries to convert to np dtype S1500, but fails because unicode characters are not supported (normally no issue because encoded to bytes first) (stata.py:2945,2956)
```

### Issue Description

The StataWriter uses two different versions of the string column to check the same thing. During _encode_strings() it checks the length of the byte-encoded column `max_len_string_array(ensure_object(encoded._values))` but when assigning numpy types it checks the (potentially) unencoded version `itemsize = max_len_string_array(ensure_object(column._values))`. This then trips up the _prepare_data() section, which expects short columns to be byte-encoded already `typ <= self._max_string_length` based on the reported type, which is not true if the encoded column > 2045 due to unicode characters such as `§` taking up two bytes.

### Expected Behavior

I don't know the internal workings of stata.py well enough to be sure, but I think the easiest fix is using the actual values when checking str length in _encode_strings(). That is, replace
```max_len_string_array(ensure_object(encoded._values))```
by
```max_len_string_array(ensure_object(self.data[col]._values))```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.1.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_Belgium.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 23.2.1
Cython                : None
pytest                : 8.3.5
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.4
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 15.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>

### Temporary fix
For users finding this topic, this refers to the following Exception
```
Exception has occurred: UnicodeEncodeError       (note: full exception trace is shown but execution is paused at: <module>)
'ascii' codec can't encode characters in position 0-1499: ordinal not in range(128)
  File ""F:\datatog\junkyard\adhoc-scripts\mwes\pandas_asciiencoding.py"", line 4, in <module> (Current frame)
    df.to_stata('temp.dta', version=118)
UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1499: ordinal not in range(128)
```

You can workaround this issue by explicitly specifying the offending columns in the `convert_strL` option.","['Bug', 'IO Stata', 'Needs Triage']",2025-06-06 10:14:19,2025-06-30 18:14:30,1,closed
61579,"`pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0`","## How to reproduce:

```
docker run --rm python:3.9 bash -c ""pip install pandas && python -c 'import pandas; print(pandas.__version__)'""
```

## Output:

```sh
% docker run --rm python:3.9 bash -c ""pip install pandas && python -c 'import pandas; print(pandas.__version__)'""

... # pip install logs

2.3.0+4.g1dfc98e16a
```

Seems related to https://github.com/pandas-dev/pandas/issues/61563#issuecomment-2947099734",['Bug'],2025-06-06 02:49:46,2025-07-07 19:28:56,9,closed
61574,BUG: 2.3.0 didn't publish wheels for musl-aarch64 (arm),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
https://pypi.org/project/pandas/2.3.0/#files
https://pypi.org/project/pandas/2.2.3/#files
```

### Issue Description

ctrl-f for musl on the first page gives 15 results
for the second page gives 36 results.
ctrl on the 2.3.0 page for ""-cp313-cp313-musllinux_1_2_aarch64.whl"" gives no results but there are results on the 2.2.3 page.j

### Expected Behavior

would like a wheel for musl / arm64

### Installed Versions

not relevant","['Bug', 'Needs Triage']",2025-06-05 19:18:15,2025-06-06 16:36:53,3,closed
61573,WEB: Test that our versions JSON is valid,"See #61572

I think this can be as simple as loading the file with `json.load` when calling `pandas_web.py`. This way, if the file is not valid JSON the CI should break. But we need to double check that `json.load` fails if an extra comma is present.","['good first issue', 'Web']",2025-06-05 18:59:22,2025-06-07 10:00:47,0,closed
61571,DOC: Version dropdown not working,"Seems like the version dropdown is not working, at least for me, after the release: https://pandas.pydata.org/docs/

Can others confirm please?

Edit: Also the search. I guess there is a javascript error making all javascript code to not run","['Docs', 'good first issue']",2025-06-05 18:40:11,2025-06-05 19:25:20,0,closed
61570,BUG: comparing strings of different dtypes errors in 2.3,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd, numpy as np
arr1 = pd.array([],pd.StringDtype(""pyarrow"", na_value=pd.NA))
arr2 = pd.array([], pd.StringDtype(""python"", na_value=np.nan))
arr1 == arr2 # NotImplementedError: eq not implemented for <class 'pandas.core.arrays.string_.StringArrayNumpySemantics'>
```

### Issue Description

This appears to be the type of issue discussed in https://github.com/pandas-dev/pandas/issues/60639. That issue was closed, but I got an error when I tried running the above reproducer on the example given in the [whatsnew](https://pandas.pydata.org/pandas-docs/version/2.3.0/whatsnew/v2.3.0.html#notable-bug-fix1) for release 2.3.

My understanding was that the issue was closed when https://github.com/pandas-dev/pandas/pull/61138 was merged to main, but it's unclear if the fix was successfully backported to the 2.3.x branch. I haven't had the time yet to try when building pandas myself from main.

### Expected Behavior

Comparisons of string arrays/series with different dtypes should not error and the return dtype should follow the behavior laid out in #60639 .

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.12.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.87.1-microsoft-standard-WSL2
Version               : #1 SMP PREEMPT_DYNAMIC Mon Apr 21 17:08:54 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.3.0
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Strings']",2025-06-05 17:57:17,2025-07-02 16:35:31,4,closed
61566,BUILD: Installation issue on Mac with M1 Pro arm64 processor. pandas_parser.cpython-311-darwin.so is using x86_64 arch,"### Installation check

- [x] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

macOS-15.5-arm64-arm-64bit

### Installation Method

Built from source

### pandas Version

pandas-3.0.0.dev0+2147.g1f6f42ac55

### Python Version

3.11.13

### Installation Logs

I can't use pandas installed either using pip or from source, on my macbook with M1 Pro arm64 processor.
I am not using rosetta in terminal and my python install is also amd64.

Please help. It looks like one library file is built with x86-x64 arch.

<details>

    import pandas as pd
../../Library/Python/3.11/lib/python/site-packages/pandas/__init__.py:45: in <module>
    from pandas.core.api import (
../../Library/Python/3.11/lib/python/site-packages/pandas/core/api.py:1: in <module>
    from pandas._libs import (
../../Library/Python/3.11/lib/python/site-packages/pandas/_libs/__init__.py:16: in <module>
    import pandas._libs.pandas_parser  # isort: skip # type: ignore[reportUnusedImport]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ImportError: dlopen(/Users/leviann/Library/Python/3.11/lib/python/site-packages/pandas/_libs/pandas_parser.cpython-311-darwin.so, 0x0002): tried: '/Users/leviann/Library/Python/3.11/lib/python/site-packages/pandas/_libs/pandas_parser.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/leviann/Library/Python/3.11/lib/python/site-packages/pandas/_libs/pandas_parser.cpython-311-darwin.so' (no such file), '/Users/leviann/Library/Python/3.11/lib/python/site-packages/pandas/_libs/pandas_parser.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))

</details>
","['Build', 'Needs Info', 'OS X']",2025-06-05 10:26:33,2025-08-14 02:02:00,2,closed
61565,BUG: RecursionError when apply generic alias as a func,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.DataFrame({'x': [1], 'y': [2]}).apply(list, axis=""columns"")
pd.DataFrame({'x': [1], 'y': [2]}).apply(list[int], axis=""columns"")
```

### Issue Description

Traceback:
```python
[... skipping similar frames: Series.apply at line 4935 (593 times), NDFrameApply.agg_or_apply_list_like at line 744 (592 times), SeriesApply.apply at line 1412 (592 times), Apply.apply_list_or_dict_like at line 630 (592 times), Apply.compute_list_like at line 369 (592 times)]

File /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/apply.py:1412, in SeriesApply.apply(self)
    ...

File /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/apply.py:630, in Apply.apply_list_or_dict_like(self)
    ...

File /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/apply.py:744, in NDFrameApply.agg_or_apply_list_like(self, op_name)
    ...

File /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/apply.py:369, in Apply.compute_list_like(self, op_name, selected_obj, kwargs)
    ...

File /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/series.py:4935, in Series.apply(self, func, convert_dtype, args, by_row, **kwargs)
    ...

File /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/apply.py:1407, in SeriesApply.apply(self)
   1404 def apply(self) -> DataFrame | Series:
   1405     obj = self.obj
-> 1407     if len(obj) == 0:
   1408         return self.apply_empty_result()
   1410     # dispatch to handle list-like or dict-like

File /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/series.py:918, in Series.__len__(self)
    914 def __len__(self) -> int:
    915     """"""
    916     Return the length of the Series.
    917     """"""
--> 918     return len(self._mgr)

File /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/internals/base.py:76, in DataManager.__len__(self)
     74 @final
     75 def __len__(self) -> int:
---> 76     return len(self.items)

RecursionError: maximum recursion depth exceeded
> /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/internals/base.py(76)__len__()
     74     @final
     75     def __len__(self) -> int:
---> 76         return len(self.items)
     77 
     78     @property

ipdb>
```

where `self.func` is regarded as list of funcs, which lead to the bug 

```python
> /opt/homebrew/envs/pandera/lib/python3.12/site-packages/pandas/core/apply.py(1412)apply()
   1410         # dispatch to handle list-like or dict-like
   1411         if is_list_like(self.func):
-> 1412             return self.apply_list_or_dict_like()
   1413 
   1414         if isinstance(self.func, str):

ipdb> p self.func
*list[int]
ipdb> p is_list_like(self.func)
True
ipdb>
```

### Expected Behavior

Expected output:
> 0    [1, 2]
> dtype: object

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2cc37625532045f4ac55b27176454bbbc9baf213
python                : 3.12.9
python-bits           : 64
OS                    : Darwin
OS-release            : 24.5.0
Version               : Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:33 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8122
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : zh_CN.UTF-8
LOCALE                : zh_CN.UTF-8

pandas                : 2.3.0
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-06-05 09:30:30,2025-06-16 23:13:26,1,closed
61564,Failed to install pandas BUILD: 2.3.0 Windows,"### Installation check

- [x] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Windows-10-10.0.22631-SP0

### Installation Method

pip install

### pandas Version

2.3.0

### Python Version

3.9.13

### Installation Logs

<details>
C:\Users\tanishq>python -V
Python 3.9.13

C:\Users\tanishq>python -c ""import platform; print(platform.platform())""
Windows-10-10.0.22631-SP0

Collecting pandas
  Downloading pandas-2.3.0.tar.gz (4.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 4.9 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [10 lines of output]
      + meson setup C:\Users\tanishq\AppData\Local\Temp\pip-install-9lil68je\pandas_8d418a38654d41a19bef484e6372f854 C:\Users\tanishq\AppData\Local\Temp\pip-install-9lil68je\pandas_8d418a38654d41a19bef484e6372f854\.mesonpy-3s9nxbqg -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=C:\Users\tanishq\AppData\Local\Temp\pip-install-9lil68je\pandas_8d418a38654d41a19bef484e6372f854\.mesonpy-3s9nxbqg\meson-python-native-file.ini
      The Meson build system
      Version: 1.8.1
      Source dir: C:\Users\tanishq\AppData\Local\Temp\pip-install-9lil68je\pandas_8d418a38654d41a19bef484e6372f854
      Build dir: C:\Users\tanishq\AppData\Local\Temp\pip-install-9lil68je\pandas_8d418a38654d41a19bef484e6372f854\.mesonpy-3s9nxbqg
      Build type: native build

      ..\meson.build:2:0: ERROR: Could not find C:\Program Files (x86)\Microsoft Visual Studio\Installer\vswhere.exe

      A full log can be found at C:\Users\tanishq\AppData\Local\Temp\pip-install-9lil68je\pandas_8d418a38654d41a19bef484e6372f854\.mesonpy-3s9nxbqg\meson-logs\meson-log.txt
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

</details>
","['Build', 'Needs Triage']",2025-06-05 04:41:38,2025-06-05 16:58:11,5,closed
61563,Failed to install pandas==2.3.0 with Python 3.9,"### Installation check

- [x] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Linux-5.10.195-1.20230921.el7.x86_64-x86_64-with-glibc2.17

### Installation Method

pip install

### pandas Version

2.3.0

### Python Version

3.9.15

### Installation Logs

<details>
(base) [root@64bf929a621d7dafeb18b348 ~]# python -c 'import platform; print(platform.platform())'
Linux-5.10.195-1.20230921.el7.x86_64-x86_64-with-glibc2.17

(base) [root@64bf929a621d7dafeb18b348 ~]# pip install pandas -U
Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.5.0)
Collecting pandas
  Downloading pandas-2.3.0.tar.gz (4.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 90.9 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [152 lines of output]
      + meson setup /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86 /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/.mesonpy-lsu89q1a -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=/tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/.mesonpy-lsu89q1a/meson-python-native-file.ini
      The Meson build system
      Version: 1.8.1
      Source dir: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86
      Build dir: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/.mesonpy-lsu89q1a
      Build type: native build
      Project name: pandas
      Project version: 2.3.0
      C compiler for the host machine: cc (gcc 4.8.5 ""cc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)"")
      C linker for the host machine: cc ld.bfd 2.27-44
      C++ compiler for the host machine: c++ (gcc 4.8.5 ""c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)"")
      C++ linker for the host machine: c++ ld.bfd 2.27-44
      Cython compiler for the host machine: cython (cython 3.1.1)
      Host machine cpu family: x86_64
      Host machine cpu: x86_64
      Program python found: YES (/opt/conda/bin/python)
      Found pkg-config: YES (/usr/bin/pkg-config) 0.27.1
      Run-time dependency python found: YES 3.9
      Build targets in project: 53

      pandas 2.3.0

        User defined options
          Native files: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/.mesonpy-lsu89q1a/meson-python-native-file.ini
          b_ndebug    : if-release
          b_vscrt     : md
          buildtype   : release
          vsenv       : true

      Found ninja-1.11.1.git.kitware.jobserver-1 at /tmp/pip-build-env-ltjugjgm/normal/bin/ninja

      Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
      /tmp/pip-build-env-ltjugjgm/overlay/bin/meson compile -C .
      + /tmp/pip-build-env-ltjugjgm/normal/bin/ninja
      [1/151] Generating pandas/_libs/intervaltree_helper_pxi with a custom command
      [2/151] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command
      [3/151] Generating pandas/_libs/algos_common_helper_pxi with a custom command
      [4/151] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command
      [5/151] Generating pandas/_libs/index_class_helper_pxi with a custom command
      [6/151] Generating pandas/_libs/algos_take_helper_pxi with a custom command
      [7/151] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command
      [8/151] Copying file pandas/__init__.py
      [9/151] Generating pandas/_libs/sparse_op_helper_pxi with a custom command
      [10/151] Compiling C object pandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p/src_vendored_ujson_lib_ultrajsonenc.c.o
      FAILED: pandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p/src_vendored_ujson_lib_ultrajsonenc.c.o
      cc -Ipandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p -Ipandas/_libs -I../pandas/_libs -I../../../pip-build-env-ltjugjgm/overlay/lib/python3.9/site-packages/numpy/_core/include -I../pandas/_libs/include -I/opt/conda/include/python3.9 -fvisibility=hidden -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -Wextra -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p/src_vendored_ujson_lib_ultrajsonenc.c.o -MF pandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p/src_vendored_ujson_lib_ultrajsonenc.c.o.d -o pandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p/src_vendored_ujson_lib_ultrajsonenc.c.o -c ../pandas/_libs/src/vendored/ujson/lib/ultrajsonenc.c
      In file included from ../pandas/_libs/src/vendored/ujson/lib/ultrajsonenc.c:43:0:
      ../pandas/_libs/include/pandas/portable.h:31:22: error: missing binary operator before token ""(""
       #elif __has_attribute(__fallthrough__)
                            ^
      [11/151] Compiling C object pandas/_libs/pandas_parser.cpython-39-x86_64-linux-gnu.so.p/src_parser_io.c.o
      [12/151] Compiling C object pandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p/src_vendored_numpy_datetime_np_datetime.c.o
      FAILED: pandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p/src_vendored_numpy_datetime_np_datetime.c.o
      cc -Ipandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p -Ipandas/_libs -I../pandas/_libs -I../../../pip-build-env-ltjugjgm/overlay/lib/python3.9/site-packages/numpy/_core/include -I../pandas/_libs/include -I/opt/conda/include/python3.9 -fvisibility=hidden -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -Wextra -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p/src_vendored_numpy_datetime_np_datetime.c.o -MF pandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p/src_vendored_numpy_datetime_np_datetime.c.o.d -o pandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p/src_vendored_numpy_datetime_np_datetime.c.o -c ../pandas/_libs/src/vendored/numpy/datetime/np_datetime.c
      ../pandas/_libs/src/vendored/numpy/datetime/np_datetime.c:57:1: error: static assertion failed: ""__has_builtin not detected; please try a newer compiler""
       _Static_assert(0, ""__has_builtin not detected; please try a newer compiler"");
       ^
      ../pandas/_libs/src/vendored/numpy/datetime/np_datetime.c: In function ‘scaleYearToEpoch’:
      ../pandas/_libs/src/vendored/numpy/datetime/np_datetime.c:343:3: warning: implicit declaration of function ‘checked_int64_sub’ [-Wimplicit-function-declaration]
         return checked_int64_sub(year, 1970, result);
         ^
      ../pandas/_libs/src/vendored/numpy/datetime/np_datetime.c: In function ‘scaleYearsToMonths’:
      ../pandas/_libs/src/vendored/numpy/datetime/np_datetime.c:347:3: warning: implicit declaration of function ‘checked_int64_mul’ [-Wimplicit-function-declaration]
         return checked_int64_mul(years, 12, result);
         ^
      ../pandas/_libs/src/vendored/numpy/datetime/np_datetime.c: In function ‘npy_datetimestruct_to_datetime’:
      ../pandas/_libs/src/vendored/numpy/datetime/np_datetime.c:425:5: warning: implicit declaration of function ‘checked_int64_add’ [-Wimplicit-function-declaration]
           PD_CHECK_OVERFLOW(checked_int64_add(months, months_adder, &months));
           ^
      [13/151] Compiling C object pandas/_libs/pandas_parser.cpython-39-x86_64-linux-gnu.so.p/src_parser_pd_parser.c.o
      [14/151] Compiling C object pandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p/src_datetime_date_conversions.c.o
      [15/151] Compiling C object pandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p/src_vendored_ujson_python_JSONtoObj.c.o
      [16/151] Compiling C object pandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p/src_datetime_pd_datetime.c.o
      [17/151] Compiling C object pandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p/src_vendored_ujson_python_ujson.c.o
      [18/151] Compiling C object pandas/_libs/pandas_datetime.cpython-39-x86_64-linux-gnu.so.p/src_vendored_numpy_datetime_np_datetime_strings.c.o
      [19/151] Compiling C object pandas/_libs/json.cpython-39-x86_64-linux-gnu.so.p/src_vendored_ujson_python_objToJSON.c.o
      [20/151] Compiling C object pandas/_libs/tslibs/parsing.cpython-39-x86_64-linux-gnu.so.p/.._src_parser_tokenizer.c.o
      [21/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/indexing.pyx
      [22/151] Compiling C object pandas/_libs/pandas_parser.cpython-39-x86_64-linux-gnu.so.p/src_parser_tokenizer.c.o
      [23/151] Compiling C object pandas/_libs/lib.cpython-39-x86_64-linux-gnu.so.p/src_parser_tokenizer.c.o
      [24/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/ccalendar.pyx
      [25/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/base.pyx
      [26/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/np_datetime.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [27/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/missing.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [28/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/dtypes.pyx
      [29/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/arrays.pyx
      [30/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/hashing.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [31/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/nattype.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/nattype.pyx:79:0: Global name __nat_unpickle matched from within class scope in contradiction to to Python 'class private name' rules. This may change in a future release.
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/nattype.pyx:79:0: Global name __nat_unpickle matched from within class scope in contradiction to to Python 'class private name' rules. This may change in a future release.
      [32/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/vectorized.pyx
      [33/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/fields.pyx
      [34/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/internals.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [35/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/conversion.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [36/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/parsing.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [37/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/timezones.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [38/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/tzconversion.pyx
      [39/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/strptime.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [40/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/parsers.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/parsers.pyx:1605:18: noexcept clause is ignored for function returning Python object
      [41/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/timestamps.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [42/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/period.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [43/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/timedeltas.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [44/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/offsets.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [45/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/lib.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [46/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/index.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [47/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/interval.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [48/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/join.pyx
      [49/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/hashtable.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [50/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/algos.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      [51/151] Compiling Cython source /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/groupby.pyx
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:188:38: noexcept clause is ignored for function returning Python object
      warning: /tmp/pip-install-gp_gpioe/pandas_8608342ddb164d0e8725d2463640de86/pandas/_libs/tslibs/util.pxd:193:40: noexcept clause is ignored for function returning Python object
      ninja: build stopped: subcommand failed.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.


</details>
","['Build', 'Needs Triage']",2025-06-05 04:11:02,2025-06-06 12:10:15,22,closed
61559,Pandas DataFrame.query Code Injection (Unpatched),"Python pandas version 2.2.3 has a vulnerability on Pandas DataFrame.query 

In order to fix the function query on DataFrame python class what are the elements to review to resolve the vulnerability CVE-2024-9880.

Regards","['expressions', 'Closing Candidate']",2025-06-04 18:45:13,2025-06-05 00:53:43,2,closed
61557,BUG: regex match in compliance tests no longer match pytest expected inputs,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
N/A
```

### Issue Description

When I run compliance tests in python-db-dtype-pandas (a support file used by python-bigquery) I am getting multiple warnings (which cause test failures) due to a recent update in how pytest handles regex matches.

In pandas release 2.2.3 there is a snippet of code:
```
def test_take_pandas_style_negative_raises(self, data, na_value):
    with pytest.raises(ValueError, match=""""):
```

Pytest returns this Warning:

```
pytest.PytestWarning: matching against an empty string will *always* pass. If you want to check for an empty message you need to pass '^$'. If you don't want to match you should pass `None` or leave out the parameter.
```

This warning shows up in association with each of these pandas tests (it may occur with other tests, but these are the only ones that my tests revealed.):
```
FAILED ...::TestGetitem::test_take_pandas_style_negative_raises
FAILED ...::TestMethods::test_argmax_argmin_no_skipna_notimplemented
FAILED ...::TestSetitem::test_setitem_invalid
FAILED ...::TestJSONArrayGetitem::test_take_pandas_style_negative_raises
FAILED ...::TestJSONArrayMethods::test_argmax_argmin_no_skipna_notimplemented
FAILED ...::TestJSONArraySetitem::test_setitem_invalid
```


### Expected Behavior

N/A

### Installed Versions

N/A","['Testing', 'Error Reporting', 'good first issue']",2025-06-04 17:11:35,2025-06-13 17:44:32,2,closed
61554,BUG: duplicated() raises error with singlton set as subset,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame([{""a"": ""foo"", ""b"": ""bar""}])
df.duplicated(subset={""a""}) # raises error
df.duplicated(subset=[""a""]) # works
df.duplicated(subset=(""a"",)) # works
df.duplicates(subset={""a"",""b""}) # works
```

### Issue Description

Providing a singleton set to the subset parameter raises an error.

### Expected Behavior

Should work normally without having to convert the input to list or tuple.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.0-26-generic
Version               : #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.0
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : 8.29.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.36
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2025-06-04 08:57:11,2025-06-11 17:52:35,7,closed
61538,usecols investigation for various I/O functions,"pasting my comment from #61386 for visibility with relevant decisionmakers

> As promised during the sync meeting today, I went and compiled how various read functions handle columns being specified. Functions that take usecols (read_csv, read_clipboard, read_excel, and read_hdf(undocumented)) don't take into account input order, whereas functions that ask for columns instead do (hdf, feather, parquet, orc, starata, sql).
> 
> Finally, there are also some that straight up don't take column specifiers.
> 
> I'd expect functions that use usecols to be using the same function in the backend, but I'd have to verify it if we're planning to standardize the parameter.
> 
> CSV attached below of functions tested (those with a read and write function in pandas)
> [does_it_use_order.csv](https://github.com/user-attachments/files/20558438/does_it_use_order.csv)","['IO CSV', 'Needs Discussion']",2025-06-02 20:16:32,2025-06-03 16:04:24,1,closed
61535,ENH: read_csv tz option,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I use pd.read_csv to grab a series of timestamp'd links interactively from a remote website. 

### Feature Description

I would like it to convert the columns specified by parse_dates to the timezone specified by wherever the /etc/localtime link points to by default in a non-deprecated manner:

> [frame.loc[:,c].dt.tz_convert('/'.join([os.getenv('TZ', os.path.realpath('/etc/localtime').split('/')[-2:])][0])) for c in frame.select_dtypes('datetime64[ns, UTC]')]`

I'd like to propose this functionality as the tz parameter to read_csv. I suspect the implementation is not python, and can't find it in my git checkout of pandas. 

### Alternative Solutions

Covered above

### Additional Context","['Enhancement', 'IO CSV', 'Needs Triage', 'Closing Candidate']",2025-06-02 05:57:05,2025-08-05 16:29:10,2,closed
61531,CI: Micromamba taking too long to resolve the environments in the CI,"Our CI jobs are frequently failing now as they timeout after 90 minutes of execution. Of those 90 minutes, 25 are spent on micromamba resolving the environment.

In the past we have fixed this by limiting the number of packages to be considered. For example, if the environment just says `numpy`, maybe there are 200 versions that will be considered. While if we say `numpy >= 2` the number can be limited to few.

I'm not sure which packages have lots of options, and we don't want to filter out the versions that make sense to install. But we should have a look and see if by adding few constraints we can get a reasonable time to solve the environment.","['CI', 'Dependencies', 'good first issue']",2025-06-01 14:08:04,2025-06-13 14:34:13,6,closed
61523,"DOC: Official ""Cheat Sheet"" shows `as_type()` method, correct signature is `astype()`","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/doc/cheatsheet/Pandas_Cheat_Sheet.pdf


### Documentation problem

The third page of the official ""Cheat Sheet"" at https://github.com/pandas-dev/pandas/blob/main/doc/cheatsheet/Pandas_Cheat_Sheet.pdf has a section called ""Changing Type"". It lists `df.as_type(type)`, however no such method exists; the correct method should be `df.astype(type)`.

### Suggested fix for documentation

The fix is straightforward: replace `as_type` with `astype`.",['Docs'],2025-05-30 21:51:59,2025-06-02 16:28:56,3,closed
61521,Misleading error message when PyTables is not installed,"I tried reading an hd5 file with the latest pandas and got this import error:

`ImportError: Missing optional dependency 'pytables'.  Use pip or conda to install pytables.`

So I tried `pip install pytables` and got this error:

```
ERROR: Could not find a version that satisfies the requirement pytables (from versions: none)
ERROR: No matching distribution found for pytables
```

So then I went searching on PyPI and apparently there are no packages named `pytables`: https://pypi.org/search/?q=pytables

I did find the [PyTables](https://github.com/PyTables/PyTables) project on GitHub though, which says that we need to use `pip install tables` to install it. After installing `tables`, the hd5 read operation worked.

So, we need to install _tables_, not _pytables_, which is definitely confusing and not obvious. I think it would be very helpful if the error message indicated this to avoid having to go through the search process above.",['Error Reporting'],2025-05-30 19:08:09,2025-06-30 18:16:15,7,closed
61516,BUG: DataFrame.sample weights not required to sum to less than 1,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

data = {'w': [100, 1, 1]}
df = pd.DataFrame(data)

df.sample(n=2, weights=df.w, replace=False)
```

### Issue Description

In order for PPS sampling without replacement to be feasible, the selection probabilities must be less than 1, i.e.

$ \frac{n \cdot w_i}{\sum w_i}< 1$

where w is the weight and n is the total number of units to be sampled. This is often not the case if you are selecting a decent proportion of all units and there is wide variance in unit size. For example, suppose you want to select 2 units with PPS without replacement from a sampling frame of 3 units with sizes 100, 1, and 1. There is no way to make the probability of selection of the first unit 100x the probability of selection of the other two units (since the max prob for the first unit is 1 and at least one of the other units must have prob >= .5).

Unfortunately, pandas df.sampling function doesn't throw an error in this case. 

### Expected Behavior

The code above should throw some sort of error like ""Some unit probabilities are larger than 1 and thus PPS sampling without replacement cannot be performed""

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Algos']",2025-05-29 22:14:58,2025-07-11 02:27:16,16,closed
61512,BUG: Cannot sort by columns named None,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]], columns=['C1', None])
df.sort_values(None) # KeyError: None
```

### Issue Description

Sorting a DataFrame by a column named `None` results in the error `KeyError: None`. This breaks e.g. plugins that depend on Pandas for viewing and sorting DataFrames (see a related DataWrangler issue [here](https://github.com/microsoft/vscode-data-wrangler/issues/496), where inconsistent behavior with columns named None has also been reported).

### Expected Behavior

A column named None should not result in inconsistent behavior where some operations work but some others don't.

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.0-32-amd64
Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.1.129-1 (2025-03-06)
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 9.2.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.10.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None","['Bug', 'Indexing', 'Sorting']",2025-05-28 19:30:52,2025-06-02 16:45:45,1,closed
61511,"DOC: Docker image provided on ""Debugging C extensions"" is out of date","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/development/debugging_extensions.html

### Documentation problem

Docker image provided on linked page includes out of date pip and meson which causes build errors without manual updates to the installed docker image.

Additionally, when brought up in the bi-weekly meeting, it was mentioned that the preferred method is to locally install cygdb so the documentation should be updated to reflect this knowledge.

### Suggested fix for documentation

Either update or remove provided docker image and add that local installations are preferred.
",['Docs'],2025-05-28 19:12:36,2025-09-22 17:15:33,6,closed
61510,BUG: VSCode go to definition doesn't work with pandas.api.extensions.register_dataframe_accessor,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

Create a pandas.api.extensions.register_dataframe_accessor, then when using try to command + click in VSCode to jump to the definition. Either no definition is found or it jumps to a file like series.pyi



Create a pandas.api.extensions.register_dataframe_accessor, then when using try to command + click in VSCode to jump to the definition. Either no definition is found or it jumps to a file like series.pyi

`my_utils/my_accessor.py`

```python
import pandas as pd

@pd.api.extensions.register_dataframe_accessor(""demo"")
class DemoAccessor:
    def __init__(self, pandas_obj):
        self._obj = pandas_obj

    def say_hello(self):
        print(""Hello from accessor!"")
```

`main.py`
```python
import pandas as pd
import sys
import importlib

# Ensure the path to the module is in sys.path
sys.path.append(""my_utils"")  # Adjust this path as needed

import my_accessor
importlib.reload(my_accessor)

# Create DataFrame and use accessor
df = pd.DataFrame({""A"": [1, 2, 3]})
df.demo.say_hello()  # This runs fine, but ""jump to definition"" doesn't work
```

### Issue Description

VSCode go to definition doesn't work with pandas.api.extensions.register_dataframe_accessor

### Expected Behavior

I can jump to the definition when command clicking and see the documentation in VSCode

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
setuptools            : 80.7.1
pip                   : 25.1.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.6
IPython               : 7.34.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Closing Candidate', 'Accessors']",2025-05-28 15:04:19,2025-06-02 16:27:31,4,closed
61509,BUG: margin for pivot_table is incorrect with NA column/index values,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
#%%
import pandas as pd

df = pd.DataFrame({""i"": [1, 2, 3],
                   ""g1"": [""a"", ""b"", ""b""],
                   ""g2"": [""x"", None, None],
                   })

df.pivot_table(index=""g1"",
               columns=""g2"",
               values=""i"",
               aggfunc=""count"",
               dropna=False, margins=True)
```

### Issue Description

The margins of a `pivot_table` are incorrect when the `index` or `columns` variables contains missing variables. In particular, for the variable with the missing value, the total is missing.

| g1  | x   | nan | Total
|-----|-----|-----|------
| a   | 1.0 |     | 1
| b   |     | 2.0 | 2
| All | 1.0 |     | 3

### Expected Behavior

Margins should also be included for columns/indices with missing values.

In particular, the table should look like this

| g1  | x   | nan | Total
|-----|-----|-----|------
| a   | 1.0 |     | 1
| b   |     | 2.0 | 2
| All | 1.0 | 2.0 | 3

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457
python                : 3.12.8.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.0-26-generic
Version               : #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.0
numpy                 : 1.26.4
pytz                  : 2025.1
dateutil              : 2.9.0.post0
setuptools            : 75.8.0
pip                   : 25.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : 8.1.3
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.5
IPython               : 8.22.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.0
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : 2.4.2
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Reshaping']",2025-05-28 13:26:06,2025-07-13 12:10:06,2,closed
61505,BUG: Mask changing value despite of no True return,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

data = {'cd_tip_fma_pgto': [pd.NA]}
df = pd.DataFrame(data, dtype='double[pyarrow]')
df['payment'] = np.nan
df['payment'] = df['payment'].mask(cond=(df['cd_tip_fma_pgto'] == 6), other='Livelo')
```

### Issue Description

Evaluating the mask method under a pd.NA, when using pyarrow, is changing the target value!

![Image](https://github.com/user-attachments/assets/c51b5acd-d68c-4d40-882e-bb6111592c66)


### Expected Behavior

The results of the compasison should not cause any change of the target value, as you see in this example :

```
data = {'cd_tip_fma_pgto': [pd.NA]}
df = pd.DataFrame(data)
df['payment'] = np.nan
df['payment'] = df['payment'].mask(cond=(df['cd_tip_fma_pgto'] == 6), other='Livelo')
```

![Image](https://github.com/user-attachments/assets/49b9b3f9-a16f-421a-8b5a-85d3da613d16)

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.0
python-bits           : 64
OS                    : Linux
OS-release            : 3.10.0-1127.19.1.el7.x86_64
Version               : #1 SMP Tue Aug 25 17:23:54 UTC 2020
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.24.2
pytz                  : 2022.7.1
dateutil              : 2.8.2
pip                   : 25.1.1
Cython                : 0.29.33
sphinx                : None
IPython               : 8.10.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.11.2
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.5.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.2
lxml.etree            : 4.9.2
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 7.4.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.10.1
sqlalchemy            : 2.0.30
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : N/A
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-05-27 16:19:19,2025-05-27 16:42:11,1,closed
61503,BUG: Inconsistent returned objects when applying groupby aggregations,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame(columns=['Group', 'Data'])
df.groupby(['Group'], as_index=False)['Data'].agg('sum')
# Returns:
# Empty DataFrame
# Columns: [Group, Data]
# Index: []
def mysum(x):
  return sum(x)
df.groupby(['Group'], as_index=False)['Data'].agg(mysum)
# Returns:
# Series([], Name: Data, dtype: object)
```

### Issue Description

When performing groupby aggregations on empty dataframe (with labeled columns), the outcome differs whether we use an internal aggregator or a custom function.
The difference of behaviour is problematic because when using internal aggregators (like 'sum'), the returned object is a dataframe with proper columns that we can select. However with custom functions, the returned object with an empty Series from which we cannot select columns.

This forces developers in this situation to check emptiness of the dataframe first.
This is not desirable from code conciseness point of view, but more importantly, we easily forget to check it and can therefore lead to errors.

### Expected Behavior

Both approaches to apply groupby aggregations should return the same object, preferably a dataframe form which we can select columns.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.10
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : 9.2.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Apply']",2025-05-27 12:05:42,2025-05-30 16:40:43,2,closed
61494,DOC: kwargs naming in pd.Series.interpolate,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.interpolate.html

### Documentation problem

Display of the `**kwargs` is looking like `''**kwargs''`

### Suggested fix for documentation

Remove the backquotes.","['Docs', 'Needs Triage']",2025-05-25 19:20:56,2025-05-25 19:23:35,1,closed
61493,ENH: Supporting a `mapper` function as the 1st argument in `DataFrame.set_axis`,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

At the moment, `DataFrame.set_axis()` only accepts `labels`:

```python
df = (
    pd.DataFrame({'A': range(3), 'B': range(10, 13)})
    .set_axis(['a', 'b'], axis=1)
)
print(df)

#    a   b
# 0  0  10
# 1  1  11
# 2  2  12
```
Which makes it difficult (or more precisely verbose, using workarounds) to use during method chaining (where available columns could be dynamic and unknown at the begining of the chain). 


### Feature Description
I suggest to allow `.set_axis` method to accept a ""mapper"" (either a `function`, `dict` or `series`) that could be used to convert an axis to another preferred axis.
The proposed enhancement could get inspiration from how `.rename_axis` works. For example,  `.set_axis` could support receiving a function to apply on the current axis of the `DataFrame` (either its `index` or `columns`, depending on the `axis` argument) and set the axis to the labels that are returned by the function (see below for an example).

Example:
```python
df = (
    pd.DataFrame({'A': range(3), 'B': range(10, 13)})
    .set_axis(lambda df: 'col' + df.columns, axis=1)

   # or an alternative signature to support
   .set_axis({'A': 'colA', 'B': 'colB'}, axis='columns')
)

#    colA  colB
# 0     0    10
# 1     1    11
# 2     2    12
```

### Alternative Solutions

There is of course a workaround for this but, it is slightly verbose to use it during method chaining:

```python
df = (
    pd.DataFrame({'A': range(3), 'B': range(10, 13)})
    .pipe(lambda df: df.set_axis('col' + df.columns, axis=1))
)
print(df)

#    colA  colB
# 0     0    10
# 1     1    11
# 2     2    12
```

### Additional Context

I think `.set_axis` in general needs a bit of API consistency update.
For example, in `DataFrame.rename_axis` arguments can be provided in two ways:
```python
df.rename_axis(index=index_mapper, columns=columns_mapper)
df.rename_axis(mapper, axis='index')
```

But, `.set_axis` does not support such calling signatures. I propose to additionally support `index=` and `columns=` calling arguments to clarify the intent and increase readability.","['Enhancement', 'Needs Triage', 'Closing Candidate']",2025-05-25 12:02:15,2025-05-27 06:37:44,3,closed
61485,BUG: zfill with pyarrow string,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pyarrow as pa

pd.Series([""A"", ""AB"", ""ABC""], dtype=pd.ArrowDtype(pa.string())).str.zfill(3)
```

### Issue Description

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/will/GitHub/various/narwhals/.venv/lib/python3.12/site-packages/pandas/core/strings/accessor.py"", line 137, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/will/GitHub/various/narwhals/.venv/lib/python3.12/site-packages/pandas/core/strings/accessor.py"", line 1818, in zfill
    result = self._data.array._str_map(f)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ArrowExtensionArray' object has no attribute '_str_map'. Did you mean: '_str_pad'?
```

### Expected Behavior

Same as other string dtypes

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Fri Jul  5 17:56:39 PDT 2024; root:xnu-10063.141.1~2/RELEASE_ARM64_T8122
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.6
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.2.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : 6.131.23
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 20.0.0
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Strings', 'Arrow']",2025-05-23 20:18:19,2025-10-05 22:21:39,3,closed
61479,BUG: read_hdf() doesn't handle datetime64[ms] properly,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd


df = pd.DataFrame(
    {
        ""dates"": [
            pd.to_datetime(""2025-05-21 18:44:22""),
            pd.to_datetime(""2025-05-21 19:12:42""),
        ],
        ""tags"": [
            12,
            45,
        ]
    },
)
df[""dates""] = df[""dates""].astype(""datetime64[ms]"")
print(df.dtypes)
print(df)

df.to_hdf(""dates.h5"", key=""dates"")
df2 = pd.read_hdf(""dates.h5"", key=""dates"")
print(df2)

df2[""corrected""] = df2[""dates""].astype(""i8"").astype(""datetime64[ms]"")
print(df2)
```

### Issue Description

Dataframes containing dtype of ""datetime64[ms]"" seem to be correctly written in hdf format, but the readback is misinterpreted as “datetime64[ns]”.

The output of the code above is:

```dates    datetime64[ms]
tags              int64
dtype: object
                dates  tags
0 2025-05-21 18:44:22    12
1 2025-05-21 19:12:42    45
                       dates  tags
0 1970-01-01 00:29:07.853062    12
1 1970-01-01 00:29:07.854762    45
                       dates  tags           corrected
0 1970-01-01 00:29:07.853062    12 2025-05-21 18:44:22
1 1970-01-01 00:29:07.854762    45 2025-05-21 19:12:42```

### Expected Behavior

Correct dates when read back.

### Installed Versions

```INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : de_DE.cp1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 80.8.0
pip                   : 25.1.1
Cython                : 3.1.1
pytest                : 8.3.5
hypothesis            : 6.131.20
sphinx                : 8.2.3
blosc                 : None
feather               : None
xlsxwriter            : 3.2.3
lxml.etree            : 5.4.0
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.6
IPython               : 8.36.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
bottleneck            : 1.5.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.5.0
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.30
tables                : 3.10.2
tabulate              : None
xarray                : 2025.4.0
xlrd                  : 2.0.1
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : 2.4.3
pyqt5                 : None```
","['Bug', 'IO HDF5', 'Needs Info', 'Closing Candidate']",2025-05-22 12:34:28,2025-05-23 17:26:53,2,closed
61478,BUG: to_latex does not escape % with percent formatter,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
print(pd.DataFrame({""x"": [0.1, 0.5, 1.0]}).to_latex(formatters={""x"": ""{:.0%}""}, escape=True))
print(pd.DataFrame({""x"": [0.1, 0.5, 1.0]}).style.format(""{:.0%}"", escape=""latex"").to_latex())
```

### Issue Description

When using `""{:.0%}""` to format floating point values as percentages, the percent signs are not correctly escaped even if explicitly specified. This applies to `DataFrame.to_latex` and `Styler.to_latex`.

Output:
```latex
\begin{tabular}{lr}
\toprule
 & x \\
\midrule
0 & 10% \\
1 & 50% \\
2 & 100% \\
\bottomrule
\end{tabular}

\begin{tabular}{lr}
 & x \\
0 & 10% \\
1 & 50% \\
2 & 100% \\
\end{tabular}
```

### Expected Behavior

```latex
\begin{tabular}{lr}
\toprule
 & x \\
\midrule
0 & 10\% \\
1 & 50\% \\
2 & 100\% \\
\bottomrule
\end{tabular}

\begin{tabular}{lr}
 & x \\
0 & 10\% \\
1 & 50\% \\
2 & 100\% \\
\end{tabular}
```

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.10
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : de_DE.cp1252

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.11
sphinx                : None
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO LaTeX']",2025-05-22 12:02:17,2025-05-30 18:24:32,6,closed
61475,BUG: More Indicative Error when pd.melt with duplicate columns,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

x = pd.DataFrame([[1, 2, 3], [3, 4, 5]], columns=[""A"", ""A"", ""B""])
pd.melt(x, id_vars=[""A""], value_vars=[""B""])
```

### Issue Description

Error raised when melting on DataFrame with duplicate column headers

```
import pandas as pd

x = pd.DataFrame([[1, 2, 3], [3, 4, 5]], columns=[""A"", ""A"", ""B""])
pd.melt(x, id_vars=[""A""], value_vars=[""B""])
```

Above raises:
```
File ""pandas\core\reshape\melt.py"", line 110, in melt
  if not isinstance(id_data.dtype, np.dtype):
                    ^^^^^^^^^^^^^
File ""pandas\core\generic.py"", line 6286, in __getattr__
  return object.__getattribute__(self, name)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DataFrame' object has no attribute 'dtype'. Did you mean: 'dtypes'?
```

From pandas\core\reshape\melt.py:108 pop method causes id_data to be assigned to a DataFrame object rather causing above AttributeError
```
for col in id_vars:
  id_data = frame.pop(col)
  if not isinstance(id_data.dtype, np.dtype):
```

When having duplicate column headers in a dataframe it raises an AttributeError, should this instead indicate a hint about melting on duplicated column headers? Possibly implement a check prior to .dtype being called?


### Expected Behavior

Error raised should be indicative of duplicated column headers.

```
for col in id_vars:
  id_data = frame.pop(col)
  if isinstance(id_data, pd.DataFrame):
    raise Exception(f""{col} is a duplicate column header"")
  if not isinstance(id_data.dtype, np.dtype):
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.4
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252
pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.3.1
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Reshaping', 'Error Reporting', 'good first issue']",2025-05-21 22:01:23,2025-05-30 16:43:56,2,closed
61474,"BUG: dataframe.to_csv calls defauly numpy to_string function, resulting in","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

array = np.ones((1000, 1000))
df = pd.DataFrame({""Data"": [array]})
df.to_csv('test.csv', index=False)
```

### Issue Description

The output file will then have the following in the cell:

array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       ...,
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.]])

### Expected Behavior

I would expect the code to run like the following code does:

import pandas as pd
import numpy as np

array = np.ones((1000, 1000))
df = pd.DataFrame({""Data"": [array]})
with np.printoptions(linewidth=1000000, threshold=np.inf):
    df.to_csv('corrected_test.csv', index=False)

Where the df.to_csv function does not call the default numpy print statement. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.20.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 5, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 75.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.15.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'IO CSV', 'Closing Candidate']",2025-05-21 20:09:11,2025-05-23 17:28:18,3,closed
61469,"BUG: pandas.pivot_table margins, dropna and observed parameters not producing expected result","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

data = {
    'column_A_1': ['A', 'B', 'A', None, 'D', 'B', 'A'],
    'column_A_2': ['G', 'F', 'J', 'J', 'J', 'F', 'G'],
    'column_A_3': ['6602', '7059', '9805', '3080', '8625', '5741', '9685'],
    'column_A_4': ['A', 'B', 'A', None, 'A', None, 'B'],
    'column_A_4': ['X', None, 'Y', None, 'Z', 'X', 'Y'],
    'column_B_1': ['1', '2', '3', '4', '5', '6', '7'],
    'column_C_1': [0, 2, 5, 9, 8, 3, 7],
    'column_C_2': [12, 75, None, 93, 89, 23, 97],
    'column_C_3': [789, 102, 425, 895, None, 795, None],
    'column_C_3': [15886, 49828, None, 9898, 8085, 9707, 8049]
}
df = pd.DataFrame(data)

pd.pivot_table(df, index=['column_A_1', 'column_A_2', 'column_A_3', 'column_A_4'], columns=['column_B_1'], values=['column_C_1', 'column_C_2', 'column_C_3'], aggfunc={'column_C_1': 'max', 'column_C_2': 'min', 'column_C_3': 'count'}, dropna=False, margins=False, observed=True)
```

### Issue Description

I have a huge dataset with similar structure to the example. I want to pivot the table grouping using the columns A as the index, the values of the columns B as the new columns and aggregate the values of the columns C. I want all columns B values to appear as columns, even if the entire column is NaN. This is because I want to coalesce values from multiple columns into one. Therefore, the parameter dropna should be equal to False. But the DataFrame I get has 336 rows with impossible combinations. For example, the first row A, F, 3080, X has the entire row filled with NaNs since this combination does not exist. 

![Image](https://github.com/user-attachments/assets/578d93a9-e906-49e6-83d6-19b49f9d1073)

This is a problem because with a small dataset I wouldn't mind. But with a fairly large dataset, numpy returns an error because it has reached the maximum list size. While reading the documentation, I noticed the parameter:

![Image](https://github.com/user-attachments/assets/a30d2b3a-2a37-46aa-954e-2422ee610d16)

I thought this parameter fixed this issue. Playing around with this parameter, it does not affect the result, it only adds a row. Here is a result of combining these two parameters.

dropna=False, margins=False (Too many rows)

![Image](https://github.com/user-attachments/assets/111b43b9-8e08-46b0-8209-ec951b0ccb5f)

dropna=True, margins=False (Missing Column B values)

![Image](https://github.com/user-attachments/assets/1739d439-380b-471f-903d-806d8df8e63b)

dropna=False, margins=True (Same as dropna=False, margins=False?)

![Image](https://github.com/user-attachments/assets/39ca067f-99e2-4079-8778-04d230e1068d)

dropna=True, margins=True (Same as dropna=True, margins=False?)

![Image](https://github.com/user-attachments/assets/6a7c7cd3-ab08-4202-b314-ac0f2b7d81b9)

I also noticed this parameter:

![Image](https://github.com/user-attachments/assets/586030fe-b167-4085-963f-3bd21c718e7d)

But it is deprecated, and the default value of True seems to be the value that I need. Forcing this parameter to True does not change the result.

![Image](https://github.com/user-attachments/assets/c43f38cd-71fc-46f7-8f5e-05eeddf48fad)

### Expected Behavior

I expect with the parameter's combination dropna=False, margins=False and observed=True to get all the rows with plausible combinations (like if I was grouping by) and all the columns with column B values and columns C values.

I don't know if this is a bug or if it is the intended way for the pivot table to work and this is an enhancement.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.6
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.235-227.919.amzn2.x86_64
Version               : #1 SMP Sat Apr 5 16:59:05 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.0
Cython                : None
sphinx                : 7.2.6
IPython               : 8.23.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.3
lxml.etree            : 5.1.0
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : 2.9.0
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 15.0.2
pyreadstat            : None
pytest                : 8.1.1
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : 2.0.29
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Reshaping', 'Duplicate Report']",2025-05-21 08:58:05,2025-06-21 14:38:09,3,closed
61466,BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
s = pd.Series(['23', '³', '⅕', ''], dtype=pd.StringDtype(storage=""pyarrow""))
s.str.isdigit()


	0
0	True
1	False
2	False
3	False

dtype: boolean
```

### Issue Description

Series.str.isdigit() with pyarrow string dtype doesn't honor unicode superscript/subscript. Which diverges with the public doc. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.isdigit.html#pandas.Series.str.isdigit

The bug only happens in Pyarrow string dtype, Python string dtype behavior is correct.


### Expected Behavior

```
import pandas as pd
s = pd.Series(['23', '³', '⅕', ''], dtype=pd.StringDtype(storage=""pyarrow""))
s.str.isdigit()
```
```
	0
0	True
1	True
2	False
3	False

dtype: boolean
```

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.12
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.123+
Version               : #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.1.2
Cython                : 3.0.12
sphinx                : 8.2.3
IPython               : 7.34.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.2
html5lib              : 1.1
hypothesis            : None
gcsfs                 : 2025.3.2
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.0
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.28.1
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.3
sqlalchemy            : 2.0.40
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.3.1
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Strings', 'Needs Discussion', 'Upstream issue', 'Arrow']",2025-05-20 20:25:32,2025-09-25 16:28:08,10,closed
61462,BUILD: Provide wheel for Windows ARM64,"### What is the current behavior?

Installation via pip requires local build.

### What is the desired behavior?

To have [native wheel for WoA](https://blogs.windows.com/windowsdeveloper/2025/04/14/github-actions-now-supports-windows-on-arm-runners-for-all-public-repos/). GitHub Actions now supports win-arm64 for free.

### How would this improve `pandas`?

Due to the library's popularity, a native version for the growing number of Windows on ARM (WoA) devices offers a better user experience.","['Build', 'Windows', 'ARM']",2025-05-19 21:21:52,2025-05-21 20:25:02,0,closed
61460,PERF: Slow Windows / Ubuntu Unit Tests during Status Checks,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

The Windows Unit tests are dangerously close to time out when running the checks that validate a PR.
The last unit test from a merged PR took 83 minutes, out of the 90 minutes before timeout: 
https://github.com/pandas-dev/pandas/actions/runs/15019196064/job/42204122221

Furthermore, the checks in the open PR below are failing due to timeout in one of the Windows Unit tests.
https://github.com/pandas-dev/pandas/pull/61457/checks?check_run_id=42474035590
As there is only one unit test failing among all the PR checks and the Ubuntu Unit test is taking the same time in this PR as in the merged PR above, it strongly suggests that there is no issue intrinsic to the code change in the PR and that the way forward is:

- To increase the 90-min timeout in the unit test config yaml
- Or, and maybe better, to reduce the total time to run unit tests; this obviously might require a lot of work, unless some low-hanging fruits are still up for grab.

~If this issue appears in all new PRs triggering the core unit tests, this requires immediate attention.~

### Installed Versions

<details>

Version independent

</details>


### Prior Performance

_No response_","['Performance', 'CI', 'Windows', 'Needs Discussion']",2025-05-19 13:37:43,2025-06-02 16:26:11,3,closed
61456,PERF: Setting an item of incompatible dtype,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

df[""feature""] = np.nan
for cluster in df[""cluster""].unique():
    df.loc[df[""cluster""] == cluster, ""feature""] = ""string""


### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-138-generic
Version               : #148-Ubuntu SMP Fri Mar 14 19:05:48 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.utf8
LOCALE                : en_GB.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.34.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.6.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 4.9.4
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2023.6.0
scipy                 : 1.15.2
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : 2025.1.2
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>


### Prior Performance

Setup

    Dataset: df with 148,858 rows

    Task: Assign ""string"" to a new column ""feature"" based on unique values in the ""cluster"" column.

    Environment: Running on LSF

Test 1: Initialize with np.nan

import numpy as np

df[""feature""] = np.nan
for cluster in df[""cluster""].unique():
    df.loc[df[""cluster""] == cluster, ""feature""] = ""string""

    Runtime: ~52.5 seconds

    Warning:

    FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. 
    Value 'string' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.

Test 2: Initialize with ""None""

df[""feature""] = ""None""
for cluster in df[""cluster""].unique():
    df.loc[df[""cluster""] == cluster, ""feature""] = ""string""

    Runtime: ~1 minute 35 seconds

    No warnings

    Observation: Slower performance despite avoiding the dtype mismatch warning.","['Indexing', 'Performance', 'Needs Info']",2025-05-19 10:55:32,2025-08-05 17:04:22,2,closed
61452,BUG: Compiler Flag Drift May Affect Pandas ABI Stability via Memory Assumptions,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

# Create a structured array with alignment-sensitive types
dtype = np.dtype([('x', np.int64), ('y', np.float64)])
arr = np.zeros(10, dtype=dtype)

# Wrap into DataFrame
df = pd.DataFrame(arr)

# Trigger complex alignment path
try:
    # Operation that depends on consistent field layout
    df_sum = df.sum(numeric_only=True)
    print(""Sum result:"", df_sum)
except Exception as e:
    print(""Failure during structured alignment test:"", e)
```

### Issue Description

### Summary
Pandas may be vulnerable to ABI and memory alignment issues caused by C23 default behaviors in GCC 15.1. Silent adoption of padding behavior changes — particularly in union or struct definitions used in NumPy or Pandas C extensions — may lead to unpredictable runtime behavior.

This issue was originally identified in NumPy and Cython. As Pandas includes both compiled Cython code and relies on NumPy for internal memory layout, it is downstream vulnerable.

These compiled pieces are sensitive to pointer alignment, ABI expectations, or padding behaviors — especially across environments.

### Reproducible Example
Please see section below

Possibly related to:
- [BUG: DataFrame constructor not compatible with array-like classes that have a 'name' attribute](https://github.com/pandas-dev/pandas/issues/61443)
- [BUG: Confusing Behavior When Assigning DataFrame Columns Using omegaconf.ListConfig](https://github.com/pandas-dev/pandas/issues/61439)
- [BUG: Some ExtensionArrays can return 0-d Elements](https://github.com/pandas-dev/pandas/issues/61433)
- [BUG: Joining Pandas with Polars dataframe produces fuzzy errormessage](https://github.com/pandas-dev/pandas/issues/61434)
- [BUG: documented usage of of str.split(...).str.get fails on dtype large_string[pyarrow]](https://github.com/pandas-dev/pandas/issues/61431)

Report for more context:
[Report](https://brytelite.github.io/BryteLite/supply-chain-report)


### Expected Behavior

Recompile NumPy and Pandas with mismatched flags.

Then run the If padding bits are not cleared correctly in C structs, or if a layout mismatch occurs due to vendor/flag drift, crashes or incorrect math results may emerge.

`CFLAGS=""-std=c23"" pip install numpy pandas --force-reinstall --no-cache-dir when #building`

### Installed Versions

NumPy latest 3.13 release, Pandas latest 3.13 release are suitable.","['Bug', 'Build']",2025-05-18 00:17:59,2025-05-18 10:21:00,3,closed
61447,BUG: read_csv silently ignores out of bounds errors when parsing date columns,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import tempfile as tmp

with tmp.TemporaryFile(mode='r+') as csv_file:
    pd.DataFrame({
        'over_and_under': [
            '2262-04-12',
            '1677-09-20',
        ]
    }).to_csv(csv_file, index=False)
    csv_file.seek(0)
    df = pd.read_csv(csv_file, parse_dates=['over_and_under'], date_format='%Y-%m-%d')
    print(df.info())
    pd.to_datetime(df['over_and_under'], format='%Y-%m-%d')
```

### Issue Description

pandas 2.2.3 `read_csv` does not raise an Exception when parsing a date column with specified _date_format_ if values are out of bounds and silently keeps the column as object dtype.
An explicit call of `to_datetime` on the column reveals the out of bounds problem which I expected to get from `read_csv`

### Expected Behavior

`read_csv` should propagate or raise an OutOfBoundsDatetime exception like `to_datetime`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Darwin
OS-release            : 24.4.0
Version               : Darwin Kernel Version 24.4.0: Fri Apr 11 18:33:47 PDT 2025; root:xnu-11417.101.15~117/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.5
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 9.2.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'IO CSV', 'Non-Nano']",2025-05-16 08:42:56,2025-05-17 21:05:50,3,closed
61445,DOC: DataFrame.unstack should accept fill_value with more types than just int/str/dict,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.unstack.html#pandas.DataFrame.unstack

### Documentation problem

Currently the docs stipulate that only `int`, `str` and `dict` are allowed for the `fill_value`, yet it seems like all the types that could be used when creating a `DataFrame` seem to pass at runtime. I have not tried them all yet but int, float, complex, timestamp are working fine.

### Suggested fix for documentation

Add all allowed types for dataframe elements for the `fill_value` field.
Happy to create the PR if this is agreed by the maintainers. I will raise the issue in the pandas-stubs repo.","['Docs', 'Reshaping']",2025-05-16 00:12:57,2025-05-19 16:00:41,3,closed
61444,BUG: DataFrame column assignment with pd.Timestamp leads to unexpected dtype and incorrect JSON output,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

date = pd.Timestamp(""2025-01-01"")
df = pd.DataFrame(columns=[""date""], index=[""a"", ""b"", ""c""])
df[""date""] = date
print(df[""date""].dtype)  # Output: datetime64[s] Expected: datetime64[ns]
print(df.to_json())  # Output: {""date"":{""a"":1696,""b"":1696,""c"":1696}}
# Expected: {""date"":{""a"":1735689600000,""b"":1735689600000,""c"":1735689600000}}
```

### Issue Description

When assigning a pd.Timestamp to a column in a DataFrame, the resulting dtype of the column is not as expected, and the output of to_json() is incorrect.

### Expected Behavior

The dtype of the date column should default to datetime64[ns] after assignment.
The output of df.to_json() should correctly represent the timestamp in milliseconds since the epoch.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.4
python-bits           : 64
OS                    : Darwin
OS-release            : 24.4.0
Version               : Darwin Kernel Version 24.4.0: Fri Apr 11 18:33:47 PDT 2025; root:xnu-11417.101.15~117/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
None

</details>
","['Bug', 'Non-Nano', 'Timestamp']",2025-05-15 07:58:40,2025-05-17 21:15:26,4,closed
61443,BUG: `DataFrame` constructor not compatible with array-like classes that have a `'name'`  attribute,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd
import vtk

poly = vtk.vtkPolyData(points=np.eye(3))
pd.DataFrame(poly.points)
```

``` python
ValueError: Per-column arrays must each be 1-dimensional

```
Originally posted in https://github.com/pyvista/pyvista/issues/7519

### Issue Description

Wrapping a `DataFrame` with the array-like object above results in an unexpected `ValueError` being raised. The cause is this line, which assumes that the input object must be a `Series` or `Index` type based on having a `'name'` attribute.

https://github.com/pandas-dev/pandas/blob/41968a550a159ec0e5ef541a610b7007003bab5b/pandas/core/frame.py#L798-L799

This assumption fails for the `VTKArray` `poly.points`, which also has a `'name'` attribute.

### Expected Behavior

No error should be raised, and the array-like input should be wrapped correctly by `DataFrame`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.2
python-bits           : 64
OS                    : Darwin
OS-release            : 23.4.0
Version               : Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_CA.UTF-8
pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : 8.1.3
IPython               : 8.36.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : 6.131.9
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Constructors']",2025-05-14 22:02:04,2025-05-19 15:54:50,5,closed
61440,ENH: Broaden `dict` to `Mapping` as replace argument,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Currently the `replace` method of `Series` allows only `dict`, but not `Mapping` inputs, as the `DataFrame` one does.

For example:

```py
from collections.abc import Mapping
import pandas as pd


df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]})

d: Mapping[int, str] = {1: ""a"", 2: ""b"", 3: ""c""}

d2: Mapping[str, Mapping[int, str]] = {""A"": d}
print(df.replace(d2))  # typechecks

print(df[""A""].replace(d))  # works but doesn't typecheck
```

### Feature Description

I guess it's enough to change from `dict` to `Mapping` in the type signature, since it seems to work even if the argument is not a dict (for example if it's a `MappingProxyType` instance).

### Alternative Solutions

I guess an alternative solution is just to type ignore the replace invocation.

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-05-14 08:29:11,2025-05-27 15:26:33,9,closed
61438,BUG:  ImportError: cannot import name 'NaN' from 'numpy' in squeeze_pro.py,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas_ta as ta
```

### Issue Description

D:\t70\duanxian>python duanxian_TDI_TSI_DIV.py

Traceback (most recent call last):

  File ""D:\t70\duanxian\duanxian_TDI_TSI_DIV.py"", line 11, in <module>

    import pandas_ta as ta # 新增：用于 ALMA 等指标

    ^^^^^^^^^^^^^^^^^^^^^^

  File ""D:\veighna_studio\Lib\site-packages\pandas_ta\__init__.py"", line 116, in <module>

    from pandas_ta.core import *

  File ""D:\veighna_studio\Lib\site-packages\pandas_ta\core.py"", line 18, in <module>

    from pandas_ta.momentum import *

  File ""D:\veighna_studio\Lib\site-packages\pandas_ta\momentum\__init__.py"", line 34, in <module>

    from .squeeze_pro import squeeze_pro

  File ""D:\veighna_studio\Lib\site-packages\pandas_ta\momentum\squeeze_pro.py"", line 2, in <module>

    from numpy import NaN as npNaN

ImportError: cannot import name 'NaN' from 'numpy' (D:\veighna_studio\Lib\site-packages\numpy\__init__.py). Did you mean: 'nan'?

### Expected Behavior

import pandas_ta success

### Installed Versions

Operating System: [Your OS, e.g., Windows 10/11]
Python Version: [Your Python version, e.g., 3.9, 3.10, 3.11 - based on your traceback, it seems like Python 3.13 based on ""cp313"" in numpy download, please confirm]
pandas_ta Version: 0.3.14b0 (Confirmed via pip show pandas_ta)
numpy Version: 2.2.5 (Confirmed via pip show numpy)
pandas Version: 2.2.3 (Confirmed via pip show pandas)","['Bug', 'Needs Triage']",2025-05-14 02:50:31,2025-05-14 16:19:36,1,closed
61432,"DOC: Series.name is just Hashable, but many column arguments require str","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

* https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.name.html
* https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.pivot.html

### Documentation problem

In the documentation, `Series.name` is [just required to be](https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.name.html) a `Hashable`. When `pandas` functions ask for a column label, however, it often asks for an `str`, e.g. in [DataFrame.pivot](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.pivot.html), where it says
> **columns**: *str or object or a list of str*

### Suggested fix for documentation

Use `Hashable` everywhere to column labels as a function argument","['Docs', 'Reshaping', 'good first issue']",2025-05-12 15:45:47,2025-05-20 21:40:33,1,closed
61428,DOC: Broken Link in IO Tools - HDF5 Data Description,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/user_guide/io.html

### Documentation problem

The link for HDF5 data description is broken and leads to a 404 error.

Current [HDF5 link](https://support.hdfgroup.org/HDF5/whatishdf5.html#gsc.tab=0)

### Suggested fix for documentation

I believe a good replacement link would be to this [Introduction to HDF5](https://support.hdfgroup.org/documentation/hdf5/latest/_intro_h_d_f5.html).

I would like to update the documentation with this link and create a pull request.","['Docs', 'IO HDF5']",2025-05-11 00:23:23,2025-05-12 16:55:17,2,closed
61424,i want to develop one feature in pandas,"### Research

- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

i want to develop one feature in pandas 

### Question about pandas

_No response_","['Usage Question', 'Needs Triage']",2025-05-10 14:14:16,2025-05-10 14:31:47,2,closed
61420,ENH: Add smart_groupby() method for automatic grouping by categorical columns and aggregating numerics,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Currently, pandas.DataFrame.groupby() requires users to explicitly specify both the grouping columns and the aggregation functions. This can be repetitive and inefficient, especially during exploratory data analysis on large DataFrames with many columns. A common use case like “group by all categorical columns and compute the mean of numeric columns” requires verbose, manual setup.

### Feature Description

Add a new method to DataFrame called smart_groupby(), which intelligently infers grouping and aggregation behavior based on the column types of the DataFrame.

Proposed behavior:
- If no parameters are passed:
  - Group by all columns of type object, category, or bool
  - Aggregate all remaining numeric columns using the mean
- Optional keyword parameters:
  -  by: specify grouping columns explicitly
  - agg: specify aggregation function(s) (default is ""mean"")
  - exclude: exclude specific columns from grouping or aggregation

### Alternative Solutions

Currently, users must write verbose code to accomplish the same:
```
group_cols = [col for col in df.columns if df[col].dtype == 'category']
agg_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
df.groupby(group_cols)[agg_cols].mean()
```

### Additional Context

_No response_","['Enhancement', 'Groupby', 'Closing Candidate']",2025-05-09 13:27:40,2025-05-14 18:59:15,3,closed
61419,BUILD: Missing Windows free-threading wheel,"### Installation check

- [x] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Windows-2022Server-10.0.20348-SP0

### Installation Method

pip install

### pandas Version

2.2.3

### Python Version

3.13.3 free-threading

### Installation Logs

<details>
$ which pip
/home/Administrator/venv/Scripts/pip
$ pip install pandas
Collecting pandas
  Downloading pandas-2.2.3.tar.gz (4.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 144.5 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [12 lines of output]
      + meson setup Z:\data\tmp\pip-install-niaom8mt\pandas_620816291b0449be8d128c83a9a99222 Z:\data\tmp\pip-install-niaom8mt\pandas_620816291b0449be8d128c83a9a99222\.mesonpy-ulxgqp76\build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=Z:\data\tmp\pip-install-niaom8mt\pandas_620816291b0449be8d128c83a9a99222\.mesonpy-ulxgqp76\build\meson-python-native-file.ini
      The Meson build system
      Version: 1.2.1
      Source dir: Z:\data\tmp\pip-install-niaom8mt\pandas_620816291b0449be8d128c83a9a99222
      Build dir: Z:\data\tmp\pip-install-niaom8mt\pandas_620816291b0449be8d128c83a9a99222\.mesonpy-ulxgqp76\build
      Build type: native build
      Project name: pandas
      Project version: 2.2.3

      ..\..\meson.build:2:0: ERROR: Could not find C:\Program Files\Microsoft Visual Studio\Installer\vswhere.exe

      A full log can be found at Z:\data\tmp\pip-install-niaom8mt\pandas_620816291b0449be8d128c83a9a99222\.mesonpy-ulxgqp76\build\meson-logs\meson-log.txt
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.

[notice] A new release of pip is available: 25.0.1 -> 25.1.1
[notice] To update, run: python.exe -m pip install --upgrade pip
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

</details>

I don't see a wheel for Windows cp313t in the list of release files  https://pypi.org/project/pandas/2.2.3/#files.
I see a job is running that should produce the wheel: https://github.com/pandas-dev/pandas/actions/runs/14920899116/job/41915964757
Perhaps the wheel was accidentally omitted in the release process?
","['Build', 'Needs Triage']",2025-05-09 12:40:24,2025-05-10 14:36:30,2,closed
61416,"BUG: df.rolling.{std, skew, kurt} gives unexpected value","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame(index=range(100))
df = df.assign(val = df.index)
df = df/1e3

df.loc[0,""val""] = 1e6
df.loc[5,""val""] = -1e6

res1 = df.rolling(20,min_periods=1).kurt()
res2 = df.iloc[1:].rolling(20,min_periods=1).kurt()

>>>res1.tail(5)
           val
95  722.329422
96  730.791755
97  739.254087
98  747.716420
99  756.178752
>>>res2.tail(5)
    val
95 -1.2
96 -1.2
97 -1.2
98 -1.2
99 -1.2
```

### Issue Description

In one of my experiments, the results of my rolling calculation of high-order moments differed. When I excluded the first data or retained the first data, the results of the rolling calculation varied greatly. I used this case to attempt to reproduce this result. The operators I tested, Including df.rolling.std, df.rolling.skew, df.rolling.kurt. I don't know what the reason is. I think for the df.rolling operator, this should be a bug

### Expected Behavior

The result of the rolling calculation, regardless of what the first one is, should the last few pieces of data not be affected by the initial data

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19044
machine               : AMD64
processor             : Intel64 Family 6 Model 106 Stepping 6, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : Chinese (Simplified)_China.936

pandas                : 2.2.3
numpy                 : 2.2.5
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
None

</details>
","['Bug', 'Window']",2025-05-09 08:42:24,2025-12-01 19:14:21,10,closed
61415,BUG: ImportError: cannot import name 'NaN' from 'numpy',"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
ImportError: cannot import name 'NaN' from 'numpy'
```

### Issue Description

ImportError: cannot import name 'NaN' from 'numpy' 



### Expected Behavior

ImportError: cannot import name 'NaN' from 'numpy' 



### Installed Versions

<details>

ImportError: cannot import name 'NaN' from 'numpy' 


</details>
","['Bug', 'Needs Triage']",2025-05-09 07:40:07,2025-05-09 17:37:22,3,closed
61412,DOC: Error in Getting started tutorials > How do I read and write tabular data?,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/getting_started/intro_tutorials/02_read_write.html

### Documentation problem

In the documentation for the Titanic dataset on this page:

https://pandas.pydata.org/docs/getting_started/intro_tutorials/02_read_write.html

It currently says:

>  ""Survived: Indication whether passenger survived. 0 for yes and 1 for no.""

This appears to be incorrect. The correct meaning is:

>  0 = did not survive
>  1 = survived

You can verify this, for example, with the entry for ""McCarthy, Mr. Timothy J."", who is listed with a 0 in the dataset and was confirmed deceased (source: https://de.wikipedia.org/wiki/Passagiere_der_Titanic).

Thanks for your great work and for maintaining the documentation!

### Suggested fix for documentation

Survived: Indication whether passenger survived. 0 for no and 1 for yes.",['Docs'],2025-05-08 21:59:48,2025-05-09 18:36:08,1,closed
61409,BUG: CVE-2020-13091,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
When will this bug be fixed?
```

### Issue Description

Bug since 2020 

### Expected Behavior

No Bug

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2025-05-08 15:40:20,2025-05-08 15:59:45,1,closed
61408,"DOC: axis argument for take says `None` is acceptable, but that is incorrect.","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.take.html#pandas.DataFrame.take

### Documentation problem

The `axis` argument is documented as:  ""axis {0 or ‘index’, 1 or ‘columns’, None}, default 0"" .  But `None` is not accepted.  So it should be removed from the docs.

See https://github.com/pandas-dev/pandas-stubs/pull/1209#discussion_r2079740441 for an example.

### Suggested fix for documentation

Remove `None` from that sentence.

","['Docs', 'Algos']",2025-05-08 13:54:54,2025-05-08 22:27:23,1,closed
61407,BUG: to_csv() quotechar/escapechar behavior differs from csv module,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import csv
import sys

data = [['a', 'b""c', 'def""'], ['a2', None, '""c']]

# no escaping
df = pd.DataFrame(data)
print(df.to_csv(sep='\t', index=False, header=False, quotechar='""', escapechar='\\', quoting=csv.QUOTE_NONE))
print(df.to_csv(sep='\t', index=False, header=False, quotechar='""', escapechar='\\', quoting=csv.QUOTE_NONE, doublequote=False))

# escaping
csv_writer = csv.writer(sys.stdout, delimiter='\t', quotechar='""', escapechar='\\', quoting=csv.QUOTE_NONE)
for r in data:
    _ = csv_writer.writerow(r)
```

### Issue Description

`to_csv()` doesn't escape `quotechar` when `quoting=csv.QUOTE_NONE`.
````
a       b""c     def""
a2              ""c
````

### Expected Behavior

`quotechar` gets escaped using `escapechar` even when `quoting=csv.QUOTE_NONE`.
This is the behavior of the csv module.
````
a       b\""c    def\""
a2              \""c
````

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.2
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22621
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.26.2
pytz                  : 2023.3.post1
dateutil              : 2.8.2
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.24.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.23
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV']",2025-05-08 13:40:42,2025-05-30 16:36:43,5,closed
61405,DOC/ENH: Add full list of argument for DataFrame.query,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html#pandas.DataFrame.query

### Documentation problem

This question arises when @MarcoGorelli wanted to fully type `DataFrame.query` in the stubs repo https://github.com/pandas-dev/pandas-stubs/issues/1173. Right now the extra arguments are passed through `**kwargs` but when we go through the code we see that they are the same as the ones in `pd.eval` (https://pandas.pydata.org/docs/reference/api/pandas.eval.html#pandas.eval).

### Suggested fix for documentation

Considering that this would help to expand the typehinting in that area and that the number of arguments is limited, would it be conceivable to expose all the arguments instead of relying on `**kwargs`?

For information this is the list of arguments that would need to be added:
```python
parser: Literal[""pandas"", ""python""] = ...,
engine: Literal[""python"", ""numexpr""] | None = ...,
local_dict: dict[_str, Any] | None = ...,
global_dict: dict[_str, Any] | None = ...,
resolvers: list[Mapping] | None = ...,
level: int = ...,
target: object | None = ...,
```

See https://github.com/pandas-dev/pandas-stubs/pull/1193 for the potential typehinting.",['Docs'],2025-05-08 01:16:47,2025-05-20 02:35:33,2,closed
61403,BUG: guess_datetime_format cannot infer iso 8601 format,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd 

# UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
pd.to_datetime(
    pd.Series(['2025-05-05 20:25:22+00:00', '2025-05-05 12:04:52+00:00'])
)

# no warning
pd.to_datetime(
    pd.Series(['2025-05-05 20:25:22+00:00'])
)

# No warning
pd.to_datetime(
    pd.Series(['2025-05-05 12:03:08+00:00', '2025-05-05 12:04:52+00:00']),
)
```

### Issue Description

When running `pd.to_datetime(pd.Series(['2025-05-05 20:25:22+00:00', '2025-05-05 12:04:52+00:00']))` the following warning is risen:

> UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.

This is because `guess_datetime_format` cannot infer a format for the first given timestamp '2025-05-05 20:25:22+00:00'. 

### Expected Behavior

No warning is risen.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.10
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-58-generic
Version               : #60~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 28 16:09:21 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.40
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Datetime', 'Warnings']",2025-05-07 09:41:27,2025-05-07 17:04:01,1,closed
61402,BUG: Duplicate columns allowed on `merge` if originating from separate dataframes,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df1 = pd.DataFrame({""col1"":[1], ""col2"":[2]})
df2 = pd.DataFrame({""col1"":[1], ""col2"":[2], ""col2_dup"":[3]})

pd.merge(df1, df2, on=""col1"", suffixes=(""_dup"", """"))
# Observe (1)

pd.merge(df1, df2, on=""col1"", suffixes=("""", ""_dup""))
# Observe (2)
```

### Issue Description

Case 1 provides the following result:
```
   col1  col2_dup  col2  col2_dup
0     1         2     2         3
```

Case 2 results in an exception:
```
pandas.errors.MergeError: Passing 'suffixes' which cause duplicate columns {'col2_dup'} is not allowed.
```

While the MergeError in this case does make sense (ideally duplicate columns should not be allowed as they might cause confusion), the same issue is observed in the first case and no exception is raised.


### Expected Behavior

Since this bug is about consistency, either of the following 2 should happen:

- An error should be raised in both cases.
- An error should not be raised in any case, and the duplicate column should be allowed.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.7
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.5
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 23.2.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Reshaping']",2025-05-07 08:50:02,2025-06-06 14:12:22,8,closed
61395,BUG: pd.to_datetime failing to parse with exception error 01-Jun-2025 in sequence with 31-May-2025,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import sys

print(f""Pandas version: {pd.__version__}"")
print(f""Python version: {sys.version}"")

df = pd.DataFrame({'day': [""31-May-2025"",""01-Jun-2025"",""02-Jun-2025""]})
pd.to_datetime(df['day'])
```

### Issue Description

gives
'Pandas version: 2.2.3'
'Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]'

ValueError: time data ""01-Jun-2025"" doesn't match format ""%d-%B-%Y"", at position 1. You might want to try:
    - passing `format` if your strings have a consistent format;
    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;
    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.
File <command-6844361422137531>, line 2
      1 df = pd.DataFrame({'day': [""31-May-2025"",""01-Jun-2025"",""02-Jun-2025""]})
----> 2 pd.to_datetime(df['day'])
File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1067, in to_datetime(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)
   1065         result = arg.map(cache_array)
   1066     else:
-> 1067         values = convert_listlike(arg._values, format)
   1068         result = arg._constructor(values, index=arg.index, name=arg.name)
   1069 elif isinstance(arg, (ABCDataFrame, abc.MutableMapping)):
File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:433, in _convert_listlike_datetimes(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)
    431 # `format` could be inferred, or user didn't ask for mixed-format parsing.
    432 if format is not None and format != ""mixed"":
--> 433     return _array_strptime_with_fallback(arg, name, utc, format, exact, errors)
    435 result, tz_parsed = objects_to_datetime64(
    436     arg,
    437     dayfirst=dayfirst,
   (...)
    441     allow_object=True,

### Expected Behavior

it parses happily and correctly with no exception
interestingly it's having the transition end of may. start of June. Starting with 01-Jun-2025 works, ending with 31-May-2025 works,
dateparser.parse is happy
I'm guessing it infers a full month from the May when in fact it is a three character abbreviation. 

### Installed Versions

<details>

running in databricks notebook - checked in a separate version of python locally, with pandas 2.2.1
'Pandas version: 2.2.3'
'Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]' for the notebook.
pd.show_versions() doesn't return anything


locally 
Pandas version: 2.2.1
Python version: 3.12.2 (main, Mar 25 2024, 11:48:28) [Clang 15.0.0 (clang-1500.3.9.4)]

and pd.show_versions() gives.

FileNotFoundError                         Traceback (most recent call last)
File /Users/J.Drummond/Documents/wip/python/truth_soc_[1](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/truth_soc_1.py:1).py:2
      1 # %%
----> [2](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/truth_soc_1.py:2) pd.show_versions()

File ~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:141, in show_versions(as_json)
    [104](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:104) """"""
    [105](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:105) Provide useful information, important for bug reports.
    [106](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:106) 
   (...)
    [138](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:138) ...
    [139](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:139) """"""
    [140](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:140) sys_info = _get_sys_info()
--> [141](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:141) deps = _get_dependency_info()
    [143](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:143) if as_json:
    [144](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:144)     j = {""system"": sys_info, ""dependencies"": deps}

File ~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:98, in _get_dependency_info()
     [96](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:96) result: dict[str, JSONSerializable] = {}
     [97](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:97) for modname in deps:
---> [98](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:98)     mod = import_optional_dependency(modname, errors=""ignore"")
     [99](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:99)     result[modname] = get_version(mod) if mod else None
    [100](https://file+.vscode-resource.vscode-cdn.net/Users/J.Drummond/Documents/wip/python/~/Documents/wip/python/.venv/lib/python3.12/site-packages/pandas/util/_print_versions.py:100) return result
...


</details>
","['Bug', 'Datetime']",2025-05-03 14:04:02,2025-05-03 15:12:00,4,closed
61392,DOC: Issue with the general expressiveness of the docs,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

Example: https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.floor.html

### Documentation problem

Throughout the docs the explanation of a function is often limited only to a circular sentence that repeats the verb that names the function and nothing else. Eg in the example of `pandas.Series.dt.floor` it basically says ""it does floor"" and the details of the docs are restricted to the individual options and outcomes after that.

### Suggested fix for documentation

In the example of floor it should first say in a richer sentence what floor actually does. It doesn't have to be anything big. I won't write an example of that because the docs didn't tell me what floor does.","['Docs', 'Needs Info']",2025-05-02 16:28:12,2025-08-05 17:09:58,4,closed
61389,"BUG: Incorrect Parsing of Timestamps in pd.to_datetime with Series with format=""ISO8601""  and UTC=True","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Single timestamp
raw = ""2023-10-15T14:30:00""
single = pd.to_datetime(raw, utc=True, format=""ISO8601"")
print(single)
# Output: 2023-10-15 14:30:00+00:00 (correct)

# Series of timestamps
series = pd.Series([0, 0], index=[""2023-10-15T10:30:00-12:00"", raw])
converted = pd.to_datetime(series.index, utc=True, format=""ISO8601"")
print(converted)
# Output: 2023-10-16 02:30:00+00:00 for the second one (incorrect)
# error depends on the previous one timezone
```

### Issue Description

When using pd.to_datetime to parse a Series of timestamps with format=""ISO8601"" and utc=True, the parsing of a timestamp without an explicit timezone offset is incorrect and appears to depend on the timezone offset of the previous timestamp in the Series. This behavior does not occur when parsing a single timestamp.

### Expected Behavior

In this configuration, behavior should not depend on the previous timestamp timezone. Result should be the same as when individually passed.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.0-34-amd64
Version               : #1 SMP Debian 5.10.234-1 (2025-02-24)
machine               : x86_64
processor             :
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 15.0.2
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.40
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'version' is not defined

</details>","['Bug', 'Datetime', 'Timezones']",2025-05-02 11:24:44,2025-05-06 18:29:20,3,closed
61377,not able to see the content in the dark mode,"<img width=""1470"" alt=""Image"" src=""https://github.com/user-attachments/assets/1f676b75-6720-4a8a-9bc3-103ebe55e205"" />


##issue in styling of the content line when turning on the dark mode.","['Docs', 'Duplicate Report']",2025-04-29 14:24:33,2025-04-30 16:21:58,3,closed
61375,BUG: dot on Arrow Series produces a Numpy object result,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

series_result = pd.Series({""a"": 1.0}, dtype=""Float64"").dot(
    pd.DataFrame({""col1"": {""a"": 2.0}, ""col2"": {""a"": 3.0}}, dtype=""Float64"")
)
series_result.dtype  # is dtype('O')

series_result_2 = pd.Series({""a"": 1.0}, dtype=""float[pyarrow]"").dot(
    pd.DataFrame({""col1"": {""a"": 2.0}, ""col2"": {""a"": 3.0}}, dtype=""float[pyarrow]"")
)
series_result_2.dtype  # same, is dtype('O')

# `DataFrame.dot` was already fixed
df_result = pd.DataFrame({""col1"": {""a"": 2.0}, ""col2"": {""a"": 3.0}}, dtype=""Float64"").T.dot(
    pd.Series({""a"": 1.0}, dtype=""Float64"")
)
df_result.dtype  # is Float64Dtype()
```

### Issue Description

`Series.dot` with Arrow or nullable dtypes returns series result with numpy object dtype. This was reported in #53979 and fixed for DataFrames in #54025.

Possibly side notes: I believe the ""real"" issue here is that the implementation uses `.values` which returns a `dtype=object` array for the DataFrame. This seems directly related to #60038 and at least somewhat related to #60301 (which is also referenced in a comment on the former).

### Expected Behavior

I would expect `Series.dot` to return the ""best"" common datatype for the input datatypes (in the examples, would expect the appropriate float dtype)

```python
import pandas as pd

series_result = pd.Series({""a"": 1.0}, dtype=""Float64"").dot(
    pd.DataFrame({""col1"": {""a"": 2.0}, ""col2"": {""a"": 3.0}}, dtype=""Float64"")
)
series_result.dtype  # would expect Float64Dtype()

series_result_2 = pd.Series({""a"": 1.0}, dtype=""float[pyarrow]"").dot(
    pd.DataFrame({""col1"": {""a"": 2.0}, ""col2"": {""a"": 3.0}}, dtype=""float[pyarrow]"")
)
series_result_2.dtype  # would expect float[pyarrow]
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.12
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 85 Stepping 7, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.5
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.1
Cython                : None
sphinx                : 8.2.3
IPython               : 9.2.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.2
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.40
tables                : 3.10.2
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-04-29 14:11:39,2025-04-29 16:20:41,0,closed
61368,BUG: Python 3.14 may not increment refcount,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import warnings

warnings.simplefilter('error')

df = pd.DataFrame(
        {'year': [2018, 2018, 2018],
         'month': [1, 1, 1],
         'day': [1, 2, 3],
         'value': [1, 2, 3]})
df['date'] = pd.to_datetime(df[['year', 'month', 'day']])
```

### Issue Description

With python 3.14 and the Pandas main branch (or 2.2.3 with `pd.options.mode.copy_on_write = ""warn""`) the above fails with:

```python
Python 3.14.0a7+ (heads/main:276252565cc, Apr 27 2025, 16:05:04) [Clang 19.1.7 ]
Type 'copyright', 'credits' or 'license' for more information
IPython 9.3.0.dev -- An enhanced Interactive Python. Type '?' for help.
Tip: You can use LaTeX or Unicode completion, `\alpha<tab>` will insert the α symbol.

In [1]: import pandas as pd

In [2]: df = pd.DataFrame(
   ...:         {'year': [2018, 2018, 2018],
   ...:          'month': [1, 1, 1],
   ...:          'day': [1, 2, 3],
   ...:          'value': [1, 2, 3]})
   ...: df['date'] = pd.to_datetime(df[['year', 'month', 'day']])
<ipython-input-2-a8566e79621c>:6: ChainedAssignmentError: A value is trying to be set on a copy of a DataFrame or Series through chained assignment.
When using the Copy-on-Write mode, such chained assignment never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy.

Try using '.loc[row_indexer, col_indexer] = value' instead, to perform the assignment in a single step.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html
  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])

In [3]: import warnings

In [4]: warnings.simplefilter('error')

In [5]: df = pd.DataFrame(
   ...:         {'year': [2018, 2018, 2018],
   ...:          'month': [1, 1, 1],
   ...:          'day': [1, 2, 3],
   ...:          'value': [1, 2, 3]})
   ...: df['date'] = pd.to_datetime(df[['year', 'month', 'day']])
---------------------------------------------------------------------------
ChainedAssignmentError                    Traceback (most recent call last)
<ipython-input-5-a8566e79621c> in ?()
      2         {'year': [2018, 2018, 2018],
      3          'month': [1, 1, 1],
      4          'day': [1, 2, 3],
      5          'value': [1, 2, 3]})
----> 6 df['date'] = pd.to_datetime(df[['year', 'month', 'day']])

~/.virtualenvs/cp314-clang/lib/python3.14/site-packages/pandas/core/frame.py in ?(self, key, value)
   4156     def __setitem__(self, key, value) -> None:
   4157         if not PYPY:
   4158             if sys.getrefcount(self) <= 3:
-> 4159                 warnings.warn(
   4160                     _chained_assignment_msg, ChainedAssignmentError, stacklevel=2
   4161                 )
   4162

ChainedAssignmentError: A value is trying to be set on a copy of a DataFrame or Series through chained assignment.
When using the Copy-on-Write mode, such chained assignment never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy.

Try using '.loc[row_indexer, col_indexer] = value' instead, to perform the assignment in a single step.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html

In [6]: pd.__version__
Out[6]: '3.0.0.dev0+2080.g44c5613568'

```

With Python 3.14 there will be an optimization where the reference count is not incremented if Python can be sure that something above the calling scope will hold a reference for the life time of a scope.  This is causing a number of failures in test suites when reference counts are checked.  In this case I think it erroneously triggering the logic that the object is a intermediary.

Found this because it is failing the mpl test suite (this snippet is extracted from one of our tests).

With py313 I do not get this failure.

### Expected Behavior

no warning 

### Installed Versions

It is mostly development versions of things, this same env with pd main also fails.

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.14.0a7+
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.2-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Thu, 10 Apr 2025 18:43:59 +0000
machine               : x86_64
processor             :
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.3.0.dev0+git20250427.4961a14
pytz                  : 2025.2
dateutil              : 2.9.0.post1.dev6+g35ed87a.d20250427
pip                   : 25.0.dev0
Cython                : 3.1.0b1
sphinx                : None
IPython               : 9.3.0.dev
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.2
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 6.0.0.alpha0
matplotlib            : 3.11.0.dev732+g8fedcea7fc
numba                 : None
numexpr               : 2.10.3.dev0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.0.dev32+g7ef189757
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.16.0.dev0+git20250427.55cae81
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : 2025.3.1
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Discussion', 'Warnings', 'Copy / view semantics']",2025-04-28 01:23:56,2025-11-26 15:48:42,28,closed
61362,QST: best way to extend/subclass pandas.DataFrame,"### Research

- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/79594258/best-way-to-extend-subclass-pandas-dataframe

### Question about pandas

I've written a [package](https://www.github.com/rwijtvliet/portfolyo) to work with energy-related timeseries. At its center is a class ([`PfLine`](https://portfolyo.readthedocs.io/en/latest/core/pfline.html)) that is essentially a wrapper around pandas.DataFrame, and it implements various methods and properties that are also available on DataFrames - like `.loc`, `.asfreq()`, `.index`, etc.

I am currently in the middle of a rewrite of this package, and think it would be a good idea to have closer integration with pandas. [This page](https://pandas.pydata.org/docs/development/extending.html) lays out several possibilities, and I am unsure which route to take - and was hoping to find some sparring here.

Let me describe a bit what I'm trying to accomplish with the `PfLine` class:

  * Behaves like a DataFrame, with specific column names allowed and some data conversion (and validation) on initialisation.

  * Is immutable to avoid data from becoming inconsistent.

  * Has additional methods.

The methods could be directly under `PfLine.method()` or under e.g. `df.pfl.method()`.

What is probably important: a way is needed for the user to specify a (still under development) configuration object (`commodity`) when initialising the PfLine. This object contains information used in coercing the data, e.g. what are the correct units and which timezones are allowed for the index.","['Usage Question', 'Closing Candidate']",2025-04-26 22:17:25,2025-08-05 03:04:18,2,closed
61361,REGR: Fix signature of GroupBy.expanding,"Ref: https://github.com/pandas-dev/pandas/pull/61352#discussion_r2060726723

#61352 replaced `*args` and `**kwargs` in the signature of `GroupBy.expanding`. However I believe further arguments need to be added. We could also revert the PR instead.","['Bug', 'Groupby', 'Regression', 'Blocker', 'Window']",2025-04-26 17:38:41,2025-04-27 11:29:29,4,closed
61360,ENH: magic_case(),"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

we basically come up issue about not knowing the case of the column, we can print and view it but to make life little more easier I got magic_case created.

we have to pass the DataFrame and the column name we know (ignoring case) and we can have this assigned to a variable
mc=magic_case(df_2,'jack')
print(mc) # JaCK
and if there are multiple names with difference in case then it throws a value error with list of names
# ValueError: Multiple columns with the same name but different cases found: ['JaCK', 'JACk']


### Feature Description

def magic_case(df, column_name, new_name=None, inplace=False):
    """"""
    Find the exact case-sensitive column name in a DataFrame and optionally rename it.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        The DataFrame to search in
    column_name : str
        The case-insensitive column name to search for
    new_name : str, optional
        If provided, the column will be renamed to this value
    inplace : bool, default False
        If True and new_name is provided, modifies the DataFrame in-place and returns None.
        If False and new_name is provided, returns a copy of the DataFrame with renamed column.
        If new_name is None, this parameter has no effect.
    
    Returns:
    --------
    str or pandas.DataFrame or None
        - If new_name is None: returns the exact case-sensitive column name
        - If new_name is provided and inplace=False: returns the DataFrame with renamed column
        - If new_name is provided and inplace=True: returns None
    
    Raises:
    -------
    ValueError
        If no matching column is found or if multiple matches are found
    """"""
    # Check if the dataframe is empty or has no columns
    if df.empty or len(df.columns) == 0:
        raise ValueError(""DataFrame is empty or has no columns"")
        
    # Strip whitespace from column names for comparison
    clean_columns = {col.lower().strip(): col for col in df.columns}
    
    # Clean and lowercase the search term
    search_term = column_name.lower().strip()
    
    # Check if the lowercase version of the input exists
    if search_term not in clean_columns:
        matches = []
        # Check for partial matches (e.g., ""jack"" might match ""jackson"")
        for col_lower, col_original in clean_columns.items():
            if search_term in col_lower or col_lower in search_term:
                matches.append(col_original)
        
        if matches:
            original_column_name = matches[0]  # Get the first partial match
        else:
            raise ValueError(f""No column matching '{column_name}' was found in the DataFrame"")
    else:
        # Check for multiple exact matches with the same spelling but different cases
        exact_matches = [col for col in df.columns if col.lower().strip() == search_term]
        if len(exact_matches) > 1:
            raise ValueError(f""Multiple columns with the same name but different cases found: {exact_matches}"")
        
        # Get the exact case-sensitive column name
        original_column_name = clean_columns[search_term]
    
    # If new_name is not provided, just return the original column name
    if new_name is None:
        return original_column_name
    
    # If new_name is provided, rename the column
    if inplace:
        df.rename(columns={original_column_name: new_name}, inplace=True)
        return None
    else:
        return df.rename(columns={original_column_name: new_name})

### Alternative Solutions

# nothing

### Additional Context

if you had anything to say - please drop mail to akvamsikrishna@outlook.com with sub: magic_case() 😅 just to identify easily and prioritize your response over others.","['Enhancement', 'Indexing', 'Closing Candidate']",2025-04-26 07:05:56,2025-04-26 18:11:39,2,closed
61356,BUG: `DataFrameGroupBy.groups` fails when Categorical indexer contains NaNs and `dropna=False`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> df = DataFrame(
...         {
...             ""cat"": Categorical([""a"", np.nan, ""a""], categories=[""a"", ""b"", ""d""]),
...             ""vals"": [1, 2, 3],
...         }
...     )
>>> g = df.groupby(""cat"", observed=True, dropna=False)
>>> result = g.groups
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/workspaces/pandas/pandas/core/groupby/groupby.py"", line 569, in groups
    return self._grouper.groups
  File ""properties.pyx"", line 36, in pandas._libs.properties.CachedProperty.__get__
  File ""/workspaces/pandas/pandas/core/groupby/ops.py"", line 710, in groups
    return self.groupings[0].groups
  File ""properties.pyx"", line 36, in pandas._libs.properties.CachedProperty.__get__
  File ""/workspaces/pandas/pandas/core/groupby/grouper.py"", line 711, in groups
    return codes, uniques
  File ""/workspaces/pandas/pandas/core/arrays/categorical.py"", line 745, in from_codes
    dtype = CategoricalDtype._from_values_or_dtype(
  File ""/workspaces/pandas/pandas/core/dtypes/dtypes.py"", line 347, in _from_values_or_dtype
    dtype = CategoricalDtype(categories, ordered)
  File ""/workspaces/pandas/pandas/core/dtypes/dtypes.py"", line 230, in __init__
    self._finalize(categories, ordered, fastpath=False)
  File ""/workspaces/pandas/pandas/core/dtypes/dtypes.py"", line 387, in _finalize
    categories = self.validate_categories(categories, fastpath=fastpath)
  File ""/workspaces/pandas/pandas/core/dtypes/dtypes.py"", line 585, in validate_categories
    raise ValueError(""Categorical categories cannot be null"")
ValueError: Categorical categories cannot be null
>>>
```

### Issue Description

When using `df.groupby(cat, dropna=False).groups`, we encounter a `ValueError`. This is counter-intuitive, as grouping operations work without an issue.

```python
>>> df = DataFrame(
...         {
...             ""cat"": Categorical([""a"", np.nan, ""a""], categories=[""a"", ""b"", ""d""]),
...             ""vals"": [1, 2, 3],
...         }
...     )
>>> g = df.groupby(""cat"", observed=True, dropna=False)
>>> g.sum()
     vals
cat      
a       4
NaN     2
>>> g.sum().index
CategoricalIndex(['a', nan], categories=['a', 'b', 'd'], ordered=False, dtype='category', name='cat')
```


### Expected Behavior

`.groups` should return a dictionary which includes the NaN as the last entry.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 41131a14324ababc5c81f194de3d9a239d120f27
python                : 3.10.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2085.g41131a1432
numpy                 : 2.2.5
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 8.35.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.3.2
html5lib              : 1.1
hypothesis            : 6.131.8
gcsfs                 : 2025.3.2
jinja2                : 3.1.6
lxml.etree            : 5.4.0
matplotlib            : 3.10.1
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyreadstat            : 1.2.8
pytest                : 8.3.5
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.3.2
scipy                 : 1.15.2
sqlalchemy            : 2.0.40
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.3
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'Missing-data', 'Categorical']",2025-04-25 20:46:45,2025-04-28 16:47:11,1,closed
61350,ENH: th elements from Styler need the row scope,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Currently, the pandas [Styler](https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.html) API can be used to create a HTML table from a dataframe. However, the tables it generates are not accessible: it fails [WCAG/H63](https://www.w3.org/WAI/WCAG21/Techniques/html/H63).

### Feature Description

Ensure the output generated by Styler is accessible.

- `th` with class `row_heading` needs the `row` scope

I use the current workaround to add this rule myself:

```
    html_root = lxml.html.fromstring(frame_style.to_html())
    for th in html_root.xpath(""//th[contains(@class, 'row_heading')]""):
        th.set(""scope"", ""row"")
```

### Alternative Solutions

- Make the styler API more flexible for adding attributes. Currently, [set_td_classes](https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.set_td_classes.html#pandas.io.formats.style.Styler.set_td_classes) and [set_table_styles](https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.set_table_styles.html#pandas.io.formats.style.Styler.set_table_styles) aren't flexible enough for this, and [set_table_attributes](https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.set_table_attributes.html#pandas.io.formats.style.Styler.set_table_attributes) can't set attributes on `th` elements themselves.


### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-04-24 18:23:37,2025-04-24 19:03:21,1,closed
61347,Request for guidance on issues for upcoming PyData Yerevan pandas sprint,"Dear Pandas Team,

I am Suren Poghosyan, an Organizational Committee Member at PyData Yerevan. As you may already know, last summer we hosted an open-source contribution sprint focused on **pandas** library, in collaboration with Patrick Höfler, at the American University of Armenia. 

We are currently planning to run a follow-up sprint independently and would appreciate your guidance on which issues in the **pandas** GitHub repository are the most appropriate for a 2-3 hour contribution session. Furthermore, feel free to share any relevant issue which you would like to proceed with, despite our previous specification and time span.

On top of that, we are reaching out to make sure we’re following the proper contribution guidelines and not creating unnecessary noise or inconveniences in the issue tracker. We aim to contribute meaningfully and respectfully.

Looking forward to contributing again and strengthening our local culture of open-source collaboration.

In addition, here are the articles about our previous sprint:
[AUA to Host Inaugural PyData Yerevan Open Source pandas Sprint ](https://newsroom.aua.am/2024/06/10/aua-to-host-inaugural-pydata-yerevan-open-source-pandas-sprint/)
[PyData Yerevan Open Source pandas Sprint](https://newsroom.aua.am/event/pydata-yerevan-open-source-pandas-sprint/)
[PyData Yerevan hosted the inaugural Open Source pandas Sprint with Patrick Höfler - Linkedin](https://www.linkedin.com/posts/pydata-yerevan_yesterday-pydata-yerevan-hosted-the-inaugural-activity-7211723009479376896-nkZw?utm_source=share&utm_medium=member_desktop&rcm=ACoAADUCpscBtHmkZbcvJGJVB6J3UtccLYAcVPM)





Best regards,
Suren Poghosyan
Organizational Committee Member
PyData Yerevan


@jorisvandenbossche @TomAugspurger @jreback @WillAyd @mroeschke @jbrockmendel @datapythonista",['Community'],2025-04-24 12:34:24,2025-08-24 13:12:29,6,closed
61346,BUG: assignment via loc silently fails with differing dtypes,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
print(pd.__version__)
df = pd.DataFrame({'foo': ['2025-04-23', '2025-04-22']})
df['bar'] = pd.to_datetime(df['foo'], format='%Y-%m-%d')
df.loc[:, 'bar'] = df.loc[:, 'bar'].dt.strftime('%Y%m%d')
print(df)

# Yields
# 2.2.3
#           foo        bar
# 0  2025-04-23 2025-04-23
# 1  2025-04-22 2025-04-22
```

### Issue Description

I expect `bar` to look like 
```
20250423
20250422
```
instead of 
```
2025-04-23
2025-04-22
```


### Expected Behavior

`bar` should look like

```
20250423
20250422
```

### Installed Versions

<details>

```
[ins] In [2]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.10
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-372.32.1.el8_6.x86_64
Version               : #1 SMP Fri Oct 7 12:35:10 EDT 2022
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.12
sphinx                : None
IPython               : 8.35.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.3.2
matplotlib            : 3.10.1
numba                 : 0.61.2
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 15.0.2
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.39
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2025.3.1
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Dtype Conversions', 'Closing Candidate']",2025-04-23 18:48:39,2025-04-26 12:21:38,13,closed
61344,BUG: Series of bools with length mismatch does not raise when used with `.iloc`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

s = pd.Series([1, 2, 3])

mask_series = pd.Series([True, False, True, True])
result = s[mask_series]

print(result)
# Output:
# 0    1
# 2    3
# dtype: int64

mask_array = np.array([True, False, True, True])
print(s[mask_array])
# IndexError: Boolean index has wrong length: 4 instead of 3
```

### Issue Description

When using `.iloc` with a boolean Series mask whose length exceeds the target, pandas does not raise an error. This is inconsistent with numpy bool indexing, which raises an IndexError.



### Expected Behavior

`.iloc` should raise if the boolean Series mask length doesn’t match the target Series length.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 25e57c34158158de2cd5d2c0843f3e5babbeb3e5
python                : 3.12.9
python-bits           : 64
OS                    : Darwin
OS-release            : 24.0.0
Version               : Darwin Kernel Version 24.0.0: Mon Aug 12 20:49:48 PDT 2024; root:xnu-11215.1.10~2/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2080.g25e57c3415
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 9.0.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.3.0
html5lib              : 1.1
hypothesis            : 6.130.4
gcsfs                 : 2025.3.0
jinja2                : 3.1.6
lxml.etree            : 5.3.1
matplotlib            : 3.10.1
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.6
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyreadstat            : 1.2.8
pytest                : 8.3.5
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.3.0
scipy                 : 1.15.2
sqlalchemy            : 2.0.10
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Indexing']",2025-04-23 18:24:12,2025-04-24 20:20:13,1,closed
61342,BUG: Concatenating data frames with `MultiIndex` with `datetime64[ms]` dtype introduces `NaT` values to the index,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

def resample_each_item(dtype) -> pd.DataFrame:
    df = pd.DataFrame(
        [
            [""A"", ""2023-01-15"", 42],
            [""A"", ""2023-01-17"", 33],
            [""B"", ""2023-02-20"", 78],
            [""B"", ""2023-02-23"", 91],
        ],
        columns=[""item_id"", ""timestamp"", ""target""],
    )
    df[""timestamp""] = pd.to_datetime(df[""timestamp""]).astype(dtype)
    df = df.set_index([""item_id"", ""timestamp""])
    resampled = []
    for item_id in [""A"", ""B""]:
        resampled.append(pd.concat({item_id: df.loc[item_id].resample(""D"", level=""timestamp"").mean()}))
    return pd.concat(resampled)

print(resample_each_item(""datetime64[ns]""))
# For datetime64[ns] all timestamps are valid
#               target
#   timestamp         
# A 2023-01-15    42.0
#   2023-01-16     NaN
#   2023-01-17    33.0
# B 2023-02-20    78.0
#   2023-02-21     NaN
#   2023-02-22     NaN
#   2023-02-23    91.0

print(resample_each_item(""datetime64[ms]""))
# For datetime64[ms] or datetime64[s] dtypes, NaT values are introduced
#               target
#   timestamp         
# A 2023-01-15    42.0
#   NaT            NaN
#   NaT           33.0
# B 2023-02-20    78.0
#   NaT            NaN
#   NaT            NaN
#   NaT           91.0
```

### Issue Description

When concatenating data frames with `MultiIndex`, where one level is of type `datetime64[ms]` or `datetime64[s]`, some timestamps are replaced with `NaT`. If the timestamps are of dtype `datetime64[ns]`, no `NaT` values are introduced.

### Expected Behavior

No `NaT` values are introduced, regardless of whether the timestamp dtype is `datetime64[ms]`, `datetime64[s]` or `datetime64[ns]`.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.132-147.221.amzn2023.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Tue Apr  8 13:14:54 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.12.3
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.12.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : 0.61.2
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.40
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Datetime', 'MultiIndex']",2025-04-23 09:29:06,2025-04-24 20:18:43,1,closed
61338,BUG: Period datatype data gets mangled up in pivoting operation,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame({
    ""id1"": [1, 2], 
    ""id2"": [10, 20], 
    ""id3"": [100, 200], 
    ""period"":[pd.Period(""2021-01""), pd.Period(""2021-03"")]
}).set_index(['id1','id2'])
result = df.unstack().stack(future_stack=True)

#fails - unexpected
assert (result.loc[df.index]==df).all().all()
```

### Issue Description

The data in the ""period"" column gets mangled up, the value associated with the first record shows up twice and the value of the second record disappears. 
The problem appears with both `future_stack=True` and `future_stack=False`.
The problem does not appear when stacking ""period"" series, only when stacking dataframe (so following unstack(), the columns are a mutliindex).

### Expected Behavior

It is expected that `df.unstack().stack()` would return the original records unchanged.
Changing period dtype to 'str' behaves as expected:
```python
import pandas as pd
df = pd.DataFrame({
    ""id1"": [1, 2], 
    ""id2"": [10, 20], 
    ""id3"": [100, 200], 
    ""period"":[pd.Period(""2021-01""), pd.Period(""2021-03"")]
}).set_index(['id1','id2'])

#succeeds - expected
df2 = df.astype({""period"": ""str""})
result = df2.unstack().stack(future_stack=True)
assert (result.loc[df2.index]==df2).all().all()
```


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.11
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.226-214.880.amzn2.x86_64
Version               : #1 SMP Tue Oct 8 16:18:15 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 9.0.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.3.0
scipy                 : 1.15.2
sqlalchemy            : 2.0.39
tables                : None
tabulate              : 0.9.0
xarray                : 2025.1.2
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-04-22 14:49:35,2025-04-22 16:01:08,1,closed
61336,ENH: IDEA  Introduce axis→0/axis→1 arrow aliases to disambiguate direction vs. label operations,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

<ENG>
The `axis` parameter currently serves two distinct purposes:

1. *Along‑axis* operations that reduce or transform values (e.g. `apply`, `sum`)
2. *Label‑targeting* operations that modify or drop index / column labels (e.g. `drop`, `rename`)

Because both use the same syntax (`axis=0` or `axis=1`), many users mis‑interpret which dimension is affected.

<한국어>
현재 axis 매개변수는 서로 다른 두 가지 목적으로 사용되고 있습니다:
값을 축소하거나 변환하는 축 방향 연산 (예: apply, sum)
인덱스/열 레이블을 수정하거나 삭제하는 레이블 대상 연산 (예: drop, rename)
두 경우 모두 동일한 구문(axis=0 또는 axis=1)을 사용하기 때문에 많은 사용자가 어떤 차원이 영향을 받는지 혼동합니다.

### Feature Description

<ENG>
Proposed API
Keep existing syntax and add an **arrow alias** that makes the “direction” explicit:

| Syntax | Meaning |
|--------|---------|
| `axis=0` *(unchanged)* | target **index labels** (delete / rename) |
| `axis=1` *(unchanged)* | target **column labels** |
| `axis→0` *(new)* | operate **along index** – treat each **column vector** |
| `axis→1` *(new)* | operate **along columns** – treat each **row vector** |

Arrow aliases are optional; existing code keeps working unchanged.

<한국어>

제안된 API
기존 구문을 유지하면서 ""방향""을 명확히 나타내는 화살표 별칭을 추가합니다

| Syntax | Meaning |
|--------|---------|
| `axis=0` *(변경없음)* | 인덱스 레이블 대상 (삭제 / 이름 변경) |
| `axis=1` *(변경없음)* | 열 레이블 대상 |
| `axis→0` *(신규)* | 인덱스 방향으로 연산 – 각 열 벡터 처리 |
| `axis→1` *(신규)* | 열 방향으로 연산 – 각 행 벡터 처리 |

화살표 별칭은 선택 사항이며, 기존 코드는 변경 없이 계속 작동합니다.



### Alternative Solutions

<ENG>

| Alias idea | Interpretation |
|------------|----------------|
| **`axis→0`** | operate **along index** (column‑wise) |
| **`axis→1`** | operate **along columns** (row‑wise) |

**Benefits**
* Greatly reduces beginner confusion around `axis`.
* Preserves full NumPy compatibility.
* Requires minimal code changes (add alias mapping in `axis_aliases`).
* 
<한국어>

| 별칭 아이디어 | 해석|
|------------|----------------|
| **`axis→0`** | 인덱스 방향으로 연산 (열 단위) |
| **`axis→1`** | 열 방향으로 연산 (행 단위) |

장점
axis에 대한 초보자의 혼란을 크게 줄입니다.
NumPy 호환성을 완전히 유지합니다.
최소한의 코드 변경만 필요합니다 (axis_aliases에 별칭 매핑 추가).

### Additional Context

See repeated questions on Stack Overflow:<br>
<https://stackoverflow.com/q/26716616>.","['Enhancement', 'Needs Discussion', 'Closing Candidate']",2025-04-22 07:31:10,2025-04-23 16:19:48,5,closed
61319,ENH: grep-like select columns of a DataFrame by a part of their names,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could grep-like select columns of a DataFrame by a part of their names, and return a subset of the original DataFrame containing only columns that match the substring.

### Feature Description

```py
from typing import List, Union
import pandas as pd


class ExtendedDF(pd.DataFrame):
    @property
    def _constructor(self):
        return ExtendedDF

    def select_by_substr(self, substr: Union[str, List[str]], *, ignore_case: bool = True) -> Union[pd.DataFrame, 'ExtendedDF']:
        """"""grep-like select columns of a DataFrame by a part of their names.

        Args:
            substr (Union[str, List[str]]): a string or a list of strings to be used as search patterns
            ignore_case (bool): if True (default), ignore search pattern case

        Returns:
            pd.DataFrame: a subset of the original DataFrame containing only columns that match the substring

        Usage:

        Consider two DataFrame objects extracted from two different sources, and thus varying in their column names:

        ```py
        df1 = pd.DataFrame({
            'Distance': [105.0, 0.0, 4.0, 1.0, 1241.0],
            'Distance_percent': [0.2, 0.0, 5.2, 11.1, 92.8],
            'Mixed': [921.0, 0.0, 52.0, 5.0, 0.0],
            'Mixed_percent': [1.9, 0.0, 67.5, 55.6, 0.0],
            'avg_diff': [121146.9, 293246.3, 212169.9, 41299.8, 29438.3],
            'med_diff': [17544.0, 1657.0, 55205.0, 95750.0, 2577.0],
        })
        df2 = pd.DataFrame({
            'distance': [105.0, 0.0, 4.0, 1.0, 1241.0],
            'distance_percent': [0.2, 0.0, 5.2, 11.1, 92.8],
            'mixed': [921.0, 0.0, 52.0, 5.0, 0.0],
            'mixed_percent': [1.9, 0.0, 67.5, 55.6, 0.0],
            'diff_avg': [121146.9, 293246.3, 212169.9, 41299.8, 29438.3],
            'diff_med': [17544.0, 1657.0, 55205.0, 95750.0, 2577.0],
        })
        df1 = ExtendedDF(df1)
        df2 = ExtendedDF(df2)
        ```
        ```
        df1
           Distance  Distance_percent  Mixed  Mixed_percent  avg_diff  med_diff
        0     105.0               0.2  921.0            1.9  121146.9   17544.0
        1       0.0               0.0    0.0            0.0  293246.3    1657.0
        2       4.0               5.2   52.0           67.5  212169.9   55205.0
        3       1.0              11.1    5.0           55.6   41299.8   95750.0
        4    1241.0              92.8    0.0            0.0   29438.3    2577.0

        df2
           distance  distance_percent  mixed  mixed_percent  diff_avg  diff_med
        0     105.0               0.2  921.0            1.9  121146.9   17544.0
        1       0.0               0.0    0.0            0.0  293246.3    1657.0
        2       4.0               5.2   52.0           67.5  212169.9   55205.0
        3       1.0              11.1    5.0           55.6   41299.8   95750.0
        4    1241.0              92.8    0.0            0.0   29438.3    2577.0
        ```

        As an analyst, I need to inspect which column is which between the two datasets:

        (a) either by defining a single string search pattern (`ignore_case=True` by default):

        ```py
        cols_to_select = 'diff'
        print('df1:')
        print(df1.select_by_substr(cols_to_select).T) # transposed for a better legibility
        print()
        print('df2:')
        print(df2.select_by_substr(cols_to_select).T) # transposed for a better legibility
        ```
        ```
        df1:
                         0         1         2        3        4
        avg_diff  121146.9  293246.3  212169.9  41299.8  29438.3
        med_diff   17544.0    1657.0   55205.0  95750.0   2577.0

        df2:
                         0         1         2        3        4
        diff_avg  121146.9  293246.3  212169.9  41299.8  29438.3
        diff_med   17544.0    1657.0   55205.0  95750.0   2577.0
        ```

        (b) or by defining a list of string search patterns (`ignore_case=True` by default):

        ```py
        cols_to_select = ['dist', 'Mix']
        print('df1:')
        print(df1.select_by_substr(cols_to_select).T) # transposed for a better legibility
        print()
        print('df2:')
        print(df2.select_by_substr(cols_to_select).T) # transposed for a better legibility
        ```
        ```
        df1:
                              0    1     2     3       4
        Mixed             921.0  0.0  52.0   5.0     0.0
        Distance          105.0  0.0   4.0   1.0  1241.0
        Mixed_percent       1.9  0.0  67.5  55.6     0.0
        Distance_percent    0.2  0.0   5.2  11.1    92.8

        df2:
                              0    1     2     3       4
        mixed_percent       1.9  0.0  67.5  55.6     0.0
        mixed             921.0  0.0  52.0   5.0     0.0
        distance          105.0  0.0   4.0   1.0  1241.0
        distance_percent    0.2  0.0   5.2  11.1    92.8
        ```

        (c) or, same as (b) but with an explicit `ignore_case=False`:

        ```py
        cols_to_select = ['dist', 'Mix']
        print('df1:')
        print(df1.select_by_substr(cols_to_select, ignore_case=False).T) # transposed for a better legibility
        print()
        print('df2:')
        print(df2.select_by_substr(cols_to_select, ignore_case=False).T) # transposed for a better legibility
        ```
        ```
        df1:
                           0    1     2     3    4
        Mixed_percent    1.9  0.0  67.5  55.6  0.0
        Mixed          921.0  0.0  52.0   5.0  0.0

        df2:
                              0    1    2     3       4
        distance_percent    0.2  0.0  5.2  11.1    92.8
        distance          105.0  0.0  4.0   1.0  1241.0
        ```
        """"""
        substr = [substr] if isinstance(substr, str) else substr
        if ignore_case:
            selected_cols = [col_name for col_name in self.columns for s in substr if s.casefold() in col_name.casefold()]
        else:
            selected_cols = [col_name for col_name in self.columns for s in substr if s in col_name]
        selected_cols = list(set(selected_cols))
        return self[selected_cols]
```

### Alternative Solutions

Idk

### Additional Context

_No response_","['Enhancement', 'Indexing', 'Needs Info']",2025-04-20 14:10:10,2025-04-21 15:49:27,3,closed
61318,ENH: inspect duplicate rows for columns that vary,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I had a function that would inspect a DataFrame that has duplicate values and yield, per each group of rows that have a duplicate value, a subset of the input DataFrame featuring only the columns that vary.

### Feature Description

```py
from typing import Union
import pandas as pd


class ExtendedDF(pd.DataFrame):
    @property
    def _constructor(self):
        return ExtendedDF

    def inspect_duplicates(self, key_col: str) -> Union[pd.DataFrame, 'ExtendedDF']:
        """"""Inspects a DataFrame that has duplicate values in the `key_col` column,
        and yields, per each group of rows that have same `key_col` value, a subset
        of the input DataFrame featuring only the columns that vary.

        Args:
            key_col (str): name of the column with duplicate values

        Yields:
            pd.DataFrame: per each group of rows that have same `key_col` value,
            yields a subset of the input DataFrame featuring only the columns that
            vary.

        Examples:

        Consider a dataset containing ramen ratings with duplicates:

        ```py
        df = pd.DataFrame({
            'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],
            'style': ['cup', 'pack', 'cup', 'cup', 'pack'],
            'rating': [4, 3.5, 4, 15, 5],
            'col_that_doesnt_change': ['so yummy' for _ in range(5)],
            'another_col_that_doesnt_change': ['mmm love it' for _ in range(5)],
        })
        df = ExtendedDF(df)
        ```
        ```
        df
             brand  style  rating col_that_doesnt_change another_col_that_doesnt_change
        0  Yum Yum    cup     4.0               so yummy                    mmm love it
        1  Yum Yum   pack     3.5               so yummy                    mmm love it
        2  Indomie    cup     4.0               so yummy                    mmm love it
        3  Indomie    cup    15.0               so yummy                    mmm love it
        4  Indomie   pack     5.0               so yummy                    mmm love it
        ```

        Inspect the duplicates using 'brand' column as the key:

        ```py
        print(
            *df.inspect_duplicates('brand')
        )
        ```
        ```
             brand  rating
        2  Indomie     4.0
        3  Indomie    15.0
        4  Indomie     5.0

             brand style  rating
        0  Yum Yum   cup     4.0
        1  Yum Yum  pack     3.5
        ```

        Inspect the duplicates using 'style' column as the key:

        ```py
        print(
            *df.inspect_duplicates('style')
        )
        ```
        ```
          style    brand
        0   cup  Yum Yum
        2   cup  Indomie
        3   cup  Indomie

          style    brand  rating
        1  pack  Yum Yum     3.5
        4  pack  Indomie     5.0
        ```

        Inspect the duplicates using 'rating' column as the key:

        ```py
        print(
            *df.inspect_duplicates('rating')
        )
        ```
        ```
           rating    brand
        0     4.0  Yum Yum
        2     4.0  Indomie
        ```

        You can also concatenate everything that is yielded into a single DataFrame:

        ```py
        print(
            pd.concat([
                *df.inspect_duplicates('brand')
            ])
        )
        ```
        ```
             brand style  rating
        0  Yum Yum   cup     4.0
        1  Yum Yum  pack     3.5
        2  Indomie   NaN     4.0
        3  Indomie   NaN    15.0
        4  Indomie   NaN     5.0
        ```
        """"""
        mark_all_dupl_mask = self.duplicated(key_col, keep=False)
        df_dupl = self.loc[mark_all_dupl_mask]
        for k in set(df_dupl[key_col].values):
            sub_df = self.loc[self[key_col] == k]
            mask_eq = sub_df.iloc[0] != sub_df.iloc[1]
            diff_cols = mask_eq.loc[mask_eq].index.values
            yield sub_df.loc[:, [key_col] + list(diff_cols)]
```

### Alternative Solutions

None

### Additional Context

Authors:

- @miraaitsaada
- @kirisakow","['Enhancement', 'Groupby', 'Closing Candidate']",2025-04-20 12:11:06,2025-04-21 15:50:36,2,closed
61312,BUG: duplicated() is reporting rows as duplicates when they aren't upon visual inspection.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
I got the creditcard.csv from Kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud 

import pandas as pd
import numpy as np

credit_card_df = pd.read_csv('creditcard.csv')

duplicated_df = credit_card_df[credit_card_df.duplicated()]

duplicated_df
```

### Issue Description

If you look at the output of the *duplicated_df* you can see rows 33 and 35 reported as duplicates when they aren't. The values are close but not exact duplicates

### Expected Behavior

Would expect these rows to not be reported as duplicates because the values in the columns that aren't named 'Time' are not identical to each other.

### Installed Versions

![Image](https://github.com/user-attachments/assets/ac5c5337-d62e-41a5-8562-4372d34cb48c)
","['Bug', 'Needs Triage', 'Closing Candidate']",2025-04-19 20:18:24,2025-04-19 21:02:03,2,closed
61311,BUG: ``'ArrowExtensionArray' object has no attribute 'max'`` when passing pyarrow-backed series to `.iloc`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [1]: import pandas as pd

In [2]: df = pd.DataFrame({""a"": [1, 2], ""c"": [0, 2], ""d"": [""c"", ""a""]})

In [3]: df.iloc[:, df['c']]  # works fine
Out[3]:
   a  d
0  1  c
1  2  a

In [4]: df = pd.DataFrame({""a"": [1, 2], ""c"": [0, 2], ""d"": [""c"", ""a""]}).convert_dtypes(dtype_backend='pyarrow')

In [5]: df.iloc[:, df['c']]  # now, it raises
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[5], line 1
----> 1 df.iloc[:, df['c']]

File ~/pandas-dev/pandas/core/indexing.py:1189, in _LocationIndexer.__getitem__(self, key)
   1187     if self._is_scalar_access(key):
   1188         return self.obj._get_value(*key, takeable=self._takeable)
-> 1189     return self._getitem_tuple(key)
   1190 else:
   1191     # we by definition only have the 0th axis
   1192     axis = self.axis or 0

File ~/pandas-dev/pandas/core/indexing.py:1692, in _iLocIndexer._getitem_tuple(self, tup)
   1691 def _getitem_tuple(self, tup: tuple):
-> 1692     tup = self._validate_tuple_indexer(tup)
   1693     with suppress(IndexingError):
   1694         return self._getitem_lowerdim(tup)

File ~/pandas-dev/pandas/core/indexing.py:975, in _LocationIndexer._validate_tuple_indexer(self, key)
    973 for i, k in enumerate(key):
    974     try:
--> 975         self._validate_key(k, i)
    976     except ValueError as err:
    977         raise ValueError(
    978             f""Location based indexing can only have [{self._valid_types}] types""
    979         ) from err

File ~/pandas-dev/pandas/core/indexing.py:1613, in _iLocIndexer._validate_key(self, key, axis)
   1610         raise IndexError(f"".iloc requires numeric indexers, got {arr}"")
   1612     # check that the key does not exceed the maximum size of the index
-> 1613     if len(arr) and (arr.max() >= len_axis or arr.min() < -len_axis):
   1614         raise IndexError(""positional indexers are out-of-bounds"")
   1615 else:

AttributeError: 'ArrowExtensionArray' object has no attribute 'max'
```

### Issue Description

`df.iloc[:, df['c']]` works for regular pandas dataframes but raises for pyarrow-backed ones

spotted in [narwhals](https://github.com/narwhals-dev/narwhals)

### Expected Behavior

```
   a  d
0  1  c
1  2  a
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 57fd50221ea3d5de63d909e168f10ad9fc0eee9b
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1979.g57fd50221e
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 8.33.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.2.0
html5lib              : 1.1
hypothesis            : 6.127.5
gcsfs                 : 2025.2.0
jinja2                : 3.1.5
lxml.etree            : 5.3.1
matplotlib            : 3.10.1
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyreadstat            : 1.2.8
pytest                : 8.3.5
python-calamine       : None
pytz                  : 2025.1
pyxlsb                : 1.0.10
s3fs                  : 2025.2.0
scipy                 : 1.15.2
sqlalchemy            : 2.0.38
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Indexing', 'Arrow']",2025-04-19 19:08:44,2025-08-05 17:21:49,1,closed
61310,BUG: no last function in window rolling,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
s = pd.Series(range(5))
s.rolling(3).last()
```

### Issue Description

The reproducible example is right from the [docs](https://pandas.pydata.org/docs/dev/reference/api/pandas.core.window.rolling.Rolling.last.html).
Same goes for agg invocations.

<ins>Typical error messages:</ins>
  * AttributeError: 'last' is not a valid function for 'Rolling' object
  * AttributeError: 'Rolling' object has no attribute 'last'

### Expected Behavior

Last if available.

### Installed Versions

commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.14.2-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Thu, 10 Apr 2025 18:43:59 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 9.1.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.2
matplotlib            : 3.10.1
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'Needs Info', 'Window', 'Closing Candidate']",2025-04-19 10:00:37,2025-04-21 02:52:26,3,closed
61309,"BUG: to_latex, when escaped=True, doesn't escape columns name","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame(data=[{""hello"":""world""}])
df.columns.name = ""hello_world""
df.to_latex(""table.tex"",escape=True)
```

### Issue Description

Contents of df.columns.name aren't escaped.

### Expected Behavior

df.columns.name should be escaped.

### Installed Versions

INSTALLED VERSIONS
------------------
commit           : 0f437949513225922d851e9581723d82120684a6
python           : 3.8.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.15.167.4-microsoft-standard-WSL2
Version          : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 2.0.3
numpy            : 1.24.4
pytz             : 2025.2
dateutil         : 2.9.0.post0
setuptools       : 44.0.0
pip              : 20.0.2
Cython           : 3.0.12
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 3.1.6
IPython          : 8.12.3
pandas_datareader: None
bs4              : None
bottleneck       : None
brotli           : None
fastparquet      : None
fsspec           : None
gcsfs            : None
matplotlib       : 3.7.5
numba            : None
numexpr          : 2.8.6
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : None
snappy           : None
sqlalchemy       : None
tables           : 3.8.0
tabulate         : None
xarray           : None
xlrd             : None
zstandard        : None
tzdata           : 2025.2
qtpy             : None
pyqt5            : None
","['Bug', 'Duplicate Report', 'Styler']",2025-04-19 09:43:14,2025-04-19 12:37:13,4,closed
61304,ENH: Add Optional Schema Definitions to Enable IDE Autocompletion,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Pandas is widely used in data-heavy workflows, and in many cases, the structure of a DataFrame is known in advance — especially when loading from sources like CSVs, databases, or APIs.

However, pandas DataFrames are fully dynamic, so IDEs and static type checkers cannot infer the structure. This limits productivity, especially in large codebases, because Column names don’t autocomplete

We’re not asking for runtime schema enforcement or data validation — we’re already familiar with Pandera and similar tools. What’s missing is a mechanism for IDEs and static tools (like Pylance and MyPy) to recognize DataFrame schemas for better code intelligence.


### Feature Description

Introduce an optional way to define column names and types for a DataFrame that tools like VS Code + Pylance can use for autocompletion and type hints.

Example syntax (suggested API):

```python
import pandas as pd
from pandas.typing import Schema  # hypothetical

class OrderSchema(Schema):
    OrderID: int
    CustomerName: str
    OrderDate: str
    Product: str
    Quantity: int
    Price: float
    Country: str

df: pd.DataFrame[OrderSchema] = pd.read_csv(""orders.csv"")

# IDE should support:
df.Country           # autocomplete & type: str
```

This would behave similarly to how TypedDict or Pydantic models enable structure-aware development, but focused on DataFrame-level constructs.

It does not need to affect runtime at all — just serve as a static hint for tooling. 


### Alternative Solutions

No

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-04-17 20:23:14,2025-04-17 20:56:31,1,closed
61300,DOC: DataFrameGroupBy.filter documentation is misleading,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html and https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.SeriesGroupBy.filter.html

### Documentation problem

Both `DataFrameGroupBy.filter` and `SeriesGroupBy.filter` state that they ""filter *elements* from groups"".

This is not true, these methods filter whole groups. If you attempt to filter individual elements within a group by returning a series of boolean you get an error:
```
df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
                          'foo', 'bar'],
                   'B' : [1, 2, 3, 4, 5, 6],
                  'C' : [2.0, 5., 8., 1., 2., 9.]})
df.groupby(""A"").filter(lambda x: x['B'] > 1).sum()
```
```
TypeError: filter function returned a Series, but expected a scalar bool
```

### Suggested fix for documentation

Suggested documentation:
```
Filter groups that don’t satisfy a criterion.

Groups are filtered if they do not satisfy the boolean criterion specified by func.
```","['Docs', 'Needs Triage']",2025-04-17 01:35:00,2025-04-17 04:34:38,1,closed
61298,"I would like to join the pandas community, but the Slack link is broken and I cannot join.","I would like to join the pandas community on Slack, so I would appreciate if you could provide me with a new invitation link.",[],2025-04-16 05:05:35,2025-04-17 16:11:22,3,closed
61296,BUG: Underscores aren't escaped in LaTeX outputs,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame([{""header_1"": 1, ""header_2"": 2}, {""header_1"": 3, ""header_2"": 4}])
print(df.to_latex(index=False))
```

### Issue Description

Underlines in strings ( _ ) should be escaped in LaTeX outputs given than underscores in LaTeX represent subscripts.

### Expected Behavior

The expected output of the provided code example _should_ be:

```latex
\begin{tabular}{rr}
\toprule
header\_1 & header\_2 \\
\midrule
1 & 2 \\
3 & 4 \\
\bottomrule
\end{tabular}
```

But instead is:

```latex
\begin{tabular}{rr}
\toprule
header_1 & header_2 \\
\midrule
1 & 2 \\
3 & 4 \\
\bottomrule
\end{tabular}
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.7
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 9.1.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.4
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.12.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-04-15 23:52:45,2025-04-15 23:55:10,1,closed
61295,BUG: df.assign no longer works with multilevel columns,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

# Creating a DataFrame with multilevel columns
arrays = [['A', 'A', 'B', 'B'], ['one', 'two', 'one', 'two']]
tuples = list(zip(*arrays))
index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = pd.DataFrame(np.random.randn(3, 4), columns=index)

# Try to create a new column using ""assign""
df = df.assign(**{('C', 'one'): [1, 2, 3]})
```

### Issue Description

The final line reports the error: TypeError: keywords must be strings

### Expected Behavior

It should create a new column (""C"", 'one') with 1,2,3 in it.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.3
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_Australia.1252

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : 7.3.7
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : 1.2.7
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : 3.1.1
zstandard             : None
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
",['Bug'],2025-04-15 22:38:25,2025-04-16 13:40:29,4,closed
61292,BUG: `values` argument ignored when also supplied to `index`/`columns` in `pivot_table`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd
from pandas import Index, MultiIndex
import pandas.testing as tm


def test_pivot_table_values_in_columns():
    """"""``values`` arg is shared between ``values`` and ``columns``.""""""
    data = [
        [""A"", 1, 50, -1],
        [""B"", 1, 100, -2],
        [""A"", 2, 100, -2],
        [""B"", 2, 200, -4],
    ]
    df = pd.DataFrame(data=data, columns=[""index"", ""col"", ""value"", ""extra""])
    result = df.pivot_table(values=""value"", index=""index"", columns=[""col"", ""value""])
    nan = np.nan
    e_data = [
        [50.0, nan, 100.0, nan],
        [nan, 100.0, nan, 200.0],
    ]
    e_index = Index(data=[""A"", ""B""], name=""index"")
    e_cols = MultiIndex.from_arrays(
        arrays=[[1, 1, 2, 2], [50, 100, 100, 200]], names=[""col"", ""value""]
    )
    expected = pd.DataFrame(data=e_data, index=e_index, columns=e_cols)
    tm.assert_frame_equal(left=result, right=expected)


def test_pivot_table_values_in_index():
    """"""``values`` arg is shared between ``values`` and ``index``.""""""
    data = [
        [""A"", 1, 50, -1],
        [""B"", 1, 100, -2],
        [""A"", 2, 100, -2],
        [""B"", 2, 200, -4],
    ]
    df = pd.DataFrame(data=data, columns=[""index"", ""col"", ""value"", ""extra""])
    result = df.pivot_table(values=""value"", index=[""index"", ""value""], columns=""col"")
    nan = np.nan
    e_data = [
        [50.0, nan],
        [nan, 100.0],
        [100.0, nan],
        [nan, 200.0],
    ]
    e_index = MultiIndex.from_arrays(
        arrays=[[""A"", ""A"", ""B"", ""B""], [50, 100, 100, 200]], names=[""index"", ""value""]
    )
    e_cols = Index(data=[1, 2], name=""col"")
    expected = pd.DataFrame(data=e_data, index=e_index, columns=e_cols)
    tm.assert_frame_equal(left=result, right=expected)


test_pivot_table_values_in_columns()  # Fails.
test_pivot_table_values_in_index()  # Fails.
```

### Issue Description

When the column supplied to `values` in [`pandas.DataFrame.pivot_table`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html) is also supplied to `index` or `columns`, the resulting `DataFrame` does not contain the aggregations of the `values` argument. If any extra column(s) are present, those columns are aggregated instead of those supplied to `values`. This is similar to issue #57876, but the additional columns result in a non-empty `DataFrame`.

### Expected Behavior

I would expect the two tests above to pass, i.e., the `values` arg is aggregated instead of the non-supplied ""extra"" column.

```python
# Expected output of ``test_pivot_table_values_in_columns``:
col       1             2       
value   50     100    100    200
index                           
A      50.0    NaN  100.0    NaN
B       NaN  100.0    NaN  200.0
```

```python
# Expected output of ``test_pivot_table_values_in_index``:
col              1      2
index value              
A     50      50.0    NaN
      100      NaN  100.0
B     100    100.0    NaN
      200      NaN  200.0
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.3
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 9.1.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Reshaping']",2025-04-15 16:18:14,2025-04-23 21:47:19,0,closed
61281,ENH: preview_csv(***.csv) for Fast First-N-Line Preview on Large Plus Size (>100GB),"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

The current `pandas.read_csv()` implementation is designed for robust and complete CSV parsing. However, even when users request only a few lines using `nrows=X`, the function:

- **Initializes the full parsing engine**
- Performs **column-wise type inference**
- Scans for **delimiter/header consistency**
- May **read a large portion or all of the file**, even for small previews

For **large datasets** (10–100GB CSVs), this results in significant I/O, CPU, and memory overhead — all when the user likely just wants a **quick preview** of the data.

This is a common pattern in:
- Exploratory Data Analysis (EDA)
- Data cataloging and profiling
- Schema validation or column sniffing
- Dashboards and notebook tooling

Currently, users resort to workarounds like:
```python
pd.read_csv(..., chunksize=5)
next(...)
```
or shell-level hacks like:

```bash
head -n 5 large_file.csv
```
These are non-intuitive, unstructured, or outside the pandas ecosystem.

### Feature Description

## Introduces a new Function

```python
pandas.preview_csv(filepath_or_buffer, nrows=5, ...)
```

 ### Goals
- Read only the first n rows + header lines
- Avoid loading or inferring types from null dataset
- No full cloumn validation
- Fallback to object dtype unless `dtype_infer = true`
- Support basic options like delimiter, encoding, header presence.

### Proposed API:
```python
def preview_csv(
    filepath_or_buffer,
    nrows: int = 5,
    delimiter: str = "","",
    encoding: str = ""utf-8"",
    has_header: bool = True,
    dtype_infer: bool = False,
    as_generator: bool = False
) -> pd.DataFrame:
    ...

```



### Alternative Solutions

| **Tool / Method**                  | **Behavior**                                                                  | **Limitation**                                                                 |
|-----------------------------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| `pd.read_csv(nrows=X)`            | Reads entire file into memory, performs dtype inference and column validation | Not optimized for quick previews; incurs overhead even for small `nrows`      |
| `pd.read_csv(chunksize=X)`        | Returns an iterator of chunks (DataFrames of size `X`)                        | Requires non-intuitive iterator handling; users often want `DataFrame` directly |
| `csv.reader + slicing`            | Python’s built-in CSV reader is lightweight and fast                          | Returns raw lists, not a DataFrame; lacks header handling and column inference |
| `subprocess.run([""head"", ""-n""])`  | OS-level utility that returns first N lines                                   | Not portable across platforms, doesn't integrate with DataFrame workflow      |
| `Polars: pl.read_csv(..., n_rows)`| Rust-based, blazing fast CSV reader                                           | Requires installing a new library; pandas users might not want to switch ecosystems |
| `Dask: dd.read_csv(...).head()`   | Lazy, out-of-core loading with chunked processing                             | Overhead of distributed engine is unnecessary for simple previews             |
| `open(...).readlines(N)`          | Naive Python read of first N lines                                            | Doesn’t handle parsing, delimiters, or schema properly                        |
| `pyarrow.csv.read_csv(...)[0:X]`  | Efficient Arrow-based preview                                                 | Requires using Apache Arrow APIs; returns Arrow tables unless converted       |


While workarounds exist, none provide a **clean, idiomatic, native pandas function** to:
- Efficiently load the first N rows
- Return a `DataFrame` immediately
- Avoid dtype inference
- Skip full file validation
- Avoid requiring third-party dependencies

A dedicated `pandas.preview_csv()` would fill this gap and offer an elegant, performant solution for quick data previews.



### Additional Context

_No response_","['Enhancement', 'IO CSV', 'Needs Discussion', 'Closing Candidate']",2025-04-13 14:08:49,2025-08-18 01:05:06,4,closed
61280,DOC: to_json for stream object,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html

### Documentation problem

Currently the docs for `to_json`method only mentions about file-like object, yet we can pass buffer which are more like stream object. This was raised on the stubs repo (https://github.com/pandas-dev/pandas-stubs/issues/1179).
Should the docs reflect the ability of not just file-like but also stream-like? It seems to be supported at run time for sure.

Thanks!

### Suggested fix for documentation

Add mention of stream-like object for the `path_or_buf` argument.","['Docs', 'IO JSON', 'Closing Candidate']",2025-04-12 16:35:16,2025-04-14 14:03:02,2,closed
61277,WEB: Remove NumFOCUS and Coiled as sponsors,"pandas is it's not really well funded these days, and we've been updating the sponsors list recently to be more accurate, and I'd like to still remove NumFOCUS and Coiled from the list. Of course I do appreciate the support that both NumFOCUS and Coiled provided and still provide to pandas, but I think having them as sponsors is at this point misleading and create the false impression that pandas is better funded than it is.

For NumFOCUS, we pay them 15% of the pandas income for financial support mostly.  We get few other things like legal support or small development grants. But I wouldn't list them as ""pandas has the support of NumFOCUS"" in periods of very low funding like now, as we probably supported NumFOCUS more than NumFOCUS supported us. For reference OpenCollective US would charge as 10%, and OpenCollective EU 8%.

For Coiled, I saw Patrick made two commits in the last 7 months, and if I'm not wrong he's not very active in PR review or other maintenance tasks recently. Truly thankful to them as Patrick could make lots of quality work for pandas sponsored by them in the past, but as of today, having them as a sponsor it's again misleading visitors of our website into thinking pandas is in a healthy financial state as it was a couple of years ago, when in practice it's not.

I hope having a more accurate list can help find new sponsors, help when we ask new grants, and maybe even help bring awareness of our current sponsors how key is their support at this point.

@pandas-dev/pandas-core @phofl please let me know if any objection, otherwise I'll move forward with this in the next few days.",['Web'],2025-04-12 13:03:38,2025-04-13 15:39:45,12,closed
61276,BUG:FutureWarning for palette parameter without hue in faceted distributions,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Sample data
tips = sns.load_dataset(""tips"")

# Faceted distribution plot
sns.displot(data=tips, x=""total_bill"", palette=""viridis"")
plt.show()
```

### Issue Description

When using faceted distributions with Seaborn and passing the `palette` parameter without assigning `hue`, a FutureWarning is raised. The warning suggests assigning `hue` and setting `legend=False` to avoid deprecation in future versions (v0.14.0). This behavior needs clarification or adjustment in Pandas' integration with Seaborn plotting functions.
observed behavior:
FutureWarning: Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.


### Expected Behavior

The warning should either be suppressed or handled gracefully within Pandas' plotting functions when interfacing with Seaborn.


### Installed Versions

<details>

/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.85+
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.2
pytz                  : 2025.2
dateutil              : 2.8.2
setuptools            : 75.2.0
pip                   : 24.1.2
Cython                : 3.0.12
pytest                : 8.3.5
hypothesis            : None
sphinx                : 8.2.3
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.3.1
html5lib              : 1.1
pymysql               : None
psycopg2              : 2.9.10
jinja2                : 3.1.6
IPython               : 7.34.0
pandas_datareader     : 0.10.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.2
gcsfs                 : 2025.3.2
matplotlib            : 3.10.0
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.28.0
pyarrow               : 18.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.40
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2025.1.2
xlrd                  : 2.0.1
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>

![Image](https://github.com/user-attachments/assets/35b68104-544b-44ac-bbf4-b8e98d8b2753)
","['Bug', 'Visualization']",2025-04-12 12:05:08,2025-04-26 12:04:01,2,closed
61273,ENH: Add `tzdata` to the `_hard_dependencies`,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

In https://github.com/pandas-dev/pandas/pull/61084#pullrequestreview-2669422152,  suggesting that `tzdata` should be added to the `_hard_dependencies` list.

### Feature Description

Extend current `_hard_dependencies` from `(""numpy"", ""dateutil"")` to `(""numpy"", ""dateutil"", ""tzdata"")`. 

### Alternative Solutions

No

### Additional Context

_No response_","['Enhancement', 'Dependencies']",2025-04-11 15:43:38,2025-04-22 16:04:23,2,closed
61272,"BUILD: Error installing pandas 2.2.3 on AIX 7.3 system (error: conflicting types for lockf64, lseek64, ftruncate64..)","### Installation check

- [x] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

AIX-3-CENSORED-powerpc-64bit

### Installation Method

pip install

### pandas Version

2.2.3

### Python Version

3.11

### Installation Logs

```
pip3.11 install --no-build-isolation pandas -vvv
```
<details>
  Running command Preparing metadata (pyproject.toml)
  + meson setup /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/.mesonpy-j4blhphr -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=/tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/.mesonpy-j4blhphr/meson-python-native-file.ini
  The Meson build system
  Version: 1.6.1
  Source dir: /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d
  Build dir: /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/.mesonpy-j4blhphr
  Build type: native build
  Project name: pandas
  Project version: 2.2.3
  C compiler for the host machine: gcc (gcc 10.3.0 ""gcc (GCC) 10.3.0"")
  C linker for the host machine: gcc ld.aix 7.3.2
  C++ compiler for the host machine: c++ (gcc 10.3.0 ""c++ (GCC) 10.3.0"")
  C++ linker for the host machine: c++ ld.aix 7.3.2
  Cython compiler for the host machine: cython (cython 3.0.8)
  Host machine cpu family: ppc
  Host machine cpu: powerpc
  Program python found: YES (/opt/freeware/bin/python3.11)
  Found pkg-config: YES (/opt/freeware/bin/pkg-config) 0.29.2
  Run-time dependency python found: YES 3.11
  Build targets in project: 53

  pandas 2.2.3

    User defined options
      Native files: /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/.mesonpy-j4blhphr/meson-python-native-file.ini
      b_ndebug    : if-release
      b_vscrt     : md
      buildtype   : release
      vsenv       : true

  Found ninja-1.12.1 at /opt/freeware/bin/ninja

  Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
  /opt/freeware/bin/meson compile -C .
  + /opt/freeware/bin/ninja
  [1/151] Generating pandas/_libs/algos_take_helper_pxi with a custom command
  [2/151] Generating pandas/_libs/algos_common_helper_pxi with a custom command
  [3/151] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command
  [4/151] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command
  [5/151] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command
  [6/151] Generating pandas/_libs/index_class_helper_pxi with a custom command
  [7/151] Generating pandas/_libs/intervaltree_helper_pxi with a custom command
  [8/151] Generating pandas/_libs/sparse_op_helper_pxi with a custom command
  [9/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/base.pyx
  [10/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/dtypes.pyx
  [11/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/np_datetime.pyx
  [12/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/ccalendar.pyx
  [13/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/nattype.pyx
  warning: /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/nattype.pyx:79:0: Global name __nat_unpickle matched from within class scope in contradiction to to Python 'class private name' rules. This may change in a future release.
  warning: /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/nattype.pyx:79:0: Global name __nat_unpickle matched from within class scope in contradiction to to Python 'class private name' rules. This may change in a future release.
  [14/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/conversion.pyx
  [15/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/parsing.pyx
  [16/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/fields.pyx
  [17/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/offsets.pyx
  [18/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/period.pyx
  [19/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/strptime.pyx
  [20/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/vectorized.pyx
  [21/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/timezones.pyx
  [22/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/arrays.pyx
  [23/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/tzconversion.pyx
  [24/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/timedeltas.pyx
  [25/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslibs/timestamps.pyx
  [26/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/indexing.pyx
  [27/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/hashing.pyx
  [28/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/internals.pyx
  [29/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/missing.pyx
  [30/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/ops_dispatch.pyx
  [31/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/index.pyx
  [32/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/interval.pyx
  [33/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/parsers.pyx
  [34/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/lib.pyx
  [35/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/ops.pyx
  [36/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/properties.pyx
  [37/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/join.pyx
  [38/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/byteswap.pyx
  [39/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/algos.pyx
  [40/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/hashtable.pyx
  [41/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/sas.pyx
  [42/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/testing.pyx
  [43/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/groupby.pyx
  [44/151] Compiling C object pandas/_libs/tslibs/base.cpython-311.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o
  [45/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/window/indexers.pyx
  [46/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/reshape.pyx
  [47/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/tslib.pyx
  [48/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/window/aggregations.pyx
  [49/151] Compiling C object pandas/_libs/tslibs/ccalendar.cpython-311.so.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o
  [50/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/sparse.pyx
  [51/151] Compiling Cython source /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d/pandas/_libs/writers.pyx
  [52/151] Compiling C object pandas/_libs/tslibs/parsing.cpython-311.so.p/.._src_parser_tokenizer.c.o
  [53/151] Compiling C object pandas/_libs/tslibs/np_datetime.cpython-311.so.p/meson-generated_pandas__libs_tslibs_np_datetime.pyx.c.o
  [54/151] Compiling C object pandas/_libs/tslibs/dtypes.cpython-311.so.p/meson-generated_pandas__libs_tslibs_dtypes.pyx.c.o
  [55/151] Compiling C object pandas/_libs/tslibs/nattype.cpython-311.so.p/meson-generated_pandas__libs_tslibs_nattype.pyx.c.o
  [56/151] Compiling C object pandas/_libs/tslibs/conversion.cpython-311.so.p/meson-generated_pandas__libs_tslibs_conversion.pyx.c.o
  pandas/_libs/tslibs/conversion.cpython-311.so.p/pandas/_libs/tslibs/conversion.pyx.c: In function '__pyx_pf_6pandas_5_libs_6tslibs_10conversion_cast_from_unit_vectorized.constprop':
  pandas/_libs/tslibs/conversion.cpython-311.so.p/pandas/_libs/tslibs/conversion.pyx.c:3064:79: warning: '__pyx_v_i' may be used uninitialized in this function [-Wmaybe-uninitialized]
   3064 |     __Pyx_GetItemInt_Fast(o, (Py_ssize_t)i, is_list, wraparound, boundscheck) :\
        |                                                                               ^~
   3065 |     (is_list ? (PyErr_SetString(PyExc_IndexError, ""list index out of range""), (PyObject*)NULL) :\
        |
  pandas/_libs/tslibs/conversion.cpython-311.so.p/pandas/_libs/tslibs/conversion.pyx.c:23754:14: note: '__pyx_v_i' was declared here
  23754 |   Py_ssize_t __pyx_v_i;
        |              ^~~~~~~~~
  [57/151] Compiling C object pandas/_libs/tslibs/fields.cpython-311.so.p/meson-generated_pandas__libs_tslibs_fields.pyx.c.o
  [58/151] Compiling C object pandas/_libs/tslibs/timezones.cpython-311.so.p/meson-generated_pandas__libs_tslibs_timezones.pyx.c.o
   [59/151] Compiling C object pandas/_libs/tslibs/strptime.cpython-311.so.p/meson-generated_pandas__libs_tslibs_strptime.pyx.c.o
  [60/151] Compiling C object pandas/_libs/tslibs/vectorized.cpython-311.so.p/meson-generated_pandas__libs_tslibs_vectorized.pyx.c.o
  [61/151] Compiling C object pandas/_libs/tslibs/period.cpython-311.so.p/meson-generated_pandas__libs_tslibs_period.pyx.c.o
  [62/151] Compiling C object pandas/_libs/arrays.cpython-311.so.p/meson-generated_pandas__libs_arrays.pyx.c.o
  [63/151] Compiling C object pandas/_libs/tslibs/tzconversion.cpython-311.so.p/meson-generated_pandas__libs_tslibs_tzconversion.pyx.c.o
  [64/151] Compiling C object pandas/_libs/tslibs/parsing.cpython-311.so.p/meson-generated_pandas__libs_tslibs_parsing.pyx.c.o
  [65/151] Compiling C object pandas/_libs/indexing.cpython-311.so.p/meson-generated_pandas__libs_indexing.pyx.c.o
  [66/151] Compiling C object pandas/_libs/tslibs/timestamps.cpython-311.so.p/meson-generated_pandas__libs_tslibs_timestamps.pyx.c.o
  [67/151] Compiling C object pandas/_libs/tslibs/timedeltas.cpython-311.so.p/meson-generated_pandas__libs_tslibs_timedeltas.pyx.c.o
  [68/151] Compiling C object pandas/_libs/hashing.cpython-311.so.p/meson-generated_pandas__libs_hashing.pyx.c.o
  [69/151] Compiling C object pandas/_libs/lib.cpython-311.so.p/src_parser_tokenizer.c.o
  [70/151] Compiling C object pandas/_libs/internals.cpython-311.so.p/meson-generated_pandas__libs_internals.pyx.c.o
  [71/151] Compiling C object pandas/_libs/pandas_datetime.cpython-311.so.p/src_vendored_numpy_datetime_np_datetime.c.o
  [72/151] Compiling C object pandas/_libs/pandas_datetime.cpython-311.so.p/src_vendored_numpy_datetime_np_datetime_strings.c.o
  [73/151] Compiling C object pandas/_libs/pandas_datetime.cpython-311.so.p/src_datetime_date_conversions.c.o
  [74/151] Compiling C object pandas/_libs/pandas_datetime.cpython-311.so.p/src_datetime_pd_datetime.c.o
  [75/151] Compiling C object pandas/_libs/pandas_parser.cpython-311.so.p/src_parser_tokenizer.c.o
  [76/151] Compiling C object pandas/_libs/pandas_parser.cpython-311.so.p/src_parser_io.c.o
  [77/151] Compiling C object pandas/_libs/pandas_parser.cpython-311.so.p/src_parser_pd_parser.c.o
  [78/151] Compiling C object pandas/_libs/missing.cpython-311.so.p/meson-generated_pandas__libs_missing.pyx.c.o
  [79/151] Compiling C object pandas/_libs/parsers.cpython-311.so.p/src_parser_tokenizer.c.o
  [80/151] Compiling C object pandas/_libs/parsers.cpython-311.so.p/src_parser_io.c.o
  [81/151] Compiling C object pandas/_libs/json.cpython-311.so.p/src_vendored_ujson_python_ujson.c.o
  [82/151] Compiling C object pandas/_libs/tslibs/offsets.cpython-311.so.p/meson-generated_pandas__libs_tslibs_offsets.pyx.c.o
  [83/151] Compiling C object pandas/_libs/json.cpython-311.so.p/src_vendored_ujson_python_JSONtoObj.c.o
  FAILED: pandas/_libs/json.cpython-311.so.p/src_vendored_ujson_python_JSONtoObj.c.o
  gcc -Ipandas/_libs/json.cpython-311.so.p -Ipandas/_libs -I../pandas/_libs -I../../../../home/USERNAME/.local/lib/python3.11/site-packages/numpy/core/include -I../pandas/_libs/include -I/opt/freeware/include/python3.11 -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -Wextra -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/json.cpython-311.so.p/src_vendored_ujson_python_JSONtoObj.c.o -MF pandas/_libs/json.cpython-311.so.p/src_vendored_ujson_python_JSONtoObj.c.o.d -o pandas/_libs/json.cpython-311.so.p/src_vendored_ujson_python_JSONtoObj.c.o -c ../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c
  In file included from /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/string.h:52,
                   from ../pandas/_libs/include/pandas/portable.h:12,
                   from ../pandas/_libs/include/pandas/vendored/ujson/lib/ultrajson.h:55,
                   from ../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c:41:
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:207:16: error: conflicting types for 'lseek64'
    207 | extern off64_t _NOTHROW(lseek64, (int, off64_t, int));
        |                ^~~~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:205:23: note: previous declaration of 'lseek64' was here
    205 | extern off_t _NOTHROW(lseek, (int, off_t, int));
        |                       ^~~~~
  In file included from /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:864,
                   from /opt/freeware/include/python3.11/Python.h:29,
                   from ../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c:43:
  /usr/include/sys/lockf.h:64:13: error: conflicting types for 'lockf64'
     64 |  extern int lockf64 (int, int, off64_t);
        |             ^~~~~~~
  /usr/include/sys/lockf.h:62:13: note: previous declaration of 'lockf64' was here
     62 |  extern int lockf (int, int, off_t);
        |             ^~~~~
  In file included from /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/string.h:52,
                   from ../pandas/_libs/include/pandas/portable.h:12,
                   from ../pandas/_libs/include/pandas/vendored/ujson/lib/ultrajson.h:55,
                   from ../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c:41:
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:937:14: error: conflicting types for 'ftruncate64'
    937 |  extern int  _NOTHROW(ftruncate64, (int, off64_t));
        |              ^~~~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:935:23: note: previous declaration of 'ftruncate64' was here
    935 |  extern int  _NOTHROW(ftruncate, (int, off_t));
        |                       ^~~~~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:994:14: error: conflicting types for 'truncate64'
    994 |  extern int  _NOTHROW(truncate64, (const char *, off64_t));
        |              ^~~~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:992:23: note: previous declaration of 'truncate64' was here
    992 |  extern int  _NOTHROW(truncate, (const char *, off_t));
        |                       ^~~~~~~~
  In file included from /opt/freeware/include/python3.11/Python.h:29,
                   from ../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c:43:
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:1013:18: error: conflicting types for 'pread64'
   1013 |  extern ssize_t  pread64(int, void *, size_t, off64_t);
        |                  ^~~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:1010:18: note: previous declaration of 'pread64' was here
   1010 |  extern ssize_t  pread(int, void *, size_t, off_t);
        |                  ^~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:1014:18: error: conflicting types for 'pwrite64'
   1014 |  extern ssize_t  pwrite64(int, const void *, size_t, off64_t);
        |                  ^~~~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:1011:18: note: previous declaration of 'pwrite64' was here
   1011 |  extern ssize_t  pwrite(int, const void *, size_t, off_t);
        |                  ^~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:1110:17: error: conflicting types for 'fclear64'
   1110 |  extern off64_t fclear64(int, off64_t);
        |                 ^~~~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:1107:15: note: previous declaration of 'fclear64' was here
   1107 |  extern off_t fclear(int, off_t);
        |               ^~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:1111:13: error: conflicting types for 'fsync_range64'
   1111 |  extern int fsync_range64(int, int, off64_t, off64_t);
        |             ^~~~~~~~~~~~~
  /opt/freeware/lib/gcc/powerpc-ibm-aix7.3.0.0/10/include-fixed/unistd.h:1108:13: note: previous declaration of 'fsync_range64' was here
   1108 |  extern int fsync_range(int, int, off_t, off_t);
        |             ^~~~~~~~~~~
  [84/151] Compiling C object pandas/_libs/json.cpython-311.so.p/src_vendored_ujson_python_objToJSON.c.o
  [85/151] Compiling C object pandas/_libs/index.cpython-311.so.p/meson-generated_pandas__libs_index.pyx.c.o
  [86/151] Compiling C object pandas/_libs/parsers.cpython-311.so.p/meson-generated_pandas__libs_parsers.pyx.c.o
  [87/151] Compiling C object pandas/_libs/lib.cpython-311.so.p/meson-generated_pandas__libs_lib.pyx.c.o
  pandas/_libs/lib.cpython-311.so.p/pandas/_libs/lib.pyx.c:91529:12: warning: '__pyx_memview_set_object' defined but not used [-Wunused-function]
  91529 | static int __pyx_memview_set_object(const char *itemp, PyObject *obj) {
        |            ^~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/lib.cpython-311.so.p/pandas/_libs/lib.pyx.c:91524:20: warning: '__pyx_memview_get_object' defined but not used [-Wunused-function]
  91524 |   static PyObject *__pyx_memview_get_object(const char *itemp) {
        |                    ^~~~~~~~~~~~~~~~~~~~~~~~
  [88/151] Compiling C object pandas/_libs/interval.cpython-311.so.p/meson-generated_pandas__libs_interval.pyx.c.o
  [89/151] Compiling C object pandas/_libs/join.cpython-311.so.p/meson-generated_pandas__libs_join.pyx.c.o
  [90/151] Compiling C object pandas/_libs/algos.cpython-311.so.p/meson-generated_pandas__libs_algos.pyx.c.o
  [91/151] Compiling C object pandas/_libs/groupby.cpython-311.so.p/meson-generated_pandas__libs_groupby.pyx.c.o
  [92/151] Compiling C object pandas/_libs/hashtable.cpython-311.so.p/meson-generated_pandas__libs_hashtable.pyx.c.o
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_complex128':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:134615:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  134615 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_complex64':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:136475:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  136475 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_float64':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:138335:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  138335 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_float32':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:140195:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  140195 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_uint64':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:142055:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  142055 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_uint32':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:143915:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  143915 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_uint16':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:145775:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  145775 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_uint8':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:147635:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  147635 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_object':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:149433:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  149433 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_16; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_int64':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:151193:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  151193 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_int32':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:153053:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  153053 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_int16':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:154913:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  154913 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_f_6pandas_5_libs_9hashtable_value_count_int8':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:156773:33: warning: comparison of integer expressions of different signedness: 'Py_ssize_t' {aka 'long int'} and 'khuint_t' {aka 'unsigned int'} [-Wsign-compare]
  156773 |   for (__pyx_t_1 = 0; __pyx_t_1 < __pyx_t_15; __pyx_t_1+=1) {
         |                                 ^
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_4__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:147685:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  147685 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_UInt8Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:147216:26: note: '__pyx_v_val' was declared here
  147216 |   __pyx_t_5numpy_uint8_t __pyx_v_val;
         |                          ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_0__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:156823:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  156823 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_Int8Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:156354:25: note: '__pyx_v_val' was declared here
  156354 |   __pyx_t_5numpy_int8_t __pyx_v_val;
         |                         ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_6__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:143965:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  143965 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_UInt32Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:143496:27: note: '__pyx_v_val' was declared here
  143496 |   __pyx_t_5numpy_uint32_t __pyx_v_val;
         |                           ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_1__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:154963:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  154963 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_Int16Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:154494:26: note: '__pyx_v_val' was declared here
  154494 |   __pyx_t_5numpy_int16_t __pyx_v_val;
         |                          ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_5__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:145825:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  145825 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_UInt16Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:145356:27: note: '__pyx_v_val' was declared here
  145356 |   __pyx_t_5numpy_uint16_t __pyx_v_val;
         |                           ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_8__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:140245:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  140245 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_Float32Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:139776:28: note: '__pyx_v_val' was declared here
  139776 |   __pyx_t_5numpy_float32_t __pyx_v_val;
         |                            ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_7__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:142105:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  142105 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_UInt64Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:141636:27: note: '__pyx_v_val' was declared here
  141636 |   __pyx_t_5numpy_uint64_t __pyx_v_val;
         |                           ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_9__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:138385:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  138385 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_Float64Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:137916:28: note: '__pyx_v_val' was declared here
  137916 |   __pyx_t_5numpy_float64_t __pyx_v_val;
         |                            ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_2__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:153103:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  153103 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_Int32Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:152634:26: note: '__pyx_v_val' was declared here
  152634 |   __pyx_t_5numpy_int32_t __pyx_v_val;
         |                          ^~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c: In function '__pyx_fuse_3__pyx_f_6pandas_5_libs_9hashtable_value_count.constprop':
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:151243:6: warning: '__pyx_v_val' may be used uninitialized in this function [-Wmaybe-uninitialized]
  151243 |     ((struct __pyx_vtabstruct_6pandas_5_libs_9hashtable_Int64Vector *)__pyx_v_result_keys->__pyx_vtab)->append(__pyx_v_result_keys, __pyx_v_val);
         |     ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/hashtable.cpython-311.so.p/pandas/_libs/hashtable.pyx.c:150774:26: note: '__pyx_v_val' was declared here
  150774 |   __pyx_t_5numpy_int64_t __pyx_v_val;
         |                          ^~~~~~~~~~~
  ninja: build stopped: subcommand failed.
  error: subprocess-exited-with-error

  Preparing metadata (pyproject.toml) did not run successfully.
  exit code: 1

  See above for output.

  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: /opt/freeware/bin/python3.11 /opt/freeware/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpwrjwf5p1
  cwd: /tmp/pip-install-m6bze4df/pandas_9acde4f69c3542ff9312ac4b80e89b4d
  Preparing metadata (pyproject.toml) ... error
error: metadata-generation-failed

Encountered error while generating package metadata.

See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
Exception information:
Traceback (most recent call last):
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/operations/build/metadata.py"", line 35, in generate_metadata
    distinfo_dir = backend.prepare_metadata_for_build_wheel(metadata_dir)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/utils/misc.py"", line 772, in prepare_metadata_for_build_wheel
    return super().prepare_metadata_for_build_wheel(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_impl.py"", line 186, in prepare_metadata_for_build_wheel
    return self._call_hook('prepare_metadata_for_build_wheel', {
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_impl.py"", line 311, in _call_hook
    self._subprocess_runner(
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 252, in runner
    call_subprocess(
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess
    raise error
pip._internal.exceptions.InstallationSubprocessError: Preparing metadata (pyproject.toml) exited with 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 180, in exc_logging_wrapper
    status = run_func(*args)
             ^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 245, in wrapper
    return func(self, options, args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 377, in run
    requirement_set = resolver.resolve(
                      ^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 95, in resolve
    result = self._result = resolver.resolve(
                            ^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 546, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 397, in resolve
    self._add_to_criteria(self.state.criteria, r, parent=None)
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 173, in _add_to_criteria
    if not criterion.candidates:
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 156, in __bool__
    return bool(self._sequence)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__
    return any(self)
           ^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>
    return (c for c in iterator if id(c) not in self._incompatible_ids)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built
    candidate = func()
                ^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 211, in _make_candidate_from_link
    self._link_candidate_cache[link] = LinkCandidate(
                                       ^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 293, in __init__
    super().__init__(
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 156, in __init__
    self.dist = self._prepare()
                ^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 225, in _prepare
    dist = self._prepare_distribution()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 304, in _prepare_distribution
    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 525, in prepare_linked_requirement
    return self._prepare_linked_requirement(req, parallel_builds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 640, in _prepare_linked_requirement
    dist = _get_prepared_distribution(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 71, in _get_prepared_distribution
    abstract_dist.prepare_distribution_metadata(
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 67, in prepare_distribution_metadata
    self.req.prepare_metadata()
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 577, in prepare_metadata
    self.metadata_directory = generate_metadata(
                              ^^^^^^^^^^^^^^^^^^
  File ""/opt/freeware/lib/python3.11/site-packages/pip/_internal/operations/build/metadata.py"", line 37, in generate_metadata
    raise MetadataGenerationFailed(package_details=details) from error
pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed
</details>
",['Build'],2025-04-11 14:00:12,2025-04-17 08:40:30,1,closed
61270,BUG: Unexpected behavior change in DataFrame.min(axis=1) with numpy.array elements in pandas 2.2 vs pandas 1.1,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

df = pd.DataFrame(
    {
        ""A"": [
            np.array([1]),
            np.array([2]),
            np.array([3]),
            np.array([4]),
            np.array([5]),
        ],
        ""B"": [10, 20, 30, 40, 50],
    }
)

df = df[[""A"", ""B""]].min(axis=1)
print(df)
```

### Issue Description

The behavior of DataFrame.min(axis=1) when a column contains numpy.array elements has changed between pandas 2.2 and pandas 1.1. I did not find any mention of this change in the changelog, and it is unclear whether this is a regression or an intentional change.

### Expected Behavior

Output on pandas 1.1
```
0    1
1    2
2    3
3    4
4    5
dtype: float64
```

Output on pandas 2.2
```
0    [1]
1    [2]
2    [3]
3    [4]
4    [5]
dtype: object
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.4
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Nested Data']",2025-04-11 10:55:24,2025-04-13 16:25:38,8,closed
61269,BUG: pandas change in style overrides defaults format for other columns,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
 
pd.set_option('display.float_format', '{:.2f}'.format)
 
pd.DataFrame(15.22345676543234567,columns=[1,2,3,4,5,6],index=['A','Z','R','T'])#.style.format({1:'{:.2%}'})
 
pd.DataFrame(15.22345676543234567,columns=[1,2,3,4,5,6],index=['A','Z','R','T']).style.format({1:'{:.2%}'})
```

### Issue Description

After setting float default number of decimal place to display to 2, the first exemple works, only showing 2 decimals for all column. However when adding a custom style to only the first columns all other columns are formatted with the default 6 decimals places. This is quite counterproductive as it means if we want to set some format we need to define all format. It would be nice to be able to change only a few formats while the unchanged are set to default.

### Expected Behavior


When specifying a format for a given columns, pandas should style use user specified default value for other columns.

### Installed Versions


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22621
machine               : AMD64
processor             : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : fr_FR.cp1252
 
pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.31.0
adbc-driver-postgresql: None
...
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
 
","['Styler', 'Closing Candidate']",2025-04-11 08:11:14,2025-04-13 20:21:25,4,closed
61268,DOC: Add documentation for `groupby.ewm()`,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/groupby.html

### Documentation problem

There is no reference for `DataFrameGroupBy.ewm()`, even though it exists in API, and Docstring can be greatly improved.

Similar to: #61254 

E.g. consider working example:

```
>>> import pandas as pd
>>> pd.__version__
'2.2.3'
>>> data = {""Class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""B""],""Value"": [10, 20, 30, 40, 50, 60],}
>>> df = pd.DataFrame(data)
>>> df
  Class  Value
0     A     10
1     A     20
2     A     30
3     B     40
4     B     50
5     B     60
>>> ewm_mean = (df.groupby(""Class"").ewm(span=2).mean().reset_index(drop=True))
>>> ewm_mean
   Value
0  10.000000
1  17.500000
2  26.153846
3  40.000000
4  47.500000
5  56.153846
```



### Suggested fix for documentation

Include reference of DataFrameGroupBy.ewm and SeriesGroupBy.ewm, like for [DataFrameGroupBy.rolling](https://pandas.pydata.org/docs/dev/reference/api/pandas.core.groupby.DataFrameGroupBy.rolling.html#pandas.core.groupby.DataFrameGroupBy.rolling)
and [SeriesGroupBy.rolling](https://pandas.pydata.org/docs/dev/reference/api/pandas.core.groupby.SeriesGroupBy.rolling.html#pandas.core.groupby.SeriesGroupBy.rolling)

Improve `groupby.ewm()` function Docstring","['Docs', 'Needs Triage']",2025-04-11 06:42:06,2025-04-14 20:05:36,1,closed
61267,BUG: Inconsistent date resolution,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import datetime
import pandas as pd
pd.__version__  # ‘2.2.2’

d = datetime.datetime(2025, 4, 10)

pd.Series([d]).dtype  # dtype('<M8[ns]')

pd.DataFrame([{'date': d}])['date'].dtype  # dtype('<M8[ns]')

df = pd.DataFrame([{'x': 0}])
df['date'] = d  # <— broadcast scalar
df['date'].dtype  # dtype('<M8[us]')  <— different date resolution!
```

### Issue Description

Date resolution differs between Series/DataFrame construction versus assignment via a scalar broadcast. 

### Expected Behavior

I’d expect the same date resolution across the examples I provided. 

### Installed Versions

<details>

commit        : d9cdd2ee5a58015e
python         : 3.11.9.final.0
python-bits : 64
OS                : Windows
OS-release  : 10
Version        : AMD64
processor    : Intel64 Family 6 Model 85 Stepping 7, GenuineIntel
byteorder     : little

pandas         : 2.2.2
numpy          : 1.26.4
Cython         : 3.0.10

</details>
","['Bug', 'Datetime']",2025-04-10 22:29:16,2025-04-13 12:31:31,2,closed
61260,API: Rename arg to func in Series.map for consistency,"The API of methods taking udf follow certain patterns that make them consistent and easier to learn and use. There are some small differences, which have been listed in #40112 and #61128.

This issue is to rename the `arg` parameter of `Series.map` to `func`, which is the name consistently used in almost all methods. In the case of `Series.map`, the argument is slightly different than others, given that `arg` or `func` can also be a `dict` or a `Series`, which will make `map` replace values from these mappings, instead of executing an elementwise udf.

This issue is for the renaming of the parameter, making the parameter consistent with other methods such as `DataFrame.apply` can be considered in another issue. But there are some cases to consider, given that the behavior of `map` is slightly different when providing a mapping, than when providing a function that maps. In particular, `map` will use `NaN` when the mapping returns `None`, but it will use `None` when the function returns `None`. Also, if we stop supporting dictionaries, users in general should just replace their code from `Series.map(my_dict)` to `Series.map(my_dict.get)`. But there are some special cases, for example when the dictionary is a `defaultdict`, `.get` will return `None`, while the current `map` implementation with a `defaultdict` will consider the default value.","['API Design', 'Apply', 'API - Consistency']",2025-04-09 17:47:36,2025-04-14 13:14:30,2,closed
61254,DOC: Add documentation for groupby.expanding(),"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

""https://pandas.pydata.org/docs/dev/reference/groupby.html""

### Documentation problem

There is no reference for `DataFrameGroupBy.expanding()`, even though it exists in API

E.g. consider working example:
```
>>> import pandas as pd
>>> pd.__version__
'2.2.3'
>>> data = {""Class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""B""],""Value"": [10, 20, 30, 40, 50, 60],}
>>> df = pd.DataFrame(data)
>>> df
  Class  Value
0     A     10
1     A     20
2     A     30
3     B     40
4     B     50
5     B     60
>>> expanding_mean = df.groupby(""Class"").expanding().mean().reset_index(drop=True)
>>> expanding_mean
   Value
0   10.0
1   15.0
2   20.0
3   40.0
4   45.0
5   50.0
```
It's undocumented behaviour

### Suggested fix for documentation

Include reference of `DataFrameGroupBy.expanding` and `SeriesGroupBy.expanding`, like for [DataFrameGroupBy.rolling](https://pandas.pydata.org/docs/dev/reference/api/pandas.core.groupby.DataFrameGroupBy.rolling.html#pandas.core.groupby.DataFrameGroupBy.rolling)
and  [SeriesGroupBy.rolling](https://pandas.pydata.org/docs/dev/reference/api/pandas.core.groupby.SeriesGroupBy.rolling.html#pandas.core.groupby.SeriesGroupBy.rolling)","['Docs', 'Groupby', 'Window']",2025-04-08 12:36:06,2025-04-14 22:41:30,2,closed
61253,BUG: Selecting the wrong first column,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
TTP_CWE_mappingDF = pd.read_csv('./658.csv', sep=',')

columns_list = TTP_CWE_mappingDF.columns.tolist()  # Returns column names as a list
print(columns_list)
print(""\n"")


TEST = TTP_CWE_mappingDF.iloc[:, [0]]



columns_list2 = TEST.columns.tolist()  # Returns column names as a list
print(columns_list2)

print(TEST)

# first_column = TTP_CWE_mappingDF.iloc[:, 0]  # All rows (:) + first column (0)
# print(first_column)
```

### Issue Description

Hi,

When downloading the MITRE CAPEC cwe .csv I tried to import it on Python to play with it a bit.
Surprisingly, when selecting the first column, the data is from the second column, and this applies to the whole dataframe; all columns are off by one. The key is correct, but the data is for the next key.


This is rather problematic, as you can imagine.

I added the .csv file I am using as well.

[658.csv](https://github.com/user-attachments/files/19648994/658.csv)

### Expected Behavior

this is the result i get:

[""'ID"", 'Name', 'Abstraction', 'Status', 'Description', 'Alternate Terms', 'Likelihood Of Attack', 'Typical Severity', 'Related Attack Patterns', 'Execution Flow', 'Prerequisites', 'Skills Required', 'Resources Required', 'Indicators', 'Consequences', 'Mitigations', 'Example Instances', 'Related Weaknesses', 'Taxonomy Mappings', 'Notes']


[""'ID""]
                                                   'ID
1    Accessing Functionality Not Properly Constrain...
11                  Cause Web Server Misclassification
112                                        Brute Force
114                               Authentication Abuse
115                              Authentication Bypass
..                                                 ...
698                        Install Malicious Extension
70       Try Common or Default Usernames and Passwords
700                          Network Boundary Bridging
94                      Adversary in the Middle (AiTM)
98                                            Phishing

[177 rows x 1 columns]



the expected result should be:

[""'ID"", 'Name', 'Abstraction', 'Status', 'Description', 'Alternate Terms', 'Likelihood Of Attack', 'Typical Severity', 'Related Attack Patterns', 'Execution Flow', 'Prerequisites', 'Skills Required', 'Resources Required', 'Indicators', 'Consequences', 'Mitigations', 'Example Instances', 'Related Weaknesses', 'Taxonomy Mappings', 'Notes']


[""'ID""]
'ID
1    
11                  
112             
114                        
115        
..          
698     
70      
700       
94        
98     

[177 rows x 1 columns]

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.7
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.11-arm64
Version               : #1 SMP Kali 6.8.11-1kali2 (2024-05-30)
machine               : aarch64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 24.1.1
Cython                : None
sphinx                : None
IPython               : 9.0.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV']",2025-04-08 12:32:18,2025-04-08 21:07:37,1,closed
61252,BUG: AttributeError: 'SparseArray' object has no attribute 'round',"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame([1.1,2.5,3,4.7], dtype = pd.SparseDtype())

hasattr(df, 'round')
# True

df.round()
# AttributeError: 'SparseArray' object has no attribute 'round'
```

### Issue Description

the sparsed `df` do has `round` method, but can not execute it

### Expected Behavior

`df` can execute `round` method

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.2
python-bits           : 64
OS                    : Linux
OS-release            : 3.10.0-1160.102.1.0.1.an7.x86_64
Version               : #1 SMP Sun Oct 29 06:40:18 CST 2023
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.utf8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Sparse']",2025-04-08 10:10:36,2025-04-08 21:14:41,2,closed
61251,PERF: future_stack is too slow,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

The future_stack is very slow compared to the previous implementation:

```python
import pandas as pd, numpy as np

df = pd.DataFrame(np.random.randn(5000, 5000))

%%time
df.stack(dropna=False)
# CPU times: user 49.4 ms, sys: 49.7 ms, total: 99.1 ms
# Wall time: 96 ms

%%time
df.stack(future_stack=True)
# CPU times: user 1.96 s, sys: 122 ms, total: 2.08 s
# Wall time: 2.08 s
```


### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.16
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-122-generic
Version               : #132-Ubuntu SMP Thu Aug 29 13:45:52 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.0
Cython                : 3.0.7
sphinx                : 7.3.7
IPython               : 8.25.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
html5lib              : None
hypothesis            : 6.129.3
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : 8.2.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : 2.0.31
tables                : 3.9.2
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None
</details>


### Prior Performance

_No response_","['Performance', 'Needs Triage']",2025-04-08 08:39:36,2025-04-08 16:18:07,1,closed
61242,No Windows free-threaded wheel available in scientific-python-nightly-wheels,"In scikit-learn we noticed there are no Windows free-threaded development wheel in [scientific-python-nightly-wheels](https://anaconda.org/scientific-python-nightly-wheels/pandas/files).

The reason seems to be that your Wheels builder has failed consistently for more than a week, see [build logs](https://github.com/pandas-dev/pandas/actions/workflows/wheels.yml?query=event%3Aschedule).

The failures only happen for Windows free-threaded i.e. `cp313t-win_amd64`. I had a quick look at one of the log there are almost 300 failures and plenty of them seem to be related to indexes with timestamps ...

One thing I did notice is that you are still using numpy development wheel for Windows free-threaded and I think using a released numpy may be good enough since numpy 2.2.4 (and probably a few earlier versions as well) has a free-threaded wheel for Windows, see [PyPI numpy info](https://pypi.org/project/numpy/#files).",[],2025-04-07 15:06:16,2025-04-09 17:56:31,2,closed
61232,ENH: The method of obtaining a certain cell or slice of the dataframe is confusing and unclear,"### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

The method of obtaining a certain cell or slice of the dataframe is confusing and unclear, such as using `loc`, `iloc`, `at`, `iat`, and the operator `[]`, etc. For example, `df.loc[row_label, col_label]` and `df.iloc[row_index, col_index]`. If `loc` is a property, it is better to be a variable but now it is a verb or something rather than a df buffer. The user wants to take the cell value, so the behavior of `loc` is like a member function and the `()` operator should be used to pass parameters. However, you are using the operator `[]`, but the object of the operator `[]` is usually an instance, which is a confusing place.

And when taking two columns or slices at the same time, for example, taking one column and one row, the expression `value = df.loc[1, 'B']`, where the operator `[]` represents the horizontal and vertical coordinate information, and taking two columns and one row, `row_data = df.loc['row_label', ['col1', 'col2']]`, where the second operator `[]` has both vertical coordinate information but behaves like a `list` or `tuple` instead of the former, this is another confusing aspect.

In mathematics, the coordinate values such as (3, 4) represent the horizontal and vertical coordinates respectively, as well as the `()` operator. I hope that the operation rules you define should conform to common customs or competition analysis or benchmarking such as numpy. thank you.

### Feature Description

n/a

### Alternative Solutions

n/a

### Additional Context

_No response_","['Enhancement', 'Needs Triage', 'Closing Candidate']",2025-04-04 16:27:25,2025-08-05 16:30:20,3,closed
61231,BUG: PyArrow timestamp type does not work with map() function,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Here are my reproduction steps that does not work with PyArrow type:

df = pd.DataFrame({""a"": pd.date_range(""2018-01-01 00:00:00"", ""2018-01-07 00:00:00"")}).astype({""a"": ""timestamp[ns][pyarrow]""})
date2pos = {date: i for i, date in enumerate(df['a'])}
df[""a""].map(date2pos)


0   NaN
1   NaN
2   NaN
3   NaN
4   NaN
5   NaN
6   NaN
Name: a, dtype: float64
```

### Issue Description

For some reason `pd.DataFrame.map()` function does not work with PyArrow `timestamp[ns][pyarrow]` type and does not map values.

### Expected Behavior

Here is an expected behavior that works with the default pandas type `datetime64[ns]`:
```
df = pd.DataFrame({""a"": pd.date_range(""2018-01-01 00:00:00"", ""2018-01-07 00:00:00"")})
date2pos = {date: i for i, date in enumerate(df['a'])}
df[""a""].map(date2pos)
```
```
0    0
1    1
2    2
3    3
4    4
5    5
6    6
Name: a, dtype: int64
```

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.7
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.3
pytz                  : 2025.1
dateutil              : 2.8.2
pip                   : 23.2.1
Cython                : None
sphinx                : None
IPython               : 8.20.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.12.2
html5lib              : None
hypothesis            : None
gcsfs                 : 2023.12.2post1
jinja2                : 3.1.3
lxml.etree            : None
matplotlib            : 3.8.2
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 14.0.2
pyreadstat            : None
pytest                : 7.4.4
python-calamine       : None
pyxlsb                : None
s3fs                  : 2023.12.2
scipy                 : 1.12.0
sqlalchemy            : 2.0.29
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Apply', 'Arrow']",2025-04-04 15:34:54,2025-09-08 20:53:03,3,closed
61224,ENH: Implement loading and dumping to and from YAML,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Hi,

I am nowadays mostly working with YAML and moved away from JSON, given that it has proven far superior in terms of userfriendlines (e.g. readability). However I am missing a quick way to load and dump data from yaml to pandas.

### Feature Description

```python
df = pd.from_yaml('/path/to/my/yaml/file.yml')
df.to_yaml('/path/to/my/yaml/file.yml')
```

### Alternative Solutions

I can write my wrapper to do all the dirty work myself for now.

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-04-03 11:30:41,2025-04-03 15:12:31,2,closed
61222,"BUG: Index name lost when using ""resample"" with pyarrow dtypes","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Create a df with DatetimeIndex called ""timestamp"" and native pandas dtype
native_df = pd.DataFrame(
    {'value': [23.5, 24.1, 22.8, 25.3, 23.9]},
    index=pd.date_range(start='2025-01-01 00:00:00', end='2025-01-01 04:00:00', freq='h'),
)
native_df.index.name = ""timestamp""

# Create a similar df with pyarrow dtypes
pyarrow_df = native_df.copy()
pyarrow_df.index = pyarrow_df.index.astype('timestamp[ns][pyarrow]')
pyarrow_df[""value""] = pyarrow_df[""value""].astype('float64[pyarrow]')

native_df.resample(""2h"").mean().reset_index()[""timestamp""] # OK
pyarrow_df.resample(""2h"").mean().reset_index()[""timestamp""] # KeyError: 'timestamp'
```

### Issue Description

The `resample` forget the name of the index when using `pyarrow` dtypes. 

By the way, I notice that `DatetimeIndex` are converted to `Index` when using `pyarrow` dtypes. Maybe it is related?

See concrete example in screenshot
![Image](https://github.com/user-attachments/assets/a9b1b5f7-907d-4910-abb4-5be186bcb2d9)



### Expected Behavior

The `resample` methods is expected to behave in the same way for `pyarrow` and `native` dtypes.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.234-225.910.amzn2.x86_64
Version               : #1 SMP Fri Feb 14 16:52:40 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : C.UTF-8
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.2
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.3.2
scipy                 : 1.15.2
sqlalchemy            : 2.0.38
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Resample', 'Arrow']",2025-04-03 10:37:17,2025-04-07 16:55:10,2,closed
61221,BUG: Exception with `unstack(sort=False)` and NA in index,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

levels1 = ['b', 'a']
levels2 = pd.Index([1, 2, 3, pd.NA], dtype=pd.Int64Dtype())
index = pd.MultiIndex.from_product([levels1, levels2], names=['level1', 'level2'])

df = pd.DataFrame(dict(value=range(len(index))), index=index)
print(df)

print(df.unstack(level='level2'))

print(df.unstack(level='level2', sort=False))
```

```
               value
level1 level2
b      1           0
       2           1
       3           2
       <NA>        3
a      1           4
       2           5
       3           6
       <NA>        7
```

```
       value
level2  <NA>  1  2  3
level1
a          3  0  1  2
b          7  4  5  6
```

```
Traceback (most recent call last):
  File ""/home/jared/tmp/./250402-pd-test.py"", line 15, in <module>
    print(df.unstack(level='level2', sort=False))
          ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jared/opt/mambaforge/envs/pd-test/lib/python3.13/site-packages/pandas/core/frame.py"", line 9928, in unstack
    result = unstack(self, level, fill_value, sort)
  File ""/home/jared/opt/mambaforge/envs/pd-test/lib/python3.13/site-packages/pandas/core/reshape/reshape.py"", line 504, in unstack
    return _unstack_frame(obj, level, fill_value=fill_value, sort=sort)
  File ""/home/jared/opt/mambaforge/envs/pd-test/lib/python3.13/site-packages/pandas/core/reshape/reshape.py"", line 537, in _unstack_frame
    return unstacker.get_result(
           ~~~~~~~~~~~~~~~~~~~~^
        obj._values, value_columns=obj.columns, fill_value=fill_value
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""/home/jared/opt/mambaforge/envs/pd-test/lib/python3.13/site-packages/pandas/core/reshape/reshape.py"", line 242, in get_result
    return self.constructor(
           ~~~~~~~~~~~~~~~~^
        values, index=index, columns=columns, dtype=values.dtype
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File ""/home/jared/opt/mambaforge/envs/pd-test/lib/python3.13/site-packages/pandas/core/frame.py"", line 827, in __init__
    mgr = ndarray_to_mgr(
        data,
    ...<4 lines>...
        typ=manager,
    )
  File ""/home/jared/opt/mambaforge/envs/pd-test/lib/python3.13/site-packages/pandas/core/internals/construction.py"", line 336, in ndarray_to_mgr
    _check_values_indices_shape_match(values, index, columns)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jared/opt/mambaforge/envs/pd-test/lib/python3.13/site-packages/pandas/core/internals/construction.py"", line 420, in _check_values_indices_shape_match
    raise ValueError(f""Shape of passed values is {passed}, indices imply {implied}"")
ValueError: Shape of passed values is (2, 4), indices imply (2, 5)
```

### Issue Description

With a `MultiIndex` level of `Int64Dtype()` containing NA values, `DataFrame.unstack()` produces the expected result with `sort=True` (default) but causes an exception with `sort=False`. This does not occur if the NA value is removed from the index.

### Expected Behavior

No exception, the returned `DataFrame` has `level` in the original order (`['b', 'a']`).

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.2
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 9.0.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Reshaping', 'ExtensionArray']",2025-04-03 00:54:08,2025-09-15 19:19:41,4,closed
61218,QST: Should the absence of tzdata package affect the performance in any way ?,"### Research

- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/search?page=3&tab=Relevance&pagesize=30&q=pandas%20AND%20tzdata%20&searchOn=3

### Question about pandas

We are on pandas 1.5.3.   We are investigating some performance bottlenecks.  At this point, we are not sure where the problem lies.  However, we have a consistent pattern of observations.

We noticed that when `tzdata==2025.2` was uninstalled, there was a severe degradation in performance (> 10x).

Upon further investigations and eliminations, we arrived at the following matrix:

## Good perf-1
```
pandas==2.2.3
tzdata==2025.2
```

## Good perf-2
```
pandas==1.5.3
tzdata==2025.2
```

## Bad perf
No `tzdata`
```
pandas==1.5.3
```

Any suggestions ?




Is there any logic in any part of Pandas that relies on `tzdata`  ?

Thanks,
Sau



","['Usage Question', 'Needs Info']",2025-04-02 15:53:10,2025-08-05 17:04:53,6,closed
61217,BUG: unstack incorrectly reshuffles data when sort=False,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
a = pd.Series([f'a{i}' for i in range(10)])
b = pd.Series([f'b{i}' for i in range(10)])

ser = pd.concat({'a': a, 'b': b})  # multi-indexed series e.g. ('a', 0) = 'a0'
df = ser.unstack(0, sort=False)  # dataframe with integer index and columns=['a','b'], some a values end up in b column and vice-versa
```

### Issue Description

When unstacking a multi-indexed series (or dataframe), passing sort=False fails to preserve the original mapping of multi-index keys to values. In other words, the resulting ""a"" column has a mix of ""a"" and ""b""-prefixed values.

### Expected Behavior

I would expect sort=False to prevent a sort of the newly produced column names but preserve the mapping of multi-index keys to values based on the following from the documentation: ""sort: Sort the level(s) in the resulting MultiIndex columns.""

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.9
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 25 Model 97 Stepping 2, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-04-02 14:12:08,2025-04-03 02:51:31,4,closed
61213,BUG: DataFrame.corr clips values when cov=True,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

In [50]: x = pd.DataFrame({""A"": [1, 2, None, 4], ""B"": [2, 4, None, 9]})

In [51]: x.cov()
Out[51]:
     A    B
A  1.0  1.0
B  1.0  1.0

In [52]: x.dropna().cov()
Out[52]:
          A     B
A  2.333333   5.5
B  5.500000  13.0
```

### Issue Description

Stemming from #61154. `DataFrame.corr` was clipped between `-1` and `1` to handle numerical precision errors. However, this was done regardless of whether `cov` equals `True` or `False`, and should instead only be done when `cov=False`.

### Expected Behavior

import pandas as pd

In [50]: x = pd.DataFrame({""A"": [1, 2, None, 4], ""B"": [2, 4, None, 9]})

In [51]: x.cov()
Out[51]:
     A    B
A  1.0  1.0
B  1.0  1.0

In [52]: x.dropna().cov()
Out[52]:
     A    B
A  1.0  1.0
B  1.0  1.0

### Installed Versions

<details>

commit                : cdc9e952f139746c2e6816997d82b389f605ec58
python                : 3.10.16
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:22 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6041
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2006.gcdc9e952f1
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 8.34.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.3.0
html5lib              : 1.1
hypothesis            : 6.130.4
gcsfs                 : 2025.3.0
jinja2                : 3.1.6
lxml.etree            : 5.3.1
matplotlib            : 3.10.1
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyreadstat            : 1.2.8
pytest                : 8.3.5
python-calamine       : None
pytz                  : 2025.2
pyxlsb                : 1.0.10
s3fs                  : 2025.3.0
scipy                 : 1.15.2
sqlalchemy            : 2.0.40
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2025-04-02 02:20:50,2025-04-02 02:38:39,0,closed
61208,BUG: OverflowError when fillna on DataFrame with a pd.Timestamp,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({
'datetime' : pd.date_range('1/1/2011', periods=3, freq='h'),
'value' : [1,2,3]
})

df.iloc[0,0] = None
df.fillna(pd.Timestamp('0001-01-01'), inplace=True)
```

### Issue Description

Issue is similar to [this closed issue without a reproducible example](https://github.com/pandas-dev/pandas/issues/56502 ).

A DataFrame that has a column with datetime64[ns] with NaT gets an error if trying to fill null values with a pd.Timestamp that lies outside the range of the given precision.

### Expected Behavior

The null values in the DataFrame should be replaced with the provided TimeStamp or an error should be provided to the user that the Timestamps have incompatible precisions and ranges.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------------
commit : 0691c5cf90477d3503834d983f69350f250a6ff7
python : 3.10.8
python-bits : 64
OS : Windows
OS-release : 10
Version : 10.0.17763
machine : AMD64
processor : Intel64 Family 6 Model 143 Stepping 8, GenuineIntel
byteorder : little
LC_ALL : None
LANG : None
LOCALE : German_Switzerland.1252

pandas : 2.2.3
numpy : 2.2.4
pytz : 2025.2
dateutil : 2.9.0.post()
pip : 25.0.1
Cython : None
sphinx : None
IPython : None
adbc-driver-postgresql: None
adbc-driver-sqlite : None
bs4 : None
blosc : None
bottleneck : None
dataframe-api-compat : None
fastparquet : None
fsspec : None
html5lib : None
hypothesis : None
gcsfs : None
jinja2 : None
lxml.etree : None
matplotlib : None
numba : None
numexpr : None
odfpy : None
openpyxl : None
pandas_gbq : None
psycopg2 : None
pymysql : None
pyarrow : None
pyreadstat : None
pytest : None
python-calamine : None
pyxlsb : None
s3fs : None
scipy : None
sqlalchemy : None
tables : None
tabulate : None
xarray : None
xlrd : None
xlsxwriter : None
zstandard : None
tzdata : 2025.2
qtpy : None
pyqt5 : None
</details>
","['Bug', 'Error Reporting', 'Timestamp']",2025-03-31 15:19:29,2025-04-14 16:59:00,3,closed
61206,BUG: round on object columns no longer raises a TypeError,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df=pd.DataFrame(data=['foo'],columns=['bar'])
df.loc[0,'bar']=0.2
df['bar'].round()

Out[4]: 
0    0.2
```

### Issue Description

pd.Series.round() appears to have changed behaviour in 2.2.3 compared to 2.1.4.
In previous versions, attempting to round a column with ""object"" dtype would raise a TypeError. In 2.2.3, round now silently returns the same column, without applying any rounding.

I'm not sure if there is some underlying change that causes this behaviour, but together with the removal of downcasting from a variety of methods (ffill, replace, fillna,...) this change in behaviour seems dangerous without any warnings.

### Expected Behavior

import pandas as pd
df=pd.DataFrame(data=['foo'],columns=['bar'])
df.loc[0,'bar']=0.2
df['bar'].round()

TypeError: loop of ufunc does not support argument 0 of type float which has no callable rint method

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.9.18
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252
pandas                : 2.2.3
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
pip                   : 23.3.1
Cython                : None
sphinx                : None
IPython               : 8.15.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
blosc                 : None
bottleneck            : 1.3.6
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.2
lxml.etree            : 4.9.3
matplotlib            : 3.8.0
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.0
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 7.4.0
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : 2023.12.0
xlrd                  : 2.0.1
xlsxwriter            : 3.1.1
zstandard             : 0.19.0
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Regression', 'Error Reporting', 'Numeric Operations']",2025-03-31 08:18:12,2025-05-21 00:33:34,4,closed
61196,BUG: `to_datetime()` warns unnecessarily that format cannot be inferred,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
pd.to_datetime(['2020-01-01T20:20:20', '2020-01-01T20:21:20'])
```

### Issue Description

This produces the following warning even though the format is inferable:

```
UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
```

Digging deeper, this warning occurs only when the year = concatenated hour and minute in the first element of the list, e.g. year = 2020, hour = 20, minute = 20.

### Expected Behavior

`to_datetime()` should behave just as it does when this unique condition does not hold.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:24 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6030
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : 7.3.7
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.12.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : 0.61.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.12.0
scipy                 : 1.15.1
sqlalchemy            : 2.0.37
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2024.11.0
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'Warnings']",2025-03-28 21:49:48,2025-03-29 12:30:42,3,closed
61194,ENH: adding a filter (and bold) to header when writing to excel,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Hello,

One of the things I always do when opening an excel file is adding filter on header. Just like that

![Image](https://github.com/user-attachments/assets/fe4dfc23-7b3b-4e40-8920-43c9cf814e98)

### Feature Description

I'm not exactly sure 

### Alternative Solutions

I see that as an option to to_excel function or to class ExcelWriter, not sure of what would be the best.

### Additional Context

_No response_","['Enhancement', 'IO Excel']",2025-03-28 17:13:59,2025-11-19 01:22:08,14,closed
61189,BUG: \0 null bytes in `str` not preserved in `pandas.CategoricalIndex` or `pandas.MultiIndex`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from sys import version_info as py_version_info
from pandas import __version__ as pd_version

assert py_version_info[:3] == (3, 13, 2)
assert pd_version == '2.2.3' or pd_version == '3.0.0.dev0+2028.gb64f438cc8'

from pandas import CategoricalIndex, MultiIndex

entities = [b'abc', b'abc\0']

# CORRECT
cat = CategoricalIndex(entities)
assert cat.tolist() == entities
assert len({*cat.tolist()}) == len({*entities})

# CORRECT
idx = MultiIndex.from_product([entities])
assert idx.get_level_values(0).tolist() == entities
assert len({*idx.get_level_values(0).tolist()}) == len({*entities})

entities = ['abc', 'abc\0']

# INCORRECT
cat = CategoricalIndex(entities)
assert cat.tolist() != entities
assert len({*cat.tolist()}) < len({*entities})

# INCORRECT
idx = MultiIndex.from_product([entities])
assert idx.get_level_values(0).tolist() != entities
assert len({*idx.get_level_values(0).tolist()}) < len({*entities})

entities = ['abc', 'abc\0def']

# INCORRECT
cat = CategoricalIndex(entities)
assert cat.tolist() != entities
assert len({*cat.tolist()}) < len({*entities})

# INCORRECT
idx = MultiIndex.from_product([entities])
assert idx.get_level_values(0).tolist() != entities
assert len({*idx.get_level_values(0).tolist()}) < len({*entities})
```

### Issue Description

When constructing a `pandas.CategoricalIndex` or `pandas.MultiIndex` from Python `str` values, any code points following a '\0' are discarded. This does not occur with `bytes` inputs.

### Expected Behavior

The null bytes should be preserved exactly.

### Installed Versions

<details>
>>> from pandas import show_versions
>>> show_versions() # trimmed

INSTALLED VERSIONS
------------------
commit                : b64f438cc8079d441331396fbac1e2dc61b26af9
python                : 3.13.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.12.20-1-lts
Version               : #1 SMP PREEMPT_DYNAMIC Sun, 23 Mar 2025 08:02:10 +0000
machine               : x86_64
processor             :
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+2028.gb64f438cc8
numpy                 : 2.3.0.dev0+git20250325.2a6f4f0
dateutil              : 2.9.0.post0
pip                   : 24.3.1
tzdata                : 2025.2
</details>","['Bug', 'Algos', 'Strings']",2025-03-27 15:38:25,2025-03-28 20:44:37,3,closed
61188,BUG: date comparison fails when series is all pd.NaT values,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from datetime import datetime

s = pd.Series([pd.NaT, ""1/1/2020 10:00:00""])
s = pd.to_datetime(s)
print(s.dt.date.le(datetime.now().date()))
# 0    False
# 1     True
# dtype: bool

s = pd.Series([pd.NaT, pd.NaT])
s = pd.to_datetime(s)
print(s.dt.date.le(datetime.now().date()))
# TypeError: Invalid comparison between dtype=datetime64[ns] and date
```

### Issue Description

When comparing a `datetime[ns]` or similar series, where all the values turn out to be `pd.NaT` values, the comparison just breaks. This is problematic, as the input series cannot necessarily be controlled beforehand, and if there's any actual non-NaT value, the comparison works. The Series `dtype` values are the same in both cases, which would make me expect that the rest of the behaviour is the same too.

### Expected Behavior

In the above code, I would expect it to return:

```python
0    False
1    False
dtype: bool
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.16
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 8.34.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.0
html5lib              : None
hypothesis            : 6.130.4
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : 0.28.0
psycopg2              : None
pymysql               : None
pyarrow               : 15.0.2
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.3.0
scipy                 : None
sqlalchemy            : 2.0.39
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'good first issue']",2025-03-27 09:24:05,2025-04-15 12:42:01,5,closed
61187,"BUG: DataFrame.min raises TypeError when column contains mixed types (e.g., np.nan and datetime)","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
import datetime

data = {
    ""dates"": [
        np.nan,
        np.nan,
        datetime.datetime(2025, 1, 3),
        datetime.datetime(2025, 1, 4),
    ],
}

df = pd.DataFrame(data)

df.min(axis=0)
```

### Issue Description

When calling DataFrame.min(axis=0) on a DataFrame with columns containing mixed types (np.nan and datetime), a TypeError is raised due to the comparison of float (from np.nan) and datetime.date. The default behavior of min should skip np.nan values when skipna=True (default), but this does not happen.
```Traceback (most recent call last):

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\test.py"", line 29, in <module>

    df.min(axis=0)

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\frame.py"", line 11643, in min

    result = super().min(axis, skipna, numeric_only, **kwargs)

             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\generic.py"", line 12388, in min

    return self._stat_function(

           ^^^^^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\generic.py"", line 12377, in _stat_function

    return self._reduce(

           ^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\frame.py"", line 11562, in _reduce

    res = df._mgr.reduce(blk_func)

          ^^^^^^^^^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\internals\managers.py"", line 1500, in reduce

    nbs = blk.reduce(func)

          ^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\internals\blocks.py"", line 404, in reduce

    result = func(self.values)

             ^^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\frame.py"", line 11481, in blk_func

    return op(values, axis=axis, skipna=skipna, **kwds)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\nanops.py"", line 147, in f

    result = alt(values, axis=axis, skipna=skipna, **kwds)

             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\nanops.py"", line 404, in new_func

    result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)

             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\pandas\core\nanops.py"", line 1098, in reduction

    result = getattr(values, meth)(axis)

             ^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""C:\Users\45217950\Downloads\GitHub\irr-cloud\.venv\Lib\site-packages\numpy\_core\_methods.py"", line 48, in _amin

    return umr_minimum(a, axis, None, out, keepdims, initial, where)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

TypeError: '<=' not supported between instances of 'float' and 'datetime.date'
```

### Expected Behavior

The min function should skip np.nan values when skipna=True (default) and return the minimum datetime value:
```
dates   2025-01-03
dtype: datetime64[ns]
```

### Installed Versions

<details>

INSTALLED VERSIONS

------------------

commit                : 0691c5cf90477d3503834d983f69350f250a6ff7

python                : 3.12.7

python-bits           : 64

OS                    : Windows

OS-release            : 10

Version               : 10.0.19045

machine               : AMD64

processor             : Intel64 Family 6 Model 85 Stepping 7, GenuineIntel

byteorder             : little

LC_ALL                : None

LANG                  : en_US.UTF-8

LOCALE                : English_United States.1252

 

pandas                : 2.2.3

numpy                 : 2.2.3

pytz                  : 2025.1

dateutil              : 2.9.0

pip                   : 24.2

Cython                : None

sphinx                : None

IPython               : None

adbc-driver-postgresql: None

adbc-driver-sqlite    : None

bs4                   : None

blosc                 : None

bottleneck            : None

dataframe-api-compat  : None

fastparquet           : None

fsspec                : None

html5lib              : None

hypothesis            : None

gcsfs                 : None

jinja2                : None

lxml.etree            : None

matplotlib            : None

numba                 : None

numexpr               : None

odfpy                 : None

openpyxl              : 3.1.5

pandas_gbq            : 0.28.0

psycopg2              : None

pymysql               : None

pyarrow               : 19.0.1

pyreadstat            : None

pytest                : None

python-calamine       : None

pyxlsb                : None

s3fs                  : None

scipy                 : 1.15.2

sqlalchemy            : None

tables                : None

tabulate              : None

xarray                : None

xlrd                  : None

xlsxwriter            : 3.2.2

zstandard             : None

tzdata                : 2025.1

qtpy                  : None

pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Needs Info', 'Reduction Operations']",2025-03-27 03:16:47,2025-03-31 02:16:58,5,closed
61186,BUG: engine calamine lost 0 when read_excel from vlookup cell,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df = pd.read_excel(r'C:\Users\ryjfgjl\Desktop\汽车计提1-2月明细(1).xlsx', sheet_name=3, na_filter=False, engine='calamine', dtype=object)
print(df)
```

### Issue Description

Excel data
![Image](https://github.com/user-attachments/assets/faac4ada-81cc-4318-b86b-ae6402677412)

df:
![Image](https://github.com/user-attachments/assets/9014cb58-0c15-4f53-af80-7a4a1888f774)

### Expected Behavior

change engine to openpyxl is correct

![Image](https://github.com/user-attachments/assets/665555b9-4f5f-417d-83c8-2c2fb0ccfcb6)

### Installed Versions

<details>

2.2.3

</details>
","['Bug', 'IO Excel', 'Closing Candidate', 'Upstream issue']",2025-03-27 01:57:03,2025-06-19 20:51:54,6,closed
61182,BUG: Negation of `.str.isnumeric()` changes `dtype` when `pd.NA` is present,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

s = pd.Series(["""", ""0"", ""123"", "" 123"", pd.NA])
print(s.str.isnumeric())
print(~s.str.isnumeric())

t = pd.Series(["""", ""0"", ""123"", "" 123""])
print(t.str.isnumeric())
print(~t.str.isnumeric())
```

### Issue Description

When `pd.NA` is present in a `Series` object, negating the `.str.isnumeric()` method changes `bool` values to `int` values.


### Expected Behavior

Negation should adhere to the [Kleene logic](https://pandas.pydata.org/docs/user_guide/boolean.html) implemented elsewhere in `pandas`.


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.16
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-1021-azure
Version               : #25-Ubuntu SMP Wed Jan 15 20:45:09 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 8.34.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.3.1
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.39
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Strings', 'Needs Discussion', 'Closing Candidate']",2025-03-26 16:35:55,2025-04-04 09:59:28,9,closed
61179,BUG: replace with np.nan unexpectedly converts pd.Timestamp to pd.NaT,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

data = {
    ""date"": [
        pd.Timestamp(""2025-01-01""),
        pd.Timestamp(""2025-01-02""),
        pd.Timestamp(""2025-01-03""),
    ],
}

df = pd.DataFrame(data)

df.replace([pd.Timestamp(""2025-01-01""), pd.Timestamp(""2025-01-02"")], np.nan)
```

### Issue Description

When using `DataFrame.replace()` to replace specific `pd.Timestamp` values with np.nan, the resulting values become `pd.NaT` instead of `np.nan`. This behavior differs from `pandas 1.1.5`, where the replaced values were `np.nan` as expected.

Output
``` 
        date
0        NaT
1        NaT
2 2025-01-03
```

### Expected Behavior

```
        date
0        NaN
1        NaN
2 2025-01-03
```


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.5
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 23.2.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Timestamp']",2025-03-26 07:57:42,2025-03-26 19:26:41,1,closed
61175,BUG: AttributeError on method call after binary operation in eval expression,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

x = pd.Series([1,2,3,5])
y = pd.Series([2,3,4])

pd.eval(""(x + y).dropna()"")
# raises AttributeError: 'BinOp' object has no attribute 'value'

# Note that something like pd.eval(""(x.dropna() + y)"") works!
```

### Issue Description

Also effects other Series methods called on the result of a binary operation, which work well if applied to one operand.

See also https://github.com/pandas-dev/pandas/issues/24670

### Expected Behavior

Should return a Series of sums with nan values removed. Generally I would expect that it should be possible to apply a Series method to the result of a binary operation as part of an eval expression.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.13.7-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Thu, 13 Mar 2025 18:12:00 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.3
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 9.0.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.1
matplotlib            : 3.10.1
numba                 : None
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.38
tables                : 3.10.2
tabulate              : None
xarray                : 2025.1.2
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'expressions']",2025-03-25 10:14:20,2025-03-31 16:52:37,5,closed
61169,ENH: add destructor to HDFStore,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

When using pd.HDFStore methods directly the store often stays open. Especially in interactive context. That can cause some permission errors:

```
In [1]: pd.Series([1,2,3]).to_hdf('foo.h5', 'x')

In [2]: pd.Series([1,2,3]).to_hdf('foo.h5', 'y')

In [3]: pd.HDFStore('foo.h5', 'r').keys()
Out[3]: ['/x', '/y']

In [4]: pd.Series([1,2,3]).to_hdf('foo.h5', 'z')
ValueError: The file 'foo.h5' is already opened, but in read-only mode.  Please close it before reopening in append mode.

In [5]: os.rename('foo.h5', 'bar.h5')
On Windows:
PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'foo.h5' -> 'bar.h5' 
```

### Feature Description

Adding destructor solves this

```
pd.io.pytables.HDFStore.__del__ = lambda self: self.close()
```

### Alternative Solutions

.

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-03-23 17:23:42,2025-03-24 16:35:22,2,closed
61165,BUG: `datetime64[s]` fails round trip using `.to_parquet` and `read_parquet`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

c = pd.Series([""2024-01-01"", ""2025-01-01"", ""2026-01-01""], dtype=""datetime64[s]"")
df0 = c.to_frame()

print(df0.dtypes)
df0.to_parquet(""test.parquet"")

df1 = pd.read_parquet(""test.parquet"")
print(df1.dtypes)
```

### Issue Description

The `dtype` changes from `datetime64[s]` to `datetime64[ms]`.

### Expected Behavior

I would expect the `dtype` to remain unchanged.

### Installed Versions

<details>
INSTALLED VERSIONS

------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.16
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-1021-azure
Version               : #25-Ubuntu SMP Wed Jan 15 20:45:09 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 8.34.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : 5.3.1
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.39
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5          
</details>
","['Bug', 'Datetime', 'IO Parquet']",2025-03-21 23:39:25,2025-03-22 11:19:36,1,closed
61161,BUG: fails to plot 2 plots with secondary y axis when index type is PeriodIndex and plot kinds are different,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import matplotlib.pyplot as plt

print(pd.__version__)

index = pd.period_range('2023', periods=3, freq='Y')
df = pd.DataFrame({
    'col1': [10, 20, 30],
    'col2': [40, 25, 10]
}, index=index)

# Workaround !!!!
# df.index = df.index.astype(str)

print(type(df.index))

fig, ax = plt.subplots()

df['col1'].plot(kind='bar', ax=ax)

ax2 = ax.twinx()

df['col2'].plot(kind='line', ax=ax2, color = 'r')

plt.show()
```

### Issue Description

I seem to have found the following bug:

It is not possible to plot 2 plots with different Y axis when the following 2 conditions are met:

* plot kinds are different (e.g. 'bar' and 'line')
* index type is **PeriodIndex**


![Image](https://github.com/user-attachments/assets/84241d4d-14db-4928-aeb3-eb7f5d77ee0c)

The easy workaround is to convert index to string:

<img width=""442"" alt=""Image"" src=""https://github.com/user-attachments/assets/2bd9f427-7887-47e3-a6d4-17322e9c64e2"" />


If the parameter 'kind' of both axis is identical, then it works as well without workound

![Image](https://github.com/user-attachments/assets/f972ef18-4749-4838-bed9-b59e84ab2b72)


### Expected Behavior

Both plots shall be possible to be seen

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.7
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Netherlands.1252

pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Visualization']",2025-03-21 19:34:07,2025-04-18 16:12:48,4,closed
61159,BUG: Faulty DatetimeIndex union,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
def test_pandas_datetime_index_union(self):
    """"""
    Demonstrates a suspected bug in pandas 2.2.3, where unions of DatetimeIndexes (and therefore pd.concats of dataframes with DatetimeIndexes)
    are returning unexpected values.

    My actual usecase (concatenating two dataframes with these DatetimeIndexes, from which I extracted these date ranges) works in
    pandas 1.5.3, but not 2.2.3.

    Interestingly, this passes in both versions if you change the dtype to datetime64[ns].

    """"""

    dti1 = DatetimeIndex(
        ['2021-10-05 17:30:00', 
         '2021-10-05 18:00:00', 
         '2021-10-05 18:30:00', 
         '2021-10-05 19:00:00', 
         '2021-10-05 19:30:00'],
        dtype='datetime64[us]', name='DATETIME', freq='30min'
    )
    dti2 = DatetimeIndex(
        ['2021-10-05 17:30:00', 
         '2021-10-05 18:00:00',  
         '2021-10-05 18:30:00', 
         '2021-10-05 19:00:00', 
         '2021-10-05 19:30:00', 
         '2021-10-05 20:00:00'], # <-- Extra datetime
        dtype='datetime64[us]', name='DATETIME', freq='30min'
    )

    union = set(dti1.union(dti2))
    expected = set(dti1) | set(dti2)
    print(f""{union=}"")
    print(f""{expected=}"")

    assert len(union) == len(expected), ""Should have all the rows from the concatenated dataframes""


def test_range_index_equality(self):
    """""" This (presumably) faulty equality check appears to be the root cause of the datetimeindex union bug above
    Note that the two stop values are different, so the RangeIndexes should not be equal.
    Interestingly, this fails in both pandas 1.5.3 and 2.2.3.
    """"""
    a = RangeIndex(start=1633455000000000, stop=1635262200000000, step=1800000000000)
    b = RangeIndex(start=1633455000000000, stop=1635264000000000, step=1800000000000)
    assert not a.equals(b)
```

### Issue Description

These tests above (details in the function doc) demonstrate the issue and what I think is the root cause.
Basically we get back what appears to be an incorrect result when taking the union of two DatetimeIndexes with different ranges.  

I traced this as far as the RangeIndex equality check in the second test, which appears to be faulty, returning True for two different `stop` values. 



### Expected Behavior

Out from first test should be (as in pandas 1.5.3):
```
union={Timestamp('2021-10-05 18:30:00'), Timestamp('2021-10-05 19:00:00'), Timestamp('2021-10-05 17:30:00'), Timestamp('2021-10-05 20:00:00'), Timestamp('2021-10-05 19:30:00'), Timestamp('2021-10-05 18:00:00')}

expected={Timestamp('2021-10-05 18:30:00'), Timestamp('2021-10-05 19:00:00'), Timestamp('2021-10-05 17:30:00'), Timestamp('2021-10-05 20:00:00'), Timestamp('2021-10-05 19:30:00'), Timestamp('2021-10-05 18:00:00')}
``` 

But the actual output in pandas 2.2.3 is (incorrectly):
```
union={Timestamp('2021-10-05 17:30:00'), Timestamp('2021-10-26 13:30:00')}

expected={Timestamp('2021-10-05 19:30:00'), Timestamp('2021-10-05 20:00:00'), Timestamp('2021-10-05 17:30:00'), Timestamp('2021-10-05 19:00:00'), Timestamp('2021-10-05 18:00:00'), Timestamp('2021-10-05 18:30:00')}

```


### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Australia.1252
pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : 4.5.0
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.3.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.6
lxml.etree            : None
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.3.0
scipy                 : 1.15.2
sqlalchemy            : 2.0.39
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'setops', 'Non-Nano']",2025-03-21 05:11:25,2025-03-22 11:36:51,3,closed
61155,BUG: Impossible creation of array with dtype=string,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.array([list('test')], dtype='string')
# ValueError: Buffer has wrong number of dimensions (expected 1, got 2)

pd.array([list('test'), list('word')], dtype='string')
# ValueError: Buffer has wrong number of dimensions (expected 1, got 2)

pd.array([list('test'), list('words')], dtype='string')
# <StringArray>
# [""['t', 'e', 's', 't']"", ""['w', 'o', 'r', 'd', 's']""]
# Length: 2, dtype: string

pd.array([list('test')])
# <NumpyExtensionArray>
# [['t', 'e', 's', 't']]
# Length: 1, dtype: object
```

### Issue Description

I'm trying to transform a list of list of strings into a `StringArray`, but the `pd.array` method with `dtype='string'` doesn't work when the second last level list contains lists of same length, raising an exception (see example). If the lists have different lengths, then the output doesn't raise an error and is correct.

In an older version of pandas (1.5.3), it produced a list of same length, but containing repeated casts of string arrays
```
<StringArray>
[
[""['t' 'e' 's' 't']"", ""['t' 'e' 's' 't']"", ""['t' 'e' 's' 't']"",
 ""['t' 'e' 's' 't']""]
]
Shape: (1, 4), dtype: string
```

### Expected Behavior

Same as pd.array([list('test')]) but with `StringArray` type.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_Europe.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 75.1.0
pip                   : 24.2
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.1
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.27.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.6.1
scipy                 : 1.13.1
sqlalchemy            : 2.0.34
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2023.6.0
xlrd                  : None
zstandard             : 0.23.0
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Strings']",2025-03-20 14:25:51,2025-05-15 16:13:22,6,closed
61153,BUG: import pandas 2.2.3 on WSL 2.4.12.0 raises broken support for `numpy.longdouble` dtype warning,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# on WSL2, ubuntu 24
import pandas

#raises
#/home/pepmts/git_projects/mts-python-monorepo/projects/mml-python-toolkit/venv/lib/python3.11/#site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature #b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class #'numpy.longdouble'> does not match any known type: falling back to type probe function.
#This warnings indicates broken support for the dtype!
#  machar = _get_machar(dtype)
```

### Issue Description

Whenever I import pandas on WSL I get this broken dtype warning
```python
>>> import pandas
/home/pepmts/git_projects/mts-python-monorepo/projects/mml-python-toolkit/venv/lib/python3.11/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\x00\xd0\xcc\xcc\xcc\xcc\xcc\xcc\xfb\xbf\x00\x00\x00\x00\x00\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.
This warnings indicates broken support for the dtype!
  machar = _get_machar(dtype)
```

### Expected Behavior

no warning

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.5
python-bits           : 64
OS                    : Linux
OS-release            : 4.4.0-26100-Microsoft
Version               : #1882-Microsoft Fri Jan 01 08:00:00 PST 2016
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.18.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Build', 'Upstream issue']",2025-03-20 10:12:53,2025-03-26 13:44:36,3,closed
61141,"BUG: astype transforms NA to ""NA""","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas

a = pandas.Series([pandas.NA], dtype = ""str"")

# This is tight
print(type(a[0]))
<class 'pandas._libs.missing.NAType'>

print(type(a.astype(""str"")[0]))
<class 'str'>
```

### Issue Description

When we work with missing data, and we do transformation from NA to ""str"", is does not keep the NA value, instead returns the string ""NA"".

### Expected Behavior

Return NA instead of ""NA""

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Linux
OS-release            : 6.12.16-gentoo-x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Tue Feb 25 08:36:23 -03 2025
machine               : x86_64
processor             : AMD Ryzen 7 5800H with Radeon Graphics
byteorder             : little
LC_ALL                : None
LANG                  : es_CL.utf8
LOCALE                : es_CL.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.3
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Missing-data', 'Strings']",2025-03-17 20:26:18,2025-03-19 12:34:58,6,closed
61140,BUG: `DataFrame.from_records()` ignores `columns` with iterator and `nrows=0`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```pycon
>>> import pandas as pd
>>> rows = []
>>> pd.DataFrame.from_records(iter(rows), columns=['a', 'b'], nrows=0)
Empty DataFrame
Columns: []
Index: []
```

### Issue Description

Passing an empty iterator to `DataFrame.from_records()` along with `nrows=0` results in a `DataFrame` with no columns, even if the `columns` argument is provided.

Oddly, the correct result is obtained with an empty iterator and `nrows > 0`. However, columns are still ignored if the iterator is non-empty and `nrows=0`.

### Expected Behavior

Should return a `DataFrame` with 0 rows and the columns provided.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.9
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.3
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'IO Data']",2025-03-17 18:13:09,2025-08-11 20:53:46,1,closed
61139,DOC: Clarification on Python 3.13 support,"### Pandas version checks

The issue doesn't still exist on the latest versions of the docs on main since 
https://pandas.pydata.org/docs/dev/getting_started/install.html#python-version-support
refers to https://scientific-python.org/specs/spec-0000/ which says Python 3.13 is supported. So the question is about the released version 2.2.3

### Location of the documentation

https://pandas.pydata.org/docs/getting_started/install.html#python-version-support

### Documentation problem


In [this issue](https://github.com/python/cpython/issues/131354) @StanFromIreland pointed out that Pandas docs say:

> #### Python version support
>
> Officially Python 3.9, 3.10, 3.11 and 3.12.

I see that there are Python 3.13 wheels, that it is tested in CI, and `pyproject.toml` includes the `Programming Language :: Python :: 3.13` classifier. Is it an accidental omission that Python 3.13 isn't included in the officially supported list? If it _isn't_ officially supported, it would be good to call this out more obviously.


### Suggested fix for documentation

Perhaps the new text in the dev docs should be backported?","['Docs', 'Closing Candidate']",2025-03-17 14:27:48,2025-03-19 13:59:25,2,closed
61136,Issue Title Here,Description of the issue goes here.,[],2025-03-17 07:55:45,2025-03-17 07:56:55,0,closed
61135,Issue Title Here,Description of the issue goes here.,[],2025-03-17 07:41:22,2025-03-17 07:42:01,0,closed
61133,ENH: Adding some common functionalities,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Some feature suggestions.

1) Pandas Plotting: Method chaining in plotting would make it really powerful.
sample:- https://github.com/maddytae/pytae/blob/master/src/plotter.ipynb
2) Select: Native R like select function in pandas would make method chaining much more easier.
sample:-https://github.com/maddytae/pytae/blob/master/src/select.ipynb
3) qry:- A dict based filtering criteria.
sample:- https://github.com/maddytae/pytae/blob/master/src/qry.ipynb
4) reshaping: More intuitive reshaping.
sample:- https://github.com/maddytae/pytae/blob/master/src/shape.ipynb
5) aggregation: easy aggregation.
sample:- https://github.com/maddytae/pytae/blob/master/src/agg_df.ipynb
6) Other utilities: Some minor useful functions.
sample:- https://github.com/maddytae/pytae/blob/master/src/other_utilities.ipynb




### Feature Description

Sample implementation:-
1) https://github.com/maddytae/pytae/blob/master/src/pytae/plotting.py
2) https://github.com/maddytae/pytae/blob/master/src/pytae/select.py
3) https://github.com/maddytae/pytae/blob/master/src/pytae/qry.py
4) https://github.com/maddytae/pytae/blob/master/src/pytae/shape.py
5) https://github.com/maddytae/pytae/blob/master/src/agg_df.ipynb
6) https://github.com/maddytae/pytae/blob/master/src/pytae/other_utilities.py

### Alternative Solutions

https://pypi.org/project/pytae/

### Additional Context

Apologies for not following conventions. I am still learning and have little experiencing contributing to open source. I have also leveraged llms for some of the codes but again I am hopeful the ideas I have share others will find some of it useful.","['Enhancement', 'Needs Triage']",2025-03-16 13:12:18,2025-03-17 16:27:14,2,closed
61126,DOC: Write user guide page on apply/map/transform methods,"There is some information in our documentation regarding how to use user defined functions in pandas. The API pages of the used methods, and these sections:

- https://pandas.pydata.org/docs/user_guide/groupby.html#aggregation-with-user-defined-functions
- https://pandas.pydata.org/docs/user_guide/gotchas.html#gotchas-udf-mutation

My understanding is that we've been mostly discouraging the use of functions like apply, or at least the community has with many posts and comments regarding `apply` is slow, which seem fair. With the work going on supporting JIT compilers on these functions (see https://github.com/pandas-dev/pandas/pull/54666 and https://github.com/pandas-dev/pandas/pull/61032) this can hopefully change, and allow in some cases for clearer code while not compromising speed.

I think it may be difficult to communicate all the information related to udf in the existing sections on group by and FAQ pages and in the API docs. A dedicated page in the users guide that guides users on when to use udf, a general idea of the API, the differences between the different methods, the options available... seems a better idea.

Also, the APIs of the different methods are quite inconsistent, and in some cases cumbersome. I think writing this page will be a good exercise to identify cases when explaining the functionality to the users is complex and not intuitive, and see if we can address them.","['Docs', 'Apply']",2025-03-15 04:03:22,2025-05-18 19:31:47,3,closed
61124,ENH: fillna enhancement with method='nearest',"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

This should be a simple follow-up to https://github.com/pandas-dev/pandas/issues/9471, enabling support for alignment with method='nearest'.

Since fillna internally uses interpolate, which already supports method='nearest', this might work right away, though it will require extensive testing.

### Feature Description

The new feature could be implemented by extending the current alignment functionality in Pandas to support `method='nearest'`. This would allow the user to align two Series or DataFrames by their indices, using the nearest available value when exact matches are not found. Here's a basic idea of how it could be implemented in pseudocode:

```python
def align_nearest(df1, df2):
    # Use a nearest neighbor search to align the indices
    df1_nearest = df1.reindex(df2.index, method='nearest')
    return df1_nearest
```

This functionality could be added as a method to the existing `pandas.DataFrame` and `pandas.Series` objects, integrating smoothly into the current API.


### Alternative Solutions

  An alternative solution would be to use the existing `interpolate` function with `method='nearest'`, which can be applied to the DataFrame or Series before performing the alignment. Additionally, third-party libraries like `fuzzywuzzy` or `scipy.spatial` could be used for more complex nearest matching.

```python
import pandas as pd
from fuzzywuzzy import process

# Example using fuzzywuzzy to find nearest match
df1 = pd.DataFrame([...])
df2 = pd.DataFrame([...])
df1['nearest'] = df1['index_column'].apply(lambda x: process.extractOne(x, df2['index_column'])[0])
```

However, native support within Pandas would likely be more efficient and user-friendly.


### Additional Context

 ","['Enhancement', 'Missing-data', 'Needs Info']",2025-03-15 01:03:57,2025-08-05 17:08:20,2,closed
61123,DOC: `read_excel` `nrows` parameter reads extra rows when tables are adjacent (no blank row),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Two tables, each header row + 3 data rows
file1 = ""test1.xlsx"" # blank row between the tables
file2 = ""test2.xlsx"" # no blank row between the tables
df1 = pd.read_excel(file1, header=0, nrows=4)
df2 = pd.read_excel(file2, header=0, nrows=4)

print(df1)
print(df2)
assert df1.shape == df2.shape

# df2 includes the header row of the following table
```

### Issue Description

Consider two Excel files with nearly identical data: two tables, each with a header row and 3 data rows. The only difference is that the first has a blank row between the tables and the second does not.

It seems that the blank line makes a difference, even when `nrows` is specified. I expect `nrows=4` to always parse 4 rows, yielding a data frame with a header and 3 data rows. Yet without a blank line, `read_excel` also includes the next row, which is the header for the next table.

[test1.xlsx](https://github.com/user-attachments/files/19254818/test1.xlsx)
[test2.xlsx](https://github.com/user-attachments/files/19254817/test2.xlsx)

### Expected Behavior

I expect `nrows=4` to always parse 4 rows regardless of context: a header and 3 data rows.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.0rc1
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.12.3
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.3
lxml.etree            : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.2.0
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Docs', 'IO Excel', 'good first issue']",2025-03-14 21:30:43,2025-03-19 20:37:24,1,closed
61122,BUG: ``Series.interpolate`` regression in latest Pandas 3.0.0 nightly (method 'linear' behaves like 'index'),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

s = pd.Series([1.0, np.nan, 3.0], index=[1, 3, 4])
s.interpolate(method='linear')
s.interpolate(method='index')
```

### Issue Description

The interpolation method 'linear' behaves like the method 'index' with current Pandas 3.0.0 nightly. This is a regression from 2.2.3.

According to the documentation (stable and dev):

> Interpolation technique to use. One of:
>
>  - ‘linear’: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes.
> [...]
>  - ‘index’: The interpolation uses the numerical values of the DataFrame’s index to linearly calculate missing values.

In the example above, the index is not linearly spaced. But both interpolation methods return the output that is expected for the 'index' method when using the latest Pandas 3.0.0 nightly.

```
>>> s.interpolate(method='linear')
1    1.000000
3    2.333333
4    3.000000
dtype: float64
>>> s.interpolate(method='index')
1    1.000000
3    2.333333
4    3.000000
dtype: float64
```

### Expected Behavior

The output should be different and ``'linear'`` should ignore the non-linearly spaced index. The expected output should be the same as with Pandas 2.2.3:

```
>>> s.interpolate(method='linear')
1    1.0
3    2.0
4    3.0
dtype: float64
>>> s.interpolate(method='index')
1    1.000000
3    2.333333
4    3.000000
dtype: float64
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : ddd0aa8dc73481017330892dfd0ea95c0dfaa1d3
python                : 3.12.1
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19044
machine               : AMD64
processor             : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 3.0.0.dev0+2010.gddd0aa8dc7
numpy                 : 2.3.0.dev0+git20250311.a651643
dateutil              : 2.9.0.post0
pip                   : 23.2.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Regression', 'Blocker']",2025-03-14 16:52:42,2025-03-29 18:01:33,7,closed
61120,BUG: .corr() values significantly higher than 1,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
data = pd.DataFrame(dict(
    x=[0, 1],
    y=[1.35951, 1.3595100000000007]
))
data.corr().max().max()
```

### Issue Description

The example above results in `1.1547005383792517`.

This is similar to https://github.com/pandas-dev/pandas/issues/35135 which was closed as ""_not an issue with pandas, but just numerical computations_"" but differently to that issue which showed minuscule differences (practically negligible), I am presenting an example where the Pearson correlation is over 15% above the maximum of 1.

I was able to reproduce this on multiple machines.

I think this might warrant a mention in the documentation.

### Expected Behavior

These two are perfectly correlated, so we would expect `1`. `1` is the result for both:
- `(data + 0.0000000000000002).corr().max().max()`
- `(data - 0.0000000000000002).corr().max().max()`

Interestingly, using `corrwith` or R leads to a different result which under-estimates the correlation (but at least is not out of range, and the relative error is smaller!):
- `data[['x']].corrwith(data['y'])` returns `0.948683`
- `cor` in R also returns 0.9486833
```
cor(c(0, 1), c(1.35951, 1.3595100000000007))
[1] 0.9486833
```


### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.4
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-55-generic
Version               : #57-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 12 23:42:21 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
pip                   : 23.1.2
Cython                : None
sphinx                : None
IPython               : 9.0.0.dev
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.3
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 15.0.0
pyreadstat            : None
pytest                : 7.4.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'cov/corr']",2025-03-14 13:05:53,2025-03-25 17:11:41,3,closed
61113,BUG: `pivot_table` drops rows and columns despite values being non-`NaN`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd
import pandas._testing as tm


def test_pivot_table_index_and_column_with_nan() -> None:
    """"""Index and columns should exist if any non-null values.

    Input data
    ----------
        row  col  val
    0  NaN  0.0    0
    1  0.0  1.0    1
    2  1.0  2.0    2
    3  2.0  3.0    3
    4  3.0  NaN    4

    Expected output
    ---------------
    col  0.0  1.0  2.0  3.0  NaN
    row                         
    NaN  0.0  NaN  NaN  NaN  NaN
    0.0  NaN  1.0  NaN  NaN  NaN
    1.0  NaN  NaN  2.0  NaN  NaN
    2.0  NaN  NaN  NaN  3.0  NaN
    3.0  NaN  NaN  NaN  NaN  4.0
    """"""
    data = {
        ""row"": [None, *range(4)],
        ""col"": [*range(4), None],
        ""val"": range(5)
    }
    df = pd.DataFrame(data)
    actual = df.pivot_table(values=""val"", index=""row"", columns=""col"")
    e_index = [None, *range(4)]
    e_columns = [*range(4), None]
    e_data = np.zeros(shape=(5, 5))
    e_data.fill(np.NaN)
    np.fill_diagonal(a=e_data, val=range(5))
    expected = pd.DataFrame(
        data=e_data,
        index=pd.Index(data=e_index, name=""row""),
        columns=pd.Index(data=e_columns, name=""col"")
    )
    tm.assert_frame_equal(left=actual, right=expected)  # fails
```

### Issue Description

Rows and columns are unexpectedly dropped while creating a pivot table with a single `NaN` index label and a single `NaN` column label and _no_ `NaN` values in the input data.

# Input data
```text
   row  col  val
0  NaN  0.0    0
1  0.0  1.0    1
2  1.0  2.0    2
3  2.0  3.0    3
4  3.0  NaN    4
```

# Actual output
```text
col  1.0  2.0  3.0
row               
0.0  1.0  NaN  NaN
1.0  NaN  2.0  NaN
2.0  NaN  NaN  3.0
```

### Expected Behavior

The docs for `dropna` state:
> Do not include columns whose entries are all NaN. If True, rows with a NaN value in any column will be omitted before computing margins.

Given that `dropna=True` by default, I'd expect all columns and rows to be present as each has _at least one_ non-`NaN` value.

# Expected output
```text
col  0.0  1.0  2.0  3.0  NaN
row                         
NaN  0.0  NaN  NaN  NaN  NaN
0.0  NaN  1.0  NaN  NaN  NaN
1.0  NaN  NaN  2.0  NaN  NaN
2.0  NaN  NaN  NaN  3.0  NaN
3.0  NaN  NaN  NaN  NaN  4.0
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 3c93d06d641621ff62453c9c7748eeec3d6d8a97
python                : 3.13.2
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252
pandas                : 3.0.0.dev0+2007.g3c93d06d64
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 9.0.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.3.0
html5lib              : 1.1
hypothesis            : 6.129.0
gcsfs                 : 2025.3.0
jinja2                : 3.1.6
lxml.etree            : 5.3.1
matplotlib            : 3.10.1
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyreadstat            : 1.2.8
pytest                : 8.3.5
python-calamine       : None
pytz                  : 2025.1
pyxlsb                : 1.0.10
s3fs                  : 2025.3.0
scipy                 : 1.15.2
sqlalchemy            : 2.0.39
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Docs', 'Missing-data', 'Reshaping']",2025-03-12 23:41:34,2025-04-02 21:28:17,9,closed
61110,ENH: Automate reading of data in chunks,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I have a file with 20Gb of data that I need to process. When I use a pandas dataframe, the full 20Gb need to be loaded. That will make the computer slow or even crash. Can this process be made more efficient by _automatically_ (very very important that the user does not have to do anything here) loads a chunk, processes it, writes it, loads the second chunk, etc. 

This is stuff is possible, it is done by [ROOT](https://root.cern/doc/master/classROOT_1_1RDataFrame.html) for instance.

### Feature Description

This would just work with the normal dataframes, there could be an option like

```python
pd.chunk_size = 100
```
which would process 100Mb at a time. So that no more than 100 Mb would be in memory.

### Alternative Solutions

Alternatively we can

```python
import ROOT

rdf = ROOT.RDataFrame('tree', 'path_to_file.root')
```

### Additional Context

_No response_","['Enhancement', 'Needs Info', 'Closing Candidate']",2025-03-12 06:58:57,2025-08-05 16:30:42,2,closed
61099,BUG: Can only compare identically-labeled Series objects (string vs. object),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
s2 = pd.Series([4, 5, 6], index=['a', 'b', 'c'])
s2.index = s2.index.astype('string')

s1 < s2  # fails

s1, s2 = s1.align(s2)
s1 < s2  # also fails

s1 = s1.reindex(s2.index)
s1 < s2  # succeeds
```

### Issue Description

When a series (or dataframe) with otherwise identical indices are compared, but the indexes are *technically* dtype(object) and dtype(string), element-wise comparison fails. In the debugger, it looks like the ExtensionArray StringArray.equals is False when comparing to a python list of strings, causing Series._indexed_same to return False.

### Expected Behavior

Ideally the string and object dtype would be comparable. This in-between state for Pandas dtypes has been quite awkward, with some libraries porting over to numpy-nullable / pyarrow dtype backends, but the Pandas library defaults not using them yet. 

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Strings', 'Index']",2025-03-10 20:18:17,2025-07-10 20:58:45,6,closed
61093,ENH: Support dict to `pd.set_option` for cleaner code,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish that I could use dict for `pd.set_option`, just like other packages typically accept dict when there are lots of option. Current pandas style follow what old programming language do, such as MATLAB.

### Feature Description

```python
import pandas as pd

options = {
    'display.precision': 2,
    'display.max_columns': 100,
    'styler.format.precision': 2,
}

# like this
pd.set_option(**options)

# or like this:
pd.set_option(options)
```

### Alternative Solutions

Current implementations is:

```python
pd.set_option('display.precision', 2)
pd.set_option('display.max_columns', 100)
pd.set_option('styler.format.precision', 2)

# or

options = {
    'display.precision': 2,
    'display.max_columns': 100,
    'styler.format.precision': 2,
}

for key, value in options.items():
    pd.set_option(key, value)
```

### Additional Context

Because if I write it like this:

```python
pd.set_option(
    'display.precision', 2,
    'display.max_columns', 100,
    'styler.format.precision', 2,
)
```

An auto formatter like `ruff` will make it into:

```python
pd.set_option(
    'display.precision',
    2,
    'display.max_columns',
    100,
    'styler.format.precision',
    2,
)
```","['Enhancement', 'API Design']",2025-03-10 03:44:46,2025-05-29 18:44:32,25,closed
61091,BUG: DataFrame.explode doesn't explode when using pyarrow large list type,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [4]: pd.DataFrame({'a': [[1, 2], [3, 4]]}, dtype=pd.ArrowDtype(pa.list_(pa.int64()))).explode('a')
Out[4]:
   a
0  1
0  2
1  3
1  4

In [5]: pd.DataFrame({'a': [[1, 2], [3, 4]]}, dtype=pd.ArrowDtype(pa.large_list(pa.int64()))).explode('a')
Out[5]:
       a
0  [1 2]
1  [3 4]
```

### Issue Description

For `list_`, it explodes correctly, but for `large_list`, it's a no-op

### Expected Behavior

```
   a
0  1
0  2
1  3
1  4
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 57fd50221ea3d5de63d909e168f10ad9fc0eee9b
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1979.g57fd50221e
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 8.33.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.2.0
html5lib              : 1.1
hypothesis            : 6.127.5
gcsfs                 : 2025.2.0
jinja2                : 3.1.5
lxml.etree            : 5.3.1
matplotlib            : 3.10.1
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyreadstat            : 1.2.8
pytest                : 8.3.5
python-calamine       : None
pytz                  : 2025.1
pyxlsb                : 1.0.10
s3fs                  : 2025.2.0
scipy                 : 1.15.2
sqlalchemy            : 2.0.38
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Reshaping', 'Arrow']",2025-03-09 15:33:23,2025-03-11 22:20:34,1,closed
61081,BUG: pd.api.types.infer_dtype on scalar input,"### Context
I was trying to identify data types in columns with mixed data types:
```python
df.map(pd.api.types.infer_dtype)
```


### Pandas version checks

- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.
- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.api.types.infer_dtype(1)
pd.api.types.infer_dtype(1.0)
pd.api.types.infer_dtype(True)
```

### Issue Description

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[8], line 1
----> 1 pd.api.types.infer_dtype(1)

File lib.pyx:1605, in pandas._libs.lib.infer_dtype()

TypeError: 'int' object is not iterable
```

### Expected Behavior

According to the documentation, pd.api.types.infer_dtype() should accept scalar input.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.5
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.1.0
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.37
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Docs']",2025-03-07 18:50:10,2025-03-13 15:38:18,9,closed
61076,PERF: why nlargest is so slower?,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

nlargest is so slow, I think this is question, maybe we should do someting to improve it.

Here, is my code, you can see nlargest use many time more than it should use.

![Image](https://github.com/user-attachments/assets/cac98d73-b0ed-4e6d-8a1d-51798c1ad96d)


### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>


### Prior Performance

_No response_","['Performance', 'Needs Info']",2025-03-07 14:31:23,2025-08-05 17:07:45,6,closed
61072,BUG: str.fullmatch behavior is not the same for object dtype and string[pyarrow] dtype,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas

test_series = pandas.Series(['asdf', 'as'], dtype='string[pyarrow]')
regex = r'((as)|(as))'
regex2 = r'(as)|(as)'

test_series.str.fullmatch(regex)
# False
# True
test_series.str.fullmatch(regex2)
# True
# True

test_series2 = pandas.Series(['asdf', 'as'], dtype=str)

test_series2.str.fullmatch(regex)
# False
# True
test_series2.str.fullmatch(regex2)
# False
# True
```

### Issue Description

As the example shows you can use the same regular expression for the str.fullmatch method when using a str dtype and string[pyarrow] dtype and get different results. This seems to stem from Apache Arrow not having a dedicated fullmatch or match, so the regular expression has to be edited with ""^"" and ""$"" characters before being delivered to its search function. There might also be some special handling in Python's fullmatch method with the ""|"" operator. Long story short, at least some regular expressions delivered to PyArrow need additional surrounding parentheses to get the same fullmatch results as when using Python's fullmatch.

I have submitted #61073 to try and address this.

### Expected Behavior

The second set of fullmatch results in the example shows the expected behavior. The str.fullmatch method should behave the same for either dtype.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.5
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 12, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.24.4
pytz                  : 2022.1
dateutil              : 2.8.2
pip                   : 25.0.1
Cython                : 3.0.11
sphinx                : 5.1.1
IPython               : 8.21.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : 4.9.1
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.9
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Strings', 'Arrow']",2025-03-07 00:19:21,2025-09-21 14:02:24,2,closed
61071,BUG: Potentially incorrect result of `df.rolling(window=...).mean()`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

if __name__ == '__main__':    
    values = [
        -3.333333e-01,
        0.,
        1.000000e+15,
        -4.874588e+14,
        6.103516e-05,
        0.,
        1.100000e+00,
        0.,
        0.,
        -8.581237e+14,
        0.,
        1.000000e+15,
        0.,
        0.,
        0.,
        0.,
        0.,
        -5.96e-03,
        0.,
        0.,
    ]

    s = pd.Series(values, dtype=np.float64)
    window = 8

    ground_truth = np.sum(values[-window:]) / window
    rolling_mean_1 = s.rolling(window=window).mean()
    rolling_mean_2 = s.iloc[1:].rolling(window=window).mean()

    # >>> ground_truth
    # np.float64(-0.000745)

    # >>> rolling_mean_1.iloc[-1]
    # np.float64(-0.01637)
    
    # >>> rolling_mean_2.iloc[-1]
    # np.float64(-0.000745)

    # The first number is ~22 times greater than both the last one and the ground_truth value, which is significant.
    # >>> rolling_mean_1.iloc[-1] / rolling_mean_2.iloc[-1]
    # np.float64(21.973154362416107)
    # >>> rolling_mean_1.iloc[-1] / ground_truth
    # np.float64(21.973154362416107)
    
    assert np.allclose(rolling_mean_2.iloc[-1], ground_truth)  # passes
    assert np.allclose(rolling_mean_1.iloc[-1], ground_truth, rtol=0, atol=1e-2)  # fails
```

### Issue Description

While using pandas' `.rolling(window=...).mean()`, there is an unexpected discrepancy when comparing two different ways of computing the rolling mean:

1. `rolling_mean_1 = s.rolling(window=window).mean()`
2. `rolling_mean_2 = s.iloc[1:].rolling(window=window).mean()`

The discrepancy is significant, as `rolling_mean_1.iloc[-1]` is approximately `22` times greater than `rolling_mean_2.iloc[-1]`, even though the values required to compute the result for the last row are the same in both cases. Additionally, the reference value `ground_truth` is almost identical to `rolling_mean_2.iloc[-1]`. 

Likely, it's related to **rounding errors**, but I don't understand how they could cause such a large discrepancy in this case, given that the absolute values of the original data are not that large (<= `1e15`)

### Expected Behavior

`assert np.allclose(rolling_mean_1.iloc[-1], ground_truth)`  should pass

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.8
python-bits           : 64
OS                    : Darwin
OS-release            : 23.4.0
Version               : Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:54 PST 2024; root:xnu-10063.101.15~2/RELEASE_ARM64_T6031
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : 6.125.2
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Window', 'Closing Candidate']",2025-03-06 20:12:51,2025-03-07 09:23:57,2,closed
61070,DOC: Outdated example of DataFrame.mean,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.mean.html#pandas.DataFrame.mean

### Documentation problem

The second example specifies no axis and shows the mean for each column. According to the documentation above, the behavior when the axis is not specified should be to aggregate the mean over both axis since version 2.0.0

I didn't check if the same mistake is present in other aggregation functions.

### Suggested fix for documentation

Replace `df.mean()` with `df.mean(axis=0)` on the example.","['Docs', 'Needs Triage']",2025-03-06 19:36:06,2025-03-06 19:39:50,1,closed
61068,BUG:  Sorting fails in specific cases with pandas==2.1.1,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Example data
data = {'A': [3, 1, 2], 'B': ['x', 'y', 'z']}
df = pd.DataFrame(data)

# Sorting operation
sorted_df = df.sort_values(by='A')
print(sorted_df)
```

### Issue Description

I’ve encountered an issue where sorting operations fail under specific conditions when using pandas==2.1.1. This behavior does not occur in pandas==1.5.3, which suggests it might be a regression or a bug introduced in newer versions.

### Expected Behavior


Additional Context:

This issue does not occur in pandas==1.5.3.
I’ve tested this in multiple environments and confirmed the behavior.
If more details are needed, I can provide additional examples or data.
Tips for Submitting an Issue:

Be Clear and Concise: Clearly describe the problem and steps to reproduce it.
Provide Code and Data: Include a minimal reproducible example to help developers debug the issue.
Check for Duplicates: Before submitting, search the [Pandas Issues](https://github.com/pandas-dev/pandas/issues) to ensure it hasn’t already been reported.
Be Polite and Professional: Open-source maintainers appreciate respectful and constructive feedback.

### Installed Versions

Environment:

Python version: [e.g., 3.9.12]
Pandas version: 2.1.1
Operating System: [ Windows 10]
","['Bug', 'Needs Info', 'Sorting']",2025-03-06 05:56:49,2025-03-27 01:22:18,5,closed
61058,"DOC: Pivot() example call incorrectly used and would give ""error: duplicate index""","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

doc/source/user_guide/reshaping.rst



### Documentation problem

the table given as an example for pivot() is wrong and cant be used. it would return ""error duplicate index"" as there are duplicate values in the column given for ""index"" parameter.

<img width=""772"" alt=""Image"" src=""https://github.com/user-attachments/assets/d7198715-b74e-4c35-88f8-5a01ee8eefe4"" />



### Suggested fix for documentation

The ""foo"" column must contain unique values ","['Reshaping', 'Error Reporting', 'Needs Info']",2025-03-05 04:39:53,2025-08-05 17:06:52,4,closed
61055,BUG:  invalid result of reindex on columns after unstack with Period data #60980,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

series1 = pd.DataFrame(
   [(0, ""s2"", pd.Period(2022)), (0, ""s1"", pd.Period(2021))],
   columns=[""A"", ""B"", ""C""]
).set_index([""A"", ""B""])[""C""]

series2 = series1.astype(str)

print(series1.unstack(""B"").reindex([""s2""], axis=1))
print(series2.unstack(""B"").reindex([""s2""], axis=1))
```

### Issue Description

The example code prints

B  s2
A      
0  2021

B  s2
A  
0  2022

### Expected Behavior

Expect the result for both pd.Period and str data to be 2022:

B  s2
A  
0  2022

B  s2
A  
0  2022

(actually observed with older Pandas 2.0.3)

### Installed Versions

<details>

INSTALLED VERSIONS
commit : https://github.com/pandas-dev/pandas/commit/0691c5cf90477d3503834d983f69350f250a6ff7
python : 3.11.10
python-bits : 64
OS : Linux
OS-release : 6.2.16
Version : https://github.com/pandas-dev/pandas/issues/1-NixOS SMP PREEMPT_DYNAMIC Tue Jan 1 00:00:00 UTC 1980
machine : x86_64
processor :
byteorder : little
LC_ALL : None
LANG : en_US.UTF-8
LOCALE : en_US.UTF-8
pandas : 2.2.3
numpy : 2.2.3
pytz : 2025.1
dateutil : 2.9.0.post0
pip : 24.0
Cython : None
sphinx : None
IPython : None
adbc-driver-postgresql: None
adbc-driver-sqlite : None
bs4 : None
blosc : None
bottleneck : None
dataframe-api-compat : None
fastparquet : None
fsspec : None
html5lib : None
hypothesis : None
gcsfs : None
jinja2 : None
lxml.etree : None
matplotlib : None
numba : None
numexpr : None
odfpy : None
openpyxl : None
pandas_gbq : None
psycopg2 : None
pymysql : None
pyarrow : None
pyreadstat : None
pytest : None
python-calamine : None
pyxlsb : None
s3fs : None
scipy : None
sqlalchemy : None
tables : None
tabulate : None
xarray : None
xlrd : None
xlsxwriter : None
zstandard : None
tzdata : 2025.1
qtpy : None
pyqt5 : None

</details>
","['Bug', 'Reshaping']",2025-03-04 18:11:53,2025-03-05 00:13:34,3,closed
61052,BUG: sometimes when using ~ and & operators for indexing it evaluated incorrectly,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

Edit [rhshadrach]: The code below does _not_ reproduce the issue.

```python
# No idea how to exactly reproduce it, but it occurs sometimes. Logic is this:
import pandas as pd
bool_df = pd.DataFrame([
    {""first"": True, ""second"": False, ""third"": True},
    {""first"": True, ""second"": True, ""third"": True},
    {""first"": True, ""second"": False, ""third"": True},
    {""first"": True, ""second"": True, ""third"": True},
    {""first"": True, ""second"": True, ""third"": True},
    {""first"": True, ""second"": False, ""third"": True},
    {""first"": True, ""second"": True, ""third"": True},
    {""first"": False, ""second"": False, ""third"": True},
    {""first"": True, ""second"": True, ""third"": True},
    {""first"": True, ""second"": False, ""third"": True},
])
bool_df = bool_df[bool_df[""third""]][[""first"", ""second""]]
# In some cases, this line prints the length of the DataFrame (10)
print(len(bool_df[(~bool_df[""first""]) & (~bool_df[""second""])])) # Sometimes prints 10

# This line prints the expected output (1)
print(len(bool_df[(bool_df[""first""] == False) & (bool_df[""second""] == False)])) # Prints 1

# Using De Morgan's law also returned with the expected output
print(len(bool_df[~((bool_df[""first""]) | (bool_df[""second""]))])) # Prints: 1
```

### Issue Description

We don't know when and why this occurs. We werre looking for any rational explanation for hours. Anyone else experienced similar? How could this be possible?
(Environment: MacBook Pro 2023, Sequoia 15.3)

### Expected Behavior

print(len(bool_df[(~bool_df[""first""]) & (~bool_df[""second""])])) # Print 1

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.1
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:23 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6031
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.3
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 9.0.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.1
matplotlib            : 3.10.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.38
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Numeric Operations', 'Needs Info']",2025-03-04 12:14:16,2025-03-07 23:49:47,6,closed
61049,BUG: Ghost Column Generation Bug in .loc[] with Series Column Selector (Pandas 2.2.2),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
print(pd.__version__)  # 2.2.2

data = {'CustomerID': [101, 102, 103, 104, 105],
        'Name': ['John', 'Alice', 'Bob', 'David', 'Mike'],
        'CreditScore': [650, 720, 710, 600, 750],
        'LoanAmount': [40000, 70000, 80000, 30000, 120000],
        'AccountType': ['Savings', 'Current', 'Current', 'Savings', 'Current']}

df = pd.DataFrame(data)

# This line generates unexpected columns without warning
df.loc[(df['AccountType'] == ""Current"") & (df['CreditScore'] > 700), df['LoanAmount']] = 90000

print(df)
```

### Issue Description

I found a hidden bug inside .loc[] method that silently generates new columns when the second parameter is a Series column selector.

### Expected Behavior

Only LoanAmount column should be updated.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.7.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_India.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 75.1.0
pip                   : 24.2
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.1
html5lib              : None
pymysql               : 1.4.6
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.27.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.6.1
scipy                 : 1.13.1
sqlalchemy            : 2.0.34
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2023.6.0
xlrd                  : None
zstandard             : 0.23.0
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Indexing']",2025-03-04 07:06:45,2025-03-04 23:52:31,2,closed
61047,"Correction: Standardize spelling of ""normalize"" in codebase","Some functions in `_normalize.py` use British spelling (""normalise"") and some use American spelling (""normalize""). To maintain consistency with standard American spelling used in Pandas, ""normalise"" should be replaced with ""normalize"".

<img width=""683"" alt=""Image"" src=""https://github.com/user-attachments/assets/eac39be2-ef14-4a11-9b10-ac9ebfd74b8f"" />",[],2025-03-04 05:45:56,2025-03-04 18:04:14,0,closed
61045,PERF: Optimize membership check in column filtering for better performance,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

### Description  
Currently, the `columns` variable is a list of hashable elements returned by `_filter_usecols`. In the dictionary comprehension at [`pandas/pandas/io/parsers/c_parser_wrapper.py#L262`](https://github.com/pandas-dev/pandas/blob/main/pandas/io/parsers/c_parser_wrapper.py#L262):  

```python
col_dict = {k: v for k, v in col_dict.items() if k in columns}
```
### Proposed Improvement
Convert columns to a set before performing the membership check, reducing lookup time to O(1):

```python
columns_set = set(columns)  # Convert once
col_dict = {k: v for k, v in col_dict.items() if k in columns_set}
```

This avoids repeated list traversal and improves performance when filtering columns.

Expected Benefits
- Faster execution when columns contains many elements.
- Improved efficiency in scenarios with frequent membership checks.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.8
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-1025-azure
Version               : #26~22.04.1-Ubuntu SMP Thu Jul 11 22:33:04 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 8.33.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : 2024.11.0
fsspec                : 2025.2.0
html5lib              : 1.1
hypothesis            : 6.127.5
gcsfs                 : 2025.2.0
jinja2                : 3.1.5
lxml.etree            : 5.3.1
matplotlib            : 3.10.1
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.1
pyreadstat            : 1.2.8
pytest                : 8.3.5
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2025.2.0
scipy                 : 1.15.2
sqlalchemy            : 2.0.38
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>


### Prior Performance

_No response_","['Performance', 'IO CSV']",2025-03-04 01:51:13,2025-04-26 18:10:40,0,closed
61043,BUG: `.str.replace()` with capture groups does not play nice with string methods,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

Code
```python
import pandas as pd
c = pd.Series(""THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG"")
x, y, z = ""\\b(FOX|THE)\\b"", ""_ABC_"", ""\\1_ABC_""
print(c.str.replace(x, y.lower(), regex=True))
print(c.str.replace(x, z.lower(), regex=True))
```

Output
```
0    _abc_ QUICK BROWN _abc_ JUMPS OVER _abc_ LAZY DOG
dtype: object
0    THE_abc_ QUICK BROWN FOX_abc_ JUMPS OVER THE_a...
dtype: object
```


### Issue Description

The `.lower()` string method inconsistently modifies the `repl` argument when the latter includes a regex capture group.


### Expected Behavior

I would expect `.lower()` to modify all characters in `repl`, including those in the capture group (or a warning stating otherwise).


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.16
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 8.33.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.1
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>","['Bug', 'Strings']",2025-03-03 22:00:55,2025-03-03 22:56:12,1,closed
61037,Metadata generation failed when installing PyQt5,"This error comes from running the dockerfile and reaches the PyQt5 in the requirements-dev.txt. From stackoverflow, this problem occur when installing PyQt5 with version after 5.15.6 in virtual environment.

Not labeling bug as it's bug from PyQt5, but since this could affect dockerfile building I'll create this issue. 
If this is not proper please close this issue. 

Reference: https://stackoverflow.com/questions/70936664/metadata-generation-failed-while-installing-pyqt5","['Build', 'OS X']",2025-03-03 14:38:15,2025-09-23 19:19:42,8,closed
61036,Correction: Adjust Dockerfile key value format,"Suggest to add ""="" for assigning variable in Dockerfile.




![Image](https://github.com/user-attachments/assets/ea01d02e-26da-4dfb-be04-13fa457b35f0)","['Build', 'Clean']",2025-03-03 14:09:36,2025-03-05 18:47:20,1,closed
61031,BUG: `ValueError: ndarray is not C-contiguous` for `cummax` on nullable dtypes,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [5]: df = pd.DataFrame({'a': [1,1,2], 'b': [4,5,6], 'i': [0,1,2]}, dtype='Int64')[::-1]

In [6]: df.groupby('a')['b'].cummax()
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[6], line 1
----> 1 df.groupby('a')['b'].cummax()

File ~/pandas-dev/pandas/core/groupby/groupby.py:4945, in GroupBy.cummax(self, numeric_only, **kwargs)
   4886 """"""
   4887 Cumulative max for each group.
   4888
   (...)
   4942 bull    6   9
   4943 """"""
   4944 skipna = kwargs.get(""skipna"", True)
-> 4945 return self._cython_transform(
   4946     ""cummax"", numeric_only=numeric_only, skipna=skipna
   4947 )

File ~/pandas-dev/pandas/core/groupby/generic.py:684, in SeriesGroupBy._cython_transform(self, how, numeric_only, **kwargs)
    681 obj = self._obj_with_exclusions
    683 try:
--> 684     result = self._grouper._cython_operation(
    685         ""transform"", obj._values, how, 0, **kwargs
    686     )
    687 except NotImplementedError as err:
    688     # e.g. test_groupby_raises_string
    689     raise TypeError(f""{how} is not supported for {obj.dtype} dtype"") from err

File ~/pandas-dev/pandas/core/groupby/ops.py:932, in BaseGrouper._cython_operation(self, kind, values, how, axis, min_count, **kwargs)
    928 assert kind in [""transform"", ""aggregate""]
    930 cy_op = WrappedCythonOp(kind=kind, how=how, has_dropped_na=self.has_dropped_na)
--> 932 return cy_op.cython_operation(
    933     values=values,
    934     axis=axis,
    935     min_count=min_count,
    936     comp_ids=self.ids,
    937     ngroups=self.ngroups,
    938     **kwargs,
    939 )

File ~/pandas-dev/pandas/core/groupby/ops.py:546, in WrappedCythonOp.cython_operation(self, values, axis, min_count, comp_ids, ngroups, **kwargs)
    542 self._validate_axis(axis, values)
    544 if not isinstance(values, np.ndarray):
    545     # i.e. ExtensionArray
--> 546     return values._groupby_op(
    547         how=self.how,
    548         has_dropped_na=self.has_dropped_na,
    549         min_count=min_count,
    550         ngroups=ngroups,
    551         ids=comp_ids,
    552         **kwargs,
    553     )
    555 return self._cython_op_ndim_compat(
    556     values,
    557     min_count=min_count,
   (...)
    561     **kwargs,
    562 )

File ~/pandas-dev/pandas/core/arrays/masked.py:1602, in BaseMaskedArray._groupby_op(self, how, has_dropped_na, min_count, ngroups, ids, **kwargs)
   1599 if how == ""rank"" and kwargs.get(""na_option"") in [""top"", ""bottom""]:
   1600     result_mask[:] = False
-> 1602 res_values = op._cython_op_ndim_compat(
   1603     self._data,
   1604     min_count=min_count,
   1605     ngroups=ngroups,
   1606     comp_ids=ids,
   1607     mask=mask,
   1608     result_mask=result_mask,
   1609     **kwargs,
   1610 )
   1612 if op.how == ""ohlc"":
   1613     arity = op._cython_arity.get(op.how, 1)

File ~/pandas-dev/pandas/core/groupby/ops.py:331, in WrappedCythonOp._cython_op_ndim_compat(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)
    329 if result_mask is not None:
    330     result_mask = result_mask[None, :]
--> 331 res = self._call_cython_op(
    332     values2d,
    333     min_count=min_count,
    334     ngroups=ngroups,
    335     comp_ids=comp_ids,
    336     mask=mask,
    337     result_mask=result_mask,
    338     **kwargs,
    339 )
    340 if res.shape[0] == 1:
    341     return res[0]

File ~/pandas-dev/pandas/core/groupby/ops.py:477, in WrappedCythonOp._call_cython_op(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)
    474     if self.how != ""rank"":
    475         # TODO: should rank take result_mask?
    476         kwargs[""result_mask""] = result_mask
--> 477     func(
    478         out=result,
    479         values=values,
    480         labels=comp_ids,
    481         ngroups=ngroups,
    482         is_datetimelike=is_datetimelike,
    483         mask=mask,
    484         **kwargs,
    485     )
    487 if self.kind == ""aggregate"" and self.how not in [""idxmin"", ""idxmax""]:
    488     # i.e. counts is defined.  Locations where count<min_count
    489     # need to have the result set to np.nan, which may require casting,
    490     # see GH#40767. For idxmin/idxmax is handled specially via post-processing
    491     if result.dtype.kind in ""iu"" and not is_datetimelike:
    492         # if the op keeps the int dtypes, we have to use 0

File groupby.pyx:2287, in pandas._libs.groupby.group_cummax()

File <stringsource>:663, in View.MemoryView.memoryview_cwrapper()

File <stringsource>:353, in View.MemoryView.memoryview.__cinit__()

ValueError: ndarray is not C-contiguous
```

### Issue Description

groupby-cummax raises for nullable integers in this case

Note that for pyarrow-backed integers it works fine

### Expected Behavior

```
2    6
1    5
0    5
Name: b, dtype: Int64
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : b552dc95c9fa50e9ca2a0c9f9cdb8757f794fedb
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+618.gb552dc95c9
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : 3.0.11
sphinx                : 8.1.3
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.2.0
html5lib              : 1.1
hypothesis            : 6.125.2
gcsfs                 : 2025.2.0
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.0
pyreadstat            : 1.2.8
pytest                : 8.3.4
python-calamine       : None
pytz                  : 2025.1
pyxlsb                : 1.0.10
s3fs                  : 2025.2.0
scipy                 : 1.15.1
sqlalchemy            : 2.0.37
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>

Spotted in [Narwhals](https://github.com/narwhals-dev/narwhals)","['Bug', 'Groupby', 'NA - MaskedArrays', 'Transformations']",2025-03-02 16:21:03,2025-11-20 19:49:00,3,closed
61030,BUG: Dependency check custom error loses information,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# in an environment where numpy fails to be imported because for example libstdc++.so.6 doesn't exist
import pandas
```

### Issue Description

I was importing pandas in an environment where `libstdc++.so.6` could not be found, which causes numpy to fail to load its C++ extensions.

The error I got was the following:
```
Traceback (most recent call last):
  File ""/home/nora/projects/rplcs-events-tournament-1/report_generator.py"", line 2, in <module>
    import pandas as pd
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/pandas/__init__.py"", line 19, in <module>
    raise ImportError(
ImportError: Unable to import required dependencies:
numpy: Error importing numpy: you should not try to import numpy from
        its source directory; please exit the numpy source tree, and relaunch
        your python interpreter from there.
```

This does not contain any information about the missing shared library. If I patch out this custom throw https://github.com/pandas-dev/pandas/blob/57fd50221ea3d5de63d909e168f10ad9fc0eee9b/pandas/__init__.py#L13 here and replace it with just `raise _e` I get the full error, which mentioned missing `libstdc++.so.6` which was very useful to debug the issue (hidden in a details tag as it's big):

<details><summary>The full error</summary>
<p>

```
Traceback (most recent call last):
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/numpy/_core/__init__.py"", line 23, in <module>
    from . import multiarray
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/numpy/_core/multiarray.py"", line 10, in <module>
    from . import overrides
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/numpy/_core/overrides.py"", line 7, in <module>
    from numpy._core._multiarray_umath import (
ImportError: libstdc++.so.6: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/numpy/__init__.py"", line 114, in <module>
    from numpy.__config__ import show_config
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/numpy/__config__.py"", line 4, in <module>
    from numpy._core._multiarray_umath import (
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/numpy/_core/__init__.py"", line 49, in <module>
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.12 from ""/home/nora/projects/rplcs-events-tournament-1/.venv/bin/python3""
  * The NumPy version is: ""2.2.3""

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: libstdc++.so.6: cannot open shared object file: No such file or directory


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/nora/projects/rplcs-events-tournament-1/report_generator.py"", line 2, in <module>
    import pandas as pd
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/pandas/__init__.py"", line 16, in <module>
    raise _e
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/pandas/__init__.py"", line 14, in <module>
    __import__(_dependency)
  File ""/home/nora/projects/rplcs-events-tournament-1/.venv/lib/python3.12/site-packages/numpy/__init__.py"", line 119, in <module>
    raise ImportError(msg) from e
ImportError: Error importing numpy: you should not try to import numpy from
        its source directory; please exit the numpy source tree, and relaunch
        your python interpreter from there.
```

</p>
</details> 

### Expected Behavior

Pandas printing the *full* error, including nested exceptions.

### Installed Versions

<details>

It can't run it because my environment is broken, but I can see the relevant code on master. Pandas version is 2.2.3

</details>
","['Bug', 'Error Reporting']",2025-03-02 10:07:09,2025-04-14 16:39:19,9,closed
61029,BUG: Printing float16 with NumPy>=2.0 gives overflow warning,"    print(pd.Series([1.0], dtype=""float16""))
    # RuntimeWarning: overflow encountered in cast np.array([1.0], dtype=""float16"") > 1e6
    # 0    1.0
    # dtype: float16

This occurs here:

https://github.com/pandas-dev/pandas/blob/bc34e2497f1471409f144fde89330a71bb8ba892/pandas/io/formats/format.py#L1446

Perhaps we just set `has_large_values` to be false for float16?","['Bug', 'Output-Formatting', 'Warnings']",2025-03-01 22:07:17,2025-03-07 23:43:08,3,closed
61025,BUG: Arugment nan_as_null is unused in __dataframe__ calls.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Code review of frame.py (line 904).
```

### Issue Description

The arugment nan_as_null is unused in __dataframe__ calls (Line 904 in [frame.py](https://github.com/pandas-dev/pandas/blob/main/pandas/core/frame.py)) and should be removed. A comment notes that ""nan_as_null is DEPRECATED and has no effect. Please avoid using it; it will be removed in a future release"". nan_as_null has now been removed but the argument is still present in the function call.

![Image](https://github.com/user-attachments/assets/7252920d-b4c8-4243-a6a6-d0d63f555737)

The issue was last touched on in [54846.](https://github.com/pandas-dev/pandas/pull/54846) but this was before nan_as_null was supposed to be removed.


### Expected Behavior

The argument can be removed if not in use for the sake of clean code.

### Installed Versions

<details>

v3.0.0.dev0-1976-g6e61cf44ee

</details>
","['Clean', 'Closing Candidate', 'Interchange']",2025-03-01 09:15:02,2025-11-08 18:15:07,3,closed
61019,BUG: df.plot() multi-column subplots & title interaction,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import matplotlib.pyplot as plt
import pandas as pd

df = pd.DataFrame([(30, 10, 10), (20, 20, 20), (10, 30, 30)], columns=list('ABC'))
df.plot(subplots= [('A','B')],kind=""bar"", stacked=True, title=[""A&B"",""C"", ""Needs another title despite no title here, removing this will error""])

print(df)
# print(pd.show_versions())
plt.show()
```

### Issue Description

df.plot() has a hard coded check to make sure there are the same number of titles when using with the ""subplots"" parameter, however subplots allows for multiple columns to be represented on one plot, so there are scenarios where the number of plots is less than the check for the number of titles

### Expected Behavior

You shouldn't need to purposefully make empty titles up to the number of columns, the check should be run on the number of subplots

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : AMD64 Family 25 Model 33 Stepping 2, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.24.2
pytz                  : 2025.1
dateutil              : 2.8.2
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 8.12.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.2
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.9
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Visualization']",2025-02-28 05:50:48,2025-05-07 16:11:20,3,closed
61018,"BUG: df.plot() ""Subplots"" changes behavior of how values are stacked using the ""Stacked"" property","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

df = pd.DataFrame([(30, 10, 10), (20, 20, 20), (10, 30, 30)], columns=list('ABC'))
df.plot(kind=""bar"", stacked=True)
df.plot(subplots= [('A','B')],kind=""bar"", stacked=True)
plt.show()
print(df)
```

### Issue Description

Using both the ""stacked"" and ""subplots"" option when drawing a bar graph changes how the bar graph is stacked.

Illustrated with image below where instead of numberically stacking the values for A and B it just physically overlays them. My guess is it doesn't properly use the ""bottom"" attribute when drawing B. 

![Image](https://github.com/user-attachments/assets/30d4655d-1a88-4321-be17-6ea88bbf2646)

### Expected Behavior

The behavior of the subplot version should be inline with when the option is not used. So the total of the values should equal to A+B for each element

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : AMD64 Family 25 Model 33 Stepping 2, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.24.2
pytz                  : 2025.1
dateutil              : 2.8.2
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 8.12.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.2
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.9
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Visualization']",2025-02-28 05:41:34,2025-04-28 20:10:29,8,closed
61016,BUG: Cannot cast float to int using map function,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df=pd.DataFrame({'VAR_NAME': {65: 'FIN4_0020', 66: 'FIN1_0021', 67: 'FIN3_0021', 68: 'FIN4_0021', 69: 'FIN1_0022', 70: 'FIN3_0022', 71: 'FIN4_0022', 72: 'FIN1_0023', 73: 'FIN3_0023', 74: 'FIN4_0023', 75: 'FIN1_0024'}, 'LYM1': {65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1}, 'LYM2': {65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1}, 'LYM3': {65: 1.0, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 73: 1.0, 74: 1.0, 75: 1.0}, 'LYM4': {65: 2, 66: 1, 67: 'T', 68: 2, 69: 1, 70: 'T', 71: 2, 72: 1, 73: 'T', 74: 2, 75: 1}})
```

### Issue Description

I can't explain the difference between the 2 ways of casting int using the map function. I just changed the order of using map function in code below and the result is different. The problem occurs when I read directly from the file as in the example below..

### Expected Behavior

```python
>>> df = pd.read_excel(excel_file, sheet_name=sheet_name)
>>> print(df[['VAR_NAME','LYM1','LYM2','LYM3','LYM4']].map(lambda x: int(x) if isinstance(x,float) else x,na_action='ignore').query('VAR_NAME==""FIN3_0022""'))
>>> print(df.query('VAR_NAME==""FIN3_0022""')[['VAR_NAME','LYM1','LYM2','LYM3','LYM4']].map(lambda x: int(x) if isinstance(x,float) else x,na_action='ignore'))

     VAR_NAME  LYM1 LYM2  LYM3 LYM4
70  FIN3_0022     1    1   1.0    T
     VAR_NAME  LYM1  LYM2  LYM3 LYM4
70  FIN3_0022     1     1     1    T


>>> df=pd.DataFrame({'VAR_NAME': {65: 'FIN4_0020', 66: 'FIN1_0021', 67: 'FIN3_0021', 68: 'FIN4_0021', 69: 'FIN1_0022', 70: 'FIN3_0022', 71: 'FIN4_0022', 72: 'FIN1_0023', 73: 'FIN3_0023', 74: 'FIN4_0023', 75: 'FIN1_0024'}, 'LYM1': {65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1}, 'LYM2': {65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1}, 'LYM3': {65: 1.0, 66: 1.0, 67: 1.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 1.0, 73: 1.0, 74: 1.0, 75: 1.0}, 'LYM4': {65: 2, 66: 1, 67: 'T', 68: 2, 69: 1, 70: 'T', 71: 2, 72: 1, 73: 'T', 74: 2, 75: 1}})
>>> df
     VAR_NAME  LYM1  LYM2  LYM3 LYM4
65  FIN4_0020     1     1   1.0    2
66  FIN1_0021     1     1   1.0    1
67  FIN3_0021     1     1   1.0    T
68  FIN4_0021     1     1   1.0    2
69  FIN1_0022     1     1   1.0    1
70  FIN3_0022     1     1   1.0    T
71  FIN4_0022     1     1   1.0    2
72  FIN1_0023     1     1   1.0    1
73  FIN3_0023     1     1   1.0    T
74  FIN4_0023     1     1   1.0    2
75  FIN1_0024     1     1   1.0    1
>>> print(df[['VAR_NAME','LYM1','LYM2','LYM3','LYM4']].map(lambda x: int(x) if isinstance(x,float) else x,na_action='ignore').query('VAR_NAME==""FIN3_0022""'))
     VAR_NAME  LYM1  LYM2  LYM3 LYM4
70  FIN3_0022     1     1     1    T
>>> print(df.query('VAR_NAME==""FIN3_0022""')[['VAR_NAME','LYM1','LYM2','LYM3','LYM4']].map(lambda x: int(x) if isinstance(x,float) else x,na_action='ignore'))
     VAR_NAME  LYM1  LYM2  LYM3 LYM4
70  FIN3_0022     1     1     1    T
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.11
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : 8.24.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.2.2
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : 2.0.38
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Dtype Conversions', 'Apply']",2025-02-27 10:54:58,2025-03-03 23:13:09,4,closed
61011,DOC: fix docstring typo,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/core/dtypes/dtypes.py

### Documentation problem

There seem to be two extra words in the _**is_dtype**_ method docstring, both in the _**PeriodDtype**_ and _**IntervalDtype**_ classes: 
```
@classmethod
    def is_dtype(cls, dtype: object) -> bool:
        """"""
        Return a boolean if we if the passed type is an actual dtype that we
        can match (via string or type)
        """"""
```

### Suggested fix for documentation

Remove the words **'if we'** in both places. ",['Docs'],2025-02-26 15:33:08,2025-02-26 19:15:14,1,closed
61010,PERF: bottleneck in `where()`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.randn(1, 1_000_000))

mask = df > 0.5
```
```python
%%timeit
_ = df.where(mask)
# 693 ms ± 3.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
perf result taken from pyinstrument:

![Image](https://github.com/user-attachments/assets/77f7afb1-b3a6-413a-ab8e-439730b644a7)

This issue seems to be related to this:
https://github.com/pandas-dev/pandas/blob/d1ec1a4c9b58a9ebff482af2b918094e39d87893/pandas/core/generic.py#L9735-L9737

When dataframe is large, this overhead of `is_bool_dtype` accumulates. Would it be better to use `cond.dtypes.unique()` instead?

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.14
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-122-generic
Version               : #132-Ubuntu SMP Thu Aug 29 13:45:52 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.0
Cython                : 3.0.7
sphinx                : 7.3.7
IPython               : 8.25.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : 8.2.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : 2.0.31
tables                : 3.9.2
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>


### Prior Performance

_No response_","['Performance', 'Needs Triage']",2025-02-26 09:47:27,2025-02-28 18:23:01,5,closed
61009,BUG: `.mul` on multi index columns doesnt work.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
data = pd.DataFrame(
    {
        ""state"": ([""vic"", ""nsw"", ""tas""] * 3 + ['vic'])*2,
        ""colour"": ['red'] * 10 + ['blue'] * 10,
        ""month"": ['mar', 'sep'] * 10,
        ""year"": [2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2022, 2023] * 2,
        ""value"": range(20),
    }
).set_index(['state','colour','year', 'month']).unstack(['state','year','month'])['value']
data.pipe(print)
""""""
state    vic  nsw  tas  vic  nsw  tas  vic  nsw  tas  vic
year    2020 2020 2020 2021 2021 2021 2022 2022 2022 2023
month    mar  sep  mar  sep  mar  sep  mar  sep  mar  sep
colour                                                   
blue      10   11   12   13   14   15   16   17   18   19
red        0    1    2    3    4    5    6    7    8    9
""""""

scaler = pd.DataFrame(
    [
        {""year"": 2020, ""month"": ""mar"", ""scale"": 0.5},
        {""year"": 2020, ""month"": ""sep"", ""scale"": 0.5},
        {""year"": 2021, ""month"": ""mar"", ""scale"": 0.5},
        {""year"": 2021, ""month"": ""sep"", ""scale"": 0.5},
        {""year"": 2022, ""month"": ""mar"", ""scale"": 0.5},
        {""year"": 2022, ""month"": ""sep"", ""scale"": 0.5},
        {""year"": 2023, ""month"": ""mar"", ""scale"": 0.5},
        {""year"": 2023, ""month"": ""sep"", ""scale"": 0.5},
    ]
).set_index(['year','month'])['scale']
scaler.pipe(print)
""""""
year  month
2020  mar      0.5
      sep      0.5
2021  mar      0.5
      sep      0.5
2022  mar      0.5
      sep      0.5
2023  mar      0.5
      sep      0.5
Name: scale, dtype: float64
""""""

mul_on_cols = data.mul(scaler, axis = 1)
mul_on_cols.pipe(print)
""""""
state   vic  tas  nsw       vic  tas  vic  tas  nsw  NaN  vic
year   2020 2020 2020 2021 2021 2021 2022 2022 2022 2023 2023
month   mar  mar  sep  mar  sep  sep  mar  mar  sep  mar  sep
colour                                                       
blue    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
red     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
""""""

mul_on_index = data.T.mul(scaler, axis = 0).T
mul_on_index.pipe(print)
""""""
state   vic  tas  nsw       vic  tas  vic  tas  nsw  NaN  vic
year   2020 2020 2020 2021 2021 2021 2022 2022 2022 2023 2023
month   mar  mar  sep  mar  sep  sep  mar  mar  sep  mar  sep
colour                                                       
blue    5.0  6.0  5.5  7.0  6.5  7.5  8.0  9.0  8.5  NaN  9.5
red     0.0  1.0  0.5  2.0  1.5  2.5  3.0  4.0  3.5  NaN  4.5
""""""
```

### Issue Description

using `.mul` on multi index columns fails, but on multi index rows, works as expected. View screenshots and code example. 

### Expected Behavior

Refer to screenshots and example

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.16
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.17.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : None
gcsfs                 : 2025.2.0
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : 0.61.0
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.15.2
sqlalchemy            : 2.0.38
tables                : None
tabulate              : 0.9.0
xarray                : 2025.1.2
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-02-26 01:07:36,2025-02-28 18:27:02,2,closed
61006,ENH: json_normalize should work with JSON,"### Feature Type

- [x] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish `pd.json_normalize` accepted JSON (as str or bytes), and not just `dict`.

Or, as a joke, there could be a `pd.dict_normalize` that only accepts JSON ;)

### Feature Description

Given a Series with JSON as str or bytes:
```
>>> df[""data""]
0                  {""value"":0.0}
1          {""value"":0.005787037}
2         {""value"":0.0115740741}
3         {""value"":0.0173611111}
```

It should be possible to parse the JSON with `pd.json_normalize`, e.g.

```
>>> pd.json_normalize(df[""data""])
            value
0        0.000000
1        0.005787
2        0.011574
3        0.017361
```

Pandas already has good JSON integration, so don't see why it can't be done.

### Alternative Solutions

From what I understand, right now it must be first parsed with some other library, e.g. with `apply`, before using `pd.json_normalize`.

```
>>> import json
>>> pd.json_normalize(df[""data""].apply(json.loads))
            value
0        0.000000
1        0.005787
2        0.011574
3        0.017361
```

### Additional Context

With better JSON/JSONB support in databases like postgres and sqlite, encountering this sort of data is becoming more common, and the intermediate `apply` step is a performance and usability issue:
```
>>> import json
>>> df = pd.read_sql(sql=query, con=conn)
>>> pd.json_normalize(df[""data""].apply(json.loads))
            value
0        0.000000
1        0.005787
2        0.011574
3        0.017361
```","['Enhancement', 'Needs Triage']",2025-02-25 16:09:43,2025-03-05 19:00:08,3,closed
61005,BUG: scatter and line matplotlib plots not compatible for sharex=True datetime plots,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
datetime_list = [datetime.datetime(year = 2025, month = 1, day = 1, hour = n) for n in range(23)]
y = [n for n in range(23)]

df = pd.DataFrame(columns = ['datetime','y'])
for i, n in enumerate(datetime_list):
    df.loc[len(df)] = [n,y[i]]

#Plotting with pandas - first subplot shows up as blank
fig, ax = plt.subplots(2, sharex=True)
df.plot.scatter(x = 'datetime', y = 'y', ax = ax[0])
df.plot(x = 'datetime', y = 'y', ax = ax[1])

#Plotting with matplotlib - this works
fig, ax = plt.subplots(2, sharex=True)
ax[0].scatter(df['datetime'],df['y'])
ax[1].plot(df['datetime'],df['y'])
```

### Issue Description

When I am trying to plot a line plot and scatter plot using df.plot and df.plot.scatter, the scatter plot shows up as blank when I select sharex=True when making the figure and axes.

```python
#Plotting with pandas - first subplot shows up as blank
fig, ax = plt.subplots(2, sharex=True)
df.plot.scatter(x = 'datetime', y = 'y', ax = ax[0])
df.plot(x = 'datetime', y = 'y', ax = ax[1])
```

![Image](https://github.com/user-attachments/assets/df1b5ee5-dcec-44dd-911e-bcfcee2500db)

However, if I plot using the standard matplotlib way of plotting (ax.scatter and ax.plot), both subplots show up correctly when I set sharex=True for the figure.

```python
#Plotting with matplotlib - this works
fig, ax = plt.subplots(2, sharex=True)
ax[0].scatter(df['datetime'],df['y'])
ax[1].plot(df['datetime'],df['y'])
```

![Image](https://github.com/user-attachments/assets/fb9a7ae4-2c51-4058-9a30-52919958cdbc)

### Expected Behavior

I would expect for the both subplots to show up, not blank. The behavior should be more similar to the traditional way of plotting with matplotlib.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.1
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Visualization']",2025-02-25 16:06:08,2025-04-09 16:28:52,6,closed
60994,BUG: `iloc` with `Series` as indexer fails for `__getitem__` but works with `__setitem__`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# __getitem__
>>> a = pd.Series([0, 1, 2])
>>> a.iloc[pd.Series([True, False, False])]
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[8], line 1
----> 1 a.iloc[pd.Series([True, False, False])]

File ~/.local/share/hatch/env/virtual/lontras/VBVTu9RT/lontras/lib/python3.11/site-packages/pandas/core/indexing.py:1191, in _LocationIndexer.__getitem__(self, key)
   1189 maybe_callable = com.apply_if_callable(key, self.obj)
   1190 maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable)
-> 1191 return self._getitem_axis(maybe_callable, axis=axis)

File ~/.local/share/hatch/env/virtual/lontras/VBVTu9RT/lontras/lib/python3.11/site-packages/pandas/core/indexing.py:1738, in _iLocIndexer._getitem_axis(self, key, axis)
   1735     key = np.asarray(key)
   1737 if com.is_bool_indexer(key):
-> 1738     self._validate_key(key, axis)
   1739     return self._getbool_axis(key, axis=axis)
   1741 # a list of integers

File ~/.local/share/hatch/env/virtual/lontras/VBVTu9RT/lontras/lib/python3.11/site-packages/pandas/core/indexing.py:1578, in _iLocIndexer._validate_key(self, key, axis)
   1576 if hasattr(key, ""index"") and isinstance(key.index, Index):
   1577     if key.index.inferred_type == ""integer"":
-> 1578         raise NotImplementedError(
   1579             ""iLocation based boolean ""
   1580             ""indexing on an integer type ""
   1581             ""is not available""
   1582         )
   1583     raise ValueError(
   1584         ""iLocation based boolean indexing cannot use ""
   1585         ""an indexable as a mask""
   1586     )
   1587 return

NotImplementedError: iLocation based boolean indexing on an integer type is not available

# __setitem__
>>> a.iloc[pd.Series([True, False, False])] = 10; a
0    10
1     1
2     2
dtype: int64
```

### Issue Description

Behavior of `loc` with a `Series` as argument shows inconsistent behavior for `__getitem__` and `__setitem__`

### Expected Behavior

Either both `__getitem__` and `__setitem__` should work or both should fail

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Linux
OS-release            : 6.12.10-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Sat, 18 Jan 2025 02:26:57 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : 8.1.3
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Indexing']",2025-02-23 16:29:36,2025-04-09 16:27:06,9,closed
60989,BUG: `Urgent` - All PR's are getting deployment errors in git pipeline,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
na
```

### Issue Description

FAILED pandas/tests/io/test_sql.py::test_xsqlite_execute_fail - pandas.errors.DatabaseError: Execution failed on sql 'INSERT INTO test VALUES(""foo"", ""bar"", 1.234)': no such column: ""foo"" - should this be a string literal in single-quotes?
FAILED pandas/tests/io/test_sql.py::test_xsqlite_execute_closed_connection - pandas.errors.DatabaseError: Execution failed on sql 'INSERT INTO test VALUES(""foo"", ""bar"", 1.234)': no such column: ""foo"" - should this be a string literal in single-quotes?
= 2 failed, 3760 passed, 372 skipped, 232245 deselected, 376 xfailed, 1 xpassed, 81 warnings in 1182.58s (0:19:42) =
/home/runner/micromamba/envs/test/lib/python3.12/site-packages/s3fs/core.py:569: DeprecationWarning: There is no current event loop
  loop = asyncio.get_event_loop()
Error: Process completed with exit code 1.
Run actions/upload-artifact@v4
With the provided path, there will be 1 file uploaded
Artifact name is valid!
Root directory input is valid!
Beginning upload of artifact content to blob storage
Uploaded bytes 39831
Finished uploading artifact content to blob storage!
SHA256 hash of uploaded artifact zip is f1126cc96347284a152f921e8e59bcadd2762e4d3872d66fe1d9881a9775948f
Finalizing artifact upload
Artifact Test results.zip successfully finalized. Artifact ID 2636217021
Artifact Test results has been successfully uploaded! Final size is 39831 bytes. Artifact ID is 2636217021
Artifact download URL: https://github.com/pandas-dev/pandas/actions/runs/13478588009/artifacts/2636217021
Run codecov/codecov-action@v4
eventName: pull_request
baseRef: pandas-dev:main | headRef: Anurag-Varma:bug#60723
==> Fork detected, tokenless uploading used
==> linux OS detected
https://cli.codecov.io/latest/linux/codecov.SHA256SUM
gpg: directory '/home/runner/.gnupg' created
gpg: keybox '/home/runner/.gnupg/pubring.kbx' created
gpg: /home/runner/.gnupg/trustdb.gpg: trustdb created
gpg: key 806BB28AED779869: public key ""Codecov Uploader (Codecov Uploader Verification Key) <security@codecov.io>"" imported
gpg: Total number processed: 1
gpg:               imported: 1
gpg: Signature made Fri Feb 21 21:18:38 2025 UTC
gpg:                using RSA key 27034E7FDB850E0BBC2C62FF806BB28AED779869
gpg: Good signature from ""Codecov Uploader (Codecov Uploader Verification Key) <security@codecov.io>"" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 2703 4E7F DB85 0E0B BC2C  62FF 806B B28A ED77 9869
==> Uploader SHASUM verified (ec031bdc38a2ca279ea1f56650b63aa6ce1365e7ee9da33b[413](https://github.com/pandas-dev/pandas/actions/runs/13477733861/job/37658956940#step:9:417)e6f175a94fe2b  codecov)
==> Running version latest
Could not pull latest version information: SyntaxError: Unexpected token '<', ""<!DOCTYPE ""... is not valid JSON
==> Running git config --global --add safe.directory /home/runner/work/pandas/pandas
/usr/bin/git config --global --add safe.directory /home/runner/work/pandas/pandas
==> Running command '/home/runner/work/_actions/codecov/codecov-action/v4/dist/codecov create-commit'
/home/runner/work/_actions/codecov/codecov-action/v4/dist/codecov create-commit --git-service github -B Anurag-Varma:bug#60723 -C 42c51ec8e4efc3c8a221625c92b22e2730b5fe7e
info - 2025-02-23 02:40:28,834 -- ci service found: github-actions
info - 2025-02-23 02:40:28,861 -- Creating a commit for an unprotected branch: Anurag-Varma:bug#60723
info - 2025-02-23 02:40:29,365 -- Process Commit creating complete
==> Running command '/home/runner/work/_actions/codecov/codecov-action/v4/dist/codecov create-report'
/home/runner/work/_actions/codecov/codecov-action/v4/dist/codecov create-report --git-service github -C 42c51ec8e4efc3c8a221625c92b22e2730b5fe7e
info - 2025-02-23 02:40:30,161 -- ci service found: github-actions
info - 2025-02-23 02:40:30,587 -- Process Report creating complete
info - 2025-02-23 02:40:30,588 -- Finished creating report successfully --- {""response"": ""{\""status\"":\""queued\""}\n""}
==> Running command '/home/runner/work/_actions/codecov/codecov-action/v4/dist/codecov do-upload'
/home/runner/work/_actions/codecov/codecov-action/v4/dist/codecov do-upload -F unittests --git-service github -n codecov-pandas -C 42c51ec8e4efc3c8a221625c92b22e2730b5fe7e
info - 2025-02-23 02:40:31,391 -- ci service found: github-actions
warning - 2025-02-23 02:40:31,[418](https://github.com/pandas-dev/pandas/actions/runs/13477733861/job/37658956940#step:9:422) -- xcrun is not installed or can't be found.
warning - 2025-02-23 02:40:31,450 -- No gcov data found.
warning - 2025-02-23 02:40:31,450 -- coverage.py is not installed or can't be found.
info - 2025-02-23 02:40:31,498 -- Found 1 coverage files to report
info - 2025-02-23 02:40:31,498 -- > /home/runner/work/pandas/pandas/coverage.xml
info - 2025-02-23 02:40:32,005 -- Your upload is now processing. When finished, results will be available at: https://app.codecov.io/github/pandas-dev/pandas/commit/42c51ec8e4efc3c8a221625c92b22e2730b5fe7e
info - 2025-02-23 02:40:32,331 -- Process Upload complete

### Expected Behavior

na

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2025-02-23 03:31:35,2025-02-24 21:39:28,8,closed
60988,BUG: The Series `.map()` function frequently fails when using dictionaries with tuple keys in various cases.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.



### Issue Description

The Series `.map()` function frequently fails when using dictionaries with tuple keys which is given as parameter to the map function. See the below examples:

Ex 1:
```Python
import pandas as pd

df = pd.DataFrame({""a"": [(1,1), (2,2), (3,4), (5,6)]})  
label_mappings = {(1,): ""A"", (2,2): ""B"", (3,4): ""A"", (5,6): ""B""}  
df[""mapped_labels""] = df[""a""].map(label_mappings)  

print(df)
# Output:
#        a        mapped_labels
# 0  (1, 1)             A
# 1  (2, 2)             B
# 2  (3, 4)             A
# 3  (5, 6)             B

# Expected Ouput:
#        a        mapped_labels
# 0  (1, 1)           NaN
# 1  (2, 2)             B
# 2  (3, 4)             A
# 3  (5, 6)             B
```



Ex 2:
```Python
import pandas as pd

df = pd.DataFrame({""a"": [(1,1), (2,2), (3,4), (5,6)]})  
label_mappings = {(2,): ""A"", (2,2): ""B"", (3,4): ""A"", (5,6): ""B""}  
df[""mapped_labels""] = df[""a""].map(label_mappings)  

print(df)
# Output:
# Produces error: pandas.errors.InvalidIndexError: Reindexing only valid with uniquely valued Index objects

# Expected Ouput:
#        a        mapped_labels
# 0  (1, 1)           NaN
# 1  (2, 2)             B
# 2  (3, 4)             A
# 3  (5, 6)             B
```


EX 3:
```Python
import pandas as pd

df = pd.DataFrame({""a"": [(1,), (2,2), (3,4), (5,6)]})  
label_mappings = {(1,None): ""A"", (2,2): ""B"", (3,4): ""A"", (5,6): ""B""}  
df[""mapped_labels""] = df[""a""].map(label_mappings)  

print(df)

# Output:
# Produces error: AssertionError: Length of new_levels (2) must be <= self.nlevels (1)

# Expected Ouput:
#        a        mapped_labels
# 0  (1, )               NaN
# 1  (2, 2)             B
# 2  (3, 4)             A
# 3  (5, 6)             B
```



EX 4:
```Python
import pandas as pd

df = pd.DataFrame({""a"": [(1,), (2,2), (3,4), (5,6)]})  
label_mappings = {(1,1): ""A"", (2,2): ""B"", (3,4): ""A"", (5,6): ""B""}   
df[""mapped_labels""] = df[""a""].map(label_mappings)  

print(df)

# Output:
# Produces error: AssertionError: Length of new_levels (2) must be <= self.nlevels (1)

# Expected Ouput:
#        a        mapped_labels
# 0  (1, )               NaN
# 1  (2, 2)             B
# 2  (3, 4)             A
# 3  (5, 6)             B
```


EX 5:
```Python
import pandas as pd

df = pd.DataFrame({""a"": [(1,1), (1,2), (3,4), (5,6)]})  
label_mappings = {(1,): ""A"", (2,2): ""B"", (3,4): ""A"", (5,6): ""B""}   
df[""mapped_labels""] = df[""a""].map(label_mappings)  

print(df)

# Output:
#        a        mapped_labels
# 0  (1, 1)             A
# 1  (1, 2)             A
# 2  (3, 4)             A
# 3  (5, 6)             B

# Expected Ouput:
#        a        mapped_labels
# 0  (1, 1)             NaN
# 1  (1, 2)             NaN
# 2  (3, 4)             A
# 3  (5, 6)             B
```

### Expected Behavior

Gave the actual outputs and expected outputs.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 6bcd30397d67c3887288c7a82c2c235ce8bc3c7f
python                : 3.10.14
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26120
machine               : AMD64
processor             : AMD64 Family 23 Model 96 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 3.0.0.dev0+1944.g6bcd30397d.dirty
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : 3.0.10
sphinx                : 7.3.7
IPython               : 8.18.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : 2024.6.1
html5lib              : 1.1
hypothesis            : 6.105.0
gcsfs                 : 2024.6.1
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : 0.61.0
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : None
pymysql               : 1.4.6
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pytz                  : 2025.1
pyxlsb                : 1.0.10
s3fs                  : 2024.6.1
scipy                 : 1.15.2
sqlalchemy            : 2.0.31
tables                : None
tabulate              : 0.9.0
xarray                : 2024.6.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO Data']",2025-02-22 22:56:33,2025-03-07 23:51:47,2,closed
60980,BUG: invalid result of reindex on columns after unstack with Period data,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

series1 = pd.DataFrame(
   [(0, ""s2"", pd.Period(2022)), (0, ""s1"", pd.Period(2021))],
   columns=[""A"", ""B"", ""C""]
).set_index([""A"", ""B""])[""C""]

series2 = series1.astype(str)

print(series1.unstack(""B"").reindex([""s2""], axis=1))
print(series2.unstack(""B"").reindex([""s2""], axis=1))
```

### Issue Description

The example code prints
```
B  s2
A      
0  2021

B  s2
A  
0  2022
```

The result with `pd.Period` data is the incorrect 2021, but with `str` data it's the correct 2022.

This only occurs with certain index values. When replacing ""s1"" with ""s3"", the effect disappears and the result is 2022 in both cases.



### Expected Behavior

Expect the result for both `pd.Period` and `str` data to be 2022:
```
B  s2
A  
0  2022

B  s2
A  
0  2022
```

(actually observed with older Pandas 2.0.3)


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Linux
OS-release            : 6.2.16
Version               : #1-NixOS SMP PREEMPT_DYNAMIC Tue Jan  1 00:00:00 UTC 1980
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.3
numpy                 : 2.2.3
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Reshaping']",2025-02-21 18:07:08,2025-03-17 16:55:32,8,closed
60978,BUG: Unexpected datetime dtype change on assignments,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

ts = pd.Timestamp.today()
df = pd.DataFrame({""A"": [ts]})
print(df.dtypes)

# B now has datetime64[us] dtype
df[""B""] = ts
print(df.dtypes)

# A remains datetime64[ns] dtype
df.loc[:, ""A""] = pd.Timestamp.today()
print(df.dtypes)

# A now changes to datetime64[us] dtype
df[""A""] = ts
print(df.dtypes)

# B remains datetime64[us] dtype
df.loc[:, ""B""] = ts
print(df.dtypes)
```

### Issue Description

Dataframe (and series) creation with a pd.Timestamp results in a dtype of `datetime64[ns]`.
However when I assign the same timestamp to a new column the dtype changes to `datetime64[us]`.
Assigning a different way maintains the original dtype.

### Expected Behavior

I would expect both assignments to maintain the same dtype of  `datetime64[ns]`.

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252
pandas                : 2.2.3
numpy                 : 1.26.0
pytz                  : 2023.3.post1
dateutil              : 2.8.2
pip                   : 22.3
Cython                : None
sphinx                : None
IPython               : 8.16.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
blosc                 : None
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.8.0
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 11.0.0
pyreadstat            : None
pytest                : 7.4.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.3
sqlalchemy            : 2.0.0
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None","['Bug', 'Needs Triage']",2025-02-21 12:52:59,2025-02-21 18:02:40,1,closed
60973,DOC: No warning in set_index() that previous index column is removed.,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html

### Documentation problem

set_index(), when applied to a DataFrame which already has a data column (non-default) assigned as index, will delete this data column from the DataFrame when assigning another data column to be the index.

While I find this behaviour inappropriate, I understand that reset_index() should be used before set_index(), in which case the original index column may be preserved.

The problem is that the documentation for set_index() does not mention this at all, so the user is left to discover the problem and then the way to avoid it.

### Suggested fix for documentation

Add a comment in the set_index documentation to clarify that setting a data column as index, when there is already a different data column serving as index, will delete that data column, unless reset_index is performed first.","['Docs', 'Indexing', 'Needs Discussion']",2025-02-20 10:41:16,2025-03-05 00:39:09,4,closed
60972,ENH: Support for Python 3.13.2 free-threading version,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

Firstly, use `python3.13t -m pip install pandas` to build and install `pandas` on VS 16.11.43 and MSVC 19.29.30158.  
Then try to import it:

```python
E:\Git-repositories\project179>python3.13t
Python 3.13.2 experimental free-threading build (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:33:40) [MSC v.1942 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
<Segmentation fault caused there>
```

### Issue Description

The line `import pandas` failed in Python 3.13 free-threading version on x64 Windows.  
As I tried to execute it, a segmentation fault caused because I've seen `werfault.exe` appeared in the task manager.  

### Expected Behavior

The line `import pandas` successfully be executed.  

### Installed Versions

<details>
pandas 2.2.3
(Note: cannot call pd.show_versions() as I couldn't import pandas at the first step)
</details>

Additionally, this is the output during compiling pandas that contains the MSVC version:
```
 + meson setup C:\Users\admin\AppData\Local\Temp\pip-install-yk54bvta\pandas_93cffcf8ab364c6d8ccc39766c5b148a C:\Users\admin\AppData\Local\Temp\pip-install-yk54bvta\pandas_93cffcf8ab364c6d8ccc39766c5b148a\.mesonpy-6btzqpgz\build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=C:\Users\admin\AppData\Local\Temp\pip-install-yk54bvta\pandas_93cffcf8ab364c6d8ccc39766c5b148a\.mesonpy-6btzqpgz\build\meson-python-native-file.ini
      The Meson build system
      Version: 1.2.1
      Source dir: C:\Users\admin\AppData\Local\Temp\pip-install-yk54bvta\pandas_93cffcf8ab364c6d8ccc39766c5b148a
      Build dir: C:\Users\admin\AppData\Local\Temp\pip-install-yk54bvta\pandas_93cffcf8ab364c6d8ccc39766c5b148a\.mesonpy-6btzqpgz\build
      Build type: native build
      Project name: pandas
      Project version: 2.2.3
      Activating VS 16.11.43
      C compiler for the host machine: cl (msvc 19.29.30158 ""用于 x64 的 Microsoft (R) C/C++ 优化编译器 19.29.30158 版"")
      C linker for the host machine: link link 14.29.30158.0
      C++ compiler for the host machine: cl (msvc 19.29.30158 ""用于 x64 的 Microsoft (R) C/C++ 优化编译器 19.29.30158 版"")
      C++ linker for the host machine: link link 14.29.30158.0
      Cython compiler for the host machine: cython (cython 3.0.12)
      Host machine cpu family: x86_64
      Host machine cpu: x86_64
      Program python found: YES (D:\Python\Python313\python3.13t.exe)
      Run-time dependency python found: YES 3.13
      Build targets in project: 53

      pandas 2.2.3

        User defined options
          Native files: C:\Users\admin\AppData\Local\Temp\pip-install-yk54bvta\pandas_93cffcf8ab364c6d8ccc39766c5b148a\.mesonpy-6btzqpgz\build\meson-python-native-file.ini
          buildtype   : release
          vsenv       : True
          b_ndebug    : if-release
          b_vscrt     : md

      Found ninja.EXE-1.11.1.git.kitware.jobserver-1 at C:\Users\admin\AppData\Local\Temp\pip-build-env-3jrexmoi\normal\Scripts\ninja.EXE

      Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
      C:\Users\admin\AppData\Local\Temp\pip-build-env-3jrexmoi\overlay\Scripts\meson compile -C .
      + meson compile
      [1/151] Generating pandas/_libs/algos_common_helper_pxi with a custom command
      [2/151] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command
      [3/151] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command
      [4/151] Generating pandas/_libs/algos_take_helper_pxi with a custom command
      [5/151] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command
      [6/151] Generating pandas/_libs/intervaltree_helper_pxi with a custom command
      [7/151] Generating pandas/_libs/index_class_helper_pxi with a custom command
      [8/151] Generating pandas/_libs/sparse_op_helper_pxi with a custom command
      [9/151] Generating pandas/__init__.py with a custom command
      [10/151] Compiling C object pandas/_libs/tslibs/parsing.cp313t-win_amd64.pyd.p/.._src_parser_tokenizer.c.obj
      ..\..\pandas\_libs\include\pandas/vendored/klib/khash.h(729): warning C4090: “函数”: 不同的“const”限定符
      [11/151] Compiling Cython source C:/Users/admin/AppData/Local/Temp/pip-install-yk54bvta/pandas_93cffcf8ab364c6d8ccc39766c5b148a/pandas/_libs/tslibs/base.pyx
      [12/151] Compiling C object pandas/_libs/tslibs/base.cp313t-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj
```
","['Enhancement', 'Build']",2025-02-20 08:44:21,2025-06-04 18:16:56,13,closed
60966,ENH: Add `pd.read_soql()` for Salesforce data retrieval,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could use Pandas to query Salesforce using SOQL (Salesforce Object Query Language) just as you do with relational databases using `pd.read_sql()`. Currently, to retrieve Salesforce data into a Pandas DataFrame, users must execute queries using simple_salesforce and manually convert results to DataFrames, which in case of multi-level queries can become quite inefficient (due to the nested format of the outputs of Salesforce REST API).

### Feature Description

The function would parallel `pd.read_sql()` and would look like this:

```python
def read_soql(
    query: str,  # Equivalent to `query` in `pd.read_sql()`, representing the SOQL query string.
    con,  # Expects a `simple_salesforce.Salesforce` object instead of an SQLAlchemy connection.
    index_col: str | list[str] | None = None,  # Same as in `pd.read_sql()`
    parse_dates=None,  # Same as in `pd.read_sql()`
    dtype: DtypeArg | None = None,  # Same as in `pd.read_sql()`
    dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default  # Same as in `pd.read_sql()`
) -> DataFrame:

    # validate connection is Salesforce object
    # validate dtype_backend is valid option

    # execute SOQL query and get all records
    # flatten output and remove metadata
    # convert records to DataFrame

    # if dtype specified: convert columns to specified types
    # if parse_dates specified: convert date columns
    # if index_col specified: set DataFrame index

    # if dtype_backend != 'numpy': convert to nullable types
    # return DataFrame
```


### Alternative Solutions

**Alternative naming**
Since `pd.read_sql()` is a convenience wrapper around `read_sql_table` and `read_sql_query`, a more ""formally"" correct name might be `read_soql_query`, as there is no corresponding `read_soql_table`. This would maintain a closer parallel to Pandas' SQL functions.  
However, I propose `read_soql` for brevity, and for consistency with other I/O functions such as `pd.read_excel()`, `pd.read_parquet()`, `pd.read_feather()`, `pd.read_orc()` etc.

### Additional Context

I am interested in developing this feature as I have already done some work towards its implementation.","['Enhancement', 'IO Data', 'Closing Candidate', 'IO Format Request']",2025-02-19 21:21:52,2025-08-14 02:14:00,6,closed
60962,BUG: `dtype` silently ignored in `MultiIndex` constructor,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
pd.MultiIndex([[1,2,3], [4,5,6]], [[0,1,2], [0,1,2]], dtype='float64')
```

### Issue Description

This outputs
```
MultiIndex([(1, 4),
            (2, 5),
            (3, 6)],
           )
```
`dtype` is ignored

### Expected Behavior

I think one of the following:
- `dtype` is respected
- the `dtype` argument is removed
- the `dtype` argument is kept but raises if it's not `None`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.5
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.3
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : None
gcsfs                 : 2025.2.0
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2025-02-19 10:45:42,2025-02-20 17:34:40,0,closed
60961,ENH: Integrate the pyspark in pandas,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could use Pandas to handle large datasets efficiently without running into memory issues. Pandas is great for data analysis but struggles with large datasets that don't fit in memory. This feature would allow seamless integration between Pandas and PySpark, enabling users to process large datasets using Spark’s distributed computing while still leveraging the familiar Pandas syntax.

### Feature Description

Seamlessly integrate Pandas with PySpark by automatically converting large Pandas DataFrames into Spark DataFrames while preserving Pandas-like syntax for efficient distributed computing. 🚀

### Alternative Solutions

import pyspark.pandas as ps
psdf = ps.DataFrame({'id': range(1000000), 'value': range(1000000)})
import dask.dataframe as dd
ddf = dd.read_csv(""large_dataset.csv"")
import modin.pandas as mpd
df = mpd.read_csv(""large_file.csv"")
import vaex
df = vaex.open(""large_file.csv"")


### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-02-19 10:27:14,2025-02-19 10:43:29,2,closed
60959,BUG: DataFrame construction fails with `pa.json_` Arrow extension type,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
$ import pandas as pd
$ import pyarrow as pa
$ arr = ['{""bar"": True, ""foo"": 10}']
$ pd.DataFrame({'json_col': arr}, pd.ArrowDtype(pa.json_(pa.string())))

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[7], line 5
      2 import pyarrow as pa
      4 arr = ['{""bar"": True, ""foo"": 10}']
----> 5 pd.DataFrame({'json_col': arr}, pd.ArrowDtype(pa.json_(pa.string())))

File ~/src/bigframes/venv/lib/python3.12/site-packages/pandas/core/frame.py:778, in DataFrame.__init__(self, data, index, columns, dtype, copy)
    772     mgr = self._init_mgr(
    773         data, axes={""index"": index, ""columns"": columns}, dtype=dtype, copy=copy
    774     )
    776 elif isinstance(data, dict):
    777     # GH#38939 de facto copy defaults to False only in non-dict cases
--> 778     mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
    779 elif isinstance(data, ma.MaskedArray):
    780     from numpy.ma import mrecords

File ~/src/bigframes/venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:503, in dict_to_mgr(data, index, columns, dtype, typ, copy)
    499     else:
    500         # dtype check to exclude e.g. range objects, scalars
    501         arrays = [x.copy() if hasattr(x, ""dtype"") else x for x in arrays]
--> 503 return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)

File ~/src/bigframes/venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:116, in arrays_to_mgr(arrays, columns, index, dtype, verify_integrity, typ, consolidate)
    114     index = _extract_index(arrays)
    115 else:
--> 116     index = ensure_index(index)
    118 # don't force copy because getting jammed in an ndarray anyway
    119 arrays, refs = _homogenize(arrays, index, dtype)

File ~/src/bigframes/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:7649, in ensure_index(index_like, copy)
   7647         return Index(index_like, copy=copy, tupleize_cols=False)
   7648 else:
-> 7649     return Index(index_like, copy=copy)

File ~/src/bigframes/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:532, in Index.__new__(cls, data, dtype, copy, name, tupleize_cols)
    528     return cls(np.asarray(data), dtype=dtype, copy=copy, name=name)
    529 elif not is_list_like(data) and not isinstance(data, memoryview):
    530     # 2022-11-16 the memoryview check is only necessary on some CI
    531     #  builds, not clear why
--> 532     raise cls._raise_scalar_data_error(data)
    534 else:
    535     if tupleize_cols:
    536         # GH21470: convert iterable to list before determining if empty

File ~/src/bigframes/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:5289, in Index._raise_scalar_data_error(cls, data)
   5284 @final
   5285 @classmethod
   5286 def _raise_scalar_data_error(cls, data):
   5287     # We return the TypeError so that we can raise it from the constructor
   5288     #  in order to keep mypy happy
-> 5289     raise TypeError(
   5290         f""{cls.__name__}(...) must be called with a collection of some ""
   5291         f""kind, {repr(data) if not isinstance(data, np.generic) else str(data)} ""
   5292         ""was passed""
   5293     )

TypeError: Index(...) must be called with a collection of some kind, extension<arrow.json>[pyarrow] was passed
```

### Issue Description

Apache Arrow v19.0 introduced the `pa.json_` extension type ([doc](https://arrow.apache.org/docs/format/CanonicalExtensions.html#json)). Currently, attempting to create a pandas DataFrame using this data type results in an error. The detailed error call stack can be observed in the reproducible example above. 

However, creating a pandas Series with pa.json_ works correctly. Here is an example that shows that Series construction works.

```Python
$ import pandas as pd
$ import pyarrow as pa
$ arr = ['{""bar"": True, ""foo"": 10}']
$ pd.Series(arr, dtype=pd.ArrowDtype(pa.json_(pa.string())))
0    {""bar"": True, ""foo"": 10}
dtype: extension<arrow.json>[pyarrow]
```

### Expected Behavior

Pandas should support DataFrame construction with the `pa.json_` Arrow extension type, consistent with its support for Series and Index objects.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.1
python-bits           : 64
OS                    : Linux
OS-release            : 6.10.11-1rodete2-amd64
Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.10.11-1rodete2 (2024-10-16)
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 23.2.1
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : None
gcsfs                 : 2025.2.0
jinja2                : None
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : 0.27.0
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : 2.0.38
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-02-18 21:08:43,2025-02-18 21:50:40,2,closed
60958,ENH: Support `pa.json_` in arrow extension type,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
$ import pandas as pd
$ import pyarrow as pa
$ pd.api.types.pandas_dtype(pd.ArrowDtype(pa.json_(pa.string()))).type

---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[2], line 4
      1 import pandas as pd
      2 import pyarrow as pa
----> 4 pd.api.types.pandas_dtype(pd.ArrowDtype(pa.json_(pa.string()))).type

File ~/src/bigframes/venv/lib/python3.12/site-packages/pandas/core/dtypes/dtypes.py:2169, in ArrowDtype.type(self)
   2167 elif isinstance(pa_type, pa.ExtensionType):
   2168     return type(self)(pa_type.storage_type).type
-> 2169 raise NotImplementedError(pa_type)

NotImplementedError: extension<arrow.json>
```

### Issue Description

Apache Arrow v19.0 introduced the `pa.json_` extension type ([doc](https://arrow.apache.org/docs/format/CanonicalExtensions.html#json)). Currently, pandas.ArrowDtype.type does not correctly handle this new type.

The `ArrowDtype.type` method is crucial for various pandas dtype APIs, including `pd.api.types.pandas_dtype()` and `pd.api.types.is_timedelta64_dtype()`. When used with `pd.ArrowDtype(pa.json_(pa.string()))`, these APIs produce unexpected results.

The issue is that the pandas `ArrowDtype.type` method should return the underlying storage type of the arrow json type.



### Expected Behavior

`pa.json_` is a standard Arrow extension type. `ArrowDtype.type` should accurately return its storage type, mirroring the behavior of other Arrow extension types.
Specifically, `pd.ArrowDtype(pa.json_(pa.string())).type` should reflect the storage type, which is `pa.string()` as shown below.

Codes to show arrow storage type for `pa.json_`:
```python
$ import pyarrow as pa
$ pa.json_(pa.string()).storage_type
DataType(string)
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.1
python-bits           : 64
OS                    : Linux
OS-release            : 6.10.11-1rodete2-amd64
Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.10.11-1rodete2 (2024-10-16)
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 23.2.1
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : None
gcsfs                 : 2025.2.0
jinja2                : None
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : 0.27.0
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : 2.0.38
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Enhancement', 'Arrow']",2025-02-18 20:47:40,2025-03-12 23:51:31,5,closed
60956,BUG: df.to_sql with append setting doesn't respect previous table deletions within the same transaction,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from sqlalchemy import create_engine

engine = create_engine("""")

with engine.begin() as connection:
    # Delete the table if it exists
    connection.execute(text(f""DROP TABLE IF EXISTS schema.table""))
    df = pd.DataFrame([{""test1"": 5}])
    # Write the DataFrame to the database
    df.to_sql(name=""table"", con=connection, if_exists=""append"", schema=""schema"", index=False, chunksize=5000)
```

### Issue Description

using sqlalchemy 2.0.38
using pandas 2.2.3

In the above example, i get the following error:

(psycopg2.errors.UndefinedColumn) column ""test1"" of relation ""jobs_PDA"" does not exist
LINE 1: INSERT INTO workflow_stage.""jobs_PDA"" (test1) VALUES (5)

### Expected Behavior

Usually, df.to_sql with if_exists='append' will create a new table if one doesn't already exist.  Since I am deleting the table in my transaction before i run df.to_sql, i expect the transaction to be commited with scheam.table being deleted, then recreated with column 'test1' int, and then populated with a single row {test1:5}.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.5
python-bits           : 64
OS                    : Darwin
OS-release            : 24.2.0
Version               : Darwin Kernel Version 24.2.0: Fri Dec  6 18:56:34 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.12.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.38
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-02-18 19:09:05,2025-02-18 22:57:14,1,closed
60954,BUG: Segmentation Fault when changing a column name in a DataFrame,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import re
import uuid
import numpy as np
import pandas as pd

## Generate example DataFrame
t = pd.date_range(start='2023-01-01 00:00', periods=10, freq='10min')
x = np.random.randn(t.size)
y = np.random.randn(t.size)
df = pd.DataFrame({
    'Timestamp': t,
    'X position (m)': x,
    'Y position (m)': y,
    'Temperature (degC)': temp,
})
df = pd.concat([
    pd.DataFrame(
        [dict(
            zip(list(df.columns),
            ['SignalId'] + [str(uuid.uuid4()) for i in range(df.columns.size - 1)]
        ))]
    ),
    df], ignore_index=True
)
df = df.set_index('Timestamp')

## Change column name inplace
for i, c in enumerate(list(df.columns)):
    newc = re.sub(r'\s+position\s+', ' ', c)
    df.columns.values[i] = newc

## Printing DataFrame to screen may generate a segmentation fault
df
```

### Issue Description

When a column name from a DataFrame is changed inplace (at the values), sometimes it leads to a *segmentation fault*. This seems more likely if the DataFrame contains mixed element types (as per example below).

Hypotheses are:
- The change in the name leads to corruption of the data in memory.
- NumPy version >2 leads to different data types that may conflict somehow with some operations.

Example:

```python
>>> import re
>>> import uuid
>>> import numpy as np
>>> import pandas as pd
>>> 
>>> t = pd.date_range(start='2023-01-01 00:00', periods=10, freq='10min')
>>> x = np.random.randn(t.size)
>>> y = np.random.randn(t.size)
>>> temp = np.random.randn(t.size)
>>> df = pd.DataFrame({
...     'Timestamp': t,
...     'X position (m)': x,
...     'Y position (m)': y,
...     'Temperature (degC)': temp,
... })
>>> df = pd.concat([
...     pd.DataFrame(
...         [dict(
...             zip(list(df.columns),
...             ['SignalId'] + [str(uuid.uuid4()) for i in range(df.columns.size - 1)]
...         ))]
...     ),
...     df], ignore_index=True
... )
>>> df = df.set_index('Timestamp')
>>> 
>>> df
                                           X position (m)                        Y position (m)                    Temperature (degC)
Timestamp                                                                                                                            
SignalId             da8a0a1b-a022-48cc-9e17-91b4b103cc5b  e92dad78-6128-45d5-8545-b45e80345da9  3106111b-0f53-4122-a89f-e1f78aac72b9
2023-01-01 00:00:00                               1.66612                              0.503874                             -0.202982
2023-01-01 00:10:00                             -1.266542                              0.141686                              0.488124
2023-01-01 00:20:00                              -0.46789                             -0.132084                             -1.011771
2023-01-01 00:30:00                              1.276952                             -0.811061                             -1.735414
2023-01-01 00:40:00                              1.178987                             -0.245169                              1.295712
2023-01-01 00:50:00                             -1.503673                               0.60517                             -0.946938
2023-01-01 01:00:00                             -1.095622                             -0.920928                             -0.233186
2023-01-01 01:10:00                             -1.276511                              0.710022                               1.94653
2023-01-01 01:20:00                             -0.470105                             -0.643144                              1.380882
2023-01-01 01:30:00                              1.426826                             -0.286228                              1.351435
>>> for i, c in enumerate(list(df.columns)):
...     newc = re.sub(r'\s+position\s+', ' ', c)
...     df.columns.values[i] = newc
... 
>>> df
Segmentation fault (core dumped)
```

### Expected Behavior

Though the operation may be debatable (the change inplace of the column name via `df.column.values[i] = new_name`), it is a valid operation without any other warning or error message. The ensuing segmentation fault is completely random (so very hard to diagnose).

Hence the expected behaviour is to either block these operations, or alternatively to fully allow those if these are to be permitted.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.11
python-bits           : 64
OS                    : Linux
OS-release            : 4.19.0-27-amd64
Version               : #1 SMP Debian 4.19.316-1 (2024-06-25)
machine               : x86_64
processor             :
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : 8.18.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : 2024.7.0
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
</details>
","['Docs', 'Index']",2025-02-18 10:07:42,2025-03-11 16:49:36,8,closed
60947,BUILD: Trouble installing pandas on Windows,"### Installation check

- [x] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Gerät PRN konnte nicht initialisiert werden.

### Installation Method

pip install

### pandas Version

2.2.3

### Python Version

3.11.9

### Installation Logs

Hello everyone,
I am currently trying to install asreview from Uni Utrecht, but there accurs a problem which seems to be related to pandas installation.
As I tried installing pandas the same error occurs. See below:
The installation is to be executed right from Windows command line.
<details>

pip install pandas
Defaulting to user installation because normal site-packages is not writeable
Collecting pandas
  Using cached pandas-2.2.3.tar.gz (4.4 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [12 lines of output]
      + meson setup C:\*********\AppData\Local\Temp\pip-install-zbkfzy9s\pandas_2ad6fccfe0c843c78cc606c094498a41 C:\*******\AppData\Local\Temp\pip-install-zbkfzy9s\pandas_2ad6fccfe0c843c78cc606c094498a41\.mesonpy-yu7welhd\build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=C:\********\AppData\Local\Temp\pip-install-zbkfzy9s\pandas_2ad6fccfe0c843c78cc606c094498a41\.mesonpy-yu7welhd\build\meson-python-native-file.ini
      The Meson build system
      Version: 1.2.1
      Source dir: C:\*********\AppData\Local\Temp\pip-install-zbkfzy9s\pandas_2ad6fccfe0c843c78cc606c094498a41
      Build dir: C:\*********\AppData\Local\Temp\pip-install-zbkfzy9s\pandas_2ad6fccfe0c843c78cc606c094498a41\.mesonpy-yu7welhd\build
      Build type: native build
      Project name: pandas
      Project version: 2.2.3

      ..\..\meson.build:2:0: ERROR: Could not parse vswhere.exe output

      A full log can be found at C:\********\AppData\Local\Temp\pip-install-zbkfzy9s\pandas_2ad6fccfe0c843c78cc606c094498a41\.mesonpy-yu7welhd\build\meson-logs\meson-log.txt
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

</details>

I hope I gathered all infromation you need and to help me.
","['Build', 'Needs Info']",2025-02-17 07:16:59,2025-08-05 17:06:18,9,closed
60942,"BUG: pd.Series.rename(..., inplace=True) returns a pd.Series and not nont","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [1]: import pandas as pd

In [2]: pd.Series([1, 2, 3])
Out[2]:
0    1
1    2
2    3
dtype: int64

In [3]: pd.Series([1, 2, 3]).rename('A')
Out[3]:
0    1
1    2
2    3
Name: A, dtype: int64

In [4]: pd.Series([1, 2, 3]).rename('A', inplace=True). # should return None
Out[4]:
0    1
1    2
2    3
Name: A, dtype: int64
```

### Issue Description

According to the [documation](https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.rename.html), inplace operations should return None and only modify the object in place without doing a deepcopy.
However when running the rename operation on `pd.Series` with `inplace=True` the type of the returns is `pd.Series` and not None.
This seems to originate from quick look up in the `_set_name` operations that return and object no matter the value of `inplace` (cf https://github.com/pandas-dev/pandas/blob/6bcd30397d67c3887288c7a82c2c235ce8bc3c7f/pandas/core/series.py#L1835-L1850).
And then the `rename` operation only returns the result of `_set_name` without distinguishing the `inplace` value.

### Expected Behavior

The docs suggest that the return of the `rename(..., inplace=True)` should be None.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.1
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:06 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.29.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Docs', 'good first issue', 'rename', 'inplace']",2025-02-16 15:42:49,2025-02-22 01:12:31,3,closed
60937,BUG: datetime64 can't be compared with pyarrow-backed variant,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df = pd.DataFrame({'a': pd.to_datetime(pd.Series(['2020-01-01']))})
df['b'] = df['a'].convert_dtypes(dtype_backend='pyarrow')
print(df['b'] < df['a'])
```

### Issue Description

Should the above be supported? it currently raises

### Expected Behavior

i think
```
0    False
dtype: bool[pyarrow]
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.3
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : 6.125.2
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'Numeric Operations']",2025-02-15 16:19:01,2025-09-24 00:50:02,6,closed
60933,BUG:  Converting string of type lxml.etree._ElementUnicodeResult to a datetime using pandas.to_datetime results in a TypeError.,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from lxml import etree
import pandas as pd


example_date = ""2025-02-05 16:59:57""
default_format = ""%Y-%m-%d %H:%M:%S""

xml_node = etree.XML(f""<date>{example_date}</date>"")
example_date_from_xml = xml_node.xpath(""/date/node()"")[0]

assert isinstance(example_date, str)
assert isinstance(example_date_from_xml, str)
assert isinstance(example_date_from_xml, etree._ElementUnicodeResult)
assert not isinstance(example_date, etree._ElementUnicodeResult)
assert example_date_from_xml == example_date

pd.to_datetime(pd.Series([example_date]))  # OK
pd.to_datetime(pd.Series([example_date_from_xml]))  # OK
pd.to_datetime(pd.Series([example_date_from_xml]), format=default_format)  # KO: TypeError: Expected unicode, got lxml.etree._ElementUnicodeResult

# Shorter way of doing this
pd.to_datetime(pd.Series([etree._ElementUnicodeResult(example_date)]))  # OK
pd.to_datetime(pd.Series([etree._ElementUnicodeResult(example_date)]), format=default_format)  # KO
```

### Issue Description

Hello,

When trying to convert a string that comes from an XML file parsing with `pandas.to_datetime`, I struggled with an unexpected `TypeError`.

I managed to write a reproducible example that both works on the latest 2.2.3 version and `3.0.0dev` with Python 3.12.8.

It looks like when I'm trying to convert datetimes in a Series initialized from a list of `lxml.etree._ElementUnicodeResult` with the argument `format`, an error is raised.

Also, using `Series.astype(str)` does not work (values are still `lxml.etre._ElementUnicodeResult`).

### Expected Behavior

No `TypeError`.

### Installed Versions

3.0.0dev

<details>
INSTALLED VERSIONS
------------------
commit                : 19ea997815d4dadf490d7052a0a3c289be898588
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.146.1-microsoft-standard-WSL2
Version               : #1 SMP Thu Jan 11 04:09:03 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 3.0.0.dev0+1943.g19ea997815
numpy                 : 2.3.0.dev0+git20250211.bbfb823
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : 5.3.1
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
</details>

2.2.3
<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.146.1-microsoft-standard-WSL2
Version               : #1 SMP Thu Jan 11 04:09:03 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : 8.1.3
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.1
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : 2025.1.2
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None
</details>","['Bug', 'Datetime']",2025-02-14 16:44:17,2025-10-07 16:40:29,5,closed
60928,ENH: Control resampling at halfyear with origin,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
s1 = pd.Series(1, pd.date_range('2025', freq='D', periods=700)).resample('2QS-JAN').sum()
s2 = pd.Series(1, pd.date_range('2025-04', freq='D', periods=700)).resample('2QS-JAN').sum()

# s1 expectedly has timestamps in january and july
# s1
# 2025-01-01    181
# 2025-07-01    184
# 2026-01-01    181
# 2026-07-01    154
# Freq: 2QS-JAN, dtype: int64    # NB frequency

# but s2 unexpectedly has timestamps in april and october
# s2
# 2025-04-01    183
# 2025-10-01    182
# 2026-04-01    183
# 2026-10-01    152
# Freq: 2QS-JAN, dtype: int64    # NB frequency

s1.index.freq == s2.index.freq   # True
```

### Issue Description

It seems there is no way to force where the period boundaries are when resampling at the 2-Quarter frequency. Resampling at `2QS-APR` gives the same results for `s1` and `s2` as those shown above.

### Expected Behavior

I'd expect the index of `s2` to also have timestamps on the first of January and July.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 6.9.3-76060903-generic
Version               : #202405300957~1738770968~22.04~d5f7c84 SMP PREEMPT_DYNAMIC Wed F
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : 7.3.7
IPython               : 8.29.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Enhancement', 'Frequency', 'Resample']",2025-02-13 22:52:27,2025-03-03 18:21:17,8,closed
60927,BUG: Unable to round-trip nested arrow extension types with `pa.Table.to_pandas`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pyarrow as pa
import cudf
import pandas as pd

In [19]: data = [{""text"": ""hello"", ""list_col"": np.asarray([1, 2], dtype=""uint32"")}]

In [20]: df = cudf.DataFrame(data)

In [21]: arrow_types_pdf = df.to_pandas(arrow_type=True)

In [22]: arrow_types_pdf
Out[22]: 
    text list_col
0  hello    [1 2]

In [23]: arrow_types_pdf.dtypes
Out[23]: 
text                    string[pyarrow]
list_col    list<item: uint32>[pyarrow]
dtype: object

In [24]: pa_table = pa.Table.from_pandas(arrow_types_pdf)

In [25]: pa_table
Out[25]: 
pyarrow.Table
text: string
list_col: list<item: uint32>
  child 0, item: uint32
----
text: [[""hello""]]
list_col: [[[1,2]]]

In [26]: pa_table.to_pandas()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[26], line 1
----> 1 pa_table.to_pandas()

File /datasets/pgali/envs/cudfdev/lib/python3.12/site-packages/pyarrow/array.pxi:887, in pyarrow.lib._PandasConvertible.to_pandas()

File /datasets/pgali/envs/cudfdev/lib/python3.12/site-packages/pyarrow/table.pxi:5132, in pyarrow.lib.Table._to_pandas()

File /datasets/pgali/envs/cudfdev/lib/python3.12/site-packages/pyarrow/pandas_compat.py:783, in table_to_dataframe(options, table, categories, ignore_metadata, types_mapper)
    780     table = _add_any_metadata(table, pandas_metadata)
    781     table, index = _reconstruct_index(table, index_descriptors,
    782                                       all_columns, types_mapper)
--> 783     ext_columns_dtypes = _get_extension_dtypes(
    784         table, all_columns, types_mapper)
    785 else:
    786     index = _pandas_api.pd.RangeIndex(table.num_rows)

File /datasets/pgali/envs/cudfdev/lib/python3.12/site-packages/pyarrow/pandas_compat.py:862, in _get_extension_dtypes(table, columns_metadata, types_mapper)
    857 dtype = col_meta['numpy_type']
    859 if dtype not in _pandas_supported_numpy_types:
    860     # pandas_dtype is expensive, so avoid doing this for types
    861     # that are certainly numpy dtypes
--> 862     pandas_dtype = _pandas_api.pandas_dtype(dtype)
    863     if isinstance(pandas_dtype, _pandas_api.extension_dtype):
    864         if hasattr(pandas_dtype, ""__from_arrow__""):

File /datasets/pgali/envs/cudfdev/lib/python3.12/site-packages/pyarrow/pandas-shim.pxi:148, in pyarrow.lib._PandasAPIShim.pandas_dtype()

File /datasets/pgali/envs/cudfdev/lib/python3.12/site-packages/pyarrow/pandas-shim.pxi:151, in pyarrow.lib._PandasAPIShim.pandas_dtype()

File /datasets/pgali/envs/cudfdev/lib/python3.12/site-packages/pandas/core/dtypes/common.py:1645, in pandas_dtype(dtype)
   1640     with warnings.catch_warnings():
   1641         # GH#51523 - Series.astype(np.integer) doesn't show
   1642         # numpy deprecation warning of np.integer
   1643         # Hence enabling DeprecationWarning
   1644         warnings.simplefilter(""always"", DeprecationWarning)
-> 1645         npdtype = np.dtype(dtype)
   1646 except SyntaxError as err:
   1647     # np.dtype uses `eval` which can raise SyntaxError
   1648     raise TypeError(f""data type '{dtype}' not understood"") from err

TypeError: data type 'list<item: uint32>[pyarrow]' not understood

In [27]: pa_table.to_pandas(ignore_metadata=True)
Out[27]: 
    text list_col
0  hello   [1, 2]
```

### Issue Description

It looks like `pa.Table.to_pandas` is not yet compatible with arrow extension types that pandas supports and the conversion back fails in `pandas_dtype` API.

### Expected Behavior

Round-trip the dataframe without errors.

### Installed Versions

<details>

In [28]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.4.0-182-generic
Version               : #202-Ubuntu SMP Fri Apr 26 12:29:36 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : 3.0.12
sphinx                : 8.1.3
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : 6.125.1
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : None
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : 7.4.4
python-calamine       : None
pyxlsb                : None
s3fs                  : 2025.2.0
scipy                 : 1.15.1
sqlalchemy            : 2.0.38
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Closing Candidate', 'Arrow']",2025-02-13 21:24:29,2025-02-13 22:24:15,1,closed
60923,BUG: `series.reindex(mi)` behaves different for series with Index and MultiIndex,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

* Create a series with `Index` and a MultiIndex to use for reindexing later
```python
>>> series = pd.Series(
...   [26.7300, 24.2550],
...   index=pd.Index([81, 82], name='a')
... )
>>> series
a
81    26.730
82    24.255
dtype: float64
>>> series.index
Index([81, 82], dtype='int64', name='a')
>>> other_index = pd.MultiIndex(
...   levels=[
...     pd.Index([81, 82], name='a'),
...     pd.Index([np.nan], name='b'),
...     pd.Index([
...       '2018-06-01', '2018-07-01'
...     ], name='c')
...   ],
...   codes=[
...     [0, 0, 1, 1],
...     [0, 0, 0, 0],
...     [0, 1, 0, 1]
...   ],
...   names=['a', 'b', 'c']
... )
>>> other_index
MultiIndex([(81, nan, '2018-06-01'),
            (81, nan, '2018-07-01'),
            (82, nan, '2018-06-01'),
            (82, nan, '2018-07-01')],
           names=['a', 'b', 'c'])
```

* `reindex` to `MultiIndex` (`other_index`) which expands `series.index` by two more levels.
* unfortunately the `reindex` sets all values of the original series to NaN which can be fixed by turning `series.index` into a 1-level `MultiIndex` first

```python

>>> series.reindex(other_index) # this removes all values of the series
a   b    c         
81  NaN  2018-06-01   NaN
         2018-07-01   NaN
82  NaN  2018-06-01   NaN
         2018-07-01   NaN
dtype: float64
```

* apply `to_mi(...)` to turn the `series.index` into a 1-level `MultiIndex`
* rerun `reindex` on the new `series` with `MultiIndex` and the values are maintained/filled as expected

```python
>>> def to_mi(series):
...   if isinstance(series.index, pd.MultiIndex):
...     series_mi = series.index
...   else:
...     level_names = [series.index.name]
...     level_values = [series.index]
...     series_mi = pd.MultiIndex.from_arrays(level_values, names=level_names)
...   series_with_mi = pd.Series(series.values, index=series_mi, name=series.name)
...   return series_with_mi
... 
>>> series_mi = to_mi(series)
>>> series_mi
a 
81    26.730
82    24.255
dtype: float64
>>> series_mi.index
MultiIndex([(81,),
            (82,)],
           names=['a'])
>>> series_mi.reindex(other_index)
a   b    c         
81  NaN  2018-06-01    26.730
         2018-07-01    26.730
82  NaN  2018-06-01    24.255
         2018-07-01    24.255
dtype: float64

```

### Issue Description

In the above case, `series.reindex(multi_index)` will turn the series values to NaN when the series has a single `Index`. However when the series index is converted to a 1-level `MultiIndex` prior to the `reindex`, the values are maintained and filled as expected.

In my opinion it shouldn't matter if a 1-level `MultiIndex` or an `Index` is used for a `reindex` - the outcomes should be the same.

As a further discussion point (here or elsewhere), this issue (and others) also begs the question why a distinction between `Index` and `MultiIndex` is necessary (I suspect there are historic reasons). I would imagine that many issues (and code) would go away if `MultiIndex` was used exclusively (even for 1-dimensional indices).

### Expected Behavior

The missing levels in `series_mi` (compared to `other_index`) are added and the values of the partial index from the original series are used to fill the places of the added indices.

```
>>> series_mi.reindex(other_index)
a   b    c         
81  NaN  2018-06-01    26.730 # from index <81> of `series` (`series_mi`)
         2018-07-01    26.730 # from index <81> of `series` (`series_mi`)
82  NaN  2018-06-01    24.255 # from index <82> of `series` (`series_mi`)
         2018-07-01    24.255 # from index <82> of `series` (`series_mi`)
dtype: float64
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 3979e954a339db9fc5e99b72ccb5ceda081c33e5
python                : 3.11.11
python-bits           : 64
OS                    : Linux
OS-release            : 6.12.11-200.fc41.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Fri Jan 24 04:59:58 UTC 2025
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_AU.UTF-8
LOCALE                : en_AU.UTF-8

pandas                : 3.0.0.dev0+1909.g3979e954a3.dirty
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : 8.1.3
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2025.2.0
html5lib              : 1.1
hypothesis            : 6.125.2
gcsfs                 : 2025.2.0
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : 0.61.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 19.0.0
pyreadstat            : 1.2.8
pytest                : 8.3.4
python-calamine       : None
pytz                  : 2025.1
pyxlsb                : 1.0.10
s3fs                  : 2025.2.0
scipy                 : 1.15.1
sqlalchemy            : 2.0.38
tables                : 3.10.2
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.2
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'MultiIndex', 'Index']",2025-02-13 01:25:54,2025-08-07 15:39:03,7,closed
60922,BUG: `pd.concat` with a slightly nontrivial join on a `DatetimeIndex` with anything other than `ns` resolution gives arbitrarily wrong results,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

idx = pd.date_range(""2025-01-29 01:36"",periods=4,freq=""1 min"",unit=""us"")
ab = pd.DataFrame(index=idx,data=dict(a=[1,2,3,4],b=[2,2,2,2]))
cd = pd.DataFrame(index=idx[:3],data=dict(c=[9,8,7],d=[6,6,6]))

abcd = pd.concat([ab,cd],axis=""columns"")
print(abcd)
assert abcd.shape[0] == 4
```

### Issue Description

The above example attempts to concatenate a 4-row `DataFrame` with a 3-row `DataFrame` by joining on the index;
the first three index values match exactly. The expected result is a 4-row `DataFrame` as follows, that you can obtain with `unit=""ns""`:
```
                     a  b    c    d
2025-01-29 01:36:00  1  2  9.0  6.0
2025-01-29 01:37:00  2  2  8.0  6.0
2025-01-29 01:38:00  3  2  7.0  6.0
2025-01-29 01:39:00  4  2  NaN  NaN
```
Instead, with `unit=""us""` the result is a 2-row `DataFrame`, where the first row is correct, and the second row is completely made up from outer space:
```
                       a    b    c    d
2025-01-29 01:36:00  1.0  2.0  9.0  6.0
2025-01-29 18:16:00  NaN  NaN  NaN  NaN
```
With `unit=""s""`, the result is similarly wrong, but with a different made-up row index:
```
                       a    b    c    d
2025-01-29 01:36:00  1.0  2.0  9.0  6.0
3926-05-28 12:16:00  NaN  NaN  NaN  NaN
```

### Expected Behavior

The correct result is returned.

### Installed Versions

I noticed the issue with the following versions: (latest python 3.13.2, latest pandas 2.2.3):
<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.2
python-bits           : 64
OS                    : Darwin
OS-release            : 24.3.0
Version               : Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : None.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : 0.61.0
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : 2025.1.2
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>

I also reproduced on python 3.11 with pandas 2.2.0","['Bug', 'Reshaping', 'Non-Nano']",2025-02-12 20:58:24,2025-02-12 21:52:10,2,closed
60921,DOC: Little contrast in documentation accordion cards,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/getting_started/index.html

### Documentation problem

Originally I reported upstream to the pydata-sphinx-theme issue tracker: https://github.com/pydata/pydata-sphinx-theme/issues/2110 

On the pandas Getting started page, https://pandas.pydata.org/docs/getting_started/index.html, I saw following when using dark theme in Firefox 134.0 (64-bit) on Ubuntu:

![Image](https://github.com/user-attachments/assets/945903f7-55f9-4fbb-8698-09a09ad73dab)

I could not really read the text of the spoiler names under Intro to pandas as they are virtually black on black, so I am forced to switch to light mode instead. (Looking at the code what I am looking at seems to be called tutorial accordion cards, but I might be wrong).

The theme developers in the upstream issue noted that the component used seems to be a custom one built by the Pandas team, so reporting this here.

**BUT** as I was in the process of reporting it here the checkbox at the top of the form had asked me to check whether the issue still persists in the _dev_ version of the documentation. Funny thing is that it doesn't. But it became worse. Now instead of the spoiler title being unreadable the spoiler body isn't:


![Image](https://github.com/user-attachments/assets/7ba563bf-40ba-43d8-98a2-c006c618017d)

### Suggested fix for documentation

The component in question or its application should be fixed so that text is readable in both dark and light themes.",['Docs'],2025-02-12 17:23:20,2025-02-12 21:44:20,1,closed
60919,ENH: Make merge_asof preserve the index,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import datetime as dt

print(pd.__version__)  # 2.2.3

left = pd.DataFrame({
    'left_date': pd.date_range(dt.date(2025, 1, 1), dt.date(2025, 1, 5)),
})

left = left.drop([0, 1])

right = pd.DataFrame({
    'right_date': pd.date_range(dt.date(2025, 1, 1), dt.date(2025, 1, 5)),
    'value': range(5)
})

merged = pd.merge_asof(left, right, left_on='left_date', right_on='right_date')

index_before = left.index.tolist()
index_after = merged.index.tolist()
print(index_before)  # [2, 3, 4] since first two dropped
print(index_after)  # should be [2, 3, 4] but is [0, 1, 2]
```

### Issue Description

merge_asof is has the unexpected side effect of resetting a RangeIndex. If there are gaps in the RangeIndex in the left dataframe ahead of the call, there will be no gaps in the dataframe resulting from the call. 

### Expected Behavior

By default, I would expect the index to be untouched, or at least expect an ignore_index keyword argument to be able to control this behavior. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.9
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 25 Model 97 Stepping 2, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Enhancement', 'Reshaping', 'Needs Discussion', 'Closing Candidate']",2025-02-12 13:48:41,2025-08-05 16:53:53,4,closed
60915,DOC: Conbench PoC benchmark is down,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

In the current [benchmark documentation](https://pandas.pydata.org/community/benchmarks.html) we have the following links for viewing the benchmarks:

- Original server: [asv](https://asv-runner.github.io/asv-collection/pandas/)
- OVH server: [asv](https://pandas.pydata.org/benchmarks/asv/) (benchmarks results can also be visualized in this [Conbench PoC](http://57.128.112.95:5000/)

This Cornbench PoC points to http://57.128.112.95:5000/ and it is currently down.

I've found an old link for a project of the same name but the repo have been deleted: https://github.com/DeaMariaLeon/Conbench-PoC

### Documentation problem

A broken link in the documentation

### Suggested fix for documentation

Either find the maintainer (I think it is [Dea Maria](https://github.com/DeaMariaLeon/)) and check how to set it up again or if the address has changed or just remove this link","['Docs', 'Benchmark']",2025-02-11 22:17:08,2025-04-13 17:58:41,7,closed
60914,BUG: Unexpected rounding behaviour,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
print (pd.DataFrame([[-0.025]]).round(2).values)
print (pd.DataFrame([[-0.025001]]).round(2).values)
print (round(-0.025, 2))
print (round(-0.025001, 2))

#output:
#[[-0.02]]
#[[-0.03]]
#-0.03
#-0.03
```

### Issue Description

I would expect all values to be -0.03 but pandas rounds -0.025 to -0.02.  I suspect this is due to [bankers rounding](https://stackoverflow.com/questions/10825926/python-3-x-rounding-behavior), which would be fine, but it also looks like pandas doesn't mimic the native python rounding behaviour, which rounds both to -0.03 - this might trip people up.   

### Expected Behavior

Round -0.025 to -0.03, or if this explicitly avoided as part of your rounding strategy, at least make your strategy match the python round function.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.15.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 75.1.0
pip                   : 22.3.1
Cython                : None
pytest                : 8.3.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.27.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Docs', 'Numeric Operations']",2025-02-11 20:35:37,2025-02-17 21:25:01,5,closed
60913,Roadmap page has numbered-only list when should be a mix of numbered and unnumbered,"[about / roadmap page in the website](https://pandas.pydata.org/about/roadmap.html) shows a numbered list of 17 items and this doesn't seem to be the expected result by looking at the code:

https://github.com/pandas-dev/pandas/blob/05de25381f71657bd425d2c4045d81a46b2d3740/web/pandas/about/roadmap.md?plain=1#L108-L145",['Docs'],2025-02-11 19:58:06,2025-07-22 16:27:35,1,closed
60911,Date format different in the same page,"https://github.com/pandas-dev/pandas/blob/02de8140251096386cbefab0186d45af0c3d8ebd/web/pandas/index.html#L124

This date format line is different from other date lines in the same page. While this is %Y-%m-%d, others are ""%b %d, %Y"".",[],2025-02-11 16:31:51,2025-02-11 17:08:51,0,closed
60910,pandas.pydata.org is down,After loading for a while I get a cloudflare timeout error.,"['Docs', 'Admin', 'Web']",2025-02-11 14:52:28,2025-02-11 17:50:56,2,closed
60909,BUG: pandas.read_excel returns dict type if sheet_name=None,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.read_excel(""test_data/test.xlsx"", sheet_name=None)
print(type(df))
```

### Issue Description

Function `pandas.read_excel` when parameter `sheet_name` is set to `None` return dict object apart from `pandas.core.frame.DataFrame`.

### Expected Behavior

It should return DF

### Installed Versions

pandas==2.2.3
","['Docs', 'IO Excel']",2025-02-11 09:28:31,2025-02-13 17:41:27,3,closed
60905,DOC: Should `pandas.api.types.is_dtype_equal()` be documented?,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

None:  It's not there!

### Documentation problem

`pandas.api.types.is_dtype_equal()` is not documented, but it is used in `tests/extension/decimal/array.py` which is used as an example `ExtensionArray` implementation

### Suggested fix for documentation

Unsure if we want this in the public API or not, but if so, we ought to fix it.

See https://github.com/pandas-dev/pandas-stubs/pull/1112 for some discussion","['Docs', 'Dtype Conversions']",2025-02-10 16:57:57,2025-05-03 20:00:54,6,closed
60903,BUG: Adding DataFrames with misaligned MultiIndex produces NaN despite fill_value=0,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Create two DataFrames with MultiIndex
index1 = pd.MultiIndex.from_tuples([('A', 'one'), ('A', 'two')])
index2 = pd.MultiIndex.from_tuples([('B', 'one'), ('B', 'two')])

df1 = pd.DataFrame([[1, 2]], columns=index1)
df2 = pd.DataFrame([[3, 4]], columns=index2)

# Adding DataFrames with different MultiIndex
result = df1.add(df2, fill_value=0)
print(result)
```

### Issue Description

I have two data frames with unaligned multi-indices. When adding the two data frames, with fill_value=0, I'd expect the missing values to be replaced with zero before performing the addition operation, as described in the documentation of `DataFrame.add`. However, the above example produces this output:

```
    A       B    
  one two one two
0 NaN NaN NaN NaN
```

The problem affects the current main branch (tested with commit https://github.com/pandas-dev/pandas/commit/e557039fda7d4325184cf76520892b3a635ec2dd). No released version is affected (yet). The behavior was introduced by https://github.com/pandas-dev/pandas/pull/60538. Before this PR, the code was producing the expected output.

### Expected Behavior

I'd expect this output:

```
A B
one two one two
0 1.0 2.0 3.0 4.0
```


### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 72fd708761f1598f1a8ce9b693529b81fd8ca252
python                : 3.10.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-130-generic
Version               : #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1839.g72fd708761
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : 8.1.3
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2024.10.0
html5lib              : 1.1
hypothesis            : 6.122.4
gcsfs                 : 2024.10.0
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 18.1.0
pyreadstat            : 1.2.8
pytest                : 8.3.4
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : 1.0.10
s3fs                  : 2024.10.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Indexing', 'Numeric Operations', 'MultiIndex']",2025-02-10 09:23:10,2025-02-10 18:23:28,3,closed
60901,ENH: Add an iterdicts() function,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could iterate through dictionaries of each row's contents in a DataFrame, the same way I could do so with namedtuples through itertuples(). Performing the `namedtuple`-to-`dict` conversion process isn't difficult normally, but in some situation (e.g. doing so in a list/set/dict comprehension), it's more convenient to have a generator that does the process automatically for you.

### Feature Description

Add a new function to DataFrames, `iterdicts`, that takes an `index` argument (equivalent to the same argument in `itertuples`) and returns each row as a dictionary the same way itertuples does so as a namedtuple. (No need to take a ""name"" argument, since that's irrelevant for dictionaries.)
```python
def iterdicts(self, index=True):
    for x in self.itertuples(index):
        yield x._asdict()
```

### Alternative Solutions

Write a custom function that does the same thing externally.
```python
def iterdicts(df, index=True):
    for x in df.itertuples(index):
        yield x._asdict()
```

EDIT (4/4/25): Alternatively, use `map` and an anonymous function to create the generator.

```python
map(lambda x: x._asdict(), df.itertuples())
```

### Additional Context

_No response_","['Enhancement', 'Needs Triage', 'Closing Candidate']",2025-02-09 18:09:41,2025-08-05 16:22:26,3,closed
60897,BUG: Memory leak when creating a df inside a loop,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import tracemalloc
import numpy as np
import time
import gc

# Start memory tracking
tracemalloc.start()

iteration = 0

Row_Number = 20000

while iteration < 1000:
    
    test_lst = [*range(12)]
    
    for i in range(12):
        
        # Create a DataFrame with X amount of rows
        df = pd.DataFrame({
            ""A"": np.arange(Row_Number),  # Sequential Row_Numbers from 0 to 999999
            ""B"": np.random.rand(Row_Number),  # Random floats between 0 and 1
            ""C"": np.random.randint(0, 100, size=Row_Number),  # Random integers between 0 and 99
            ""D"": np.random.choice([""apple"", ""banana"", ""cherry""], size=Row_Number),  # Random categories
            ""E"": np.random.randn(Row_Number)  # Normally distributed random Row_Numbers
        })

        test_lst[i] = df # The bug also appears without appending to list

        del df # Deleting df at the end of loop doesnt affect memory leak
  
    del test_lst # Deleting list at the end of loop doesnt affect memory leak
        
    time.sleep(0.01)
    
    iteration += 1

    # Check memory usage for 3rd party packages
    if iteration % 1 == 0:
    
        snapshot = tracemalloc.take_snapshot()
        
        # Get memory statistics **without filtering** first
        top_stats = snapshot.statistics(""lineno"")
        
        print(f""\n[ Memory Snapshot at iteration {iteration} ]"")
        for stat in top_stats[:5]:  # Show top memory-consuming locations
            print(stat)
```

### Issue Description

By using tracemalloc (a tool to track memory usage in loops), I can see that pandas doesnt release memory when creating dfs inside a loop. The problem seems to come from pandas\core\internals\blocks around line 228. Would be nice if anyone could find a fix to this.

### Expected Behavior

That the memory doesnt leak

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.1
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : Norwegian Bokmål_Norway.1252

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : 8.1.3
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 19.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : 2.4.2
pyqt5                 : None

</details>
","['Bug', 'Performance', 'Windows', 'Constructors', 'Closing Candidate']",2025-02-09 12:21:56,2025-08-05 16:52:48,17,closed
60886,DOC: should `DateOffset` always be used instead of `BaseOffset`?,"There's currently 4 pages in the docs which mention BaseOffset:
- https://pandas.pydata.org/docs/reference/api/pandas.Period.asfreq.html
- https://pandas.pydata.org/docs/reference/api/pandas.Period.now.html
- https://pandas.pydata.org/docs/reference/api/pandas.tseries.frequencies.to_offset.html

However, `BaseOffset` isn't user-facing

Should they just be replaced with `DateOffset`?

This is also relevant for `pandas-stubs` which uses `BaseOffset` in a few places

@jbrockmendel is this ok to do?

Based on 

https://github.com/pandas-dev/pandas/blob/0c4ca3a9e4baa9b4fa8cbc81c57f2e2996636c10/pandas/_libs/tslibs/offsets.pyx#L1620-L1632

they should be interchangeable?","['Docs', 'Typing']",2025-02-08 14:27:28,2025-03-24 16:41:46,2,closed
60885,BUG: pandas to_datetime() returns incorrect isoweek conversion in week 53 when only 52 weeks exist,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.to_datetime(""2024-53-1"", format=""%G-%V-%u"")
```

### Issue Description

When using python [format codes](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes) to resolve an isoweek notation (_""%G-%V-%u""_) back to an iso date, to_datetime() incorrectly works with non-existing weeks. In the example below the third call should not return a valid date, as no week 53 exists in the isoclander year 2024 (the first lines are non-isoclandar, the last line is the first week in isocalendar 2025, which are all correct).

![Image](https://github.com/user-attachments/assets/8a50f2d0-3b41-42ce-96f4-edcc50bda818)

This behavior can be seen in others years with 52 isoweeks as well, e.g. pd.to_datetime(""2023-53-1"", format=""%G-%V-%u"") also returns the date of the first isoweek of 2024.

The python standard library correctly raises an error when the same thing is tried:

![Image](https://github.com/user-attachments/assets/f26a9c3e-370d-420c-9db8-96d75cf5d573)


### Expected Behavior

Raise an error for misformatted date string.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_Germany.1252

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.2
Cython                : None
sphinx                : 8.0.2
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'datetime.date']",2025-02-08 14:09:01,2025-02-12 18:20:28,1,closed
60881,BUG: Unhandled Rust panic when processing sheets with missing formatting data,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
1. Make an excel document using Microsoft Excel. Rename the document's extension to .zip from .xlsx and extract the data as a zip file. The file contents will be extracted in a new directory. Make a modification to the xl/styles.xml file to remove the name attribute from any instance of the cellStyles tag, so it looks something like this:

<cellStyles count=""1""><cellStyle xfId=""0"" builtinId=""0"" /></cellStyles></styleSheet>

2. Select all the files within the directory and zip them back up, renaming the output zip file to .xlsx

3. Attempt to load the file as an ExcelFile object in pandas using the following code:

try:
	e = pd.ExcelFile($YOUR_FILE_NAME, engine=""openpyxl"")
except Exception as exc: 
	print(exc)
	try:
		e = pd.ExcelFile($YOUR_FILE_NAME, engine=""openpyxl"")
	except Exception as ex:
		print(ex)

You should get something like:

-traceback
Traceback (most recent call last):
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/platform_flow_common/tasks/flows/preliminary/excel.py"", line 367, in create_data_frame_sheet_file_object
    excel = pd.ExcelFile(path_or_buffer=excel_file, engine=pandas_engine)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/pandas/io/excel/_base.py"", line 1567, in __init__
    self._reader = self._engines[engine](
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py"", line 553, in __init__
    super().__init__(
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/pandas/io/excel/_base.py"", line 573, in __init__
    self.book = self.load_workbook(self.handles.handle, engine_kwargs)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py"", line 572, in load_workbook
    return load_workbook(
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/reader/excel.py"", line 346, in load_workbook
    reader.read()
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/reader/excel.py"", line 299, in read
    apply_stylesheet(self.archive, self.wb)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py"", line 198, in apply_stylesheet
    stylesheet = Stylesheet.from_tree(node)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py"", line 103, in from_tree
    return super(Stylesheet, cls).from_tree(node)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/descriptors/serialisable.py"", line 87, in from_tree
    obj = desc.expected_type.from_tree(el)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/descriptors/serialisable.py"", line 87, in from_tree
    obj = desc.expected_type.from_tree(el)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/descriptors/serialisable.py"", line 103, in from_tree
    return cls(**attrib)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/styles/named_styles.py"", line 229, in __init__
    self.name = name
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/descriptors/base.py"", line 46, in __set__
    raise TypeError(msg)
TypeError: <class 'openpyxl.styles.named_styles._NamedCellStyle'>.name should be <class 'str'> but value is <class 'NoneType'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/platform_flow_common/tasks/flows/preliminary/excel.py"", line 371, in create_data_frame_sheet_file_object
    excel = pd.ExcelFile(path_or_buffer=excel_file)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/pandas/io/excel/_base.py"", line 1567, in __init__
    self._reader = self._engines[engine](
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py"", line 553, in __init__
    super().__init__(
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/pandas/io/excel/_base.py"", line 573, in __init__
    self.book = self.load_workbook(self.handles.handle, engine_kwargs)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py"", line 572, in load_workbook
    return load_workbook(
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/reader/excel.py"", line 346, in load_workbook
    reader.read()
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/reader/excel.py"", line 299, in read
    apply_stylesheet(self.archive, self.wb)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py"", line 198, in apply_stylesheet
    stylesheet = Stylesheet.from_tree(node)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py"", line 103, in from_tree
    return super(Stylesheet, cls).from_tree(node)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/descriptors/serialisable.py"", line 87, in from_tree
    obj = desc.expected_type.from_tree(el)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/descriptors/serialisable.py"", line 87, in from_tree
    obj = desc.expected_type.from_tree(el)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/descriptors/serialisable.py"", line 103, in from_tree
    return cls(**attrib)
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/styles/named_styles.py"", line 229, in __init__
    self.name = name
  File ""/Users/username/.local/share/virtualenvs/platform-excel-flow-v1_0-xHjD71YU/lib/python3.9/site-packages/openpyxl/descriptors/base.py"", line 46, in __set__
    raise TypeError(msg)
TypeError: <class 'openpyxl.styles.named_styles._NamedCellStyle'>.name should be <class 'str'> but value is <class 'NoneType'>

And then you get:

PanicException: index out of bounds: the len is 0 but the index is 0
```

### Issue Description

My company processes excel files, and we often encounter errors in the way a file is constructed. We raise specific errors in cases when the file is encrypted, when the file cannot be opened, when there are formatting errors, etc. so we can notify other team members or clients that something is wrong with their data source. I observed a Rust panic which is not caught when using pandas-calamine engine to load an excel sheet that has structural formatting errors. This is a concern because it doesn't appear there's any way to handle the exception in Python, and thus we cannot surface the right kind of error.

Not sure if this belongs as an issue on pandas or on pyO3 since the rust bindings are managed through that library...

### Expected Behavior

I would expect a different kind of exception to be raised, one native to the Python environment. It doesn't matter what.

### Installed Versions

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.9.16
python-bits           : 64
OS                    : Darwin
OS-release            : 22.6.0
Version               : Darwin Kernel Version 22.6.0: Wed Jul  5 22:22:05 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2025.2.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.0
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.38
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2025-02-07 23:53:42,2025-02-08 01:11:19,1,closed
60879,TST/CI: Investigate numba=0.61 test segfaults on Ubuntu ARM jobs,"In https://github.com/pandas-dev/pandas/pull/60847, we're skipping tests that use numba=0.61 on Ubuntu ARM jobs because they are causing segfaults. We should come up with a minimum reproducible example that be reported to numba so ideally we won't have to skip these tests in future numba releases.","['CI', 'Segfault', 'numba']",2025-02-07 23:31:51,2025-10-13 21:05:04,1,closed
60870,PERF: Regression in groupby ops from adding skipna,"https://github.com/rhshadrach/asv-runner/issues/42

Due to #60752 - cc @snitish","['Groupby', 'Missing-data', 'Performance', 'Regression']",2025-02-06 21:30:07,2025-02-10 18:24:55,2,closed
60869,ENH: unset_index method,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

In many contexts I've had the need to ""unset"" a data frame index prior to passing the data frame to an index-blind method. My main requirements for such an index unset are:
- the output should have the most trivial index possible, i.e. an unnamed range index
- if the input has only an unnamed range index, the ""unset"" operation should be a no-op
- if the input has a named index, it should be kept as a new column (or columns for a multi-index)
- if the input is not a simple RangeIndex and is unnamed, raise a value error to remind me to decide whether that index is supposed to be meaningful (typically I'd name it if so, or do `.reset_index(drop=True)` if not).

Usage of the `.reset_index` method to cover all of these requirements in a general sense requires a bit of nuance:
- calling `df.reset_index()` introduces a new column named simply ""index"" when users forget to use `drop=True` even when the index is [trivial] unnamed range index. In a stricter/safer world, you might want an error instead when you're about to construct a data frame column from an unnamed index.
- the requirement to specify `drop` at all is sometime onerous; it typically would be preferable to have a method that's a no-op in case the existing index is already an unnamed range index.

### Feature Description

Define a new `unset_index` method on DataFrame something like 

```
def unset_index(self) -> Self:
    if is_unnamed_range_index(df.index):
        return df
    if index_has_any_unnamed_col(df.index):
        raise ValueError(
            ""At least one column of the index is unnamed while the index itself is not a RangeIndex. ""
            ""Set the names of the index columns before calling unset, or just call reset_index(drop=True) directly.""
        )
    return df.reset_index(drop=False, allow_duplicates=False)
```

### Alternative Solutions

Until something like this is available in pandas, I'm using the [unset](https://github.com/zkurtz/pandahandler/blob/7d4320512c8015a5f70e903a9fcc0e62ce339aff/pandahandler/indexes.py#L47-L77) method in pandahandler.

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-02-06 19:40:35,2025-08-27 18:24:20,3,closed
60845,DOC: Update Bodo project description in ecosystem page,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/community/ecosystem.html#out-of-core

### Documentation problem

Bodo is now open source so the description is out of date.

### Suggested fix for documentation

Bodo description needs to be updated. Submitting a PR.","['Docs', 'Needs Triage']",2025-02-04 15:46:25,2025-02-04 17:55:18,0,closed
60838,BUG: Unknown Error - Getting from Databricks SQL Python - From PyArrow module (pyarrow.lib.ArrowException),"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
arr = arr.to_numpy(zero_copy_only=False)
```

### Issue Description

**Description:**
Hi there,

I'm getting this behaviour from databricks sql connectors due to it's dependency of Pandas. Though of posting it here to get any idea to fix it. B'coz I'm getting the exception when I try to convert the data with utf-8 encoded string to numpy array in pandas and py-arrow. Also Just curious to know is there any specific version of pandas or py-arrow that I can use to fix this issue. Any help would be appreciated here!

**Versions:**
Python: 3.11.9
Databricks SQL Connector: 3.3.0


**Code:**
Below is the code I'm using to fetch data from databricks which has some utf-8 encoded char (�) in the data.

``` 
sql_query = f"""""" SELECT Column FROM table LIMIT 10 """"""

host = os.getenv(""DATABRICKS_HOST"")
http_path = os.getenv(""DATABRICKS_HTTP_PATH"")

connection = sql.connect(server_hostname=host, http_path=http_path)
with conn.cursor() as cursor:
        cursor.execute(sql_query)
        response = fetchmany(cursor)

```

**Error:**

  ```
response = fetchmany(cursor)
               ^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/my_agent/app_helper/db_helper.py"", line 35, in fetchmany
    query_results = cursor.fetchmany(fetch_size)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/my_agent_connectors/dq_databricks/dependencies/databricks/sql/client.py"", line 976, in fetchmany
    return self.active_result_set.fetchmany(size)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/my_agent_connectors/dq_databricks/dependencies/databricks/sql/client.py"", line 1235, in fetchmany
    return self._convert_arrow_table(self.fetchmany_arrow(size))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/my_agent_connectors/dq_databricks/dependencies/databricks/sql/client.py"", line 1161, in _convert_arrow_table
    df = table_renamed.to_pandas(
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow/array.pxi"", line 884, in pyarrow.lib._PandasConvertible.to_pandas
  File ""pyarrow/table.pxi"", line 4192, in pyarrow.lib.Table._to_pandas
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/pyarrow/pandas_compat.py"", line 776, in table_to_dataframe
    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/pyarrow/pandas_compat.py"", line 1131, in _table_to_blocks
    return [_reconstruct_block(item, columns, extension_columns)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/pyarrow/pandas_compat.py"", line 1131, in <listcomp>
    return [_reconstruct_block(item, columns, extension_columns)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/pyarrow/pandas_compat.py"", line 736, in _reconstruct_block
    pd_ext_arr = pandas_dtype.__from_arrow__(arr)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/pandas/core/arrays/string_.py"", line 230, in __from_arrow__
    arr = arr.to_numpy(zero_copy_only=False)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow/array.pxi"", line 1581, in pyarrow.lib.Array.to_numpy
  File ""pyarrow/error.pxi"", line 91, in pyarrow.lib.check_status
pyarrow.lib.ArrowException: Unknown error: Wrapping Ruta 204 � Zapote failed
 ```

### Expected Behavior

It should support and parse the utf-8 encoded characters as well.

### Installed Versions



INSTALLED VERSIONS
------------------
commit           : 8dab54d6573f7186ff0c3b6364d5e4dd635ff3e7
python           : 3.11.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 23.6.0
Version          : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T8122
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.5.2
numpy            : 1.23.4
pytz             : 2022.1
dateutil         : 2.9.0.post0
setuptools       : 65.5.0
pip              : 24.0
Cython           : 3.0.10
pytest           : 7.4.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.9.9
jinja2           : 3.1.4
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
brotli           : None
fastparquet      : None
fsspec           : None
gcsfs            : None
matplotlib       : None
numba            : None
numexpr          : None
odfpy            : None
openpyxl         : 3.1.3
pandas_gbq       : None
pyarrow          : 16.1.0
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : None
snappy           : None
sqlalchemy       : 1.4.52
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
zstandard        : None
","['Bug', 'IO SQL', 'Needs Info', 'Arrow']",2025-02-03 12:22:09,2025-08-05 17:04:03,5,closed
60831,BUG: `pd.Series.groupby` issues `FutureWarning`,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Some index that are not integers
index = pd.date_range(start='2000-01-01', periods=3, freq='YS')

# Integer as a name
data = pd.Series([1, 2, 3], index=index, name=2)

data.groupby(data) # FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
```

### Issue Description

The warning comes from this line:
https://github.com/pandas-dev/pandas/blob/0691c5cf90477d3503834d983f69350f250a6ff7/pandas/core/groupby/grouper.py#L1015

Here, `gpr.name` is `2`, which results in the warning. If the index consists of integers, this means `obj[gpr.name]` will actually return the line (which then fails the comparison).


I have not checked on the main branch, but the same line might be present here:
https://github.com/pandas-dev/pandas/blob/d72f165eb327898b1597efe75ff8b54032c3ae7b/pandas/core/groupby/grouper.py#L853

### Expected Behavior

Don't issue `FutureWarning`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22621
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : 8.32.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'Warnings']",2025-02-02 18:03:53,2025-02-22 15:38:38,7,closed
60823,"BUG: pd.HDFStore(file, mode='a') increases file size unnecessarily when file exists and dataframe contains string-based columns","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import os
import warnings
import numpy as np
import pandas as pd

warnings.filterwarnings(""ignore"")  # PerformanceWarning

iterations = 10

print(""'w' mode'"")
file = ""w.h5""
for i in range(iterations):
    # Create a large random DataFrame
    df = pd.DataFrame(np.random.rand(1000, 100))
    df = df.assign(label=""hellothere"")

    # Save and reload the DataFrame with 'w' mode
    with pd.HDFStore(file, mode=""w"") as store:
        store.put(""data"", df)

    with pd.HDFStore(file, mode=""r"") as store:
        read_df = store.get(""data"")

    assert df.equals(read_df)

    print(f""'{file}' size: {os.path.getsize(file)}"")

print(""'a' mode'"")
file = ""a.h5""
for i in range(iterations):
    # Create a large random DataFrame
    df = pd.DataFrame(np.random.rand(1000, 100))
    df = df.assign(label=""hellothere"")

    # Save and reload the DataFrame with 'a' mode
    with pd.HDFStore(file, mode=""a"") as store:
        store.put(""data"", df)

    with pd.HDFStore(file, mode=""r"") as store:
        read_df = store.get(""data"")

    assert df.equals(read_df)

    print(f""'{file}' size: {os.path.getsize(file)}"")

print(""'a' mode, only numerical'"")
file = ""a_numerical.h5""
for i in range(iterations):
    # Create a large random DataFrame
    df = pd.DataFrame(np.random.rand(1000, 100))

    # Save and reload the DataFrame with 'a' mode
    with pd.HDFStore(file, mode=""a"") as store:
        store.put(""data"", df)

    with pd.HDFStore(file, mode=""r"") as store:
        read_df = store.get(""data"")

    assert df.equals(read_df)

    print(f""'{file}' size: {os.path.getsize(file)}"")

print(""'a' mode, only strings'"")
file = ""a_strings.h5""
for i in range(iterations):
    # Create a large random DataFrame
    df = pd.DataFrame({""label"": [""hellothere""] * 1000})

    # Save and reload the DataFrame with 'a' mode
    with pd.HDFStore(file, mode=""a"") as store:
        store.put(""data"", df)

    with pd.HDFStore(file, mode=""r"") as store:
        read_df = store.get(""data"")

    assert df.equals(read_df)

    print(f""'{file}' size: {os.path.getsize(file)}"")
```

### Issue Description

When saving dataframes to HDF5 using pd.HDFStore(file, 'a') and the dataframe to be stored contains string data columns, the size of the resulting file will increase if the file already existed even if it should not. This behavior impacts a situation where an existing .h5 file is being updated with more data. This is a **low-priority bug**, since the issue is avoidable in this situation by using the 'w' open mode instead. 

Example output:
```
'w' mode'
'w.h5' size: 3974104
'w.h5' size: 3974104
'w.h5' size: 3974104
'w.h5' size: 3974104
'w.h5' size: 3974104
'a' mode'
'a.h5' size: 3974104
'a.h5' size: 4776152
'a.h5' size: 5578200
'a.h5' size: 6380248
'a.h5' size: 7182296
'a' mode, only numerical'
'a_numerical.h5' size: 815240
'a_numerical.h5' size: 815312
'a_numerical.h5' size: 815312
'a_numerical.h5' size: 815312
'a_numerical.h5' size: 815312
'a' mode, only strings'
'a_strings.h5' size: 1070008
'a_strings.h5' size: 1076152
'a_strings.h5' size: 1080248
'a_strings.h5' size: 1084344
'a_strings.h5' size: 1088440
```

As we can see, as identically-sized data is being written into a.h5 and a_strings.h5, the size of the file increases.

Apologies in advance for not providing a fix, but since the workaround is very easy I didn't want to spend much time digging into it. Also sorry if it's written in documentation somewhere to avoid using the 'a' option. 

### Expected Behavior

Opening an HDFStore in 'a' mode and overwriting keys with dataframes containing string values does not lead to unnecessarily increasing file size.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-52-generic
Version               : #53-Ubuntu SMP PREEMPT_DYNAMIC Sat Jan 11 00:06:25 UTC 2025
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.3
numpy                 : 2.2.2
pytz                  : 2025.1
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : None
tables                : 3.10.2
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2025.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO HDF5']",2025-02-01 06:58:06,2025-03-31 21:17:07,4,closed
60816,BUG: Union of two DateTimeIndexes is incorrectly calculated,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas import DatetimeIndex

l = DatetimeIndex(['2023-05-24 00:00:00+00:00', '2023-05-24 00:15:00+00:00',
               '2023-05-24 00:30:00+00:00', '2023-05-24 00:45:00+00:00',
               '2023-05-24 01:00:00+00:00'],
              dtype='datetime64[ms, UTC]', name='ts', freq='15min') 

r = DatetimeIndex(['2023-05-24 00:00:00+00:00', '2023-05-24 00:30:00+00:00',
               '2023-05-24 01:00:00+00:00'],
              dtype='datetime64[ms, UTC]', name='ts', freq='30min') 

union = r.union(l)

print(union)

assert len(union) == len(l)
assert all(r.union(l) == l)
```

### Issue Description

The union of two datetime-indexes as given in the reproducible example is calculated incorrectly, the result on newer Pandas versions is

```python
DatetimeIndex(['2023-05-24 00:00:00+00:00', '2051-11-29 16:00:00+00:00',
               '2080-06-06 08:00:00+00:00'],
              dtype='datetime64[ms, UTC]', name='ts', freq='15T')
```

The first failing version is the one I put into ""Installed Versions"". The error happens exactly from Pandas 2.1.0 onwards, Pandas 1.* and up to 2.0.3 work fine. Neither the numpy nor the Python version matter.

### Expected Behavior

The expected result in the given case is that `l` is returned.

### Installed Versions

INSTALLED VERSIONS
------------------
commit              : ba1cccd19da778f0c3a7d6a885685da16a072870
python              : 3.10.16.final.0
python-bits         : 64
OS                  : Linux
OS-release          : 6.12.10-200.fc41.x86_64
Version             : #1 SMP PREEMPT_DYNAMIC Fri Jan 17 18:05:24 UTC 2025
machine             : x86_64
processor           : 
byteorder           : little
LC_ALL              : None
LANG                : en_US.UTF-8
LOCALE              : en_US.UTF-8

pandas              : 2.1.0
numpy               : 1.26.4
pytz                : 2024.2
dateutil            : 2.9.0.post0
tzdata              : 2025.1
","['Bug', 'Regression', 'Needs Discussion', 'Non-Nano']",2025-01-29 15:45:23,2025-06-02 17:24:35,7,closed
60815,DOC: Missing documentation for `Styler.columns` and `Styler.index`,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.io.formats.style.Styler.html#pandas.io.formats.style.Styler

### Documentation problem

The attributes `columns` and `index` are not documented for the `Styler` class.

### Suggested fix for documentation

Document those attributes.


Initially reported here in `pandas-stubs` : https://github.com/pandas-dev/pandas-stubs/issues/1102","['Docs', 'Styler']",2025-01-29 15:25:29,2025-02-21 18:06:56,5,closed
60812,Next Pandas Release Question,"Will there be another patch release any time soon?
I see the last regular release 2.2.3 was in September.

Specifically, I am interested in picking up this bugfix https://github.com/pandas-dev/pandas/issues/60102
Considering if I will have to backport it",[],2025-01-28 22:58:36,2025-04-17 09:46:00,2,closed
60810,BUG: Inconsistent dtype with GroupBy for StrDtype and all missing values,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> df = pd.DataFrame({""a"": [""a""] * 3, ""b"": pd.Series([None] * 3, dtype=pd.StringDtype(na_value=np.nan))})
>>> df
   a    b
0  a  NaN
1  a  NaN
2  a  NaN
>>> df.groupby(""a"").sum()
   b
a   
a  0
>>> df.groupby(""a"").sum().dtypes
b    str
dtype: object
>>> df.groupby(""a"").min()
    b
a    
a NaN
>>> df.groupby(""a"").min().dtypes
b    float64
dtype: object
```

### Issue Description

The sum reduction return type is partially discussed in https://github.com/pandas-dev/pandas/issues/60229 but I didn't see anything for `min` 

Note that this discrepancy is the root cause of the test failure shown at https://github.com/pandas-dev/pandas/blob/dec6eb29b35c884e78c82525e1bb30280208714c/pandas/tests/resample/test_resampler_grouper.py#L465

@rhshadrach 

### Expected Behavior

I think in all cases here we should still be returning a `str` type.

### Installed Versions

'3.0.0.dev0+1824.g8d6d29cac3.dirty'
","['Bug', 'Groupby', 'Strings']",2025-01-28 21:39:07,2025-04-19 15:22:38,1,closed
60803,BUG: Most recent conda-forge package limited to python 3.9?,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
docker run --rm -it mambaorg/micromamba
micromamba install pandas -c conda-forge -c defaults
```

### Issue Description

Sorry if this issue is in the wrong place! It seems there were previously conda-forge packages for linux-64 up to python v3.13 but the latest package only has v3.9:

![Image](https://github.com/user-attachments/assets/ba6daf88-d266-40c5-b559-9df0170ca135)

https://anaconda.org/conda-forge/pandas/files

What this means is that installing pandas via the example above chooses to install python 3.9 unless the user selects a newer version of python, which is probably suboptimal?

![Image](https://github.com/user-attachments/assets/9ccc86ce-599c-462d-8513-12a2bc7f8012)

Thanks for your consideration!

### Expected Behavior

Conda-forge packages for all supported versions of python released at the same time?

### Installed Versions

```python
>>> pd.show_versions()                                                                                                                                        [11/1762]
                                         
INSTALLED VERSIONS          
------------------          
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.9.21
python-bits           : 64  
OS                    : Linux
OS-release            : 6.1.119-129.201.amzn2023.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Tue Dec  3 21:07:35 UTC 2024
machine               : x86_64
processor             :     
byteorder             : little
LC_ALL                : C.UTF-8
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 25.0
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
```","['Bug', 'Build']",2025-01-27 23:33:33,2025-01-30 21:10:38,9,closed
60802,"DOC: Specify what ""non-null"" means in DataFrame.info()","### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html

### Documentation problem

Non-null is not specific

### Suggested fix for documentation

Link to documentation or specify exactly what non-null means. In particular, for float64s NaN are considered ""null"". And does it also represent NULLs in the Nullable integer types? https://pandas.pydata.org/docs/user_guide/integer_na.html

Pandas is not consistent with its terminology of NA, NULL, and NaN. 
NaN is a floating point value that is not in the IEEE standard as a missing value.
R uses NA consistently and SQL uses NULL consistently in 3VL. ","['Docs', 'Missing-data']",2025-01-27 22:18:02,2025-04-10 16:04:04,5,closed
60801,"ENH: Add a ratio() Method for Series and dataframes. Alternatively, consider removing similar methods at this functionality level (e.g. diff() and pct_change())","### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [x] Removing existing functionality in pandas


### Problem Description

We propose adding a ratio() method to pandas. 
Alternatively, if this request is deemed unnecessary because it is a ""one-liner,"" we suggest re-evaluating whether diff() and pct_change() (other one-liners) should remain in the library for consistency.

===
Currently, pandas provides the diff() method to calculate differences between consecutive elements in a Series or DataFrame. However, there is no equivalent method for calculating ratios. 
This absence creates the following issues:
1. Users must implement ratios manually with df / df.shift(), which is repetitive, error-prone, and less readable than a dedicated method.
2. Users are often tempted to misuse pct_change() as a substitute for ratios, but pct_change() introduces an adjustment ((current / previous - 1)) and is misleadingly named.

Adding a ratio() method would simplify a fundamental operation, enhance code clarity, and align pandas' functionality with its philosophy of making common operations easy and intuitive.

### Feature Description

The ratio() method would calculate the ratio of consecutive elements in a Series or DataFrame along a specified axis, similar to how diff() computes differences.

Behavior:
* By default, ratio() divides the current element by the element n rows before it (n is defined by the periods parameter, default is 1).
* Missing values (e.g., from shift()) would propagate as NaN, consistent with pandas behavior.

Example Usage:

```
# Input data
df = pd.DataFrame({""col"": [2, 4, 8, 16]})

# Current workaround
df[""ratios""] = df[""col""] / df[""col""].shift(1)

# Proposed functionality
df[""ratios""] = df[""col""].ratio()

# Expected output
#     col  ratios
# 0     2     NaN
# 1     4     2.0
# 2     8     2.0
# 3    16     2.0
```


### Alternative Solutions

1. Existing Functionality (df / df.shift()):
This is the current workaround but is repetitive, less readable, and more prone to user error.

2. Misusing pct_change():
While pct_change() appears similar, it computes (current / previous - 1), which is not a true ratio. This method is often misinterpreted and introduces unnecessary complexity.

3. 3rd Party Packages:
For the basic functionality - simpler to use the 1-liner
For optimized or more generic functionality: We are not aware of any 3rd party package for this.

### Additional Context

1. Parity with Existing Methods:
Pandas already provides diff() for additive differences and both cumsum() and cumprod() for cumulative operations. 
A ratio() method would align pandas’ API by addressing a gap for multiplicative differences.

2. Confusion with pct_change():
Many users misuse pct_change() because there is no direct alternative for calculating ratios. Adding a ratio() method would eliminate this source of confusion.

3. Broader Use Case:
Ratios are widely used in finance, analytics, and scientific computing, making this a valuable addition to pandas' core functionality.

4. Implementation, testing and efficiency:
We propose the viewpoint that ratio() is the natural geometric equivalent to the existing diff() in all possible respects, and thus the requirements and implementation considerations should follow similar lines (save for specific, division-related caveats that will be dealt with as needed).

","['Enhancement', 'Needs Triage']",2025-01-27 16:35:25,2025-08-27 18:25:36,1,closed
60800,BUG: Cannot connect to odoo,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.test()
```

### Issue Description

I found an error in pandas, after I installed it I couldn't connect it to one of the modules in odoo. I then checked it and I found some small modules in pandas that were Fatal Error, so I tested the pandas module and the results were like this
""725 failed, 171537 passed, 25922 skipped, 3935 deselected, 1003 xfailed, 77 xpassed, 3 warnings, 729 errors""

### Expected Behavior

I'm just a regular user but I understand enough about modules like this, sorry I can't give a clear answer

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Info', 'Closing Candidate']",2025-01-27 14:26:06,2025-07-23 09:20:44,5,closed
60798,"BUG: replace(to_replace=pd.NaT, value=None) different from replace({pd.NaT: None})","### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df = pd.read_pickle(""example.pkl"")

df
                        ts               Start                 End
23 2025-01-27 09:49:44.045 2025-01-27 09:49:44                 NaT
28 2025-01-27 06:50:56.046 2025-01-27 06:50:54 2025-01-27 06:50:56


df.replace(to_replace=pd.NaT, value=None)
                        ts               Start                 End
23 2025-01-27 09:49:44.045 2025-01-27 09:49:44                 NaT
28 2025-01-27 06:50:56.046 2025-01-27 06:50:54 2025-01-27 06:50:56

df.replace({pd.NaT: None})
                            ts                Start                  End
23  2025-01-27 09:49:44.045000  2025-01-27 09:49:44                 None
28  2025-01-27 06:50:56.046000  2025-01-27 06:50:54  2025-01-27 06:50:56

df.replace(to_replace=pd.NaT, value=None).dtypes
ts       datetime64[ns]
Start    datetime64[ns]
End      datetime64[ns]
dtype: object

df.replace({pd.NaT: None}).dtypes
ts       object
Start    object
End      object
dtype: object
```

### Issue Description

In my application I read data from a database via asyncpg and then process it with pandas.
Recently I encountered an issue where the replace command changes the datatypes of unrelated columns if I use it with a dictionary argument. 
Using replace with the arguments ""to_replace"" and ""value"", however, works. 

Somehow my dataframe is weird, I was not able to create a pure code example to reproduce this and only saving and loading my dataframe as a pickle file made it reproducible. However, due to GitHub limitations I cannot share the pickle file here which is probably due to pickle files being unsafe to unpickle from untrusted sources.
I did try to recreate the issue in code and also using other file formats but that somhow seems to loose important metadata that causes the issue.


This is the metadata of the dataframe as it shows in the VS-code debugger:
![Image](https://github.com/user-attachments/assets/f85d4906-04ef-46d9-95c8-f4df75e2aa48)

### Expected Behavior

I would expect that these two results are the same:

`df.replace(to_replace=pd.NaT, value=None).dtypes`
`df.replace({pd.NaT: None}).dtypes`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.5
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.0
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Duplicate Report', 'replace']",2025-01-27 12:58:16,2025-01-27 21:35:33,1,closed
60794,BUG: Bug in mask method when handling pd.NA with Int64Dtype,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from pandas import Series
from pandas.api.extensions import Int64Dtype
series = Series([None, 1, 2, None, 3, 4, None], dtype=Int64Dtype())
result = series.mask(series <= 2, -99)
print(result)
```

### Issue Description

I am encountering an issue with the mask method in pandas when it is used with a Series of type Int64Dtype. Specifically, when trying to mask pd.NA values, they are being replaced, which is not the expected behavior. I expected the pd.NA values to remain unchanged, but they are being incorrectly filled.

### Expected Behavior

Series([None, -99, -99, None, 3, 4, None], dtype=Int64Dtype())

### Installed Versions

python: 3.11.1
pandas: 2.1.3
","['Bug', 'Duplicate Report', 'NA - MaskedArrays']",2025-01-26 11:53:00,2025-01-27 21:36:27,2,closed
60786,ENH: generic `save` and `read` methods for DataFrame,"### Feature Type

- [x] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Currently, pandas has separate IO methods for each file format (to_csv, read_parquet, etc.). This requires users to:
- Remember multiple method names
- Change code when switching formats

### Feature Description

A unified `save`/`read` API would simplify common IO operations while maintaining explicit control when needed:
- File type is inferred from the filepath extension, but a `format` arg can be passed to be explicit, raising an error in some cases where the inferred file type disagrees with passed file type.
- Both methods accept `**kwargs` and pass them along to the underlying file-type-specific pandas IO methods. 
- Optionally, support some basic translation across discrepancies in arg names in existing IO methods (i.e. ""usecols"" in `read_csv` vs ""columns"" in `read_parquet`).

```
# Simplest happy path:
df.save('data.csv')  # Uses to_csv
df = pd.read('data.parquet')  # Uses read_parquet

# Optionally, be explicit about expected file type
df.save('data.csv', format=""csv"")  # Uses to_csv
df = pd.read('data.parquet', format=""parquet"")  # Uses read_parquet

# Raises ValueError for conflicting format info:
df.save('data.csv', format='parquet')  # Conflicting types
df.save('data.txt', format='csv')  # .txt implies text format

# Reading allows overrides for misnamed files (or should we require users to rename their files properly first?)
df = pd.read('mislabeled.txt', format='parquet')

# Not sure if we should allow save when inferred file type is not a standard type:
df.save('data', format='csv')  # No extension, needs type
df.save('mydata.unknown', format='csv')  # Unclear extension
```


### Alternative Solutions

Existing functionality is OK, just not the simplest to use.

### Additional Context

_No response_","['Enhancement', 'IO Data', 'Needs Discussion', 'Closing Candidate']",2025-01-25 01:18:47,2025-08-05 16:20:37,4,closed
60779,BUG: pd.read_csv Incorrect Checksum validation for COMPOSITE Checksum,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import base64
import zlib

import awswrangler as wr
import boto3
import pandas as pd

# DL DEV
AWS_ACCESS_KEY_ID = <Redacted>
AWS_SECRET_ACCESS_KEY = <Redacted>
AWS_SESSION_TOKEN = <Redacted>
session_west = boto3.Session(
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    aws_session_token=AWS_SESSION_TOKEN,
    region_name=""eu-west-1"",
)

client = session_west.client(""s3"")
localpath = <Redacted>
bigfile = ""bigfile.csv""
smallfile = ""smallfile.csv""
bucket = ""checksum-test-bucket""
s3path = ""checksum-test""

for filetype in [smallfile, bigfile]:
    with open(f""{localpath}{filetype}"", ""rb"") as file:

        # Calculate CRC32 ourselves for reference
        crcval = zlib.crc32(file.read())
        crc_bytes = crcval.to_bytes(4, ""big"")
        crc = base64.b64encode(crc_bytes).decode(""utf-8"")
        print(f""{filetype} - {crc}"")

    with open(f""{localpath}{filetype}"", ""rb"") as file:

        client.put_object(
            Bucket=bucket, Key=f""{s3path}/put_object/{filetype}"", Body=file
        )

    client.upload_file(
        Bucket=bucket,
        Key=f""{s3path}/upload_file/{filetype}"",
        Filename=f""{localpath}{filetype}"",
    )

for filetype in [smallfile, bigfile]:
    for upload_method in [""put_object"", ""upload_file""]:

        path = f""s3://{bucket}/{s3path}/{upload_method}/{filetype}""
        print(path)
        try:
            fw: pd.DataFrame = wr.s3.read_csv(
                path=path,
                dtype=""object"",
                boto3_session=session_west,
            )
            print(fw.shape)
        except Exception as e:
            print(f""wrangler failed - {e}"")

        try:
            fp = pd.read_csv(
                path,
                storage_options={
                    ""key"": AWS_ACCESS_KEY_ID,
                    ""secret"": AWS_SECRET_ACCESS_KEY,
                    ""token"": AWS_SESSION_TOKEN,
                },
            )
            print(fp.shape)
        except Exception as e:
            print(f""Pandas fail - {e}"")

        try:
            client = session_west.client(""s3"")
            fb = client.get_object(
                Bucket=bucket,
                Key=f""{s3path}/{upload_method}/{filetype}"",
                ChecksumMode=""ENABLED"",
            )
            print(f'{fb[""ChecksumCRC32""]} - {fb[""ChecksumType""]}')
        except Exception as e:
            print(f""boto error - {e}"")
```

### Issue Description

Boto3 >=1.36.0 has modified behaviour to add CRC32 checksum by default where supported.

When accessing s3 objects with pd.read_csv any s3 object that has created a COMPOSITE checksum fails reading as the checksum compared against is the FULL_OBJECT checksum. 
Composite checksum appears to be calculated when an object exceeds ~10Mb when using boto3 upload_file(), seemingly it switches to a multi-part upload behind the scenes at that threshold. Other explicit multi-part uploads will presumably have the same behaviour.

Included test using both Pandas and Awswrangler for completeness

Output for failing versions
```pip show boto3 botocore s3transfer pandas awswrangler| egrep 'Name:|Version:'
Name: boto3
Version: 1.36.5
Name: botocore
Version: 1.36.5
Name: s3transfer
Version: 0.11.2
Name: pandas
Version: 2.2.3
Name: awswrangler
Version: 3.11.0


smallfile.csv - CbsfmA==
bigfile.csv - vGPIeA==
s3://checksum-test-bucket/checksum-test/put_object/smallfile.csv
(1461, 91)
(1461, 91)
CbsfmA== - FULL_OBJECT
s3://checksum-test-bucket/checksum-test/upload_file/smallfile.csv
(1461, 91)
(1461, 91)
CbsfmA== - FULL_OBJECT
s3://checksum-test-bucket/checksum-test/put_object/bigfile.csv
(20467, 91)
(20467, 91)
vGPIeA== - FULL_OBJECT
s3://checksum-test-bucket/checksum-test/upload_file/bigfile.csv
wrangler failed - Expected checksum DIoExg== did not match calculated checksum: vGPIeA==
Pandas fail - Expected checksum DIoExg== did not match calculated checksum: vGPIeA==
DIoExg==-2 - COMPOSITE
```

Using boto3 <1.36 all scenarios from the example code work

Test with older version
```pip install ""boto3<1.36.0""	```

Output from working version
```show boto3 botocore s3transfer pandas awswrangler| egrep 'Name:|Version:'
Name: boto3
Version: 1.35.99
Name: botocore
Version: 1.35.99
Name: s3transfer
Version: 0.10.4
Name: pandas
Version: 2.2.3
Name: awswrangler
Version: 3.11.0


smallfile.csv - CbsfmA==
bigfile.csv - vGPIeA==
s3://checksum-test-bucket/checksum-test/put_object/smallfile.csv
(1461, 91)
(1461, 91)
None - None
s3://checksum-test-bucket/checksum-test/upload_file/smallfile.csv
(1461, 91)
(1461, 91)
None - None
s3://checksum-test-bucket/checksum-test/put_object/bigfile.csv
(20467, 91)
(20467, 91)
None - None
s3://checksum-test-bucket/checksum-test/upload_file/bigfile.csv
(20467, 91)
(20467, 91)
None - None```


### Expected Behavior

When reading using pd.read_csv the checksum calculated for comparison should be aware of whether the stored checksum is FULL_OBJECT or COMPOSITE and handle it correctly. 



### Installed Versions

<details>

Working versions
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.11
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-1021-aws
Version               : #23~22.04.1-Ubuntu SMP Tue Dec 10 16:50:46 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.utf8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.12.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 16.0.0
pyreadstat            : None
pytest                : 8.2.0
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.12.0
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
None

Failing versions
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.11
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-1021-aws
Version               : #23~22.04.1-Ubuntu SMP Tue Dec 10 16:50:46 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.utf8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.12.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 16.0.0
pyreadstat            : None
pytest                : 8.2.0
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.12.0
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
None
</details>
","['Bug', 'IO CSV', 'IO Network', 'Needs Triage']",2025-01-24 11:48:54,2025-01-27 12:34:43,2,closed
60773,BUG: datetime64 column has incorrect unit when created with pd.Timestamp ,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
ts_list = [""2021-01-01 00:00:00"", ""2021-01-01 00:00:01"", ""2021-01-01 00:00:02""]
df = pd.DataFrame(
    {
        ""ts1"": [pd.Timestamp(ts) for ts in ts_list],
        ""ts2"": pd.to_datetime(ts_list),
    }
).assign(
    ts3=pd.Timestamp(ts_list[0]),
    ts4=pd.Timestamp(ts_list[0], unit=""ns""),
    ts5=pd.to_datetime(ts_list[0]),
)
```

### Issue Description

Then

```
df.dtypes
```

gives 

```
ts1    datetime64[ns]
ts2    datetime64[ns]
ts3     datetime64[s]
ts4     datetime64[s]
ts5    datetime64[ns]
dtype: object
```

### Expected Behavior

I would expect the above to output 

```
ts1    datetime64[ns]
ts2    datetime64[ns]
ts3     datetime64[ns]
ts4     datetime64[ns]
ts5    datetime64[ns]
dtype: object
```

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Darwin
OS-release            : 24.2.0
Version               : Darwin Kernel Version 24.2.0: Fri Dec  6 19:02:41 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6030
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.24.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : None
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.12.2
html5lib              : 1.1
hypothesis            : 6.123.17
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.8.4
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : 7.4.4
python-calamine       : None
pyxlsb                : None
s3fs                  : 2023.12.2
scipy                 : 1.13.0
sqlalchemy            : 2.0.36
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2025.1.1
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Needs Triage']",2025-01-23 13:04:34,2025-01-23 21:00:55,3,closed
60770,BUG: arrow backend get wrong result,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
### Describe the bug, including details regarding any error messages, version, and platform.

when pandas has a null column，compare will get a False，

import duckdb as dd
df=dd.sql(""select null as id"").df()
df['id']>1

0    False
Name: id, dtype: bool

but change to arrow, will get NA, how to get False?

import pyarrow as pa
import pandas as pd
df2=pa.Table.from_pandas(df).to_pandas(types_mapper=pd.ArrowDtype,use_threads=True)
df2['id']>1

0    <NA>
Name: id, dtype: bool[pyarrow]

### Component(s)

Python
```

### Issue Description

pandas2.2.3，use arrow backend，
got NA，need False，
how to got same result？

### Expected Behavior

df['id']>1，want return False

### Installed Versions

pandas2.2.3
","['Bug', 'Missing-data', 'Arrow']",2025-01-23 07:37:39,2025-01-25 02:11:56,1,closed
60769,BUG: value_counts() method in panda Series,"### Pandas version checks

- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

s = pd.Series([3, 1, 2, 3, 4, np.nan])
s.value_counts(normalize=True)
```

### Issue Description

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[737], [line 1](vscode-notebook-cell:?execution_count=737&line=1)
----> [1](vscode-notebook-cell:?execution_count=737&line=1) s = pd.Series([3, 1, 2, 3, 4, np.nan])
      [2](vscode-notebook-cell:?execution_count=737&line=2) s.value_counts(normalize=True)

TypeError: 'Index' object is not callable

### Expected Behavior

Must create a series and count the unique values

### Installed Versions

","['Bug', 'Needs Triage']",2025-01-23 03:44:51,2025-01-23 03:55:53,0,closed
60750,BUG: `pd.Series.isnumeric()` doesn't work on decimal value strings,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""string_values"": [""1"", ""1.0"", ""1.1""]})
df.string_values.str.isnumeric()
```

### Issue Description

The series method `.isnumeric()` only works on integer strings. If a string number is decimal, it will return `False`. When running the example below, the following is returned: 

<img width=""609"" alt=""Image"" src=""https://github.com/user-attachments/assets/9cdd0a8e-4a74-4e2f-ba07-44914a085b4d"" />

This is the docs description for the method: 

<img width=""758"" alt=""Image"" src=""https://github.com/user-attachments/assets/0c10d350-56af-4699-8fcb-2f20a739e28a"" />


### Expected Behavior

Running the method on decimal strings should return `True`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.49-linuxkit-pr
Version               : #1 SMP PREEMPT Thu May 25 07:27:39 UTC 2023
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.1
sqlalchemy            : 2.0.37
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Docs', 'Strings', 'good first issue']",2025-01-21 22:51:24,2025-02-21 01:12:42,20,closed
60747,DOC: Reinstate the JupyterLite-based live shell for the pandas website,"## Description

Hello, I've recently been looking into the JupyterLite shell for [the `pandas` website's Getting Started page](https://pandas.org/getting_started.html) that briefly used to serve as an interactive endpoint for users browsing the website. It was discussed in https://github.com/pandas-dev/pandas/issues/46682 and added in #47428, subsequently [reported to be a bit slow back then](https://github.com/pandas-dev/pandas/issues/47530), and was removed as a result in https://github.com/pandas-dev/pandas/pull/49807.

I'd like to propose reinstating this shell for the website (either on the same page, or elsewhere on the [docs website's landing page](https://pandas.pydata.org/docs/index.html) via [the `jupyterlite-sphinx` project](https://github.com/jupyterlite/jupyterlite-sphinx), similar to https://github.com/matplotlib/matplotlib/pull/22634), and wish to seek thoughts from the `pandas` maintainers via this issue on whether it would be a good idea to do so for usage by newcomers.

## Rationale and additional context

- In early 2025, it has been a lot of time by now, and while the world of Python running in WebAssembly still experimental, we've since then made a bunch of improvements across the Pyodide and JupyterLite ecosystems across many past releases – both for improving the stability of the shell, if not its speed, and for being able to run `pandas` code within it.
- As the one who helped add the WASM CI job for Pandas last year via $57896, this is a related area in terms of `pandas`'s usage within Pyodide, and I would be happy to maintain the shell if it's added and establish some relevant automations towards its upkeep.
- We have been working on similar improvements to contemporary shells, such as those that exist and have been retained on the websites for [NumPy](https://numpy.org/)and [SymPy](https://live.sympy.org/), recently

xref: https://github.com/Quansight-Labs/czi-scientific-python-mgmt/issues/134

Thank you for your time! :)

<hr>

P.S. Here's a short [example](https://jupyterlite.github.io/demo/repl/index.html?&code=import%20pandas%20as%20pd%0Adf%20%3D%20pd.DataFrame(%0A%20%20%20%20%5B%5B1,%202%5D,%20%5B4,%205%5D,%20%5B7,%208%5D%5D,%0A%20%20%20%20index%3D%5B%27cobra%27,%20%27viper%27,%20%27sidewinder%27%5D,%0A%20%20%20%20columns%3D%5B%27max_speed%27,%20%27shield%27%5D%0A)%0Adf.loc%5B%5B%27viper%27,%20%27sidewinder%27%5D%5D%0A&kernel=python&execute=1), which takes ~7.5 seconds for me to load on a decently stable connection – but even for those with throttled connections, it should be easy to add a small admonition before it that just says ""This is an experimental playground"", or just prefix the word ""Experimental"" before the heading.

P.P.S. I noticed that a similar approach has been taken by the Ibis project; they have an admonition on this page: https://ibis-project.org/tutorials/browser/repl that states that it is experimental at the moment.

cc: @jtpio for visibility, as he was among those who collaborated on (and led) this effort previously through the issues and PRs linked.

<hr>

The description and rationale have been copied over with minor changes from my recent message on 18/01/2025 in the `pandas` Slack workspace: https://pandas-dev-community.slack.com/archives/C03PH1SU1M1/p1737168137448029 as suggested by @rhshadrach, which should help this proposal receive greater visibility.","['Enhancement', 'Web']",2025-01-21 13:29:59,2025-03-04 01:25:36,2,closed
60743,BUG: cannot use random.choice on index in python 3.11.0,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import random
import pandas as pd
df=pd.DataFrame([[0,1],[2,3]])
random.choice(df.index)
```

### Issue Description

This code produces the ValueError shown below. When I use python 3.10.x or 3.11.10, the error does not occur. I don't understand why the behavior would be different for Python 3.11.0.


---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[3], line 5
      3 df=pd.DataFrame([[0,1],[2,3]])
      4 pd.__version__
----> 5 random.choice(df.index)

File ~/miniconda3/envs/py3110/lib/python3.11/random.py:369, in Random.choice(self, seq)
    367 def choice(self, seq):
    368     """"""Choose a random element from a non-empty sequence.""""""
--> 369     if not seq:
    370         raise IndexError('Cannot choose from an empty sequence')
    371     return seq[self._randbelow(len(seq))]

File ~/miniconda3/envs/py3110/lib/python3.11/site-packages/pandas/core/indexes/base.py:3190, in Index.__nonzero__(self)
   3188 @final
   3189 def __nonzero__(self) -> NoReturn:
-> 3190     raise ValueError(
   3191         f""The truth value of a {type(self).__name__} is ambiguous. ""
   3192         ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""
   3193     )

ValueError: The truth value of a RangeIndex is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

### Expected Behavior

Should not raise an error

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.0
python-bits           : 64
OS                    : Darwin
OS-release            : 24.1.0
Version               : Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage']",2025-01-21 04:15:05,2025-01-21 04:44:05,2,closed
60740,BUG: stack with future_stack=True and empty list,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame()
old_working = df.stack(level=[])  # does nothing as list of levels is empty

new_broken = df.stack(level=[], future_stack=True)   # tries to stack despite the empty list
```

### Issue Description

There is an unexpected behavioral change in stack with future_stack=True. The future_stack variant tries to stack even though the list of levels to stack is empty. The error message is `ValueError: Cannot remove 1 levels from an index with 1 levels: at least one level must be left.`



### Expected Behavior

For me, this happens in code where the levels to stack are determined by a function - and the case of ""empty list"" seems to be forgotten in the new implementation. So could this behavior be restored in the new stack implementation?

### Installed Versions

Tested with pandas 2.2.3.","['Bug', 'Reshaping']",2025-01-20 17:29:45,2025-02-04 03:27:34,3,closed
60728,BUG: pd.Series fails to cast datetime series containing only NaT to timedelta type,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

example = pd.Series(
    pd.Series([pd.NaT], dtype=""datetime64[ns]""), dtype=""timedelta64[ns]""
)
print(example.dtype)
```

### Issue Description

The above snipper outputs `datetime64[ns]`, which is against the documented behavior for `pd.Series`:

> **dtype** : str, numpy.dtype, or ExtensionDtype, optional
Data type for the output Series.

Reference https://github.com/theOehrly/Fast-F1/issues/674

### Expected Behavior

Expect the output series `example` to have `timedelta64[ns]` data type. Or for the snippet to emit an error or warning indicating this conversion is not available.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.4
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.167.4-microsoft-standard-WSL2
Version               : #1 SMP Tue Nov 5 00:21:55 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.26.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Dtype Conversions', 'Constructors']",2025-01-17 20:11:29,2025-02-08 19:00:14,9,closed
60727,DOC: Add comparison with SPSS,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/getting_started/comparison/index.html

### Documentation problem

I teach pandas, and have a number of students that come in with experience in [SPSS](https://en.wikipedia.org/wiki/SPSS). We have comparison guides with a number of other related tools, but none that speaks to SPSS.

### Suggested fix for documentation

Add a ""Comparison to SPSS"" page","['Docs', 'good first issue']",2025-01-17 18:18:52,2025-01-22 18:41:46,1,closed
60723,BUG: Concatenating dataframe and series with `ignore_index = True` drops the series name,"### Pandas version checks

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({'a': [0, 1], 'b': [2, 3]})
s = pd.Series([4, 5], name='c')
pd.concat([df, s])   # columns are ['a', 'b', 'c']
pd.concat([df, s], ignore_index=True)  # columns are ['a', 'b', 0]
pd.concat([df, s.to_frame()])  # columns are ['a', 'b', 'c']
pd.concat([df, s.to_frame()], ignore_index=True)  # columns are ['a', 'b', 'c']
```

### Issue Description

When I concatenate a dataframe with a series and pass `ignore_index=True`, the series' name does not show up in the resulting dataframe.  This is surprising, because the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) for `ignore_index` says, ""Note the index values on the other axes are still respected in the join.""

This seems similar to #56257. That concerned the case in which the name of the series is the same as the name of one of the columns of the dataframe but did not involve `ignore_index`. That issue has a stale PR (#56362) that looks like it might fix this one, too.

### Expected Behavior

Doing `pd.concat([df, s], ignore_index=True)` should preserve the name of the series.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.2
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
None
</details>
","['Bug', 'Reshaping']",2025-01-16 19:50:02,2025-03-07 01:04:19,6,closed
60714,BUG: why arrow only work on mac arm？,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.options.mode.copy_on_write = True
def df_merge(
    left,
    right,
    how: Literal[""left"", ""right"", ""inner"", ""outer"", ""cross""] = ""inner"",
    on=None,
    left_on=None,
    right_on=None,
    left_index: bool = False,
    right_index: bool = False,
    sort: bool = False,
    suffixes=(""_x"", ""_y""),
    copy: bool = True,
    indicator: bool = False,
    validate=None,
):
    if not pd.api.types.is_dtype_backend(left, ""pyarrow""):
        left = pa.Table.from_pandas(left).to_pandas()
    if not pd.api.types.is_dtype_backend(right, ""pyarrow""):
        right = pa.Table.from_pandas(right).to_pandas()
```


### Issue Description

I have a python project use pickle file ，pandas2.1，
when it run in x86 centos7，cost 107s，
but only need 71s in mac m2，
and I upgrade pandas to 2.2.3，and set：pd.options.mode.copy_on_write = True
and edit function df_merge（which is the most cost time fuciton），
change df to arrow first。
then only need 35s。
but，same in x86 centos7，still need 100s，why arrow not work？

### Expected Behavior

use arrow twice quickly in x86 centos7，but no effect！

### Installed Versions

2.2.3","['Bug', 'Closing Candidate', 'ARM', 'OS X', 'Arrow']",2025-01-13 02:58:45,2025-11-08 18:19:35,5,closed
60705,"DOC: io.rst description and code inconsistent, plus the description is for deprecated behaviour","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/user_guide/io.html#reading-html-content

<blockquote>

Read in the content of the file from the above URL and pass it to `read_html` as a string:

```py
In [317]: html_str = """"""
   .....:          <table>
   .....:              <tr>
   .....:                  <th>A</th>
   .....:                  <th colspan=""1"">B</th>
   .....:                  <th rowspan=""1"">C</th>
   .....:              </tr>
   .....:              <tr>
   .....:                  <td>a</td>
   .....:                  <td>b</td>
   .....:                  <td>c</td>
   .....:              </tr>
   .....:          </table>
   .....:      """"""
   .....: 

In [318]: with open(""tmp.html"", ""w"") as f:
   .....:     f.write(html_str)
   .....: 

In [319]: df = pd.read_html(""tmp.html"")

In [320]: df[0]
Out[320]: 
   A  B  C
0  a  b  c
```

</blockquote>

### Documentation problems

#### Problem 1

The ""above URL"" is

```py
url = 'https://www.sump.org/notes/request/' # HTTP request reflector
```

but data from that URL is not what's used in the code.

#### Problem 2

""pass it to `read_html` as a string"" is not what's being demonstrated in the code.

#### Problem 3

`read_html` can take an HTML string, but that behaviour is deprecated, per [its docs](https://pandas.pydata.org/docs/dev/reference/api/pandas.read_html.html):

> Deprecated since version 2.1.0: Passing html literal strings is deprecated. Wrap literal string/bytes input in `io.StringIO`/`io.BytesIO` instead.


### Suggested fix for documentation

I'm not sure!","['Docs', 'Needs Triage']",2025-01-12 20:22:00,2025-10-27 17:28:38,3,closed
60695,BUG: Series constructor from dictionary drops key (index) levels when not all keys have same number of entries,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.Series({(""l1"",):""v1"", (""l1"",""l2""): ""v2""})
# Out[30]: 
# l1    v1
# l1    v2
# dtype: object

# the reason is that the Series constructor uses internally MultiIndex.from_tuples in the following way (note that the input is a tuple of tuples!):
pd.MultiIndex.from_tuples(((""l1"",), (""l1"",""l2"")))
# Out[32]: 
# MultiIndex([('l1',),
#             ('l1',)],
#            )

# compare to the following which produces the expected result:
pd.MultiIndex.from_tuples([(""l1"",), (""l1"",""l2"")])
# Out[33]: 
# MultiIndex([('l1',  nan),
#             ('l1', 'l2')],
#            )

# Note: this was tested with latest release and current master
```


### Issue Description

When calling the `Series` constructor with a dict where the keys are tuples, a series with `MulitIndex` gets created. However, if the number of entries in the keys is not the same, key entries from keys with more than the minimum number get dropped. This is in several ways problematic, especially if this produces duplicated index values / keys which is not expected because it was called with a dict (which has per definition unique keys).

### Expected Behavior

The `MultiIndex` of the new series has nan-padded values.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.16
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-51-generic
Version               : #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'MultiIndex', 'good first issue']",2025-01-11 19:14:38,2025-03-07 23:51:46,26,closed
60692,ENH: Make pd.Timestamp.astimezone() default to local timezone,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```py
import pandas as pd
pd.Timestamp.now().astimezone()
```

However, for datetime objects this is no problem, it uses the local timezone as default:

```py
from datetime import datetime
datetime.now().astimezone()
```


### Issue Description

It would be great if `Timestamp.astimezone()` would work like it does for the original `datetime`, so that e.g. a function that accepts a datetime doesn't have to treat pd.Timestamp (which inherits datetime) differently

### Expected Behavior

`Timestamp.astimezone` should work like it does for the original `datetime` (choosing the local timezone)

### Installed Versions

<details>

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2022.7.1
dateutil              : 2.8.2
Cython                : 3.0.11
pytest                : 7.4.0

</details>
","['Enhancement', 'API Design', 'Needs Discussion', 'Closing Candidate', 'Localization']",2025-01-11 12:06:50,2025-08-05 16:31:23,3,closed
60690,ENH: frozensets are shown in parentheses (like tuples),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.
    - Query: `is:issue in:title frozenset`
- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.
- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.

### Reproducible Example

```python
s = pd.Series([frozenset([1])])
print(s)
```

### Issue Description

```
0    (1)
dtype: object
```

### Expected Behavior

The same as `s.map(repr)`:
```
0    frozenset({1})
dtype: object
```

Or if you insist on an abbreviated option, maybe something like this:
```
0    f{1}
dtype: object
```

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 3aba767f3ac4507185d911ed120a49969cdee63d
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.4.0-204-generic
Version               : #224-Ubuntu SMP Thu Dec 5 13:38:28 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : fr_CA.UTF-8
LOCALE                : fr_CA.UTF-8

pandas                : 3.0.0.dev0+1815.g3aba767f3a
numpy                 : 2.1.0.dev0+git20240403.e59c074
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.22.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : 2024.1
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```

</details>
","['Enhancement', 'Output-Formatting']",2025-01-09 23:55:12,2025-02-05 17:49:32,2,closed
60688,BUG: ValueError in pandas.DataFrame.replace with regex on single-row DataFrame with None/NaN,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""ticker"": [""#1234#""], ""name"": [None]})

df.replace({col: {r""^#"": ""$""} for col in df.columns}, regex=True)  # raises
df.fillna("""").replace({col: {r""^#"": ""$""} for col in df.columns}, regex=True)  # works
df.astype(str).replace({col: {r""^#"": ""$""} for col in df.columns}, regex=True)  # works
df.astype(pd.StringDtype()).replace({col: {r""^#"": ""$""} for col in df.columns}, regex=True)  # works
```


### Issue Description

Using replace with a regex pattern on a single-row DataFrame containing `None` values raises the following error:
```
ValueError: cannot call `vectorize` on size 0 inputs unless `otypes` is set
```

### Expected Behavior

The replace function should handle `None` values gracefully without requiring a manual fill or type conversion.

### Installed Versions


<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.27766
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Denmark.1252

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Missing-data', 'replace']",2025-01-09 21:51:42,2025-01-10 17:17:22,3,closed
60685,"ENH: Better error message for trying to convert to ""double,"" type","### Feature Type

- [ ] Adding new functionality to pandas

- [x] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I (don't ask how 😅) came over the fact that this:

```python
pd.DataFrame({""a"": [1, 2, 3]}).astype(""hurdy-gurdy"")
```
Raises a helpful 'TypeError: data type ""hurdy-gurdy"" not understood message'

But this doesn't happen if the incorrect type is ""double,"" (I'm not sure why but I assume something around the fact that pandas needs to check for types such as ""double[pyarrow]"").

This code:
```python
pd.DataFrame({""a"": [1, 2, 3]}).astype(""double,"")
```

Raises the error: ""TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"".



### Feature Description

Throw the safe ""double,"" is not a valid type error message

### Alternative Solutions

Probably involves digging slightly into the part of the code that's throwing an error - I'd love to put in a PR if this is something that'd be accepted?

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2025-01-09 10:42:37,2025-02-06 12:52:01,4,closed
60684,BUG: Pandas to_sql 'Engine' object has no attribute 'cursor',"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from tqdm import tqdm
from datetime import datetime
import sys
from azure.storage.blob import ContainerClient
from azure.core.exceptions import ResourceNotFoundError, ClientAuthenticationError
from azure.identity import ClientSecretCredential
import re
from dotenv import load_dotenv
import pickle
import io
import pandas as pd
from sqlalchemy import create_engine
import urllib
import logging
import sys
import time
import sqlalchemy
from typing import Sequence, Hashable
from collections import defaultdict
import os
from azure.storage.blob import BlobServiceClient
from sqlalchemy import text
import numpy as np
from sqlalchemy.dialects import mssql
import polars as pl
import csv
import pandas as pd
import io
import os
from datetime import datetime
import tqdm as tqdm
import sqlalchemy
from sqlalchemy import create_engine, inspect

from sqlalchemy import create_engine
from sqlalchemy.engine import URL
import pandas as pd


load_dotenv()
SERVER = os.environ.get(""SERVER"")
DATABASE = os.environ.get(""DATABASE"")
UID = os.environ.get(""UID"")
PASSWORD = os.environ.get(""PASSWORD"")

# Construct the connection string for Azure SQL Server
params = urllib.parse.quote_plus(
    'DRIVER={ODBC Driver 18 for SQL Server};'
    f'SERVER={SERVER};'
    f'DATABASE={DATABASE};'
    f'UID={UID};'
    f'PWD={PASSWORD}'
)

engine = create_engine(f'mssql+pyodbc:///?odbc_connect={params}')
df.to_sql(custom_table_name, 
                            con=engine, 
                            if_exists = 'append',  # Options: 'fail', 'replace', 'append'
                            index = False,         # Set to True if you want to include index as a column
                            #dtype = outputdict_dtype 
                            )
```


### Issue Description

BUG: Pandas to_sql 'Engine' object has no attribute 'cursor'

### Expected Behavior

BUG: Pandas to_sql 'Engine' object has no attribute 'cursor'

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.4
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 151 Stepping 5, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.26.0
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : 7.3.7
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.2.1
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.24.0
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 14.0.2
pyreadstat            : None
pytest                : 7.4.4
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.6.1
scipy                 : 1.13.1
sqlalchemy            : None
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2023.6.0
xlrd                  : None
xlsxwriter            : 3.2.0
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'IO SQL', 'Needs Info']",2025-01-09 07:57:11,2025-07-23 09:35:18,6,closed
60678,BUG: boolean series .isin([pd.NA])] inconsistent for series length,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
ser = pd.Series(True, index=range(10_000_000), dtype='boolean')
ser.iloc[0] = pd.NA
ser.iloc[1] =False
result_short = ser.iloc[:10].isin([False, pd.NA])
assert result_short.sum() == 2
result_long = ser.isin([False, pd.NA])
```


### Issue Description

The behavior of the `.isin()` function on nullable boolean `boolean` series is inconsistent for `pd.NA`:  
For short series, this returns a series with `True` for every null values in the original series  
```python
>>> pd.Series([True, False, pd.NA], dtype='boolean').isin([pd.NA])
0    False
1    False
2     True
dtype: boolean
```
For long series, this raises a `TypeError: boolean value of NA is ambiguous`   

```python
>>> pd.Series(True, index=range(1_000_000), dtype='boolean').isin([pd.NA])
0         False
1         False
2         False
3         False
4         False
          ...  
999995    False
999996    False
999997    False
999998    False
999999    False
Length: 1000000, dtype: boolean
>>> pd.Series(True, index=range(1_000_001), dtype='boolean').isin([pd.NA])
Traceback (most recent call last):
  File ""<python-input-34>"", line 1, in <module>
    pd.Series(True, index=range(1_000_001), dtype='boolean').isin([pd.NA])
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File ""/homes/yoavr/.conda/envs/pandas_latest/lib/python3.13/site-packages/pandas/core/series.py"", line 5559, in isin
    result = algorithms.isin(self._values, values)
  File ""/homes/yoavr/.conda/envs/pandas_latest/lib/python3.13/site-packages/pandas/core/algorithms.py"", line 505, in isin
    return comps_array.isin(values)
           ~~~~~~~~~~~~~~~~^^^^^^^^
  File ""/homes/yoavr/.conda/envs/pandas_latest/lib/python3.13/site-packages/pandas/core/arrays/masked.py"", line 971, in isin
    result = isin(self._data, values_arr)
  File ""/homes/yoavr/.conda/envs/pandas_latest/lib/python3.13/site-packages/pandas/core/algorithms.py"", line 545, in isin
    return f(comps_array, values)
  File ""/homes/yoavr/.conda/envs/pandas_latest/lib/python3.13/site-packages/pandas/core/algorithms.py"", line 534, in f
    return np.logical_or(np.isin(c, v).ravel(), np.isnan(c))
                         ~~~~~~~^^^^^^
  File ""/homes/yoavr/.conda/envs/pandas_latest/lib/python3.13/site-packages/numpy/lib/_arraysetops_impl.py"", line 1132, in isin
    return _in1d(element, test_elements, assume_unique=assume_unique,
           ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 invert=invert, kind=kind).reshape(element.shape)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/homes/yoavr/.conda/envs/pandas_latest/lib/python3.13/site-packages/numpy/lib/_arraysetops_impl.py"", line 982, in _in1d
    mask |= (ar1 == a)
             ^^^^^^^^
  File ""missing.pyx"", line 392, in pandas._libs.missing.NAType.__bool__
TypeError: boolean value of NA is ambiguous
``` 

### Expected Behavior

`.isin([pd.NA])` should either always work or it should always raise an error.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.1
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-88-generic
Version               : #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : C.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None



</details>
","['Bug', 'Missing-data', 'isin']",2025-01-08 14:42:55,2025-01-22 21:28:30,5,closed
60673,BUG: DataFrame.sort_values() by 2 columns and a key function produces incorrect results,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame.from_records(
    [[let, num] for let in ""DCBA"" for num in [2, 1]], columns=[""let"", ""num""]
)
print(df)

r1 = df.sort_values([""let"", ""num""])
print(r1)


def key_func(s: pd.Series) -> pd.Series:
    result = s.sort_values()
    return result


r2 = df.sort_values([""let"", ""num""], key=key_func)
print(r2)
```


### Issue Description

When providing a `key` argument to `sort_values()` or `sort_index()`, and specifying more than one column, the results are not sorted correctly.

In the above code, the output is:
```text
  let  num
0   D    2
1   D    1
2   C    2
3   C    1
4   B    2
5   B    1
6   A    2
7   A    1
  let  num
7   A    1
6   A    2
5   B    1
4   B    2
3   C    1
2   C    2
1   D    1
0   D    2
  let  num
0   D    2
1   D    1
2   C    2
3   C    1
4   B    2
5   B    1
6   A    2
7   A    1
```
- The first DF is the original DF
- The second DF is the sorted DF without a `key` function.  The results are first sorted on the column `let`, then to break ties, sorted on the column `num`
- The third DF is the result of using a key function that sorts each column (based on the specification in the API - the function has to return a sorted column).  The result is a DF that is not sorted.  It should be the same as the second DF


### Expected Behavior

The result of the sort with a `key` argument in this case should be the same as without the function.  When specifying the `key` argument with more than one column, the result should be hierarchically sorted.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.14
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.8.4
numba                 : None
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : 1.2.8
pytest                : N/A
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.15.0
sqlalchemy            : 2.0.36
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.11.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Docs', 'Sorting']",2025-01-07 20:36:31,2025-01-21 19:11:36,3,closed
60672,BUG: read_csv: Columns Silently Forced into Multi Index ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

data = """"""13.9,130,200,1000,2,1
12.1,120,200,1001,2,2
0.7,110,200,1000,2,3
"""""" 

df = pd.read_csv(
    pd.io.common.StringIO(data),
    header=None,
    names=[""test1"",""test2"",""test3""],
    sep="","",
    encoding=""utf-8-sig"",
    index_col=None
)
print(df.shape)
print(df)
```


### Issue Description

When calling pd.read_csv and using the arg 'names=...', if a user inputs fewer names than columns in the dataset, the names are applied to the final columns of the data and the first are silently placed into a multi index. 

In my example, a clean csv with 6 columns is read using read_csv, where only 3 names are provided. The result is the final 3 columns (3,4,5) are named using the provided names and the first 3 are made into a multi index. No warning or error is raised. 

### Expected Behavior

My understanding is that the expected behavior is that the first columns will receive naming priority and the following ones will be named 'unnamed_column' or something along those lines. 



### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.11
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 25 Model 97 Stepping 2, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.31.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV']",2025-01-07 18:11:06,2025-01-07 21:56:25,2,closed
60670,ENH: DataFrame.to_sql progress bar or something for check status,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

DataFrame.to_sql progress bar or something for check status

### Feature Description

DataFrame.to_sql progress bar or something for check status

### Alternative Solutions

DataFrame.to_sql progress bar or something for check status

### Additional Context

DataFrame.to_sql progress bar or something for check status","['Enhancement', 'Needs Triage']",2025-01-07 04:10:26,2025-01-07 17:42:48,2,closed
60668,ENH: Add support for executing UDF's using Bodo as the engine,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Applying User Defined Functions (UDFs) to a DataFrame can be very slow when evaluated using the default python engine. Passing `engine=""numba""` and leveraging Numba's Just-in-Time (JIT) compiler to transform the UDF application into an optimized binary can improve performance, however there are several limitations to the Numba UDF engine including:

* Limited set of dtypes supported (only supports numpy dtypes, does not support ExtensionDtypes)
* Parallel execution not supported (unless `raw=True`)
* Difficulty troubleshooting issues due to lengthy stack traces and hard-to-read error messages.

Adding support for the Bodo engine would solve the above issues and provide a good complement to the capabilities of the currently supported engines (Python and Numba).

Bodo uses an auto-parallelizing JIT compiler to transform Python code into highly optimized, parallel binaries with an MPI backend, allowing it to scale to very large data sizes with minimal extra work required from the user (large speedups on both laptops and clusters). Bodo is also built for Pandas and supports DataFrame, Series and Array Extension types natively.



### Feature Description

Allow passing the value `""bodo""` to the `engine` parameter in `DataFrame.apply` and add an `apply_bodo` method which accepts the user defined function and creates a jit function to do the apply and calls it. For example:
In `pandas/core/apply.py`
``` py
class FrameApply(NDFrameApply):
...
    def apply_series_bodo(self) -> DataFrame | Series:
        bodo = import_optional_dependency(""bodo"")

        engine_kwargs = bodo_get_jit_arguments(self.engine_kwargs)

        @bodo.jit(**engine_kwargs)
        def do_apply(obj, func, axis):
            return obj.apply(func, axis)

        result = do_apply(self.obj, self.func, self.axis)
        return result
```
This approach could also be applied to other API's that accepts a UDF and engine argument.

### Alternative Solutions

Users could execute their UDF using a Bodo JIT'd function. For example:
``` py
import bodo
import pandas as pd

def f(x):
  return x.A // x.B if x.B != 0 else 0

@bodo.jit
def apply_udf(df, func):
  return df.apply(func, axis=1)

df = pd.DataFrame({""A"": [1,2,3,4,5], ""B"": [0, 1, 2, 2, 2]})

result = apply_udf(df, f)
```
While this approach is fine, it has it's downsides such as requiring a larger code rewrite which could make it more difficult to quickly experiment with different engines.

### Additional examples:

To demonstrate the value that the Bodo engine would bring to Pandas users, consider the following examples which highlight some of the limitations of the current engines (Python and Numba):

Numba does not support non-numeric column types such as strings and has limited support for Pandas APIs inside the UDF. The following example would not compile with the numba engine: 

``` py
df = pd.DataFrame({""A"": [""Hello"", ""Hi""] * 5, ""B"": [1, 2, 3, 4, 5] * 2, ""C"": [5,4,3,2,1] * 2})

def g(x):
    last_cols = pd.Series([x.B, x.C], index=[""red"", ""blue""])
    return x.A + "" "" +str(last_cols.idxmin())

print(df.apply(g, axis=1, engine=""bodo""))
```

Bodo also natively supports Timestamp (with timezone) and DateOffset types inside UDFs whereas numba only supports datetime64/timedelta64:

``` py
tz = ""US/Pacific""

df = pd.DataFrame(
    {
        ""order_date"": pd.array([
            pd.Timestamp('2017-01-06T12', tz=tz),
            pd.Timestamp('2018-11-23T12', tz=tz),
            pd.Timestamp('2017-10-02T12', tz=tz),
            pd.Timestamp('2025-01-28T12', tz=tz),
            ])
    }
)

def f(x):
    dayofweek = x.order_date.dayofweek

    if dayofweek in [0, 1, 2, 3]:
        return ""weekday""
    elif dayofweek == 4:
        return ""friday""
    else:
        return ""weekend""

df[""time_bucket""] = df.apply(f, axis=1, engine=""bodo"")
```

``` py
tz = ""US/Pacific""

df = pd.DataFrame(
    {
        ""order_date"": pd.array([
            pd.Timestamp('2018-01-06T12', tz=tz),
            pd.Timestamp('2018-02-23T12', tz=tz),
            pd.Timestamp('2018-11-02T12', tz=tz),
            pd.Timestamp('2018-12-28T12', tz=tz),
            ])
    }
)

def f(x):
    month = x.order_date.month

    if month == 12:
        return None

    return x.order_date + pd.DateOffset(months=1)

df[""next_order_this_year""] = df.apply(f, axis=1, engine=""bodo"")
```

Lastly, ExtensionDtypes are not supported by the Numba engine, so this example with `pyarrow.decimal128` type would not compile:

``` py
from decimal import Decimal

import pandas as pd
import pyarrow as pa

df = pd.DataFrame(
    {
        ""A"": pd.array(
            [Decimal(""0.000000000000001""), Decimal(""5""), Decimal(""0.1"")],
            dtype=pd.ArrowDtype(pa.decimal128(32, 18)),
        )
    }
)


def f(x):
    if x.A < Decimal(""0.00000002""):
        return ""low""
    elif x.A < Decimal(""0.5""):
        return ""med""
    else:
        return ""high""

df[""bucket""] = df.apply(f, engine=""numba"", raw=True, axis=1)
```

For large Dataframes, the Bodo engine can offer performance benefits over the Python-based engine because it can translate functions into optimized machine code that executes in parallel. For example, consider this UDF which preprocesses text data by taking sections from two passages and joining them together to form a new passage:

``` py
import time
import pandas as pd

rand_df = pd.read_parquet(""random_strings.pq"")

def combine_str(x):
    midA = len(x.A) // 2
    midB = len(x.B) // 2
    return x.A[midA:] + x.B[:midB]

for engine in (""bodo"", ""python""):
    start = time.time()
    res = rand_df.apply(combine_str, axis=1, engine=engine)
    end = time.time()
    print(f""total execution time of {engine} engine:"", end-start)
```

For this example, we can randomly generate the string data to use:

``` py
import string
import random

import pandas as pd
import pyarrow as pa


NUM_STRINGS = 1_000_000

def get_random_string():
    length = random.randint(80, 120)
    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))

random_string_A = [get_random_string() for _ in range(NUM_STRINGS)]
random_string_B = [get_random_string() for _ in range(NUM_STRINGS)]

string_dtype = pd.ArrowDtype(pa.large_string())

rand_df = pd.DataFrame(
    {
        ""A"": pd.array(random_string_A, dtype=string_dtype),
        ""B"": pd.array(random_string_B, dtype=string_dtype)
    }
)

print(rand_df.head())
rand_df.to_parquet(""random_strings.pq"")
```

Running on my laptop I saw:

```
total execution time of bodo engine: 8.819313049316406
total execution time of python engine: 45.07858085632324
``` 

Which represents a 5x improvement for 1 million strings.


### Additional Context

Relevant links:
[Bodo's documentation](https://docs.bodo.ai/latest/)
[Bodo's github repo](https://github.com/bodo-ai/Bodo)
[Proof-of-concept PR that adds support for `engine=""bodo""` in `df.apply`.](https://github.com/pandas-dev/pandas/pull/60622)",['Enhancement'],2025-01-06 22:17:55,2025-03-14 15:03:12,10,closed
60664,"BUG: Pandas resets counter when using filterwarning ""once""","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import warnings
import pandas as pd

warnings.filterwarnings(""once"", category=UserWarning)

warnings.warn(""This is a warning"", UserWarning)
warnings.warn(""This is a warning"", UserWarning)
warnings.warn(""This is a second warning"", UserWarning)
warnings.warn(""This is a second warning"", UserWarning)
pd.DataFrame()
warnings.warn(""This is a warning"", UserWarning)
warnings.warn(""This is a warning"", UserWarning)
warnings.warn(""This is a second warning"", UserWarning)
warnings.warn(""This is a second warning"", UserWarning)
```


### Issue Description

Using filterwarnings with action 'once' should only print a warning of a specific category and text once. But calling pd.DataFrame() or other pandas functions (like pd.read_csv) makes both warnings shown twice. Deleting pd.DataFrame yields the expected behaviour.

I read issue #31978. This has been closed saying that it is a PyCharm issue, but I am using VSCode and I verified my example in termnial both from Windows and Ubuntu.

### Expected Behavior

Both warnings (""This is a warning"" and ""This is a second warning"") should be shown only once each.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.153.1-microsoft-standard-WSL2
Version               : #1 SMP Fri Mar 29 23:14:13 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Closing Candidate', 'Upstream issue']",2025-01-06 15:25:55,2025-08-05 16:21:00,6,closed
60662,BUG: pd.testing.assert_frame_equal has AssertionError with same dtypes,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pyarrow as pa
import numpy as np
from datetime import datetime, timedelta

df = pd.DataFrame({
   ""product_id"": pd.Series(
       [""PROD_"" + str(np.random.randint(1000, 9999)) for _ in range(100)], 
       dtype=pd.StringDtype(storage=""pyarrow"")
   ),
   ""transaction_timestamp"": pd.date_range(
       start=datetime.now() - timedelta(days=30), 
       periods=100, 
       freq='1H'
   ),
   ""sales_amount"": pd.Series(
       np.round(np.random.normal(500, 150, 100), 2),
       dtype=pd.Float64Dtype()
   ),
   ""customer_segment"": pd.Series(
       np.random.choice(['Premium', 'Standard', 'Basic'], 100),
       dtype=pd.StringDtype(storage=""pyarrow"")
   ),
   ""is_repeat_customer"": pd.Series(
       np.random.choice([True, False], 100, p=[0.3, 0.7])
   )
})

def types_mapper(pa_type):
  if pa_type == pa.string():
      return pd.StringDtype(""pyarrow"")

df = df.convert_dtypes(dtype_backend=""pyarrow"")
df_pa = pa.Table.from_pandas(df).to_pandas(types_mapper=types_mapper)
pd.testing.assert_frame_equal(df, df_pa)
```
```


### Issue Description

The dtypes are seemingly the same but I get the following error.

```

AssertionError: Attributes of DataFrame.iloc[:, 0] (column name=""product_id"") are different

Attribute ""dtype"" are different
[left]:  string[pyarrow]
[right]: string[pyarrow]

```

### Expected Behavior

I expected this code to not return an AssertionError.

### Installed Versions

<details>

pandas                : 2.2.2

</details>
","['Bug', 'Needs Triage']",2025-01-05 17:11:53,2025-01-06 17:44:36,0,closed
60657,BUG: vulnerabilities found in the latest pandas v2.2.3,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Scan the latest version using Sonatype/Aqua security scanners.
Expected 2 vulns to be reported: http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-9880,
http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13091
```


### Issue Description

There are 2 critical security vulnerabilities found in v2.2.3:
http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-9880,
http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13091.

We didn't get response on email sent to [security@tidelift.com](mailto:security@tidelift.com).
These 2 issues block upcoming release of our project, could your team take a look and fix them asap?

Thanks

### Expected Behavior

Should pass Sonatype and Aqua security scanners with no issues found.

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Info']",2025-01-04 14:31:36,2025-01-05 14:09:45,3,closed
60651,QST: ,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/62061221/how-to-shift-row-values-up-and-replaces-the-nan-values-with-it-in-pandas

### Question about pandas

how to <html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/Client/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/Client/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">


AAA | bbb | ffgdf | jhkhgk
-- | -- | -- | --
213srsd | ghjgh45 | hd4 | dfsh32
hgfd32 | gfh | gfdh5443 |  
gfhdfg |   | 54dfgdg |  
  |   |   |  
  |   |   |  
  |   | 564ggeg |  
  | zsa12 | gfd | dfgh123
  |   | fdgh | fdghfd
435zzzz |   | fgdhg |  
fghf323423 |   |  
  |   |   |  
  |   |   |  
  | sdfdgf123 |  
  |   |   |  
  |   | dfghdfh2324 |  
  |   | df23443 |  



</body>

</html>

to 

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/Client/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/Client/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">


AAA | bbb | ffgdf | jhkhgk
-- | -- | -- | --
213srsd | ghjgh45 | hd4 | dfsh32
hgfd32 | gfh | gfdh5443 | dfgh123
gfhdfg | zsa12 | 54dfgdg | fdghfd
435zzzz | sdfdgf123 | 564ggeg |  
fghf323423 | gfd |  
  |   | fdgh |  
  |   | fgdhg |  
  |   | dfghdfh2324
  |   | df23443 |  



</body>

</html>

by Pandas?","['Usage Question', 'Needs Info', 'Closing Candidate']",2025-01-03 07:27:59,2025-01-30 21:09:35,1,closed
60646,BUG: max on `axis=1` returns wrong values on type `datetime64[ns]` when `NaT` is present in values,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame([""NaT"", ""2024-04-16 09:20:00.123456789""], dtype=""datetime64[ns]"")
df.max(axis=1)

# prints:
#
# 0                             NaT
# 1   2024-04-16 09:20:00.123456768
# dtype: datetime64[ns]
```


### Issue Description

The max (axis=1) of a DataFrame of one column should return that column (type `pd.Series`).
However, when the `dtype` is `datetime64[ns]` and the column contains a `NaT`, there are small differences with the original and returned column.
In the MWE the difference is 21 nanoseconds.

Note: This only happens if there is a `NaT` in column values.

### Expected Behavior

Expected behavior is to print
```
0                             NaT
1   2024-04-16 09:20:00.123456789
dtype: datetime64[ns]
```

### Installed Versions

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Linux
OS-release            : 4.14.355-271.569.amzn2.x86_64
Version               : #1 SMP Tue Nov 5 10:11:37 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.0
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.3.1
Cython                : 0.29.30
sphinx                : None
IPython               : 8.21.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.8.1
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 15.0.2
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.10.0
scipy                 : 1.14.1
sqlalchemy            : None
tables                : 3.8.0
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
```","['Bug', 'Datetime', 'Dtype Conversions', 'Reduction Operations']",2025-01-02 12:55:13,2025-02-05 17:46:48,4,closed
60645,DOC: pandas.DataFrame.aggregate return value,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.aggregate.html#pandas.DataFrame.aggregate

### Documentation problem

The documentation of pandas.DataFrame.aggregate() method says:

    The return can be:

    * scalar : when Series.agg is called with single function
    * Series : when DataFrame.agg is called with a single function
    * DataFrame : when DataFrame.agg is called with several functions
 
But 
df = pd.DataFrame([[1]]) ; type(df.agg(lambda x: 3*x))
returns pandas.core.frame.DataFrame even though .agg() was called with a single function

### Suggested fix for documentation

I'd love to offer a fix, but the reason I was looking up the docs was that I'd like to know what .agg() does exactly...","['Docs', 'Duplicate Report']",2025-01-02 12:12:12,2025-01-04 14:30:58,2,closed
60639,API (string dtype): comparisons between different string classes,"Some comparisons between different classes of string (e.g. `string[pyarrow]` and `str`) raise. Resolving this is straightforward except for what class should be returned. I would expect it should always be the left obj, e.g. `string[pyarrow] == str` should return `string[pyarrow]` whereas `str == string[pyarrow]` should return `str`. Is this the concensus?

We currently run into issues with how Python handles subclasses with comparison dunders.

```python
lhs = pd.array([""x"", pd.NA, ""y""], dtype=""string[pyarrow]"")
rhs = pd.array([""x"", pd.NA, ""y""], dtype=pd.StringDtype(""pyarrow"", np.nan))

print(lhs.__eq__(rhs))
# <ArrowExtensionArray>
# [True, <NA>, True]
# Length: 3, dtype: bool[pyarrow]

print(lhs == rhs)
# [ True False  True]
```

The two results above differ because `ArrowStringArrayNumpySemantics` is a proper subclass of `ArrowStringArray` and therefore Python first calls `rhs.__eq__(lhs)`.

We can avoid this by special casing this particular case in `ArrowStringArrayNumpySemantics`, but I wanted to open up an issue for discussion before proceeding.

cc @WillAyd @jorisvandenbossche 
","['Numeric Operations', 'Strings', 'Needs Discussion', 'API - Consistency']",2025-01-01 21:22:07,2025-05-19 15:51:40,27,closed
60638,DOC: PyArrow documentation for round trip conversions with pandas,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html#pyarrow

### Documentation problem

This issue was first brought up: https://github.com/pandas-dev/pandas/issues/50074

The documentation mentions two different approaches.

First is using StringDtype: 

```
import pandas as pd
import pyarrow as pa
df = pd.DataFrame({""x"": [""foo"", ""bar"", ""baz""]}, dtype=pd.StringDtype(""pyarrow""))
df_pa = pa.Table.from_pandas(df).to_pandas()
pd.testing.assert_frame_equal(df, df_pa)
```

Second is using ArrowDtype:

```
import pandas as pd
import pyarrow as pa
df = pd.DataFrame({""x"": [""foo"", ""bar"", ""baz""]}, dtype=pd.ArrowDtype(pa.string()))
df_pa = pa.Table.from_pandas(df).to_pandas()
pd.testing.assert_frame_equal(df, df_pa)
```

However these both have assertion errors. 

Using astype as shown below doesn't have the assertion error.

```
import pandas as pd
import pyarrow as pa
df = pd.DataFrame({""x"": [""foo"", ""bar"", ""baz""]}, dtype=""string[pyarrow]"")
df_pa = pa.Table.from_pandas(df).to_pandas().astype(""string[pyarrow]"")
pd.testing.assert_frame_equal(df, df_pa)
```

The two approaches mentioned in the documentation are also mentioned in the issue from 2022 as working versions / fixes. However I think these approaches may not work with the current version of pandas. 

### Suggested fix for documentation

Documentation should be updated to reflect the `.astype(""string[pyarrow]"")` as possibly being the best practice approach for this situation. ","['Docs', 'Needs Triage']",2025-01-01 17:17:09,2025-01-05 11:40:33,0,closed
60610,"DOC: In `User Guide` /`Chart visualization`/`Controlling the labels`, A blank cell.","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/user_guide/visualization.html#controlling-the-labels

### Documentation problem

A blank cell.

![image](https://github.com/user-attachments/assets/38f8471b-361b-43d2-a7cd-913319c44c33)


### Suggested fix for documentation

It is no sense that blank cell.
It is better to delete.",['Docs'],2024-12-27 16:30:38,2024-12-30 15:08:41,1,closed
60609,to_sql doesn't seem to allow upload of df with some columns when the target table has a calculated column,"I am trying to append a dataframe with several columns to a MS SQL table with a calculated field. I get the following error on attempt: 
""sqlalchemy.exc.ProgrammingError: (pyodbc.ProgrammingError) ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The column ""Fieldname"" cannot be modified because it is either a computed column or is the result of a UNION operator. (271) (SQLExecDirectW); [42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Statement(s) could not be prepared. (8180)')""

This issue seems to have been addressed in the AWS sdk using the use_column_names parameter. See here - https://github.com/aws/aws-sdk-pandas/issues/2170 

That parameter doesn't seem to exist when using a sqlalchemy pyodbc connection. - ""TypeError: to_sql() got an unexpected keyword argument 'use_column_names'""

Is there something similar that exists for a sqlalchemy pyodbc connection? ",[],2024-12-26 18:14:51,2025-01-02 14:37:34,2,closed
60606,BUILD: Remove tests folder in package distribution,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

macOS-15.1-x86_64-i386-64bit

### Installation Method

pip install

### pandas Version

2.2.3

### Python Version

3.11.9

### Installation Logs

I discovered that the `tests` folder is included in the package distribution, which I believe is not a good practice.

Consider this real scenario: I use `pandas` in an AWS Lambda function, where the layer size limitation is 250 MB. This means all dependencies must be smaller than this limit, necessitating a reduction in the package size.

Here is what `setuptools` says,

> final users will end up installing not only timmins.foo, but also docs and tests.tests_foo. A simple way to fix this is to adopt the aforementioned [src-layout](https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#src-layout), or make sure to properly configure the include and/or exclude accordingly.


## How to reproduce it

```
$ pip install pandas -t test-install
$ cd test-install
$ du -sh * | sort -rh
 38M	tests
 21M	_libs
 12M	core
3.6M	io
692K	plotting
224K	_testing
208K	util
140K	__pycache__
116K	compat
112K	tseries
108K	_config
 64K	errors
 48K	conftest.py
 48K	api
 24K	pyproject.toml
 24K	_version.py
 16K	_typing.py
 12K	__init__.py
8.0K	arrays
4.0K	testing.py
4.0K	_version_meson.py
```

## Reference

AWS Lambda limitation: 250 MB [The maximum size of the contents of a deployment package, including layers and custom runtimes. (unzipped)](https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html)

Python setup tool: [exclude tests](https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#finding-simple-packages)

","['Build', 'Needs Triage']",2024-12-26 03:07:01,2024-12-26 16:54:35,1,closed
60604,BUG: Single Index of Tuples as Output on Tuple Groupings,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df = pandas.DataFrame({""A"" : [1,2,3,1,2,3], ""B"": [4,5,6,4,5,6], ""C"" : [7,8,9,7,8,9]})
df = df.set_index(['A', 'B'])
df.groupby(lambda x : (x[0], x[1])).aggregate('sum')
```


### Issue Description

This issue appears to be similar to #24786 except that here a lambda is used to create the tuples. The [User Guide for Pandas](https://pandas.pydata.org/docs/user_guide/groupby.html) states that 
> The result of the aggregation will have the group names as the new index. In the case of multiple keys, the result is a MultiIndex by default.

Thus I was expecting the tuples to be automatically combined to form a ```MultiIndex``` as opposed to an ```Index``` with tuples as indices. Internally, the ```Index.map()``` [call](https://github.com/pandas-dev/pandas/blob/8a5344742c5165b2595f7ccca9e17d5eff7f7886/pandas/core/groupby/grouper.py#L511) converts the produced list of tuples to a ```MultiIndex```, but when it [produces the aggregation index](https://github.com/pandas-dev/pandas/blob/8a5344742c5165b2595f7ccca9e17d5eff7f7886/pandas/core/groupby/ops.py#L753), it appears to produce a single ``Index``` instead.

I wasn't sure if this was intended behaviour, or a bug; I apologize if it is the former!

### Expected Behavior

```python
df = pandas.DataFrame({""A"" : [1,2,3,1,2,3], ""B"": [4,5,6,4,5,6], ""C"" : [7,8,9,7,8,9]})
df = df.set_index(['A', 'B'])
df.groupby(['A', 'B']).aggregate('sum')
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.1
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.18363
machine               : AMD64
processor             : AMD64 Family 21 Model 96 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Canada.1252

pandas                : 2.2.3
numpy                 : 2.2.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None</details>
","['Bug', 'Groupby']",2024-12-24 23:37:57,2024-12-26 21:47:33,1,closed
60602,QST: Does the project consider DataFrame.query() arbitrary code execution to be a security vulnerability?,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/79304226/should-i-manually-patch-the-pandas-dataframe-query-vulnerability-or-wait-for-a

(To clarify, this question was written by another user.)

### Question about pandas

Hi, I saw [this question on StackOverflow](https://stackoverflow.com/questions/79304226/should-i-manually-patch-the-pandas-dataframe-query-vulnerability-or-wait-for-a), which is about a public CVE, [CVE-2024-9880](https://huntr.com/bounties/a49baae1-4652-4d6c-a179-313c21c41a8d).

The basic premise of the CVE is that if an attacker controls the `expr` argument to DataFrame.query(), then arbitrary code execution can be achieved.

The example given in the CVE is 

<details>

```python
import pandas as pd


df = pd.DataFrame({'a': [1, 2, 3], 'b': ['error_details', 'confidential_info', 'normal']})


query = '@pd.core.frame.com.builtins.__import__(""os"").system(""""""ping google.com #"""""")'
try:
    engine = ""python""
    result = df.query(query,local_dict={},engine=""python"",).index
except Exception as e:
    print(f'Error: {e}')
```

</details>

However, this is not minimal, and a more minimal construction would be 

```python
import pandas as pd

df = pd.DataFrame()

expr = '@pd.compat.os.system(""""""echo foo"""""")'
result = df.query(expr, engine='python')
```

(The report also says that `engine='python'` is required, but both `engine='python'` and `engine='numexpr'` worked in my testing.)

My question is about Pandas's security model. What security guarantees does Pandas make about DataFrame.query() with an attacker-controlled `expr`?

My intuition about this is ""none, don't do that,"" but I'm wondering what the Pandas project thinks.","['Usage Question', 'expressions', 'Closing Candidate']",2024-12-24 19:46:22,2024-12-26 22:47:06,29,closed
60600,BUG: loc[] returns object type instead of float,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame([['a',1.,2.],['b',3.,4.]])
df.loc[0,[1,2]].dtypes
df[[1,2]].loc[0].dtypes
```


### Issue Description

`df.loc[0,[1,2]]` results in a Series of type `dtype('O')`, while `df[[1,2]].loc[0]` results in a Series of type `dtype('float64')`.

### Expected Behavior

I would expect `df.loc[0,[1,2]]` to be of type `float64`, same as `df[[1,2]].loc[0]`. The current behavior seems to encourage chaining instead of canonical referencing.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.8
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Thu Sep 12 23:35:10 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_ARM64_T6030
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : 8.1.3
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.12.0
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.5
lxml.etree            : 5.3.0
matplotlib            : 3.10.0
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.12.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.11.0
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : N/A
pyqt5                 : None

</details>
","['Bug', 'Indexing', 'Dtype Conversions']",2024-12-23 23:43:44,2025-06-30 18:25:44,13,closed
60592,BUG: inconsistent behavior in the DataFrame.agg() when use custom aggregate function,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

data = {
    ""Product"": [""apple"", ""pear"", ""banana""],
    ""Sales"": [100, 150, 200],
    ""Price"": [1, 1.5, 2],
}
df = pd.DataFrame(data)

def func1(x):
    # x is Series
    y = x.sum()
    return y

def func2(x):
    # x is a single value
    y = 0
    return y


agg_funcs = {
    ""Sales"": func1,
    ""Price"": func2
}

df_column_spec_agg = df.agg(agg_funcs)
print(df_column_spec_agg)
```


### Issue Description

in ```func1()``` x is Series, but in ```func2()``` x is a single value.

and with an error:
```
ValueError: cannot perform both aggregation and transformation operations simultaneously
```


### Expected Behavior

```type(x)``` should be same in ```func1()``` and ```func2()```

### Installed Versions

<details>
openSUSE Leap 15.6

Python 3.12.8 (main, Dec 11 2024, 08:52:29) [GCC 13.3.0] on linux

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 6.4.0-150600.23.30-default
Version               : #1 SMP PREEMPT_DYNAMIC Sat Dec  7 08:37:53 UTC 2024 (8c25a0a)
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.2.0
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Apply']",2024-12-20 14:47:31,2024-12-20 15:11:45,1,closed
60589,BUG: Test failures with 2.2.3 on Fedora Rawhide,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
============================= test session starts ==============================
platform linux -- Python 3.13.1, pytest-8.3.4, pluggy-1.5.0 -- /usr/bin/python3
cachedir: /builddir/build/BUILD/python-pandas-2.2.3-build/pandas-2.2.3/_empty/pytest-cache
hypothesis profile 'ci' -> deadline=None, suppress_health_check=[HealthCheck.too_slow, HealthCheck.differing_executors], database=DirectoryBasedExampleDatabase(PosixPath('/builddir/build/BUILD/python-pandas-2.2.3-build/pandas-2.2.3/_empty/.hypothesis/examples'))
rootdir: /builddir/build/BUILD/python-pandas-2.2.3-build/BUILDROOT/usr/lib64/python3.13/site-packages/pandas
configfile: pyproject.toml
plugins: asyncio-0.24.0, xdist-3.6.1, hypothesis-6.104.2
asyncio: mode=Mode.STRICT, default_loop_scope=None
created: 1/1 worker
```
```
=================================== FAILURES ===================================
____________________ test_array_inference[data7-expected7] _____________________
[gw0] linux -- Python 3.13.1 /usr/bin/python3
data = [datetime.datetime(2000, 1, 1, 0, 0, tzinfo=<DstTzInfo 'CET' LMT+0:18:00 STD>), datetime.datetime(2001, 1, 1, 0, 0, tzinfo=<DstTzInfo 'CET' LMT+0:18:00 STD>)]
expected = <DatetimeArray>
['2000-01-01 00:00:00+01:00', '2001-01-01 00:00:00+01:00']
Length: 2, dtype: datetime64[ns, CET]
    @pytest.mark.parametrize(
        ""data, expected"",
        [
            # period
            (
                [pd.Period(""2000"", ""D""), pd.Period(""2001"", ""D"")],
                period_array([""2000"", ""2001""], freq=""D""),
            ),
            # interval
            ([pd.Interval(0, 1), pd.Interval(1, 2)], IntervalArray.from_breaks([0, 1, 2])),
            # datetime
            (
                [pd.Timestamp(""2000""), pd.Timestamp(""2001"")],
                DatetimeArray._from_sequence([""2000"", ""2001""], dtype=""M8[ns]""),
            ),
            (
                [datetime.datetime(2000, 1, 1), datetime.datetime(2001, 1, 1)],
                DatetimeArray._from_sequence([""2000"", ""2001""], dtype=""M8[ns]""),
            ),
            (
                np.array([1, 2], dtype=""M8[ns]""),
                DatetimeArray._from_sequence(np.array([1, 2], dtype=""M8[ns]"")),
            ),
            (
                np.array([1, 2], dtype=""M8[us]""),
                DatetimeArray._simple_new(
                    np.array([1, 2], dtype=""M8[us]""), dtype=np.dtype(""M8[us]"")
                ),
            ),
            # datetimetz
            (
                [pd.Timestamp(""2000"", tz=""CET""), pd.Timestamp(""2001"", tz=""CET"")],
                DatetimeArray._from_sequence(
                    [""2000"", ""2001""], dtype=pd.DatetimeTZDtype(tz=""CET"", unit=""ns"")
                ),
            ),
            (
                [
                    datetime.datetime(2000, 1, 1, tzinfo=cet),
                    datetime.datetime(2001, 1, 1, tzinfo=cet),
                ],
                DatetimeArray._from_sequence(
                    [""2000"", ""2001""], dtype=pd.DatetimeTZDtype(tz=cet, unit=""ns"")
                ),
            ),
            # timedelta
            (
                [pd.Timedelta(""1h""), pd.Timedelta(""2h"")],
                TimedeltaArray._from_sequence([""1h"", ""2h""], dtype=""m8[ns]""),
            ),
            (
                np.array([1, 2], dtype=""m8[ns]""),
                TimedeltaArray._from_sequence(np.array([1, 2], dtype=""m8[ns]"")),
            ),
            (
                np.array([1, 2], dtype=""m8[us]""),
                TimedeltaArray._from_sequence(np.array([1, 2], dtype=""m8[us]"")),
            ),
            # integer
            ([1, 2], IntegerArray._from_sequence([1, 2], dtype=""Int64"")),
            ([1, None], IntegerArray._from_sequence([1, None], dtype=""Int64"")),
            ([1, pd.NA], IntegerArray._from_sequence([1, pd.NA], dtype=""Int64"")),
            ([1, np.nan], IntegerArray._from_sequence([1, np.nan], dtype=""Int64"")),
            # float
            ([0.1, 0.2], FloatingArray._from_sequence([0.1, 0.2], dtype=""Float64"")),
            ([0.1, None], FloatingArray._from_sequence([0.1, pd.NA], dtype=""Float64"")),
            ([0.1, np.nan], FloatingArray._from_sequence([0.1, pd.NA], dtype=""Float64"")),
            ([0.1, pd.NA], FloatingArray._from_sequence([0.1, pd.NA], dtype=""Float64"")),
            # integer-like float
            ([1.0, 2.0], FloatingArray._from_sequence([1.0, 2.0], dtype=""Float64"")),
            ([1.0, None], FloatingArray._from_sequence([1.0, pd.NA], dtype=""Float64"")),
            ([1.0, np.nan], FloatingArray._from_sequence([1.0, pd.NA], dtype=""Float64"")),
            ([1.0, pd.NA], FloatingArray._from_sequence([1.0, pd.NA], dtype=""Float64"")),
            # mixed-integer-float
            ([1, 2.0], FloatingArray._from_sequence([1.0, 2.0], dtype=""Float64"")),
            (
                [1, np.nan, 2.0],
                FloatingArray._from_sequence([1.0, None, 2.0], dtype=""Float64""),
            ),
            # string
            (
                [""a"", ""b""],
                pd.StringDtype()
                .construct_array_type()
                ._from_sequence([""a"", ""b""], dtype=pd.StringDtype()),
            ),
            (
                [""a"", None],
                pd.StringDtype()
                .construct_array_type()
                ._from_sequence([""a"", None], dtype=pd.StringDtype()),
            ),
            # Boolean
            ([True, False], BooleanArray._from_sequence([True, False], dtype=""boolean"")),
            ([True, None], BooleanArray._from_sequence([True, None], dtype=""boolean"")),
        ],
    )
    def test_array_inference(data, expected):
        result = pd.array(data)
>       tm.assert_equal(result, expected)
../../BUILDROOT/usr/lib64/python3.13/site-packages/pandas/tests/arrays/test_array.py:377: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
left = array(['1999-12-31T23:42:00.000000000', '2000-12-31T23:42:00.000000000'],
      dtype='datetime64[ns]')
right = array(['1999-12-31T23:00:00.000000000', '2000-12-31T23:00:00.000000000'],
      dtype='datetime64[ns]')
err_msg = None
    def _raise(left, right, err_msg) -> NoReturn:
        if err_msg is None:
            if left.shape != right.shape:
                raise_assert_detail(
                    obj, f""{obj} shapes are different"", left.shape, right.shape
                )
    
            diff = 0
            for left_arr, right_arr in zip(left, right):
                # count up differences
                if not array_equivalent(left_arr, right_arr, strict_nan=strict_nan):
                    diff += 1
    
            diff = diff * 100.0 / left.size
            msg = f""{obj} values are different ({np.round(diff, 5)} %)""
>           raise_assert_detail(obj, msg, left, right, index_values=index_values)
E           AssertionError: DatetimeArray._ndarray are different
E           
E           DatetimeArray._ndarray values are different (100.0 %)
E           [left]:  [1999-12-31T23:42:00.000000000, 2000-12-31T23:42:00.000000000]
E           [right]: [1999-12-31T23:00:00.000000000, 2000-12-31T23:00:00.000000000]
../../BUILDROOT/usr/lib64/python3.13/site-packages/pandas/_testing/asserters.py:684: AssertionError
```

Full build log with more test failures is here: https://kojipkgs.fedoraproject.org//work/tasks/9043/126999043/build.log
```


### Issue Description

We are updating Fedora to pandas 2.2.3 and numpy 2.0.5 but are getting test failures.

### Expected Behavior

No test failures

### Installed Versions

<details>

2.2.3

</details>
","['Bug', 'Localization']",2024-12-19 04:30:31,2025-10-23 12:36:47,14,closed
60587,BUG: OutOfBoundsDatetime for large future dates like 9990-12-31,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame({
    'account-start': ['2017-02-03', '2017-03-03', ""9990-12-31""],
    'client': ['Alice Anders', 'Bob Baker', 'Charlie Chaplin'],
    'balance': [-1432.32, 10.43, 30000.00],
    'rank': [52, 525, 32],
})
df[""account-start""] = pd.to_datetime(df[""account-start""], format=""%Y-%m-%d"")
```


### Issue Description

Getting an error as OutOfBoundsDatetime

### Expected Behavior

it should parse the date and covert the '9990-12-31' to datetime dtype

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.13.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 58.1.0
pip                   : 24.3.1
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.5.0
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.31
tables                : None
tabulate              : 0.9.0
xarray                : 2024.6.0
xlrd                  : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2024-12-18 07:58:48,2024-12-20 10:16:47,3,closed
60583,BUG: `Timestamp.normalize()` can overflow silently,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.
    - Query: `is:issue normalize overflow`
- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.
- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.

### Issue description

I was trying to get the minimum date, so I tried `pd.Timestamp.min.normalize()` before realizing that would be out of range, but what's weird is that it overflows without warning:

```py
>>> pd.Timestamp.min
Timestamp('1677-09-21 00:12:43.145224193')
>>> pd.Timestamp.min.normalize()
Timestamp('2262-04-11 23:34:33.709551616')
```

For reference:
```py
>>> pd.Timestamp.max
Timestamp('2262-04-11 23:47:16.854775807')
```

Compare that to `.floor('D')`, which I thought would be equivalent, but it raises:

```py
>>> pd.Timestamp.min.floor('D')
OverflowError: value too large


The above exception was the direct cause of the following exception:

OutOfBoundsDatetime: Cannot round 1677-09-21 00:12:43.145224193 to freq=<Day> without overflow
```

as well as `.round('D')` (same error).

Sidenote: What I really wanted was `.ceil('D')`, which gets the first whole day:

```py
>>> pd.Timestamp.min.ceil('D')
Timestamp('1677-09-22 00:00:00')
```

### Installed Versions

<details>

```none
INSTALLED VERSIONS
------------------
commit                : d41884b2dd0823dc6288ab65d06650302e903c6b
python                : 3.12.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.4.0-202-generic
Version               : #222-Ubuntu SMP Fri Nov 8 14:45:04 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : fr_CA.UTF-8
LOCALE                : fr_CA.UTF-8

pandas                : 3.0.0.dev0+1777.gd41884b2dd
numpy                 : 2.1.0.dev0+git20240403.e59c074
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.22.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : 2024.1
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Timestamp']",2024-12-16 23:06:17,2025-09-10 18:55:12,6,closed
60581,BUG: pandas.api.types.is_datetime64_any_dtype returns True for 'M' str,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas
pandas.api.types.is_datetime64_any_dtype('M')  # True
```


### Issue Description

The string 'M' cannot be converted to a datetime using pandas.to_datetime(). It raises the error
```
Given date string M not likely a datetime present at position 0
```

I'm unsure what 'M' is supposed to represent? 'Y', 'D' all return False, so I don't think it's strftime strings, perhaps there's another significance I'm missing?

### Expected Behavior

I think the 'M' char on its own, should return False

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit           : 0f437949513225922d851e9581723d82120684a6
python           : 3.11.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.26100
machine          : AMD64
processor        : Intel64 Family 6 Model 186 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : English_United Kingdom.1252

pandas           : 2.0.3
numpy            : 1.26.4
pytz             : 2024.1
dateutil         : 2.9.0.post0
setuptools       : 68.2.2
pip              : 23.3.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 3.1.4
IPython          : 8.26.0
pandas_datareader: None
bs4              : 4.12.3
bottleneck       : None
brotli           : None
fastparquet      : None
fsspec           : None
gcsfs            : None
matplotlib       : None
numba            : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : None
snappy           : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
zstandard        : None
tzdata           : 2024.1
qtpy             : None
pyqt5            : None

</details>
","['Bug', 'Datetime', 'Closing Candidate']",2024-12-16 14:58:52,2024-12-16 22:24:24,3,closed
60580,"BUG: when I assign value of 1-dim np.array holding single instance, it results in 0-dim array instance","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
arr = np.array(['one'], dtype=""object"")
df = pd.DataFrame({'col1': [None]}, index=[100])
df.at[100, 'col1'] = arr
```


### Issue Description

when I assign value of 1-dim np.array holding single instance, it results in 0-dim array instance:

>>> df.at[100, 'col1']
array('one', dtype=object)

### Expected Behavior

>>> df.at[100, 'col1']
array(['one'], dtype=object)

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.7
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.2.0
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage', 'Nested Data']",2024-12-16 14:32:17,2024-12-16 17:28:46,3,closed
60578,BUG: Reading Excel File - unsupported operand type(s) for +: 'NoneType' and 'int',"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
LakehousePath = f'abfss://{Workspace}@onelake.dfs.fabric.microsoft.com/Bronze.Lakehouse/'

if len(SourceFileList) > 0:
    for SourceFile in SourceFileList:
        LakeHouseFilePath = LakehousePath + SourceFile

        PandasDataFrame = pandas.read_excel(LakeHouseFilePath, engine='openpyxl', sheet_name=0, header=None, skiprows=0, usecols='B,C,D', dtype={'B:str','C:np.float32','D:np.float32'}, na_filter=False)
```


### Issue Description

If its a valid excel spreadsheet then it should load - there shouldnt be any need to any fiddling about - at the very least to STR

Just because a CELL has a formula in it that points to a BLANK cell should NOT cause a failure

Just because a CELL has  a DIV/0 Error in it should NOT cause failure

### Expected Behavior

The Excel file loads 

### Installed Versions

Whatever is the latest in Fabric
","['Bug', 'IO Excel']",2024-12-16 00:53:23,2024-12-17 13:53:40,6,closed
60576,DOC: fix docstring validation errors for pandas.core and pandas.errors,"> follow up on issues  #59698, #59458 and #58063 pandas has a script for validating docstrings:
> 
> https://github.com/pandas-dev/pandas/blob/b0192c70610a9db593968374ea60d189daaaccc7/ci/code_checks.sh#L86-L100
> 
> Currently, some methods fail docstring validation check. The task here is:
> 
> * take 2-4 methods
> * run: `scripts/validate_docstrings.py <method-name>`
> * run `pytest pandas/tests/scalar/test_nat.py::test_nat_doc_strings`
> * fix the docstrings according to whatever error is reported
> * remove those methods from `code_checks.sh` script
> * commit, push, open pull request
> 
> Example:
> 
> ```
>  scripts/validate_docstrings.py pandas.Timedelta.ceil
> ```
> 
> pandas.Timedelta.ceil fails with the SA01 and ES01 errors
> 
> ```
> ################################################################################
> ################################## Validation ##################################
> ################################################################################
> 
> 2 Errors found for `pandas.Timedelta.ceil`:
>         ES01    No extended summary found
>         SA01    See Also section not found
> ```
> 
> Please don't comment `take` as multiple people can work on this issue. You also don't need to ask for permission to work on this, just comment on which methods are you going to work.
> 
> If you're new contributor, please check the [contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)
> 
> thanks @natmokval for giving me the idea for this issue.",['Closing Candidate'],2024-12-15 14:08:53,2025-01-13 15:31:59,1,closed
60575,DOC: fix docstring validation errors for pandas.core and pandas.errors,"> follow up on issues  #59698, #59458 and #58063 pandas has a script for validating docstrings:
> 
> https://github.com/pandas-dev/pandas/blob/b0192c70610a9db593968374ea60d189daaaccc7/ci/code_checks.sh#L86-L100
> 
> Currently, some methods fail docstring validation check. The task here is:
> 
> * take 2-4 methods
> * run: `scripts/validate_docstrings.py <method-name>`
> * run `pytest pandas/tests/scalar/test_nat.py::test_nat_doc_strings`
> * fix the docstrings according to whatever error is reported
> * remove those methods from `code_checks.sh` script
> * commit, push, open pull request
> 
> Example:
> 
> ```
>  scripts/validate_docstrings.py pandas.Timedelta.ceil
> ```
> 
> pandas.Timedelta.ceil fails with the SA01 and ES01 errors
> 
> ```
> ################################################################################
> ################################## Validation ##################################
> ################################################################################
> 
> 2 Errors found for `pandas.Timedelta.ceil`:
>         ES01    No extended summary found
>         SA01    See Also section not found
> ```
> 
> Please don't comment `take` as multiple people can work on this issue. You also don't need to ask for permission to work on this, just comment on which methods are you going to work.
> 
> If you're new contributor, please check the [contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)
> 
> thanks @natmokval for giving me the idea for this issue.",[],2024-12-15 13:58:58,2024-12-15 14:07:24,0,closed
60573,BUG: NameError: name 'pa' is not defined despite `pyarrow` is installed,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pyarrow as pa

data = {
    ""foo"": [""A"", ""B"", ""C""],
    ""prop"": [0.5, 0.8, 0.7]
}

df = pd.DataFrame(data)

df[""prop""].astype('float64[pyarrow]')
```


### Issue Description

Cannot change a column type to `float64[pyarrow]`. The error I get is:
```
NameError: name 'pa' is not defined
```

### Expected Behavior

I'd expect to get the column converted properly
```
0    0.5
1    0.8
2    0.7
Name: prop, dtype: double[pyarrow]
```

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.8
python-bits           : 64
OS                    : Darwin
OS-release            : 24.1.0
Version               : Darwin Kernel Version 24.1.0: Thu Oct 10 21:02:45 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Closing Candidate', 'Arrow']",2024-12-15 11:15:59,2025-08-05 16:52:15,11,closed
60568,BUG:  ValueError when printing a DataFrame's column having a DataFrame in its attrs,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

n=50

a = pd.DataFrame(np.random.randint(0, 10, size=(n,n)))
b = pd.DataFrame(np.random.randint(0, 10, size=(5,5)))

a.attrs['b'] = b

a[0]
```


### Issue Description

same error as in #60455 (which was already solved for DataFrames by #60459) but here output is a Series

### Expected Behavior

selected column is printed abbreviated form, no KeyError

<details>
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/core/series.py"", line 1437, in __repr__
    return self.to_string(**repr_params)
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/util/_decorators.py"", line 332, in wrapper
    return func(*args, **kwargs)
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/core/series.py"", line 1534, in to_string
    formatter = fmt.SeriesFormatter(
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/io/formats/format.py"", line 226, in __init__
    self._chk_truncate()
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/io/formats/format.py"", line 248, in _chk_truncate
    series = concat((series.iloc[:row_num], series.iloc[-row_num:]))
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/core/reshape/concat.py"", line 440, in concat
    return _get_result(
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/core/reshape/concat.py"", line 547, in _get_result
    return result.__finalize__(
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/core/generic.py"", line 6070, in __finalize__
    have_same_attrs = all(obj.attrs == attrs for obj in objs[1:])
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/core/generic.py"", line 6070, in <genexpr>
    have_same_attrs = all(obj.attrs == attrs for obj in objs[1:])
  File ""/py/envs/pandas-dev/lib/python3.10/site-packages/pandas/core/generic.py"", line 1496, in __bool__
    raise ValueError(
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</details>

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 9501650e22767f8502a1e3edecfaf17c5769f150
python                : 3.10.16
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.10-2-MANJARO
Version               : #1 SMP PREEMPT_DYNAMIC Mon, 25 Nov 2024 05:29:44 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8

pandas                : 0+untagged.1.g9501650
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : 3.0.11
sphinx                : 8.1.3
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.11.0
fsspec                : 2024.10.0
html5lib              : 1.1
hypothesis            : 6.122.3
gcsfs                 : 2024.10.0
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.4
numba                 : 0.60.0
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 18.1.0
pyreadstat            : 1.2.8
pytest                : 8.3.4
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : 1.0.10
s3fs                  : 2024.10.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'metadata']",2024-12-14 20:47:46,2024-12-16 19:03:09,1,closed
60567,BUG: groupby with dropna=False and pa.dictionary drops NA values,"```python
df = pd.DataFrame({'A': ['a1', pd.NA]}, dtype=pd.ArrowDtype(pa.dictionary(pa.int32(), pa.utf8())))
print(df.groupby(""A"", dropna=False)[[""A""]].first())
#      A
# A     
# a1  a1
```

There should be a 2nd row with the NA value since `dropna=False`.","['Groupby', 'Missing-data', 'Arrow']",2024-12-14 16:29:58,2025-02-24 23:29:41,0,closed
60564,BUG: The isna function returns False for NaN values in a column of type 'double [pyarrow]'.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df_arrow = pd.DataFrame([[0, 0]], dtype=""double[pyarrow]"", columns=['a', 'b'])
df_arrow['c'] = df_arrow.a / df_arrow.b
df_arrow.isna()
```


### Issue Description

The isna function returns False for NaN values in the column `c`, which is of type `double [pyarrow]`. The output of reproducible example is:

```
       a      b      c
0  False  False  False
```
It is clear that column `c` is 0/0, which is NaN, so it should be `True`. 

Furthermore, if I assign the variable `n` to reference to column `c` and call pd.isna, it returns True.

```
n = df_arrow.iloc[0,2]
pd.isna(n)
```
Output of this case is `True`.

### Expected Behavior

If I take use of numpy instead of pyarrow as dtype backend, it's as expected.
Expected behavior is:
```
import pandas as pd
df_np = pd.DataFrame([[0, 0]], columns=['a', 'b'])
df_np['c'] = df_np.a / df_np.b
df_np.isna()
```
The output is:
```
       a      b     c
0  False  False  True
```


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Fri Nov 15 15:12:37 PST 2024; root:xnu-10063.141.1.702.7~1/RELEASE_ARM64_T6030
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : zh_CN.UTF-8
LOCALE                : zh_CN.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : 8.24.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Duplicate Report', 'Arrow', 'PDEP missing values']",2024-12-14 10:44:50,2024-12-14 12:48:31,1,closed
60563,BUG: value_counts() returns error/wrong result with PyArrow categorical columns with nulls,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pyarrow as pa

# First case: just one column. It gives the error below
pd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )

# Second case: more than one column. It gives the wrong result below
pd.concat( [
    pd.DataFrame( { 'A': [ 'a1', 'a2' ], 'B': [ 'b1', pd.NA ] }, dtype = pd.ArrowDtype( pa.string() ) ),
    pd.DataFrame( { 'C': [ 'c1', 'c2' ], 'D': [ 'd1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) )
], axis = 1 ).value_counts( dropna = False )
```


### Issue Description

### First Case
It gives the following error:

```python-traceback
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[6], line 1
----> 1 pd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )

File C:\Python\Lib\site-packages\pandas\core\frame.py:7519, in DataFrame.value_counts(self, subset, normalize, sort, ascending, dropna)
   7517 # Force MultiIndex for a list_like subset with a single column
   7518 if is_list_like(subset) and len(subset) == 1:  # type: ignore[arg-type]
-> 7519     counts.index = MultiIndex.from_arrays(
   7520         [counts.index], names=[counts.index.name]
   7521     )
   7523 return counts

File C:\Python\Lib\site-packages\pandas\core\indexes\multi.py:533, in MultiIndex.from_arrays(cls, arrays, sortorder, names)
    530     if len(arrays[i]) != len(arrays[i - 1]):
    531         raise ValueError(""all arrays must be same length"")
--> 533 codes, levels = factorize_from_iterables(arrays)
    534 if names is lib.no_default:
    535     names = [getattr(arr, ""name"", None) for arr in arrays]

File C:\Python\Lib\site-packages\pandas\core\arrays\categorical.py:3069, in factorize_from_iterables(iterables)
   3065 if len(iterables) == 0:
   3066     # For consistency, it should return two empty lists.
   3067     return [], []
-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))
   3070 return list(codes), list(categories)

File C:\Python\Lib\site-packages\pandas\core\arrays\categorical.py:3069, in <genexpr>(.0)
   3065 if len(iterables) == 0:
   3066     # For consistency, it should return two empty lists.
   3067     return [], []
-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))
   3070 return list(codes), list(categories)

File C:\Python\Lib\site-packages\pandas\core\arrays\categorical.py:3042, in factorize_from_iterable(values)
   3037     codes = values.codes
   3038 else:
   3039     # The value of ordered is irrelevant since we don't use cat as such,
   3040     # but only the resulting categories, the order of which is independent
   3041     # from ordered. Set ordered to False as default. See GH #15457
-> 3042     cat = Categorical(values, ordered=False)
   3043     categories = cat.categories
   3044     codes = cat.codes

File C:\Python\Lib\site-packages\pandas\core\arrays\categorical.py:451, in Categorical.__init__(self, values, categories, ordered, dtype, fastpath, copy)
    447 if dtype.categories is None:
    448     if isinstance(values.dtype, ArrowDtype) and issubclass(
    449         values.dtype.type, CategoricalDtypeType
    450     ):
--> 451         arr = values._pa_array.combine_chunks()
    452         categories = arr.dictionary.to_pandas(types_mapper=ArrowDtype)
    453         codes = arr.indices.to_numpy()

AttributeError: 'Index' object has no attribute '_pa_array'
```
Indeed, the same error is returned also if no `pd.NA` is present.

### Second case
It gives the following result:

```python
A   B     C   D 
a1  b1    c1  d1    1
a2  <NA>  c2  d1    1
Name: count, dtype: int64
```

**Note that in second line D is d1 and not `<NA>`.**

A more complete example in this JupyterLab notebook: [value_counts() Bug.pdf](https://github.com/user-attachments/files/18133225/value_counts.Bug.pdf)


### Expected Behavior

The expected behavior is analogous to the result obtained with the NumPy backend.

### First case

```python
A  
a1     1
<NA>    1
Name: count, dtype: int64
```

### Second case

```python
A   B     C   D 
a1  b1    c1  d1    1
a2  <NA>  c2  <NA>    1
Name: count, dtype: int64
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.8
python-bits           : 64
OS                    : Windows
OS-release            : 2019Server
Version               : 10.0.17763
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.29.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Algos', 'Categorical', 'good first issue', 'Arrow']",2024-12-14 00:59:58,2025-02-19 17:13:50,10,closed
60554,BUG: alpine 3.21 install issue,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
using docker image python:3.11.11-alpine3.21

then running pip3 install pandas==1.4.2

gives error about C source compilation issue while using python:3.11.11-alpine3.20 is ok
```


### Issue Description

see above

### Expected Behavior

install works

### Installed Versions

n/a","['Bug', 'Build', 'Needs Info', 'Closing Candidate']",2024-12-12 21:32:29,2025-08-05 16:23:35,1,closed
60552,ENH: sklearn.model_selection.ShuffleSplit compatible wit pandas.MultiIndex,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could smoothly apply `sklearn.model_selection.ShuffleSplit `  with a `pandas.Dataframe` that has a `pandas.MultiIndex`. I tried to apply it, but it throws some weird `KeyError`.

### Feature Description

 This is my pseudo code how I would expect it to work:
```
from sklearn.model_selection import ShuffleSplit
from sktime.datatypes import get_examples
df = get_examples(mtype=""pd-multiindex"", as_scitype=""Panel"")[0]
splitter = ShuffleSplit(n_splits=3, random_state=42)
split = splitter.split(df.index.levels[0]) 
train_indexes = []
test_indexes = []
for train_index, test_index in split:
    train_indexes.append(train_index)
    test_indexes.append(test_index)
x_train, x_test = (df.loc[train_indexes[0]], df.loc[test_indexes[0]])
```

### Alternative Solutions

Currently I only to a train_test_split as described [here](https://gist.github.com/shaypal5/3e34e85bd89d65d4ac118daa9a42b174) without any cross validation.

### Additional Context

It would also be nice to extend this to the other split methods of sklearn.","['Enhancement', 'MultiIndex', 'Needs Info']",2024-12-12 14:36:28,2024-12-13 14:25:08,2,closed
60550,ENH: Passing a single value to `.describe(percentiles = [0.25])` returns 25th- and 50th-percentile,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

# creating a single series dataframe
frame = pd.DataFrame(np.array([1, 2, 3, 4, 5, 100]))

# getting the describe with single percentile value
frame.describe(percentiles = [0.25])
```


### Issue Description

Using a single percentile value below 50 for `percentiles` for data frame describe function returns 50th percentile data by default, while the same is not reflected when the value is more than 50.

```python
# considering the above dataframe in example
>>> frame.describe(percentiles = [0.25])
                0
count    6.000000
mean    19.166667
std     39.625329
min      1.000000
25%      2.250000
50%      3.500000
max    100.000000
>>> frame.describe(percentiles = [0.35])
                0
count    6.000000
mean    19.166667
std     39.625329
min      1.000000
35%      2.750000
50%      3.500000
max    100.000000
>>> frame.describe(percentiles = [0.51])
                0
count    6.000000
mean    19.166667
std     39.625329
min      1.000000
50%      3.500000
51%      3.550000
max    100.000000
```

### Expected Behavior

Should return only given percentile value instead.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.4
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_India.1252

pandas                : 2.2.3
numpy                 : 2.2.0
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Enhancement', 'good first issue']",2024-12-12 09:20:43,2025-03-21 21:13:05,11,closed
60542,DOC: Incorrect deprecation example for `is_period_dtype`,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.api.types.is_period_dtype.html#pandas.api.types.is_period_dtype

### Documentation problem

Suggests the user use `isinstance(dtype, pd.Period)` instead, when they really need to use `Use isinstance(dtype, pd.PeriodDtype)`

### Suggested fix for documentation

Update message to reference correct class",['Docs'],2024-12-11 19:12:28,2024-12-17 01:51:52,0,closed
60534,BUG: to_json overflows when date does not fit in NS date. ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import datetime as dt
df = pd.DataFrame(data={'date': [dt.datetime(2999, 1, 1)]}).to_json()
```


### Issue Description

When converting dataframes to json then dates that do not fit inside a NS precision timestamp will overflow. 

```
---------------------------------------------------------------------------
OverflowError                             Traceback (most recent call last)
Cell In[46], [line 1](vscode-notebook-cell:?execution_count=46&line=1)
----> [1](vscode-notebook-cell:?execution_count=46&line=1) df = pd.DataFrame(data={'date': [dt.datetime(2999, 1, 1)]}).to_json()

File ~/code/datastores/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)
    [327](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:327) if len(args) > num_allow_args:
    [328](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:328)     warnings.warn(
    [329](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:329)         msg.format(arguments=_format_argument_list(allow_args)),
    [330](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:330)         FutureWarning,
    [331](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:331)         stacklevel=find_stack_level(),
    [332](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:332)     )
--> [333](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333) return func(*args, **kwargs)

File ~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2702, in NDFrame.to_json(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)
   [2699](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2699) config.is_nonnegative_int(indent)
   [2700](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2700) indent = indent or 0
-> [2702](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2702) return json.to_json(
   [2703](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2703)     path_or_buf=path_or_buf,
   [2704](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2704)     obj=self,
   [2705](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2705)     orient=orient,
   [2706](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2706)     date_format=date_format,
   [2707](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2707)     double_precision=double_precision,
   [2708](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2708)     force_ascii=force_ascii,
   [2709](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2709)     date_unit=date_unit,
   [2710](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2710)     default_handler=default_handler,
   [2711](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2711)     lines=lines,
   [2712](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2712)     compression=compression,
   [2713](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2713)     index=index,
   [2714](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2714)     indent=indent,
   [2715](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2715)     storage_options=storage_options,
   [2716](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2716)     mode=mode,
   [2717](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2717) )

File ~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:210, in to_json(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)
    [197](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:197) else:
    [198](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:198)     raise NotImplementedError(""'obj' should be a Series or a DataFrame"")
    [200](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:200) s = writer(
    [201](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:201)     obj,
    [202](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:202)     orient=orient,
    [203](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:203)     date_format=date_format,
    [204](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:204)     double_precision=double_precision,
    [205](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:205)     ensure_ascii=force_ascii,
    [206](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:206)     date_unit=date_unit,
    [207](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:207)     default_handler=default_handler,
    [208](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:208)     index=index,
    [209](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:209)     indent=indent,
--> [210](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:210) ).write()
    [212](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:212) if lines:
    [213](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:213)     s = convert_to_line_delimits(s)

File ~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:263, in Writer.write(self)
    [261](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:261) def write(self) -> str:
    [262](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:262)     iso_dates = self.date_format == ""iso""
--> [263](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:263)     return ujson_dumps(
    [264](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:264)         self.obj_to_write,
    [265](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:265)         orient=self.orient,
    [266](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:266)         double_precision=self.double_precision,
    [267](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:267)         ensure_ascii=self.ensure_ascii,
    [268](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:268)         date_unit=self.date_unit,
    [269](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:269)         iso_dates=iso_dates,
    [270](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:270)         default_handler=self.default_handler,
    [271](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:271)         indent=self.indent,
    [272](https://vscode-remote+ssh-002dremote-002bepyc-002dts.vscode-resource.vscode-cdn.net/home/skaae/code/datastores/~/code/datastores/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:272)     )

OverflowError: Overflow occurred in npy_datetimestruct_to_datetime
```

### Expected Behavior

Convert date to json

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-49-generic
Version               : #49-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov  4 02:06:24 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.9.3
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.10
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : None
tabulate              : None
xarray                : 2024.11.0
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-12-10 11:02:00,2024-12-10 11:32:25,1,closed
60522,BUG: `ListAccessor` does not preserve series name,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pyarrow as pa

df = pd.DataFrame({
    ""a"": [[1,2,3], [4,5]],
    ""b"": [""hello"", ""pandas""]
}).astype({""a"": pd.ArrowDtype(pa.list_(pa.int64()))})

df[""a""].list.len().name, df[""b""].str.len().name
# (None, ""b"")
```


### Issue Description

`ListAccessor` nullifies the series name (`list.len()` is just an example, other available methods lead to the same outcome).

### Expected Behavior

Series name to be preserved, as per other accessor (see `.str.len()` in the example)

### Installed Versions

TL;DR for relevant:
pandas                : 2.2.3
pyarrow               : 18.1.0
numpy                 : 2.1.3


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.2
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.133.1-microsoft-standard-WSL2
Version               : #1 SMP Thu Oct 5 21:02:42 UTC 2023
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 8.29.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
html5lib              : None
hypothesis            : 6.122.1
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.1.0
pyreadstat            : None
pytest                : 8.3.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None","['Bug', 'Series', 'Arrow']",2024-12-07 22:26:50,2024-12-09 18:35:23,2,closed
60517,DOC: Convert v to conv_val in function for pytables.py,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

pandas\pandas\core\computation\pytables.py

### Documentation problem

Many instances of just v in this function. Wanted to clarify throughout

### Suggested fix for documentation

Change v to conv_val","['Clean', 'Dependencies']",2024-12-07 07:58:29,2024-12-09 18:32:31,1,closed
60515,DOC: methods in see also section in the pandas.DataFrame.shape and  pandas.DataFrame.ndim are not hyperlinks,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ndim.html

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html


### Documentation problem

In the see also section the ndarray.shape and ndarray.ndim method is listed, but it is not hyperlinks and thus the reader cannot navigate with ease but has to look for them instead.


### Suggested fix for documentation

Add numpy.ndarray.shape and numpy.ndarray.ndim in the docstring.
",['Docs'],2024-12-07 06:39:22,2024-12-08 14:04:33,1,closed
60506,BUG: pandas.to_datetime produces wrong/strange results on 32-bit float data for 6-column format,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from datetime import datetime, UTC
import pandas as pd

start = datetime(2024, 1, 1)
end = datetime(2025, 1, 1)
samples = 10


df = pd.DataFrame([
    [  2024,  1,  7, 11, 42, 13],
    [  2024,  9, 19, 11, 54, 20],
    [  2024,  9, 17,  1, 22,  0],
    [  2024,  1, 24, 21, 59, 55],
    [  2024,  6, 15, 12, 27, 30],
    [  2024,  9, 26, 23, 58, 26],
    [  2024,  6,  6,  0, 19, 59],
    [  2024,  1,  8,  2,  7, 43],
    [  2024,  2, 16, 16, 20, 13],
    [  2024, 12, 22, 23, 54,  4]])

df.columns = ['year', 'month', 'day', 'hour', 'minute', 'second']


ts = pd.to_datetime(df, utc=True)
ts32 = pd.to_datetime(df.astype('float32'), utc=True)
ts64 = pd.to_datetime(df.astype('float64'), utc=True)

print (ts - ts32)

assert ts.equals(ts64)
assert ts.equals(ts32)
```


### Issue Description

When constructing datetime from 6-column format, and the data is stored at 32-bit floats pandas.to_datetime silently produces strange (off by one day) results. 
pandas.to_datetime should either produce correct results or throw an Exception. Correct results would be preferred :)

### Expected Behavior

from datetime import datetime, UTC
import pandas as pd

start = datetime(2024, 1, 1)
end = datetime(2025, 1, 1)
samples = 10


df = pd.DataFrame([
    [  2024,  1,  7, 11, 42, 13],
    [  2024,  9, 19, 11, 54, 20],
    [  2024,  9, 17,  1, 22,  0],
    [  2024,  1, 24, 21, 59, 55],
    [  2024,  6, 15, 12, 27, 30],
    [  2024,  9, 26, 23, 58, 26],
    [  2024,  6,  6,  0, 19, 59],
    [  2024,  1,  8,  2,  7, 43],
    [  2024,  2, 16, 16, 20, 13],
    [  2024, 12, 22, 23, 54,  4]])

df.columns = ['year', 'month', 'day', 'hour', 'minute', 'second']


ts = pd.to_datetime(df, utc=True)
ts32 = pd.to_datetime(df.astype('float32'), utc=True)
ts64 = pd.to_datetime(df.astype('float64'), utc=True)

print (ts - ts32)

assert ts.equals(ts64)
assert ts.equals(ts32)


### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.7
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_Denmark.1252

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime']",2024-12-06 09:06:18,2024-12-09 18:46:15,2,closed
60504,BUG:  concatenation is allowing duplicate columns when axis=1,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df1=pd.DataFrame({'a':1,'b':2},index=[0])
df2=pd.DataFrame({'a':1,'b':2},index=[0])
df_new=pd.concat([df1,df2],axis=1)
```


### Issue Description

When concatenation is done by axis=1 the duplicate columns are added up without raising an error then this will be very confusing as these are not identified as different 

### Expected Behavior

If the columns are the same before concatenating then in new df these should be some unique identifier 
let's say: a(1), b(1) like this 

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Reshaping']",2024-12-06 05:52:50,2024-12-08 14:20:04,8,closed
60500,ENH: 3D Dataframe,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I had a 3rd dimension for data

### Feature Description

Create a new class that could handle this. Similar to the existing Dataframe just allowing a third dimension

### Alternative Solutions

Put it into a list or Series like object and require the ""coordinate"" of the dataframe you want to get

### Additional Context

- `__str__` would return the foremost Dataframe
- `print(x,y,z)` would print like how it currently does, would work like `print(dataframe.get_coord(x,y,z))`
- `to_*` and `from_*` would then allow for the data to be responded in various ways e.g. [{x1:y1, x2:y2} for row in z] etc
- `get_coord(x,y,z)` would return the data at that coordinate with a `None` working as a return that dataframe e.g. `get_coord(1,5,None)` would return the series of x=1 y=5","['Enhancement', 'Needs Triage']",2024-12-05 19:50:36,2024-12-05 20:24:41,2,closed
60499,BUG: Presence of `pd.NA` value in `pd.Series` prevents values from being rounded,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.Series([1.123, 2.123, pd.NA]).round(0)
```


### Issue Description

Presence of `pd.NA` value in `pd.Series` prevents values from being rounded.
```
>>> pd.Series([1.123, 2.123, pd.NA]).round(0)
0    1.123
1    2.123
2     <NA>
dtype: object
```

### Expected Behavior

I would expect the same behavior as with `np.nan` values:
```
>>> pd.Series([1.123, 2.123, np.nan]).round(0)
0    1.0
1    2.0
2    NaN
dtype: float64
```

### Installed Versions

<details>

/Users/<USER>/miniforge3/envs/nlp/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.16.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 24.1.0
Version               : Darwin Kernel Version 24.1.0: Thu Oct 10 21:05:14 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 1.25.2
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 68.2.2
pip                   : 23.2.1
Cython                : None
pytest                : 7.4.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.3
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.16.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.10.0
gcsfs                 : None
matplotlib            : 3.8.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 14.0.1
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Constructors']",2024-12-05 18:29:58,2024-12-05 21:22:37,1,closed
60498,BUG: MultiIndex alignment solved for rows but not columns,"### Pandas version checks
- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.
- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas)

### Issue Description
Hi the alignment of row MultiIndex was solved with #54875 thank you,
but it could it please work for a column MultiIndex as well. The same `AssertionError: Length of new_levels (2) must be <= self.nlevels (1)` still happens for the 2nd half of my example.

### Reproducible Example
```python
# Adjusted from  https://github.com/pandas-dev/pandas/issues/54875
# that has been solved for row MultiIndex.
# Would like it to work for column MultiIndex as well.
import pandas as pd
import numpy as np

# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
# This part is just to create the example dataframes. It could be more elegant.

Lev1L = [""1A"", ""1B""]
Lev2L = [""2A"", ""2B""]
rix1 = pd.MultiIndex.from_product([Lev1L, Lev2L], names=[""Lev1"", ""Lev2""])
rix2 = pd.MultiIndex.from_product([Lev1L], names=[""Lev1""])
ColL = [""C1"", ""C2""]
df1 = pd.DataFrame(data=100, index=rix1, columns=ColL)
df2 = pd.DataFrame(data=np.array([[0.1, 0.2], [0.25, 0.45]]), index=rix2, columns=ColL)
df1.columns.name = ""Lev10""
df2.columns.name = ""Lev10""
df1t = df1.T
df2t = df2.T

# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
# ROW alignment .. works

print(""\n"" + ""-"" * 10 + "" ROW MultiIndex\n"")
print(df1)
print()
print(df2)
df3 = df1.mul(df2)
print()
print(df3)

# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
# COL alignment .. fails

print(""\n"" + ""-"" * 10 + "" COL MultiIndex\n"")
print(df1t)
print()
print(df2t)

df3t = df1t.mul(df2t)  #  AssertionError: Length of new_levels (2) must be <= self.nlevels (1)
print()
print(df3t)

""""""
---------- ROW MultiIndex

Lev10       C1   C2
Lev1 Lev2
1A   2A    100  100
     2B    100  100
1B   2A    100  100
     2B    100  100

Lev10    C1    C2
Lev1
1A     0.10  0.20
1B     0.25  0.45

Lev10        C1    C2
Lev1 Lev2
1A   2A    10.0  20.0
     2B    10.0  20.0
1B   2A    25.0  45.0
     2B    25.0  45.0

---------- COL MultiIndex

Lev1    1A        1B
Lev2    2A   2B   2A   2B
Lev10
C1     100  100  100  100
C2     100  100  100  100

Lev1    1A    1B
Lev10
C1     0.1  0.25
C2     0.2  0.45
""""""
```

### Expected Behavior
```python
""""""
Lev1    1A          1B
Lev2    2A   2B     2A    2B
Lev10
C1     10.0  10.0   25.0  25.0
C2     20.0  20.0   45.0  45.0
""""""
```

### Installed versions
```
commit : https://github.com/pandas-dev/pandas/commit/a36c44e129bd2f70c25d5dec89cb2893716bdbf6
python : 3.12.6
python-bits : 64
OS : Windows
OS-release : 11
Version : 10.0.22631
machine : AMD64
processor : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder : little
LC_ALL : None
LANG : None
LOCALE : English_South Africa.1252

pandas : 3.0.0.dev0+1757.ga36c44e129
numpy : 2.1.3
dateutil : 2.9.0.post0
pip : 24.2
Cython : None
sphinx : None
IPython : None
adbc-driver-postgresql: None
adbc-driver-sqlite : None
bs4 : None
blosc : None
bottleneck : None
fastparquet : None
fsspec : None
html5lib : None
hypothesis : None
gcsfs : None
jinja2 : None
lxml.etree : None
matplotlib : None
numba : None
numexpr : None
odfpy : None
openpyxl : None
psycopg2 : None
pymysql : None
pyarrow : None
pyreadstat : None
pytest : None
python-calamine : None
pytz : 2024.2
pyxlsb : None
s3fs : None
scipy : None
sqlalchemy : None
tables : None
tabulate : None
xarray : None
xlrd : None
xlsxwriter : None
zstandard : None
tzdata : 2024.2
qtpy : None
pyqt5 : None
```","['Bug', 'Indexing']",2024-12-05 14:21:26,2025-01-15 16:19:39,3,closed
60497,DOC: Typo in user guide merge section,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/user_guide/merging.html

### Documentation problem

There is a typo in the keys section summary. it currently says "" The keys argument cane override the column names when creating a new DataFrame based on existing Series"" 

### Suggested fix for documentation

I believe ""cane"" should be ""can"".","['Docs', 'Reshaping']",2024-12-05 13:18:12,2024-12-05 21:25:21,3,closed
60496,ENH: astype(object) does not convert numpy strings to str,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas
import numpy 

df_with_numpy_values = pandas.DataFrame(
    {
        ""col_int"": [numpy.int64(1), numpy.int64(2)],
        ""col_float"": [numpy.float64(1.5), numpy.float64(2.5)],
        ""col_bool"": [numpy.bool_(True), numpy.bool_(False)],
        ""col_str"": [numpy.str_(""a""), numpy.str_(""b"")],
    }
)

df_as_object = df_with_numpy_values.astype(object)

for column in df_as_object.columns:
    for value in df_as_object[column]:
        assert type(value) in (
            int,
            float,
            str,
            bool,
        ), f""Value {value} in column {column} is not a Python type, but {type(value)}""
```


### Issue Description

Calling .astype(object) on a dataframe with numpy values converts the types of the values to the python equivalents, except for numy.str_. 

### Expected Behavior

I would expect that values with numpy.str_ would be turned into str.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-126-generic
Version               : #136-Ubuntu SMP Wed Nov 6 10:38:22 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 22.0.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
",['Strings'],2024-12-05 13:02:11,2025-07-26 19:03:29,4,closed
60495,BUG: Building pandas via pip fails in AIX due to use of lower meson version,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
If someone build master branch of pandas via pip in AIX, then they will receive the error as follows:

 pip3.9 install pandas -v

mdlist = compiler.get_command_to_archive_shlib()
    File ""/tmp/pip-build-env-y55d21vm/overlay/lib/python3.9/site-packages/mesonbuild/compilers/compilers.py"", line 948, in get_command_to_archive_shlib
      return self.linker.get_command_to_archive_shlib()
  AttributeError: 'NoneType' object has no attribute 'get_command_to_archive_shlib'
  The Meson build system
  Version: 1.2.1
  Source dir: /tmp
Found ninja-1.11.1 at /opt/freeware/bin/ninja

  Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
  /tmp/pip-build-env-y55d21vm/overlay/bin/meson compile -C .

  ERROR: Unhandled python exception

      This is a Meson bug and should be reported!
  error: subprocess-exited-with-error
  
  Preparing metadata (pyproject.toml) did not run successfully.
  exit code: 2
  
  See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: /opt/freeware/bin/python3.9 /opt/freeware/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpd79b_9px
  cwd: /tmp/pip-install-ihktll3n/pandas_3bab19dae760400388e20f1843d43267
  Preparing metadata (pyproject.toml) ... error
error: metadata-generation-failed

Encountered error while generating package metadata.

See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```


### Issue Description

This is a Python exception that happened, which I have fixed [here](https://github.com/mesonbuild/meson/pull/12352)

### Expected Behavior

pip3.9 install pandas should install pandas in AIX.

I would kindly request the community to increase the minimum required version to 1.6.0 if possible, and this will build Pandas in AIX and other targets as well. 

I tested with making the change in pyproject.toml from ""meson==1.2.1"" to ""meson==1.6.0"" in AIX and it works.

Requesting the community to kindly consider this, and I will raise a pull request once you are okay.



","['Build', 'Dependencies']",2024-12-05 09:17:26,2025-01-01 01:43:47,7,closed
60494,"BUG: df.query(""`1` == 0.1"") failed with UndefinedVariableError: name 'BACKTICK_QUOTED_STRING_1' is not defined","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({1:[0.1, 0.2]})
df.query(""`1` == 0.1"")
```


### Issue Description

The above code raise a UndefinedVariableError: name 'BACKTICK_QUOTED_STRING_1' is not defined


### Expected Behavior

returns the first row

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.14
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-122-generic
Version               : #132-Ubuntu SMP Thu Aug 29 13:45:52 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.0
Cython                : 3.0.7
sphinx                : 7.3.7
IPython               : 8.25.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : 8.2.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : 2.0.31
tables                : 3.9.2
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None
</details>
","['Bug', 'expressions']",2024-12-05 02:15:27,2025-04-01 16:50:17,2,closed
60492,DOC: Fix the navigation bar,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html#

### Documentation problem

pandas.DataFrame.head and pandas.DataFrame.tail appear twice in the navigation bar, and they jump to the same location.

### Suggested fix for documentation

I think the duplication in the navigation bar is a small bug, consider removing duplicate links in the documentation.","['Docs', 'Needs Triage']",2024-12-04 13:15:06,2024-12-09 18:31:42,2,closed
60490,"DOC: DeprecationWarning on ""Intro to data structures"" user guide","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/user_guide/dsintro.html

### Documentation problem

I got warning at `In [48]: data = np.zeros((2,), dtype=[(""A"", ""i4""), (""B"", ""f4""), (""C"", ""a10"")])`. Warning is

>  ""DeprecationWarning: Data type alias 'a' was deprecated in NumPy 2.0. Use the 'S' alias instead.""

### Suggested fix for documentation

Following the warning's instructions, I changed `(""C"",""a10"")` to `(""C"",""S10"")`, and the warning resolved.","['Docs', 'Needs Triage']",2024-12-04 09:59:15,2024-12-06 18:12:09,3,closed
60489,BUG: pd.DataFrame incorrectly aligns Series with mismatched DatetimeIndex after resample (Pandas 2.2.3),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
data = {
    '0': pd.Series(
        data=[0.000209, 0.000100],
        index=pd.date_range(""2024-11-07 16:00:00"", periods=2, freq=""8h"", name=""time"").astype('datetime64[ms]'),
        name=""0""
    ).resample('8h').last(),
    '1': pd.Series(
        data=[0.001012, 0.000461],
        index=pd.date_range(""2023-11-22 16:00:00"", periods=2, freq=""8h"", name=""time"").astype('datetime64[ms]'),
        name=""1""
    ).resample('8h').last(),
}

print(pd.DataFrame(data))

                            0         1
time                                   
2023-11-22 16:00:00       NaN  0.001012
2024-11-07 16:00:00  0.000209       NaN
2936-07-12 00:00:00       NaN       NaN
2937-06-28 00:00:00       NaN       NaN
```


### Issue Description

When creating a pd.DataFrame from two pd.Series with mismatched DatetimeIndex (after resample), the resulting DataFrame does not align the indices correctly. Instead, it includes unexpected datetime values.

### Expected Behavior

```python
                            0         1
time                                   
2023-11-22 16:00:00       NaN  0.001012
2023-11-23 00:00:00       NaN  0.000461
2024-11-07 16:00:00  0.000209       NaN
2024-11-08 00:00:00  0.000100       NaN
```

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-36-generic
Version               : #36-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun 10 10:49:14 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Datetime']",2024-12-04 07:01:25,2024-12-31 13:26:51,5,closed
60488,BUG: no type provided for pandas.api.extensions.no_default,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from pandas.testing import assert_frame_equal
from pandas.api.extensions import NoDefault, no_default

def my_wrapper(a: pd.DataFrame, b: pd.DataFrame, atol: NoDefault = no_default) -> None:
    """"""Example function that cannot be written""""""
    assert_frame_equal(a, b, atol=atol)
    return
```


### Issue Description

No type is provided for the <code>pandas.api.extensions.no_default</code> sentinel.

This is a problem when trying to type annotate functions or methods that forward arguments into Pandas functions that accept the <code>no_default</code> sentinel.

### Expected Behavior

To be able to import and use the type <code>NoDefault</code> of <code>no_default</code>

### Installed Versions

<details>
<pre>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.7.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 25 Model 1 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252
pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 75.1.0
pip                   : 24.2
Cython                : None
pytest                : 8.3.3
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.3.0
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.27.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.5.0
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : 2.0.34
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</pre>
</details>
","['Enhancement', 'Typing']",2024-12-04 00:50:30,2025-01-14 01:56:33,3,closed
60485,DOC: Closed parameter not intuitively documented in DataFrame.rolling,"### Pandas version checks

- [x] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.rolling.html

### Documentation problem

I believe the parameter `closed` is not very intuitively documented.

(I'm using Pandas 2.2.2 on a macOS Sequoia)

#### Window size used for `closed` should be `window+1`
For this parameter to work, the actual window size should be thought of window+1. So for instance when `window=3` this is how closed should be thought:
- `closed='right'`: from a window size of 4 (window=3+1), take the current element and 2 (4-2) elements just before the current one. Totals to 3 elements.
- `closed='left'`: from a window size of 4 (window=3+1), don't take the current element but take the 3 (4-1) elements just before the current one. Totals to 3 elements.
- `closed='both'`: from a window size of 4 (window=3+1), take the current element and 3 (4-1) elements just before the current one. Totals to 4 elements.

#### `closed='neither'`
Either 'neither' isn't working or what it does isn't straight forward to me. See examples below, both examples return `NaN` in every position.

Intuitively I would guess this parameter would diminish the window size by two from a window size of window+1. So if window=3 it would mean the actual calculation would be done in 3+1-2=2 window but as you see below I only get `NaN`.

#### Example 1: mean()
```python
import pandas as pd

df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8]})

# Rolling mean with 'left' closed
df['rolling_mean_left'] = df['A'].rolling(window=3, closed='left').mean()

# Rolling mean with 'both' closed
df['rolling_mean_both'] = df['A'].rolling(window=3, closed='both').mean()

# Rolling mean with 'right' closed
df['rolling_mean_right'] = df['A'].rolling(window=3, closed='right').mean()

# Rolling mean with neither closed
df['rolling_mean_neither'] = df['A'].rolling(window=3, closed='neither').mean()

df
```

<img width=""692"" alt=""Capture d’écran 2024-12-03 à 14 17 33"" src=""https://github.com/user-attachments/assets/01b16b76-9472-4ef8-bbf1-b631299cd525"">

#### Example 2: sum()
```python
import pandas as pd

df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8]})

# Rolling sum with 'left' closed
df['rolling_sum_left'] = df['A'].rolling(window=3, closed='left').sum()

# Rolling sum with 'both' closed
df['rolling_sum_both'] = df['A'].rolling(window=3, closed='both').sum()

# Rolling sum with 'right' closed
df['rolling_sum_right'] = df['A'].rolling(window=3, closed='right').sum()

# Rolling sum with neither closed
df['rolling_sum_neither'] = df['A'].rolling(window=3, closed='neither').sum()

df
```

<img width=""653"" alt=""Capture d’écran 2024-12-03 à 14 18 32"" src=""https://github.com/user-attachments/assets/088313cd-3efe-429e-bb31-eda2a7d869e2"">

### Suggested fix for documentation

I would suggest stating that the window size taken into consideration for `closed` is actually the parameter `window` + 1, then what's stated in the docs would make sense. OR, actually use the actual `window` parameter which would make way much more sense to me. From the current docs:
> - If 'right', the first point in the window is excluded from calculations.
> - If 'left', the last point in the window is excluded from calculations.
> - If 'both', no point in the window is excluded from calculations.

Maybe even add an image example like the ones I posted above.

As for 'neither', I don't have suggestions as I don't fully understand it from my testing.

**Finally**, I don't like the name `closed` for the parameter, is doesn't mean much to me. I would maybe prefer something like `ends` or `ends_used`. I believe it would be more intuitive.","['Docs', 'Window']",2024-12-03 19:33:08,2025-02-08 12:52:15,7,closed
60471,BUG: DataFrameGroupBy.apply ignores group_keys setting when empty,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df = pd.DataFrame({'A': 'a a b'.split(), 'B': [1, 2, 3], 'C': [4, 6, 5]})
g1 = df.groupby('A', group_keys=False)

df = pd.DataFrame({'A': [], 'B': [], 'C': []})
g2 = df.groupby('A', group_keys=False)
g3 = df.groupby('A', group_keys=True)

r1 = g1.apply(lambda x: x / x.sum())
r2 = g2.apply(lambda x: x / x.sum())
r3 = g3.apply(lambda x: x / x.sum())

print(r1.index) # Index([0, 1, 2], dtype='int64')
print(r2.index) # Index([], dtype='float64', name='A')
print(r3.index) # Index([], dtype='float64', name='A')
```


### Issue Description

The group_keys parameter has no effect when the source dataframe is empty

### Expected Behavior

group_keys=False should not include the group keys into the index regardless of whether the source dataframe is empty
I would expect results such as:
```python
print(r2.index) # Index([], dtype='float64')
print(r2.index) # RangeIndex(start=0, stop=0, step=1)
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.11
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 141 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : es_ES.cp1252

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 23.0.1
Cython                : None
sphinx                : None
IPython               : 8.30.0
adbc-driver-postgresql: None
...
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'Apply']",2024-12-02 16:40:19,2024-12-06 18:13:47,2,closed
60467,QST: Is Using pandas.test() Equivalent to Running pytest Directly?,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/79244236/is-using-pandas-test-equivalent-to-running-pytest-directly

### Question about pandas


I'm working with the pandas codebase and using the _tester.py module located at pandas/util/_tester.py to execute tests. Specifically, I'm calling the pandas.test() function to run the test suite. Here’s how I’m doing it:
```
import pandas
pandas.test()
```
I can run the same tests directly with a pytest command, for example:
```
python3.12 -m pytest --cov=pandas --cov-report=term-missing --cov-branch pandas/tests/api/test_api.py::TestApi::test_api_indexers
```
Are these two approaches (pandas.test() and running pytest directly) functionally equivalent?

Does using pandas.test() introduce any additional overhead compared to directly invoking pytest? For example:

Does the wrapper preprocess or filter the test suite in any way?

Are there any significant differences in performance or the way results are handled?

In large test suites, would one approach be more efficient or recommended over the other?

Thanks in advance!","['Usage Question', 'Needs Triage']",2024-12-02 13:18:15,2024-12-02 18:23:34,1,closed
60466,BUG: why arrow no affect in Intel chips？,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.options.mode.copy_on_write = True

def ensure_arrow_format(df):
        if not isinstance(df._mgr, pd.core.internals.ArrayManager):  # 检查是否已经是 Arrow 格式
            return pa.Table.from_pandas(df).to_pandas()
        return df
```


### Issue Description

I updated pandas from 2.1.4 to 2.2.3,
and open copy on write,
and convert to arrow before df merge,
then the cost from 61s to 36s,
faster.

### Expected Behavior

My pc is mac mini m2, from 61s to 36s,
but in x86 centos or win10, only from 109 to 102,
why no affect in intel cpu?

### Installed Versions

pandas 2.2.3","['Performance', 'Needs Info', 'Arrow']",2024-12-02 07:11:25,2025-07-23 09:26:39,3,closed
60456,QST: Request for a method to map test functions to the functions they test,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/79239968/request-for-a-method-to-map-test-functions-to-the-functions-they-test

### Question about pandas



I am looking for a way to efficiently map test functions to the specific functions or modules they test. This would help in understanding the coverage and relationships between the test suite and the codebase. Specifically, I'd like to:

Identify which test functions are associated with which functions or modules in the pandas library.

Determine if there's a built-in tool, strategy, or script to extract this mapping.

If there is no such tool or methodology currently available, could you guide us on how this might be achieved?

This would greatly help in improving test coverage analysis.

I have been trying to use git blame and commit history analysis to trace which test functions were added or modified alongside new or updated functions in the codebase. The goal is to map these test functions to the specific functions or modules they test.

Thank you

","['Usage Question', 'Closing Candidate']",2024-11-30 16:06:11,2025-02-27 01:28:37,3,closed
60455,BUG: ValueError when executing a DataFrame with another DatFrame in its attrs,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

n=50

a = pd.DataFrame(np.random.randint(0, 10, size=(n,n)))
b = pd.DataFrame(np.random.randint(0, 10, size=(5,5)))

a.attrs['b'] = b

a
```


### Issue Description

Dear pandas Team,

Adding a DataFrame `b` to the attrs of another Dataframe `a` raises ValueError if dimensions of `a` are >20
Possibly related to  #51280, #60357, #60351

Error log: 
<details>
---------------------------------------------------------------------------
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/frame.py in ?(self)
   1210             self.info(buf=buf)
   1211             return buf.getvalue()
   1212 
   1213         repr_params = fmt.get_dataframe_repr_params()
-> 1214         return self.to_string(**repr_params)

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/util/_decorators.py in ?(*args, **kwargs)
    329                     msg.format(arguments=_format_argument_list(allow_args)),
    330                     FutureWarning,
    331                     stacklevel=find_stack_level(),
    332                 )
--> 333             return func(*args, **kwargs)

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/frame.py in ?(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)
   1372         """"""
   1373         from pandas import option_context
   1374 
   1375         with option_context(""display.max_colwidth"", max_colwidth):
-> 1376             formatter = fmt.DataFrameFormatter(
   1377                 self,
   1378                 columns=columns,
   1379                 col_space=col_space,

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/io/formats/format.py in ?(self, frame, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, max_rows, min_rows, max_cols, show_dimensions, decimal, bold_rows, escape)
    465         self.max_cols_fitted = self._calc_max_cols_fitted()
    466         self.max_rows_fitted = self._calc_max_rows_fitted()
    467 
    468         self.tr_frame = self.frame
--> 469         self.truncate()
    470         self.adj = printing.get_adjustment()

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/io/formats/format.py in ?(self)
    651         """"""
    652         Check whether the frame should be truncated. If so, slice the frame up.
    653         """"""
    654         if self.is_truncated_horizontally:
--> 655             self._truncate_horizontally()
    656 
    657         if self.is_truncated_vertically:
    658             self._truncate_vertically()

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/io/formats/format.py in ?(self)
    669         col_num = self.max_cols_fitted // 2
    670         if col_num >= 1:
    671             left = self.tr_frame.iloc[:, :col_num]
    672             right = self.tr_frame.iloc[:, -col_num:]
--> 673             self.tr_frame = concat((left, right), axis=1)
    674 
    675             # truncate formatter
    676             if isinstance(self.formatters, (list, tuple)):

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/reshape/concat.py in ?(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    391         copy=copy,
    392         sort=sort,
    393     )
    394 
--> 395     return op.get_result()

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/reshape/concat.py in ?(self)
    687             if not self.copy and not using_copy_on_write():
    688                 new_data._consolidate_inplace()
    689 
    690             out = sample._constructor_from_mgr(new_data, axes=new_data.axes)
--> 691             return out.__finalize__(self, method=""concat"")

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/generic.py in ?(self, other, method, **kwargs)
   6269             # propagate attrs only if all concat arguments have the same attrs
   6270             if all(bool(obj.attrs) for obj in other.objs):
   6271                 # all concatenate arguments have non-empty attrs
   6272                 attrs = other.objs[0].attrs
-> 6273                 have_same_attrs = all(obj.attrs == attrs for obj in other.objs[1:])
   6274                 if have_same_attrs:
   6275                     self.attrs = deepcopy(attrs)
   6276 

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/generic.py in ?(.0)
-> 6273                 have_same_attrs = all(obj.attrs == attrs for obj in other.objs[1:])

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/generic.py in ?(self)
   1575     @final
   1576     def __nonzero__(self) -> NoReturn:
-> 1577         raise ValueError(
   1578             f""The truth value of a {type(self).__name__} is ambiguous. ""
   1579             ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""
   1580         )

ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/frame.py in ?(self)
   1232             min_rows = get_option(""display.min_rows"")
   1233             max_cols = get_option(""display.max_columns"")
   1234             show_dimensions = get_option(""display.show_dimensions"")
   1235 
-> 1236             formatter = fmt.DataFrameFormatter(
   1237                 self,
   1238                 columns=None,
   1239                 col_space=None,

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/io/formats/format.py in ?(self, frame, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, max_rows, min_rows, max_cols, show_dimensions, decimal, bold_rows, escape)
    465         self.max_cols_fitted = self._calc_max_cols_fitted()
    466         self.max_rows_fitted = self._calc_max_rows_fitted()
    467 
    468         self.tr_frame = self.frame
--> 469         self.truncate()
    470         self.adj = printing.get_adjustment()

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/io/formats/format.py in ?(self)
    651         """"""
    652         Check whether the frame should be truncated. If so, slice the frame up.
    653         """"""
    654         if self.is_truncated_horizontally:
--> 655             self._truncate_horizontally()
    656 
    657         if self.is_truncated_vertically:
    658             self._truncate_vertically()

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/io/formats/format.py in ?(self)
    669         col_num = self.max_cols_fitted // 2
    670         if col_num >= 1:
    671             left = self.tr_frame.iloc[:, :col_num]
    672             right = self.tr_frame.iloc[:, -col_num:]
--> 673             self.tr_frame = concat((left, right), axis=1)
    674 
    675             # truncate formatter
    676             if isinstance(self.formatters, (list, tuple)):

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/reshape/concat.py in ?(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    391         copy=copy,
    392         sort=sort,
    393     )
    394 
--> 395     return op.get_result()

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/reshape/concat.py in ?(self)
    687             if not self.copy and not using_copy_on_write():
    688                 new_data._consolidate_inplace()
    689 
    690             out = sample._constructor_from_mgr(new_data, axes=new_data.axes)
--> 691             return out.__finalize__(self, method=""concat"")

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/generic.py in ?(self, other, method, **kwargs)
   6269             # propagate attrs only if all concat arguments have the same attrs
   6270             if all(bool(obj.attrs) for obj in other.objs):
   6271                 # all concatenate arguments have non-empty attrs
   6272                 attrs = other.objs[0].attrs
-> 6273                 have_same_attrs = all(obj.attrs == attrs for obj in other.objs[1:])
   6274                 if have_same_attrs:
   6275                     self.attrs = deepcopy(attrs)
   6276 

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/generic.py in ?(.0)
-> 6273                 have_same_attrs = all(obj.attrs == attrs for obj in other.objs[1:])

~/py/envs/jupyter/lib/python3.13/site-packages/pandas/core/generic.py in ?(self)
   1575     @final
   1576     def __nonzero__(self) -> NoReturn:
-> 1577         raise ValueError(
   1578             f""The truth value of a {type(self).__name__} is ambiguous. ""
   1579             ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""
   1580         )

ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

</details>

### Expected Behavior

well, no ValueError :-)

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.10.13-3-MANJARO
Version               : #1 SMP PREEMPT_DYNAMIC Tue Oct  8 03:24:49 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.30.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>","['Bug', 'Output-Formatting', 'metadata', 'Needs Triage']",2024-11-30 14:50:24,2024-12-03 20:32:16,1,closed
60445,Unable to build from source with Python 3.14 free-threading and Cython 3.1.0a1,"```
@clin1234 ➜ /workspaces $ venv/bin/pip install -U --pre pandas
Collecting pandas
  Using cached pandas-2.2.3.tar.gz (4.4 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [67 lines of output]
      + meson setup /tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9 /tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9/.mesonpy-9k_13c5e/build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=/tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9/.mesonpy-9k_13c5e/build/meson-python-native-file.ini
      The Meson build system
      Version: 1.2.1
      Source dir: /tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9
      Build dir: /tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9/.mesonpy-9k_13c5e/build
      Build type: native build
      Project name: pandas
      Project version: 2.2.3
      C compiler for the host machine: cc (gcc 11.4.0 ""cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0"")
      C linker for the host machine: cc ld.bfd 2.38
      C++ compiler for the host machine: c++ (gcc 11.4.0 ""c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0"")
      C++ linker for the host machine: c++ ld.bfd 2.38
      Cython compiler for the host machine: cython (cython 3.0.11)
      Host machine cpu family: x86_64
      Host machine cpu: x86_64
      Program python found: YES (/workspaces/venv/bin/python3.14-nogil)
      Found pkg-config: /usr/bin/pkg-config (0.29.2)
      Run-time dependency python found: YES 3.14
      Build targets in project: 53
      
      pandas 2.2.3
      
        User defined options
          Native files: /tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9/.mesonpy-9k_13c5e/build/meson-python-native-file.ini
          buildtype   : release
          vsenv       : True
          b_ndebug    : if-release
          b_vscrt     : md
      
      Found ninja-1.11.1.git.kitware.jobserver-1 at /tmp/pip-build-env-a79wao_d/normal/bin/ninja
      
      Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
      /tmp/pip-build-env-a79wao_d/overlay/bin/meson compile -C .
      + /tmp/pip-build-env-a79wao_d/normal/bin/ninja
      [1/151] Generating pandas/_libs/algos_take_helper_pxi with a custom command
      [2/151] Generating pandas/_libs/algos_common_helper_pxi with a custom command
      [3/151] Generating pandas/__init__.py with a custom command
      [4/151] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command
      [5/151] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command
      [6/151] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command
      [7/151] Generating pandas/_libs/sparse_op_helper_pxi with a custom command
      [8/151] Generating pandas/_libs/index_class_helper_pxi with a custom command
      [9/151] Generating pandas/_libs/intervaltree_helper_pxi with a custom command
      [10/151] Compiling Cython source /tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9/pandas/_libs/tslibs/base.pyx
      [11/151] Compiling C object pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o
      FAILED: pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o
      cc -Ipandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p -Ipandas/_libs/tslibs -I../../pandas/_libs/tslibs -I../../../../pip-build-env-a79wao_d/overlay/lib/python3.14t/site-packages/numpy/_core/include -I../../pandas/_libs/include -I/usr/include/python3.14t -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -w -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o -MF pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o.d -o pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o -c pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c
      pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c:2102:80: error: unknown type name ‘__pyx_vectorcallfunc’; did you mean ‘vectorcallfunc’?
       2102 | static CYTHON_INLINE PyObject *__Pyx_PyVectorcall_FastCallDict(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw);
            |                                                                                ^~~~~~~~~~~~~~~~~~~~
            |                                                                                vectorcallfunc
      pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c:8658:69: error: unknown type name ‘__pyx_vectorcallfunc’; did you mean ‘vectorcallfunc’?
       8658 | static PyObject *__Pyx_PyVectorcall_FastCallDict_kw(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw)
            |                                                                     ^~~~~~~~~~~~~~~~~~~~
            |                                                                     vectorcallfunc
      pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c:8703:80: error: unknown type name ‘__pyx_vectorcallfunc’; did you mean ‘vectorcallfunc’?
       8703 | static CYTHON_INLINE PyObject *__Pyx_PyVectorcall_FastCallDict(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw)
            |                                                                                ^~~~~~~~~~~~~~~~~~~~
            |                                                                                vectorcallfunc
      pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c: In function ‘__Pyx_CyFunction_CallAsMethod’:
      pandas/_libs/tslibs/base.cpython-314t-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c:9392:6: error: unknown type name ‘__pyx_vectorcallfunc’; did you mean ‘vectorcallfunc’?
       9392 |      __pyx_vectorcallfunc vc = __Pyx_CyFunction_func_vectorcall(cyfunc);
            |      ^~~~~~~~~~~~~~~~~~~~
            |      vectorcallfunc
      [12/151] Compiling Cython source /tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9/pandas/_libs/tslibs/ccalendar.pyx
      [13/151] Compiling Cython source /tmp/pip-install-anpdkgs_/pandas_239c94188d0f49c383da3a300cba66e9/pandas/_libs/tslibs/dtypes.pyx
      ninja: build stopped: subcommand failed.
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```",['Python 3.14'],2024-11-28 23:21:10,2025-09-07 07:48:51,8,closed
60439,BUG: Incorrect casting of int to float when np.nan assigned to Series with nullable boolean index,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

abc=pd.Series([1,2,3],index=np.array([False,True,pd.NA], dtype=pd.BooleanDtype),dtype=""int64"")
abc
Out[1]:
False    1
True     2
<NA>     3
dtype: int64

abc.loc[[True,False,False]]=np.NaN
abc
Out[2]:
False    NaN
True     2.0
<NA>     3.0
dtype: float64

```


### Issue Description

A Series with Nullable Boolean dtype index casts the entire series to float64 when it's originally int64.

### Expected Behavior

Everything else remains the same besides NaN.
```
import pandas as pd
import numpy as np

abc=pd.Series([1,2,3],index=np.array([False,True,pd.NA], dtype=pd.BooleanDtype),dtype=""int64"")
abc
Out[1]:
False    1
True     2
<NA>     3
dtype: int64

abc.loc[[True,False,False]]=np.NaN
abc
Out[2]:
False    NaN
True     2
<NA>     3
dtype: int64
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.5
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.24.3
pytz                  : 2023.3
dateutil              : 2.8.2
pip                   : 22.1.2
Cython                : None
sphinx                : None
IPython               : 8.13.2
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.2
lxml.etree            : None
matplotlib            : 3.8.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage', 'Closing Candidate']",2024-11-28 16:21:34,2024-11-29 07:11:36,4,closed
60438,QST: when I import pandas as pd then i get an error,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

when I import pandas as pd then i get an error

### Question about pandas

<ipython-input-26-7dd3504c366f>:1: DeprecationWarning: 
Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),
(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)
but was not found to be installed on your system.
If this would cause problems for you,
please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466
![Screenshot (15)](https://github.com/user-attachments/assets/25be51d1-4c1a-4489-9417-ffe979d7ff23)
",['Usage Question'],2024-11-28 14:48:52,2024-11-28 21:10:12,1,closed
60431,BUG: inconsistent processing of datetime in format '%Y-%m-%dT%H:%M:%S' depending on whether there is a time or it is 00:00:00,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
##filename should be a csv loaded with following values:
# ID	Start_Date	Report_Status_Effective_Date
# 1	2023-04-27T10:00:00	2023-05-26T00:00:00
# 2	2026-08-12T22:00:00	2026-08-13T00:00:00
# 3	2028-03-23T04:00:00	2028-03-23T00:00:00
# 4	2022-12-28T12:00:00	2023-01-17T00:00:00
# 5	2022-02-09T09:33:00	2023-01-04T00:00:00
# 6	2023-06-21T05:00:00	2023-07-11T00:00:00
# 7	2024-11-28T03:00:00	2024-11-28T00:00:00
# 8	2024-03-21T13:00:00	2024-06-27T00:00:00
# 9	2023-12-01T12:00:00	2024-01-22T00:00:00

df = pd.read_csv(filename, dtype=str, engine='pyarrow', keep_default_na=False)
print(df)
```


### Issue Description

The outcome of the above inconsistently removes the ""T"" from the date format, like so:

            START_DATE STATUS_EFFECTIVE_DATE
0  2023-04-27T10:00:00   2023-04-27 00:00:00
1  2026-08-12T22:00:00   2026-08-13 00:00:00
2  2028-03-23T04:00:00   2028-03-23 00:00:00
3  2022-12-28T12:00:00   2022-12-28 00:00:00
4  2022-02-09T09:33:00   2022-02-09 00:00:00
5  2023-06-21T05:00:00   2023-06-21 00:00:00
6  2024-11-28T03:00:00   2024-11-28 00:00:00
7  2024-03-21T13:00:00   2024-03-21 00:00:00
8  2023-12-01T12:00:00   2023-12-01 00:00:00
9  2024-11-19T09:00:00   2024-10-30 00:00:00

Note above how the ""T"" remains in the left most column where there are time values, and is removed on the right column where all time values are 00:00:00
This makes it impossible to map the dates consistently with the format  '%Y-%m-%dT%H:%M:%S' .
Note this issue requires all values in a column to have time = 00:00:00

### Expected Behavior

The two columns should still have the ""T"" between date and time

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-1025-azure
Version               : #26~22.04.1-Ubuntu SMP Thu Jul 11 22:33:04 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 7.4.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Duplicate Report', 'IO CSV', 'Closing Candidate', 'Arrow']",2024-11-27 16:04:52,2024-12-01 14:55:48,2,closed
60429,DOC: Missing 'pickleshare' package when running 'sphinx-build' command,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/106f33cfce16f4e08f6ca5bd0e6e440ec9a94867/requirements-dev.txt#L26

### Documentation problem

After installing `requirements-dev.txt` and `pandas` from sources, I tried to run `sphinx-build` command to build pandas docs. However, I noticed that there are lots of such warnings:

```bash
UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.
  bkms = self.shell.db.get('bookmarks',{})
UsageError: %bookmark -d: Can't delete bookmark 'ipy_savedir'
```

The followings are the commands I used:

```bash
git clone --branch=main https://github.com/pandas-dev/pandas.git pandas-main
cd pandas-main
git log -1 --pretty=format:""%H%n%s""
conda create --prefix ./.venv --channel conda-forge --yes
conda activate ./.venv
conda install python=3.10 --yes
export PYTHONNOUSERSITE=1
python -m pip install --requirement=requirements-dev.txt --progress-bar=off --verbose
python -m pip install . --no-build-isolation --no-deps -Csetup-args=--werror --progress-bar=off --verbose
export LANG=en_US.UTF-8
sphinx-build -b html -v -j 4 -c doc/source doc/source doc/build/html
```

Log file of the above commands:

[log-sphinx-build-pandas-docs.txt](https://github.com/user-attachments/files/17928982/log-sphinx-build-pandas-docs.txt)

### Suggested fix for documentation

It seems that this issue is caused by https://github.com/ipython/ipython/issues/14237. Therefore, my suggested fix is to add `pickleshare` requirement in `environment.yml` and `requirements-dev.txt`. 

Just like NumPy demonstrated here: [requirements/doc_requirements.txt#L12-L14](https://github.com/numpy/numpy/blob/4e8f724fbc136b1bac1c43e24d189ebc45e056eb/requirements/doc_requirements.txt#L12-L14)

### Versions and Platforms

- OS version: Kubuntu 24.04
- Current Branch: [`main`](https://github.com/numpy/numpy/tree/main)
- Latest Commit: 106f33cfce16f4e08f6ca5bd0e6e440ec9a94867
- Conda version: `24.9.2`
- Python version: `3.10.15`
- iPython version: `8.29.0`","['Build', 'Docs']",2024-11-27 04:36:42,2024-12-03 21:21:21,4,closed
60428,ENH:  Add a safe Option to hash_pandas_object with Default Value Set to True,"### Feature Type

- [X] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

The current implementation of hash_pandas_object does not meet collision resistance requirements, although this is known to the developers. However, it is not prominently documented, and the function is already widely used in many downstream AI platforms, such as MLflow, AutoGluon, and others. These platforms use pandas_hash_object to convert DataFrame structures and then apply MD5 or SHA-256 for uniqueness checks, enabling caching and related functionalities. This makes these platforms more vulnerable to malicious datasets.

Therefore, I propose adding a safe option with a default value set to True. This would directly benefit the security of a large number of downstream applications. If not, the documentation should explicitly state that the function does not provide collision resistance and should not be used for caching or similar tasks.

### Feature Description

``` 
def hash_pandas_object(,,,,, safe=True):
        if safe == True:
            safe_hash_pandas_object(,,,,,)
        else:
             # Existing code
```

### Alternative Solutions

Alternatively, if users need to modify the function themselves, they can use to_pickle() to serialize the DataFrame before hashing.

```
df_bytes = df.to_pickle()
hash_object = hashlib.sha256(df_bytes)
```

### Additional Context

autogluon code:
https://github.com/autogluon/autogluon/blob/082d8bae7343f02e9dc9ce3db76bc3f305027b10/common/src/autogluon/common/utils/utils.py#L176

mlflow code at:
https://github.com/mlflow/mlflow/blob/615c4cbafd616e818ff17bfcd964e8366a5cd3ed/mlflow/data/digest_utils.py#L39

graphistry code at:
https://github.com/graphistry/pygraphistry/blob/52ea49afbea55291c41962f79a90d74d76c721b9/graphistry/util.py#L84

Developer discussion on pandas functionality: https://github.com/pandas-dev/pandas/issues/16372#issuecomment-428545609  

Documentation link for `hash_pandas_object`: https://pandas.pydata.org/docs/reference/api/pandas.util.hash_pandas_object.html#pandas.util.hash_pandas_object

one demo:
```
import pandas as pd
# Define two data dictionaries
data1 = {
    'A': [1604090909467468979, 2],
    'B': [4, 4]
}
data2 = {
    'A': [1, 2],
    'B': [3, 4]
}
# Convert dictionaries to DataFrame
df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)
# Calculate the hash value for each DataFrame
hash_df1 = pd.util.hash_pandas_object(df1)
hash_df2 = pd.util.hash_pandas_object(df2)

```","['Enhancement', 'hashing', 'Closing Candidate']",2024-11-27 02:33:19,2025-08-05 16:26:45,9,closed
60427,BUG: combine_first reorders columns,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd


df = pd.DataFrame({""B"": [1, 2, 3], ""A"": [4, 5, 6]}, index=[""a"", ""b"", ""c""])

print(df)  # B first, then A
print()

df_ = pd.DataFrame({""A"": [7]}, index=[""b""])

print(df.combine_first(df_))  # A first, then B
print()

print(df_.combine_first(df))  # A first, then B
print()


print(df_.combine_first(df)[df.columns])  # Workaround
```


### Issue Description

I wouldn't expect `combine_first` to reorder the columns alphabetically, but it does.

Bug might be a stretch, but it's certainly unexpected and awkward.

### Expected Behavior

Preserve the column order, as show in `# Workaround`.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.13.0+
python-bits           : 64
OS                    : Darwin
OS-release            : 21.6.0
Version               : Darwin Kernel Version 21.6.0: Wed Oct  4 23:55:28 PDT 2023; root:xnu-8020.240.18.704.15~1/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Reshaping']",2024-11-26 21:38:24,2025-01-27 20:54:26,3,closed
60425,BUG: timestamp value is different with pd.Timestamp and regular python datetime (tz naive with same value),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

ori_ts = 1719850245.23
pd_date = pd.Timestamp(ori_ts, unit='s')
py_date = pd_date.to_pydatetime()

pd_ts = pd_date.timestamp()
py_ts = py_date.timestamp()

delta = abs(pd_ts - py_ts)

print(delta)
```


### Issue Description

with the same tz and time values the given timestamp is not the same.
I'm in France and the delta between both is 7200 seconds corresponding to the GMT+2 tz

### Expected Behavior

In the documentation pd.Timestamp will behave like the datetime, i was expecting to have the same value

### Installed Versions

I have installed the 2.2.2 version and i have tried with older version and also with the 1.x version and the result is the same
","['Bug', 'Needs Triage']",2024-11-26 16:09:50,2024-11-29 15:07:14,2,closed
60424,BUG: DataFrame.agg() on empty dataframe returns unexpected result,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

index_empty = pd.MultiIndex.from_product([[], []], names=[""one"", ""two""])
df_empty = pd.DataFrame({""a"": [], ""b"": []}, index=index_empty)
df_empty.agg(lambda row: row.name[0], axis=1)
```


### Issue Description

The above returns:
```
Empty DataFrame
Columns: [a, b]
Index: []
```

### Expected Behavior

The expected result would be an empty series:
```
Series([], dtype: float64)
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.3
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Belgium.1252
pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.8.4
numba                 : None
numexpr               : 2.10.2
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : 1.4.6
pyarrow               : 18.0.0
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.10.0
xlrd                  : None
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Apply']",2024-11-26 15:41:54,2024-11-26 21:01:06,3,closed
60421,BUG: pd.read_json fails with newer versions (>1.26.4) of numpy,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import io

json_data = '{""name"": [""John"", ""Jane"", ""Bob""],""age"": [25, 30, 35],""city"": [""New York"", ""San Francisco"", ""Chicago""]}'

df = pd.read_json(io.StringIO(json_data))
print(df)
```


### Issue Description

With numpy versions larger than 1.26.4 (haven't tested every version), it appears to fail in [StringFormatter._join_multiline ](https://github.com/pandas-dev/pandas/blob/main/pandas/io/formats/string.py#L127), because of the call to 
`np.array([self.adj.len(x) for x in idx]).max()`. 

Similar call is in line [130](https://github.com/pandas-dev/pandas/blob/main/pandas/io/formats/string.py#L130). 

It seems broken in numpy as `np.array([0,3]).max()` fails with the same error. Reported it [here](https://github.com/numpy/numpy/issues/27857).

A quick fix is to just change it to:
`np.max(np.array([self.adj.len(x) for x in idx])`



### Expected Behavior

Should print out the dataframe (which it does with numpy==1.26.4):

```
   name  age           city
0  John   25       New York
1  Jane   30  San Francisco
2   Bob   35        Chicago
```

### Installed Versions

<details>

pandas                : 2.2.2
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
setuptools            : 56.0.0
pip                   : 21.1
pytest                : 8.3.3
scipy                 : 1.14.1
sqlalchemy            : 2.0.35
xlrd                  : 2.0.1
tzdata                : 2024.2

</details>
","['Bug', 'Needs Triage']",2024-11-26 14:14:29,2024-11-26 15:04:04,1,closed
60410,DOC: incorrect formula for half-life of exponentially weighted window,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/user_guide/window.html#exponentially-weighted-window

### Documentation problem

![Screenshot 2024-11-24 22 51 05](https://github.com/user-attachments/assets/29b37e07-c500-40c9-b93e-2bcb1df0283f)
in the documentation for alpha as a function of half-life the formula says 1-exp^(log(0.5)/h)

it should be either exp(log(0.5)/h) or e^(log(0.5)/h) but not exp^(log(0.5)/h)

### Suggested fix for documentation

I suggest changing it to 1-e^(log(0.5)/h)","['Docs', 'Needs Triage']",2024-11-25 03:53:39,2024-11-25 18:36:09,0,closed
60396,CI/BUG: `comment_commands.yml` failing due to invalid `trim()` ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
comment anything on an issue in this sub, workflow will return as FAIL
```


### Issue Description

We have recently merged a past MR (#60359) regarding using `trim()` on `comment_commands.yml`. Turns out there are no ""Trim"" or similar related command on github workflows file...

> The workflow is not valid. .github/workflows/comment-commands.yml (Line: 14, Col: 9): Unrecognized function: 'trim'. Located at position 39 within expression: (!github.event.issue.pull_request) && trim(github.event.comment.body) == 'take'

Failed workflow example: https://github.com/pandas-dev/pandas/actions/runs/11973824956/workflow

Discussion regarding split or trim command on github workflow: https://stackoverflow.com/questions/64049306/github-actions-how-to-trim-a-string-in-a-condition

### Expected Behavior

comment_commands workflow should work properly

### Installed Versions

NA","['Bug', 'Needs Triage']",2024-11-22 13:56:14,2024-11-22 18:56:42,1,closed
60394,ENH: Add first_inverted and last_inverted options to keep in DataFrame.duplicated,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I suggest adding options `first_inverted` and `last_inverted` as `keep` options to function `pandas.DataFrame.duplicated`. Below an example of how it would work and what it would return.

df = pd.DataFrame({
    'brand': ['Yum Yum', 'Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],
    'style': ['cup', 'cup', 'cup', 'cup', 'pack', 'pack'],
    'rating': [4, 4, 4, 3.5, 15, 5],
})

df.duplicated(keep='first_inverted')

0     True
1    False
2    False
3    False
4    False
5    False
dtype: bool

### Feature Description

.

### Alternative Solutions

.

### Additional Context

_No response_","['Enhancement', 'Needs Discussion', 'duplicated', 'Closing Candidate']",2024-11-22 08:21:07,2025-08-05 16:37:01,5,closed
60393,"BUG: Printing of a pandas Series of complex numbers with e-notation has broken between 2.0.x and older versions and the newer 2.1.x, 2.2.x versions","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
input_array = [(1.8816e-09+0j), (3.39676e-09+0j)]
print(""input_array=\n"", input_array)
print(""pd.Series(input_array)=\n"", pd.Series(input_array))
```


### Issue Description

We use the pandas Series method to store a list of complex numbers but when upgrading from the pandas 1.3, 1.5, or 2.0 versions we are seeing a regression issue where in pandas 2.1.x and 2.2.x and latest 2.2.3 version the print out is adding extra white space to the imaginary value's e-notation portion.

example:
`import pandas as pd`
`input_array = [(1.8816e-09+0j), (3.39676e-09+0j)]`
`print(""input_array=\n"", input_array)`
`print(""pd.Series(input_array)=\n"", pd.Series(input_array))`


For example in pandas 1.3, 1.5, 2.0.3 we see the following output that we expect from the following example:

`input_array=`
` [(1.8816e-09+0j), (3.39676e-09+0j)]`
`pd.Series(input_array)=`
` 0    1.881600e-09+0.000000e+00j`
`1    3.396760e-09+0.000000e+00j`
`dtype: complex128`

But now in pandas version 2.1.x, 2.2.x, and latest 2.2.3 we are getting the output instead:
`input_array=`
` [(1.8816e-09+0j), (3.39676e-09+0j)]`
`pd.Series(input_array)=`
` 0    1.881600e-09+0.000000e+                    00j`
`1    3.396760e-09+0.000000e+                    00j`
`dtype: complex128`

Thanks.

### Expected Behavior

The expected output is the following:
`input_array=`
` [(1.8816e-09+0j), (3.39676e-09+0j)]`
`pd.Series(input_array)=`
` 0    1.881600e-09+0.000000e+00j`
`1    3.396760e-09+0.000000e+00j`
`dtype: complex128`

Where the `+0j` portion of the complex value is printed as `+0.000000e+00j` 
not what it is printed now as: `0.000000e+                    00j`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.2
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 141 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
Thanks","['Bug', 'Output-Formatting', 'Complex']",2024-11-22 03:24:45,2024-11-26 00:01:49,2,closed
60391,ENH: Decimal year,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could use pandas to quickly convert datetime/Timestamp objects to ""decimal year"" floating point numbers for subsequent visualization and analysis.

A number of plotting packages (e.g., GeoPandas, matplotlib) encounter issues when casting datetime/Timestamp objects to float.  For example, I often encounter errors when trying to create a choropleth map to visualize a GeoDataFrame column containing datetime objects.  Decimal years also simplify the legend/colorbar labels.

![example decimal year map](https://github.com/user-attachments/assets/463015bd-6290-4ac8-b436-005295fc794a)


### Feature Description

This is a simple function to accomplish this.  It's not perfect, but does the job.  Would need to re-implement as a Timestamp and/or dt accessor property (`dt.decyear`).  Should be relatively simple, I think.

```
#Decimal year (useful for plotting)
from datetime import datetime as dt
import time
def toYearFraction(date):
    def sinceEpoch(date): # returns seconds since epoch
        return time.mktime(date.timetuple())
    s = sinceEpoch

    year = date.year
    startOfThisYear = dt(year=year, month=1, day=1)
    startOfNextYear = dt(year=year+1, month=1, day=1)

    yearElapsed = s(date) - s(startOfThisYear)
    yearDuration = s(startOfNextYear) - s(startOfThisYear)
    fraction = yearElapsed/yearDuration

    return date.year + fraction
```

### Alternative Solutions

Define and apply a custom function:
`df['dt_col_decyear'] = df['dt_col'].apply(toYearFraction)`

### Additional Context

When attempting to plot column containing datetime values...

`gdf.plot(column='dt_col', legend=True)`

```
File [~/sw/miniconda3/envs/shean_py3/lib/python3.12/site-packages/geopandas/plotting.py:175](http://localhost:8888/lab/tree/src/stereo-lidar_archive_search/notebooks/~/sw/miniconda3/envs/shean_py3/lib/python3.12/site-packages/geopandas/plotting.py#line=174), in _plot_polygon_collection(ax, geoms, values, color, cmap, vmin, vmax, autolim, **kwargs)
    172 collection = PatchCollection([_PolygonPatch(poly) for poly in geoms], **kwargs)
    174 if values is not None:
--> 175     collection.set_array(np.asarray(values))
    176     collection.set_cmap(cmap)
    177     if ""norm"" not in kwargs:

File [~/sw/miniconda3/envs/shean_py3/lib/python3.12/site-packages/matplotlib/cm.py:452](http://localhost:8888/lab/tree/src/stereo-lidar_archive_search/notebooks/~/sw/miniconda3/envs/shean_py3/lib/python3.12/site-packages/matplotlib/cm.py#line=451), in ScalarMappable.set_array(self, A)
    450 A = cbook.safe_masked_invalid(A, copy=True)
    451 if not np.can_cast(A.dtype, float, ""same_kind""):
--> 452     raise TypeError(f""Image data of dtype {A.dtype} cannot be ""
    453                     ""converted to float"")
    455 self._A = A
    456 if not self.norm.scaled():

TypeError: Image data of dtype object cannot be converted to float
```","['Enhancement', 'Datetime', 'Closing Candidate']",2024-11-21 21:39:02,2025-08-05 16:33:24,9,closed
60389,BUG: Cannot `shift` Intervals that are not `closed='right'` (the default),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.Series([pd.Interval(1, 2, closed='left'), pd.Interval(2, 3, closed='left')]).shift(1)
```


### Issue Description

[This line](https://github.com/pandas-dev/pandas/blob/1c986d6213904fd7d9acc5622dc91d029d3f1218/pandas/core/arrays/interval.py#L1058) in `shift` creates an empty `IntervalArray` without specifying which side the intervals are closed on. When that array and the one being shifted get concatenated, the following exception is raised:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/wmorrison/.local/share/pyenv/versions/3.11.5/envs/3.11.5-enspired@aws_lambda/lib/python3.11/site-packages/pandas/core/generic.py"", line 11228, in shift
    new_data = self._mgr.shift(periods=periods, fill_value=fill_value)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/wmorrison/.local/share/pyenv/versions/3.11.5/envs/3.11.5-enspired@aws_lambda/lib/python3.11/site-packages/pandas/core/internals/base.py"", line 312, in shift
    return self.apply_with_block(""shift"", periods=periods, fill_value=fill_value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/wmorrison/.local/share/pyenv/versions/3.11.5/envs/3.11.5-enspired@aws_lambda/lib/python3.11/site-packages/pandas/core/internals/managers.py"", line 363, in apply
    applied = getattr(b, f)(**kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/wmorrison/.local/share/pyenv/versions/3.11.5/envs/3.11.5-enspired@aws_lambda/lib/python3.11/site-packages/pandas/core/internals/blocks.py"", line 2020, in shift
    new_values = self.values.T.shift(periods=periods, fill_value=fill_value).T
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/wmorrison/.local/share/pyenv/versions/3.11.5/envs/3.11.5-enspired@aws_lambda/lib/python3.11/site-packages/pandas/core/arrays/interval.py"", line 1097, in shift
    return self._concat_same_type([a, b])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/wmorrison/.local/share/pyenv/versions/3.11.5/envs/3.11.5-enspired@aws_lambda/lib/python3.11/site-packages/pandas/core/arrays/interval.py"", line 1045, in _concat_same_type
    raise ValueError(""Intervals must all be closed on the same side."")
ValueError: Intervals must all be closed on the same side.
```

### Expected Behavior

The following `pd.Series[Interval]` should be returned, closed on the same side as the original Series

```
0           NaN
1    [1.0, 2.0)
dtype: interval
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.5
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-48-generic
Version               : #48~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  7 11:24:13 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Interval', 'Transformations']",2024-11-21 14:09:50,2024-11-25 18:40:39,2,closed
60388,BUG: cannot locate multiIndex column when NaT is in column ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

from datetime import datetime

data = dict(Date=[datetime(2024, 11, 1), datetime(2024, 11, 1), datetime(2024, 11, 2), datetime(2024, 11, 2)], sub=['a', 'b', 'c', 'd'], value1=[1,2,3,4], value2=[5,6,7,8])
df = pd.DataFrame(data)
pivot_table = df.pivot(index='sub', columns='Date', values=['value1', 'value2'])

df2 = pivot_table.reset_index()

# Checked following ways to locate column, but all failed

df2[df2.columns[0]]

# df2.loc[:, df2.columns[0]]
```


### Issue Description

Hi,

I was transforming some dataframes and found that when there is NaT in multiIndex columns, I cannot locate that column but got keyError instead. The stack is below:

`
Traceback (most recent call last):
  File ""/Users/meg/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/indexes/multi.py"", line 3053, in get_loc
    return self._engine.get_loc(key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""index.pyx"", line 776, in pandas._libs.index.BaseMultiIndexCodesEngine.get_loc
  File ""index.pyx"", line 167, in pandas._libs.index.IndexEngine.get_loc
  File ""index.pyx"", line 196, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 2152, in pandas._libs.hashtable.UInt64HashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 2176, in pandas._libs.hashtable.UInt64HashTable.get_item
KeyError: 33

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/meg/codes/leetcode/testpandas.py"", line 11, in <module>
    df2[df2.columns[0]]
    ~~~^^^^^^^^^^^^^^^^
  File ""/Users/meg/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/frame.py"", line 4101, in __getitem__
    return self._getitem_multilevel(key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/meg/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/frame.py"", line 4159, in _getitem_multilevel
    loc = self.columns.get_loc(key)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/meg/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/indexes/multi.py"", line 3055, in get_loc
    raise KeyError(key) from err
KeyError: ('sub', NaT)
`

### Expected Behavior

pandas should return target column without error.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Darwin
OS-release            : 24.1.0
Version               : Darwin Kernel Version 24.1.0: Thu Oct 10 21:05:14 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8103
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Indexing', 'Missing-data', 'MultiIndex', 'PDEP missing values']",2024-11-21 13:35:16,2024-12-03 00:14:58,3,closed
60385,ENH: value_counts to produce both count and normalized,"### Feature Type

- [X] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I would like pandas to have a feature for when I need to see both the count and relative counts of a Series at once.

### Feature Description

Now, I can get the count by value_counts and the relative count by passing the normalize parameter to that function. Sometimes it is more interesting to see them at once, probably using a new parameter to this function. Maybe editing the normalize parameter to handle three states, raw, relative, or both.

### Alternative Solutions

Using two consecutive calls to value_counts by different values for normalize could provide the functionality.

### Additional Context

_No response_","['Enhancement', 'Algos', 'Closing Candidate']",2024-11-21 07:51:28,2025-04-24 21:34:42,1,closed
60384,DOC: Missing type hint for squeeze method,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/core/generic.py

### Documentation problem

The squeeze method is missing a type hint. 

### Suggested fix for documentation

Adding a type hint to the squeeze method to be consistent with the rest of the code. ",['Typing'],2024-11-21 06:33:11,2024-11-26 21:28:40,1,closed
60380,BUG: errant behavior of ~ operator within an apply statement,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""a"": [1, 2, np.nan, 3]})

df = df.assign(not_null_assign = lambda x: (~pd.isna(x.a))) # correct

df['not_null_apply'] = df.apply(lambda x: True if (~pd.isna(x.a)) else False, axis=1) #incorrect

df['not_null_apply_and'] = df.apply(lambda x: True if (~pd.isna(x.a) & (1==1)) else False, axis=1) # correct
```


### Issue Description

The use of the bitwise NOT ~ seems inconsistent in the above examples. I wonder if there is a bug in how pandas interprets apply when ~ is used within the conditional statement.

### Expected Behavior

I would expect all three to correctly report whether the value is null.

### Installed Versions

INSTALLED VERSIONS
------------------
commit           : 4bfe3d07b4858144c219b9346329027024102ab6
python           : 3.9.12.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 23.4.0
Version          : Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:06 PST 2024; root:xnu-10063.101.15~2/RELEASE_ARM64_T8103
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.4.2
numpy            : 1.21.5
pytz             : 2021.3
dateutil         : 2.8.2
pip              : 21.2.4
setuptools       : 61.2.0
Cython           : 0.29.28
pytest           : 7.1.1
hypothesis       : None
sphinx           : 4.4.0
blosc            : None
feather          : None
xlsxwriter       : 3.0.3
lxml.etree       : 4.8.0
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 3.1.2
IPython          : 8.3.0
pandas_datareader: None
bs4              : 4.11.1
bottleneck       : 1.3.4
brotli           : 
fastparquet      : None
fsspec           : 2022.02.0
gcsfs            : None
markupsafe       : 2.0.1
matplotlib       : 3.5.1
numba            : 0.55.1
numexpr          : 2.8.1
odfpy            : None
openpyxl         : 3.0.9
pandas_gbq       : None
pyarrow          : None
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : 1.7.3
snappy           : None
sqlalchemy       : 1.4.32
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 2.0.1
xlwt             : 1.3.0
zstandard        : None
","['Bug', 'Apply']",2024-11-20 23:01:52,2024-11-21 21:56:22,1,closed
60379,BUG: `DataFrameGroupBy.agg` has different behavior when input function receives keyword argument,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""a"": [1, 2], ""b"": [4, 5]}, index=pd.Index(list(""AB""), name=""group""))


def _agg_func(*args, **kwargs):
    for arg in args:
        print(""ARG:\n"")
        print(arg)
        print()
    return None


# _agg_func will receive a pandas Series of column ""a"", then after it, column ""b""
df.groupby(level=""group"").agg(_agg_func)

# _agg_func will receive a DataFrame with columns ""a"" and ""b"" for each group
df.groupby(level=""group"").agg(_agg_func, some_kwarg=""abc"")
```


### Issue Description

When performing an aggregation over groups, my colleagues and I observed that `DataFrameGroupBy.agg` has inconsistent behavior when the function passed to the `agg` method has or not keyword arguments.

If `agg` receives the function `_agg_func` without keyword arguments, then the input to `_agg_func` will be a `pandas.Series`. If keyword arguments are passed, then the input to `_agg_func` is a `pandas.DataFrame`.

### Expected Behavior

As per the [docs](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html), the parameter `func` should be able to receive a `pandas.DataFrame` as input, hence, I would expect that the input for `_agg_func` should _always_ be a `pandas.DataFrame`, independently if the function receives or not a keyword argument.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.133.1-microsoft-standard-WSL2
Version               : #1 SMP Thu Oct 5 21:02:42 UTC 2023
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 23.3.1
Cython                : None
sphinx                : None
IPython               : 8.29.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 18.0.0
pyreadstat            : None
pytest                : 8.3.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.32
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'Apply', 'duplicated']",2024-11-20 21:20:25,2024-11-21 22:07:21,1,closed
60370,ENH: Improve Code Quality in pandas/core/reshape Module,"## Summary

Refactor the pandas/core/reshape module to improve code quality by reducing duplication, replacing hard-coded values, and simplifying complex conditionals.

## Problem Description

The pandas/core/reshape module implements key reshaping functions (pivot, melt, and unstack) used in data manipulation workflows. A review of pivot.py and melt.py reveals a couple of areas where code quality could be improved:

**Nested Conditionals:**
  * In melt.py, nested conditionals add complexity, making the code harder to read and maintain.
  * Suggestion: Refactor these conditionals into smaller, more modular functions.

**Hard-Coded Values:**
  * In pivot.py, hard-coded strings (e.g., ""All"" for margins) reduce flexibility.
  * Suggestion: Replace hard-coded values with constants for maintainability.
  
## Relevant File
* **melt.py**
* **pivot.py**

## Proposed Solution
**Refactor Nested Conditionals in melt.py**
* Nested Conditional in `ensure_list_vars()`
  * Before:
  ```python
  def ensure_list_vars(arg_vars, variable: str, columns) -> list:
    if arg_vars is not None:
        if not is_list_like(arg_vars):
            return [arg_vars]
        elif isinstance(columns, MultiIndex) and not isinstance(arg_vars, list):
            raise ValueError(
                f""{variable} must be a list of tuples when columns are a MultiIndex""
            )
        else:
            return list(arg_vars)
    else:
        return []
    ```
    * After:
    ```python
    def ensure_list_vars(arg_vars, variable: str, columns) -> list:
    if arg_vars is None:
        return []

    if not is_list_like(arg_vars):
        return [arg_vars]

    if isinstance(columns, MultiIndex) and not isinstance(arg_vars, list):
        raise ValueError(
            f""{variable} must be a list of tuples when columns are a MultiIndex""
        )

    return list(arg_vars)
    ```
* Nested Conditional in `melt()` for `id_vars`:
  * Before:
  ```python
  if id_vars or value_vars:
    if col_level is not None:
        level = frame.columns.get_level_values(col_level)
    else:
        level = frame.columns
    labels = id_vars + value_vars
    idx = level.get_indexer_for(labels)
    missing = idx == -1
    if missing.any():
        missing_labels = [
            lab for lab, not_found in zip(labels, missing) if not_found
        ]
        raise KeyError(
            ""The following id_vars or value_vars are not present in ""
            f""the DataFrame: {missing_labels}""
        )
    if value_vars_was_not_none:
      frame = frame.iloc[:, algos.unique(idx)]
    else:
      frame = frame.copy(deep=False)
  else:
    frame = frame.copy(deep=False)
  ```
  * After:
  ```python
  def validate_and_get_level(frame, id_vars, value_vars, col_level):
    level = frame.columns.get_level_values(col_level) if col_level is not None else frame.columns
    labels = id_vars + value_vars
    idx = level.get_indexer_for(labels)
    missing = idx == -1
    if missing.any():
        missing_labels = [lab for lab, not_found in zip(labels, missing) if not_found]
        raise KeyError(
            ""The following id_vars or value_vars are not present in ""
            f""the DataFrame: {missing_labels}""
        )
    return idx

  if id_vars or value_vars:
      idx = validate_and_get_level(frame, id_vars, value_vars, col_level)
      if value_vars_was_not_none:
          frame = frame.iloc[:, algos.unique(idx)]
  else:
      frame = frame.copy(deep=False)
  ```
* Nested Conditionals for Setting `var_name` in `melt()`:
  * Before:
  ```python
  if var_name is None:
    if isinstance(frame.columns, MultiIndex):
        if len(frame.columns.names) == len(set(frame.columns.names)):
            var_name = frame.columns.names
        else:
            var_name = [f""variable_{i}"" for i in range(len(frame.columns.names))]
    else:
        var_name = [
            frame.columns.name if frame.columns.name is not None else ""variable""
        ]
  elif is_list_like(var_name):
    if isinstance(frame.columns, MultiIndex):
        if is_iterator(var_name):
            var_name = list(var_name)
        if len(var_name) > len(frame.columns):
            raise ValueError(
                f""{var_name=} has {len(var_name)} items, ""
                f""but the dataframe columns only have {len(frame.columns)} levels.""
            )
    else:
        raise ValueError(f""{var_name=} must be a scalar."")
  else:
    var_name = [var_name]
  ```
  After:
  ```python
  def determine_var_name(frame, var_name):
    if var_name is None:
        return _default_var_name(frame)
    if is_list_like(var_name):
        _validate_list_var_name(var_name, frame)
        return list(var_name)
    return [var_name]

  def _default_var_name(frame):
    if isinstance(frame.columns, MultiIndex):
        if len(frame.columns.names) == len(set(frame.columns.names)):
            return frame.columns.names
        return [f""variable_{i}"" for i in range(len(frame.columns.names))]
    return [frame.columns.name or ""variable""]

  def _validate_list_var_name(var_name, frame):
    if isinstance(frame.columns, MultiIndex):
        if is_iterator(var_name):
            var_name = list(var_name)
        if len(var_name) > len(frame.columns):
            raise ValueError(
                f""{var_name=} has {len(var_name)} items, ""
                f""but the dataframe columns only have {len(frame.columns)} levels.""
            )
    else:
        raise ValueError(f""{var_name=} must be a scalar."")

  var_name = determine_var_name(frame, var_name)
  ```
* Benefits:
  * Improves readability:
  Simplifies the main function, making the logic clearer and easier to follow.
  * Makes the logic easier to test and maintain:
  Enables independent testing of each helper function, ensuring robust behavior.
  * Separation of concerns:
  Each helper function is now responsible for a single, well-defined task, aligning with the principle of single responsibility.

**Replace Hard-Coded Values in pivot.py**
  * Before:
  ```python
  # Hard-coded string for margins
  margins_name: Hashable = ""All""
  ```
  * After:
  ```python
  # Define a constant for the hard-coded value
  MARGIN_NAME = ""All""
  
  # Use the constant in the code
  margins_name: Hashable = MARGIN_NAME:
  ```
  * Benefits:
    * Makes the code more readable and maintainable.
    * Centralizes the value so it can be reused or modified easily.

## Testing
**Unit Testing Helper Functions:**
Write focused tests for each new helper function to validate their behavior under expected, edge, and erroneous inputs. For example:
* Ensure validate_and_get_level() correctly identifies missing variables and raises KeyError.
* Test determine_var_name() with var_name=None, scalar inputs, and multi-level columns.

**Regression Testing Parent Functions:**
Run all pre-existing tests for the parent functions (e.g., melt()) to confirm they maintain their functionality after the refactor.

**Edge Cases:**
Include additional tests for edge scenarios, such as:
* Empty id_vars or value_vars.
* DataFrames with unusual column configurations like MultiIndex or missing names.

## Labels
* `ENH`
* `Code Quality`

## Compliance with Contributing Guide
* **Focus:** The issue is specific and addresses code quality improvements without scope creep.
* **Clarity:** Includes actionable suggestions and a clear implementation path.

### Please provide feedback and let me know if you would like further refinements!",[],2024-11-20 07:21:32,2024-12-03 01:37:45,2,closed
60369,DOC: Fix docstring typo,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/core/series.py

### Documentation problem

The docstring for the __arrow_c_stream__ method in the Series class uses the word ""behaviour"".

### Suggested fix for documentation

Suggested to rewrite as ""behavior"", which is the American English spelling, to maintain consistency with the rest of the Pandas codebase.",['Docs'],2024-11-20 07:05:18,2024-11-22 20:15:05,1,closed
60368,DOC: methods in see also section in the  pandas.DataFrame.size  are not hyperlinks,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.size.html

### Documentation problem

In the see also section the `ndarray.size` method is listed, but it is not hyperlinks and thus the reader cannot navigate with ease but has to look for them instead. 

### Suggested fix for documentation

Add numpy.ndarray.size in the docstring. ","['Docs', 'Needs Triage']",2024-11-20 05:15:40,2024-12-16 20:10:23,1,closed
60366,DOC: Update variables a and b to names consistent with comment documentation,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/core/computation/expressions.py

### Documentation problem

Lines 234 and 235 explain what a and b are in detail (left and right operands), but there are many of those same variables earlier in the file, making it harder to understand what they represent.

### Suggested fix for documentation

Assuming a and b represent right and left operands throughout each function, change these variable names to right_op and left_op instead throughout all functions to have more descriptive variable names","['Clean', 'expressions']",2024-11-20 02:33:27,2024-12-09 18:37:06,1,closed
60364,DOC: Add missing links to optional dependencies in getting_started/install.html,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/getting_started/install.html

### Documentation problem

On the “Installation” page, links are provided to the GitHub pages for the required dependencies and some of the optional dependencies, but the optional dependencies in the tables from “Visualization” onward do not link to the GitHub pages of the projects. Links are provided to the optional HTML-related dependencies are present, but not in the dependency table.

### Suggested fix for documentation

Add links to the library names which link to their respective repositories to make the page more consistent.","['Build', 'Docs']",2024-11-19 20:05:36,2024-12-02 19:09:40,8,closed
60363,DOC: Add examples for float_format in to_csv documentation,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html

### Documentation problem

The float_format parameter in to_csv is explained but lacks examples. Users might struggle to understand how to apply this parameter effectively without concrete examples in the documentation.

### Suggested fix for documentation

I suggest adding examples for float_format to make the documentation more beginner-friendly. Examples could include:

```
# Format floats to two decimal places
df.to_csv(""example1.csv"", float_format=""%.2f"")

# Use scientific notation
df.to_csv(""example2.csv"", float_format=""{:.2e}"".format)
```","['Docs', 'IO CSV']",2024-11-19 18:11:21,2024-12-03 20:31:36,1,closed
60361,DOC: Update URL in web/pandas/about/index.md,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/index.md

### Documentation problem

I discovered that the URL pointing to ""all of our contributors"" on line 9 in the `pandas/web/pandas/about/index.md` file is invalid, and I would like to address this issue. This problem might be due to an incomplete URL. It works fine when accessed from https://pandas.pydata.org/about/, but it causes problems when clicked from the source code on GitHub.

Current state:
Thank you to [all of our contributors](https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/team.html).
(Due to the original URL leading to different pages when navigating from the issue interface and the source code interface, I have modified the URL to direct to the page for source code navigation instead.)

What I propose:
Thank you to [all of our contributors](https://pandas.pydata.org/about/team.html).

If you agree with my suggestion, I will submit a PR myself.

### Suggested fix for documentation

Removed the invalid URL, replaced it with a new URL.","['Web', 'Closing Candidate']",2024-11-19 15:11:33,2024-11-20 04:21:24,3,closed
60360,WEB: Donations page doesn't work,"Seems like NumFOCUS stopped supporting the old donations system we had embedded in our website. We should probably get rid of the page, and make the link to the donations go to the donations page in opencollective.",['Web'],2024-11-19 03:00:21,2025-02-24 07:42:07,0,closed
60358,ENH: Strip/Trim `github.event.comment.user.login` on `issue_assign` job in `comment_commands.yml`,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Comments containing ""take"" with leading spaces gets skipped by the `issue_assign` job. Got a couple of experiences with this because I usually click Enter by reflex after typing in a comment. So my comment becomes`take\n`, which gets skipped by this job

### Feature Description

https://github.com/pandas-dev/pandas/blob/6a7685faf104f8582e0e75f1fae58e09ae97e2fe/.github/workflows/comment-commands.yml#L14
Add `trim()` command inside `github.event.comment.body`
```
    if: (!github.event.issue.pull_request) && trim(github.event.comment.body) == 'take'
```

### Alternative Solutions

NA; Enhancement is straight forward

### Additional Context

_No response_","['Enhancement', 'CI']",2024-11-19 00:58:22,2024-11-21 16:31:20,2,closed
60355,BUG: index.has_duplicates on a subset of a dataframe sometimes returns an incorrect outcome,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame([0, 0], index=['A','A'])

df.iloc[0:1, :].index.has_duplicates  # <-- this returns False (correctly)
df.index.has_duplicates               # <-- this somehow alters the outcome       
df.iloc[0:1, :].index.has_duplicates  # <-- exact same statement, but this returns True (incorrectly)
```


### Issue Description

The value of index.has_duplicates of a subset of a dataframe depends on whether we run index.has_duplicates on the full dataframe first. 

In the example, `df.iloc[0:1, :]` only has a single row, so it cannot have duplicates. The first call to `df.iloc[0:1, :].index.has_duplicates` correctly returns False. However, after we queried the same property on the full dataframe, `df.index.has_duplicates`, the exact same statement on the subset now returns True. 

### Expected Behavior

I would expect `df.iloc[0:1, :]` to always return False. It has only 1 row, so it cannot have duplicates.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.7
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Indexing', 'Needs Info']",2024-11-18 16:41:58,2024-11-19 07:48:06,2,closed
60353,"BUG: dataframe.to_hdf function incompatible with timestamps of format ""datetime64[us, UTC]""","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

if __name__ == ""__main__"":
    dataframe = pd.DataFrame(
        {
            ""start_time"": [
                pd.to_datetime(""2024-08-26 15:13:14.700000+00:00""),
                pd.to_datetime(""2024-08-26 15:14:14.700000+00:00""),
            ]
        }
    )
    dataframe[""start_time_us""] = dataframe.start_time.astype(""datetime64[us, UTC]"")

    dataframe.to_hdf(""test.hdf"", key=""Annotations"", mode=""w"")

    recovered_dataframe = pd.read_hdf(""test.hdf"", key=""Annotations"")

    pd.testing.assert_frame_equal(dataframe, recovered_dataframe)
```


### Issue Description

Dumping a dataframe with a column of datetime64[us, UTC] datetype to an HDF file seems to write datetime[ns, UTC] into the file. When recovering the data from the HDF file it seems that the dates are wrong, which is probably caused by an erroneous interpretation of the values as datetime[ns, UTC].

### Expected Behavior

datetime64[us, UTC] values which have been written from a dataframe into an HDF file using the to_hdf function should be recoverable using the pd.read_hdf function.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-118-generic
Version               : #128-Ubuntu SMP Fri Jul 5 09:28:59 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : 2024.5.0
fsspec                : 2024.9.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.7.3
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : 3.10.1
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO HDF5', 'Timezones', 'Needs Tests']",2024-11-18 13:20:21,2025-03-12 23:50:31,5,closed
60351,"ENH: Copy attrs on join (possibly depending on left, right, etc.)","### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

df.join() does not retain the attrs of the dataset. Given that the attrs manual states that ""many operations that create new datasets will retain attrs"", this seems like an omission.

### Feature Description

Join is different from concat because there is a clear dataframe that the operation is on. Therefore, it would seem natural if df.join() would retain the attrs of the initial dataframe.

### Alternative Solutions

It would also be possible to make the attrs dependent on ""how"" but this would only be natural for ""left"" and ""right"".

### Additional Context

_No response_","['Enhancement', 'Reshaping', 'Needs Info', 'metadata']",2024-11-18 04:26:12,2025-06-30 18:21:52,8,closed
60348,"DOC (string dtype): update user guide page ""Working with text data""","With the new default string dtype in pandas 3.0, we should update the documentation to properly reflect this. The main page about working with string data is https://pandas.pydata.org/pandas-docs/version/2.2/user_guide/text.html

This page currently mentions `object` dtype vs nullable `StringDtype`, some differences, and then shows most examples using `dtype=""string""`.

We should update that page to reflect that there is now a default `""str""` dtype, add a historical note about `object` dtype being the default before pandas 3.0 (and that you can still encounter this, and then how to convert to str dtype; and refer to the upgrade guide), mention the differences of ""str"" vs ""string"" dtype, etc","['Docs', 'Strings']",2024-11-17 12:38:51,2025-11-04 17:28:15,5,closed
60343,"BUG (string): contruction of Series / Index fails from dict keys when ""str"" dtype is specified explicitly","When not specifying a dtype (inferring the type), construction of `Index` or `Series` from dict keys goes fine:

```python
>>> pd.options.future.infer_string = True
>>> d = {""a"": 1, ""b"": 2}
>>> pd.Index(d.keys())
Index(['a', 'b'], dtype='str')
```

But if you explicitly specify the dtype, then it fails:
```python
>>> pd.Index(d.keys(), dtype=""str"")
...

File ~/scipy/repos/pandas/pandas/core/arrays/string_arrow.py:206, in ArrowStringArray._from_sequence(cls, scalars, dtype, copy)
    203     return cls(pc.cast(scalars, pa.large_string()))
    205 # convert non-na-likes to str
--> 206 result = lib.ensure_string_array(scalars, copy=copy)
    207 return cls(pa.array(result, type=pa.large_string(), from_pandas=True))

File lib.pyx:727, in pandas._libs.lib.ensure_string_array()

File lib.pyx:822, in pandas._libs.lib.ensure_string_array()

ValueError: Buffer has wrong number of dimensions (expected 1, got 0)
```

The reason is that at that point we pass the data directly to the dtype's array `_from_sequence` instead of first pre-processing the data into a numpy array, and `_from_sequence` calling `ensure_string_array` directly doesn't seem to be able to handle dict keys (although we do call `np.asarray(..)` inside `ensure_string_array`, so not entirely sure what is going wrong)
","['Bug', 'Strings', 'Constructors']",2024-11-17 08:31:05,2025-01-26 11:29:26,9,closed
60341,BUG: to_datetime wraps datetime64[ps] as if it were datetime64[ns]   ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

print(""Good:"", pd.to_datetime([np.datetime64(1901901901901, ""as"")]))
print(""Good:"", pd.to_datetime([np.datetime64(1901901901901, ""fs"")]))
print("" Bad:"", pd.to_datetime([np.datetime64(1901901901901, ""ps"")]))
print(""Good:"", pd.to_datetime([np.datetime64(1901901901901, ""ns"")]))
print(""Good:"", pd.to_datetime([np.datetime64(1901901901901, ""us"")]))
print(""Good:"", pd.to_datetime([np.datetime64(1901901901901, ""ms"")]))


# Good: DatetimeIndex(['1970-01-01 00:00:00.000001901'], dtype='datetime64[ns]', freq=None)
# Good: DatetimeIndex(['1970-01-01 00:00:00.001901901'], dtype='datetime64[ns]', freq=None)
#  Bad: DatetimeIndex(['1970-01-01 00:31:41.901901901'], dtype='datetime64[ns]', freq=None)
# Good: DatetimeIndex(['1970-01-01 00:31:41.901901901'], dtype='datetime64[ns]', freq=None)
# Good: DatetimeIndex(['1970-01-23 00:18:21.901901'], dtype='datetime64[ns]', freq=None)
# Good: DatetimeIndex(['2030-04-08 18:05:01.901000'], dtype='datetime64[ns]', freq=None)
```


### Issue Description

Wrapping np.datetime64[ps] data with pd.to_datetime is wrongly converted as if it were np.datetime64[ns].

### Expected Behavior

`DatetimeIndex(['1970-01-01 00:00:01.901901901'], dtype='datetime64[ns]', freq=None)`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.5.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-122-generic
Version               : #132-Ubuntu SMP Thu Aug 29 13:45:52 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8
pandas                : 2.2.2
numpy                 : 2.1.1
pytz                  : 2024.2
dateutil              : 2.9.0
setuptools            : 73.0.1
pip                   : 24.2
Cython                : None
pytest                : 8.3.3
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.28.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.9.0
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : 2024.10.0
xlrd                  : None
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Non-Nano', 'Timestamp']",2024-11-16 20:27:52,2024-11-20 14:47:34,6,closed
60340,DEPR: deprecate / warn about raising an error in __array__ when copy=False cannot be honore,"The numpy 2.0 changed the behavior of the `copy` keyword in `__array__`, and especially making `copy=False` to be strict (raising an error when a zero-copy numpy array is not possible). 
We only adjusted pandas to update the `copy` handling now in https://github.com/pandas-dev/pandas/pull/60046 (issue https://github.com/pandas-dev/pandas/issues/57739).

But that also introduced a breaking change for anyone doing `np.array(ser, copy=False)` (and who hasn't updated that when updating to numpy 2.0), which historically has always worked fine and could silently give a copy anyway.

The idea would be to still include a FutureWarning about this first before raising the error (as now in main) in pandas 3.0.
 
See https://github.com/pandas-dev/pandas/pull/60046#issuecomment-2457749926 for more context",['Compat'],2024-11-16 19:39:50,2025-01-13 08:33:09,12,closed
60338,API: creating DataFrame with no columns: object vs string dtype columns?,"A typical case we encounter in the tests is starting from an empty DataFrame, and then adding some columns. 

Simplied example of this pattern:

```python
df = pd.DataFrame()
df[""a""] = values
...
```

The dataframe starts with an empty `Index` columns, and the default dtype for an empty Index is `object` dtype. And then inserting string labels for the actual columns into that Index object, preserves the `object` dtype.

As long as we used object dtype for string column names, this was perfectly fine. But now that we will infer `str` dtype for actual string column names, it gets a bit annoying that the pattern above does not result in `str` but `object` colums.

This is not the best pattern, so maybe it's OK this does not give the ideal result. But at the same since we even use it quite regularly in our own tests, I suppose this is not _that_ uncommon.

","['API Design', 'Strings', 'Index']",2024-11-16 17:19:29,2025-04-09 18:35:43,9,closed
60322,"BUG: Specifying `hour` param, but not year, month, day in pandas.Timestamp() sets hour-value as minutes","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# returns False
pd.Timestamp(2020, 1, 1, hour=1) == pd.Timestamp(year=2020, month=1, day=1, hour=1)
```


### Issue Description

When not explicitly defining keyword args for `year`, `month` and `day`, but doing so for the `hour` sets the value provided in the `hour` as minutes instead. The values for year, month and day are still correct though. 

### Expected Behavior

I'd expect the value provided in the `hour` param to be set to the hour, and not the minutes. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Duplicate Report', 'Timestamp']",2024-11-15 11:40:32,2024-11-17 13:40:59,2,closed
60309,BUG: assignment fails with copy_on_write = True,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.options.mode.copy_on_write = True
# pd.options.mode.copy_on_write = ""warn""
dftest = pd.DataFrame({""A"":[1,4,1,5], ""B"":[2,5,2,6], ""C"":[3,6,1,7]})
df=dftest[[""B"",""C""]]
df.iloc[[1,3],:] = [[2, 2],[2 ,2]]
```


### Issue Description

The result is the following error output:

Traceback (most recent call last):
  File ""/home/rolf/jupyter/venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py"", line 1429, in setitem
    values[indexer] = casted
    ~~~~~~^^^^^^^^^
ValueError: shape mismatch: value array of shape (2,2) could not be broadcast to indexing result of shape (2,)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/rolf/code/python/bug.py"", line 7, in <module>
    df.iloc[[1,3],:] = [[2, 2],[2 ,2]]
    ~~~~~~~^^^^^^^^^
  File ""/home/rolf/jupyter/venv/lib/python3.12/site-packages/pandas/core/indexing.py"", line 911, in __setitem__
    iloc._setitem_with_indexer(indexer, value, self.name)
  File ""/home/rolf/jupyter/venv/lib/python3.12/site-packages/pandas/core/indexing.py"", line 1944, in _setitem_with_indexer
    self._setitem_single_block(indexer, value, name)
  File ""/home/rolf/jupyter/venv/lib/python3.12/site-packages/pandas/core/indexing.py"", line 2218, in _setitem_single_block
    self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/rolf/jupyter/venv/lib/python3.12/site-packages/pandas/core/internals/managers.py"", line 409, in setitem
    self.blocks[0].setitem((indexer[0], np.arange(len(blk_loc))), value)
  File ""/home/rolf/jupyter/venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py"", line 1432, in setitem
    raise ValueError(
ValueError: setting an array element with a sequence.


### Expected Behavior

Should not give an error. When uncommenting the copy_on_write=""warn"" line the code runs with no error message as expected (I'd expect a warning in this case, but thats a separate issue)

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-47-generic
Version               : #47-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 27 21:40:26 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : de_DE.UTF-8
LOCALE                : de_DE.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Indexing', 'Regression']",2024-11-14 09:54:06,2025-09-22 02:33:33,9,closed
60307,DOC: Dataframe.from_records should not say that passing in a DataFrame for data is allowed,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.from_records.html#pandas.DataFrame.from_records

### Documentation problem

The first text in the docstring says (emphasis at the end is mine)

> Convert structured or record ndarray to DataFrame.
>
> Creates a DataFrame object from a structured ndarray, sequence of
> tuples or dicts, or **DataFrame**.

However, starting in 2.1.0, passing in a DataFrame has been deprecated. In 2.1.0 it would raise a FutureWarning; in main it will raise a TyperError.

The documentation between 2.1.0 and main appear to have been updated to remove text in the Parameters section of the docstring that still said a DataFrame could be passed in for data, but the text in the initial section of the docstring was not.

### Suggested fix for documentation

Change the initial docstring text to be:

> Convert structured or record ndarray to DataFrame.
>
> Creates a DataFrame object from a structured ndarray or sequence of
> tuples or dicts.","['Docs', 'IO Data', 'good first issue']",2024-11-13 22:05:30,2024-11-14 17:18:58,2,closed
60303,BUG: inconsistent treatment of overflows between groupby.sum() and groupby.apply(lambda: _grp: _grp.sum()) and DataFrame.resample,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd
s = pd.Series([1.797693e+308, 1.797693e+308, 1.797693e+308])
s.groupby([1, 1, 1]).sum()

1   NaN
dtype: float64

s.groupby([1, 1, 1]).apply(lambda _grp: _grp.sum())
1    inf
dtype: float64

df = pd.DataFrame({""col"": [1.797693e+308, 1.797693e+308, 1.797693e+308]}, index=pd.DatetimeIndex([pd.Timestamp(""1970-01-01 00:00:00.000000000""), pd.Timestamp(""1970-01-01 00:00:00.000000001""), pd.Timestamp(""1970-01-01 00:00:00.000000002"")]))
df.resample('1min').agg(col=(""col"", ""sum""))

col
1970-01-01	NaN
```


### Issue Description

I was initially doing resampling with some large numbers and saw that when an overflow happens during a sum aggregator the float result becomes `NaN` instead of `inf`. I did some digging and I believe the problem is similar to #53606 so I did some testing with groupby similar to what was done in #53606.

This behavior also contradicts what pure python `sum` and `numpy.sum` methods do as they both return `inf`.

### Expected Behavior

Return infinity (or -infinity) in case of overflow.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.9
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : 8.29.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : 6.72.4
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'Reduction Operations']",2024-11-13 16:45:24,2025-11-25 19:00:19,1,closed
60301,API: return value of `.values` for Series with the future string dtype (numpy array vs extension array),"Historically, the `.values` attribute returned a numpy array (except for categoricals). When we added more ExtensionArrays, for certain dtypes (e.g. tz-aware timestamps, or periods, ..) the EA could more faithfully represent the underlying values instead of the lossy conversion to numpy (e.g for tz-aware timestamps we decided to return a numpy object dtype array instead of ""datetime64[ns]"" to not lose the timezone information). At that point, instead of ""breaking"" the behaviour of `.values`, we decided to add an `.array` attribute that then always returns the EA.

But for generic ExtensionArrays (external, or non-default EAs like the masked ones or the Arrow ones), the `.values` has always already directly returned the EA as well. So in those cases, there is no difference between `.values` and `.array`.

Now to the point: with the new default `StringDtype`, the current behaviour is indeed to also always return the EA for both `.values` and `.array`.

This means this is one of the breaking changes for users when upgrading to pandas 3.0, that for a column which is inferred as string data, the `.values` no longer returns a numpy array.

**Are we OK with this breaking change now?**  
Or, we could also decide to keep `.values` return the numpy array with `.array` returning the EA. 

Of course, when we would move to use EAs for all dtypes (which is being considered in the logical dtypes and missing values PDEP discussions), then we would have this breaking change as well (or at least need to make a decision about it). But, that could also be a reason to not yet do it for the string dtype now, if we would change it for all dtypes later.

cc @pandas-dev/pandas-core 

","['Docs', 'API Design', 'Strings']",2024-11-13 14:36:21,2025-08-26 21:36:59,12,closed
60298,ENH: Unique key detection function,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Hello,
This is a feature I haven't seen in any data prepation/etl. The core feature is to detect the unique key in a dataframe. More than often, you have to deal with a dataset without knowing what's make a row unique. This can lead to misinterpret the data, cartesian product at join and other funny stuff.

### Feature Description

How do I imagine that ?

Entry parameters; one dataframe, ability to specify a max number of field for combination (empty or 0=no max). 
Algo : it tests the count distinct every combination of field versus the count of rows

Result : a dataframe with one row by field combination that works. If no result : ""no field combination is unique. check for duplicate or need for aggregation upstream"".


ex : 

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/saubert/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/saubert/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\,"";
	mso-displayed-thousand-separator:"" "";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">


order_id | line_id | amount | customer | site
-- | -- | -- | -- | --
1 | 1 | 100 | A | U_250
1 | 2 | 12 | A | U_250
1 | 3 | 45 | A | U_250
2 | 1 | 75 | A | U_250
2 | 2 | 12 | A | U_250
3 | 1 | 15 | B | U_250
4 | 1 | 45 | B | U_251



</body>

</html>

The user will previously select every field but excluding Amount (he knows that Amount would have no sense in key)

The algo will test the following key
-each separate field
-each combination of two fields
-each combination of three fields
-each combination of four fields

to match the number of row (7)
And gives something like that

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/saubert/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/saubert/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\,"";
	mso-displayed-thousand-separator:"" "";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">


choice | number of fields | field combination
-- | -- | --
very good | 2 | order_id,line_id
average | 3 | order_id,line_id, customer
average | 3 | order_id,line_id, site
bad | 4 | order_id,line_id, site, customer
… | … | ….



</body>

</html>


Best regards,

Simon

### Alternative Solutions

N/A

### Additional Context

_No response_","['Enhancement', 'Closing Candidate']",2024-11-13 09:52:08,2025-04-24 21:34:01,2,closed
60287,"Change default string storage from ""python"" to ""pyarrow"" (if installed) for for NA-variant of StringDtype","Historically, the default value for the string storage (globally configurable through `pd.options.mode.string_storage`) of `StringDtype` was `""python""`, and users needed to explicitly ask for `""pyarrow""`. For example:

```python
>>> ser = pd.Series([""a"", ""b""], dtype=""string"")
>>>  ser.dtype
string[python]
```

and this is still the behaviour on `main`.

For the new NaN-variant of `StringDtype`, however, we implemented the default string storage option `""auto""` meaning ""use pyarrow if installed, otherwise use python"". So on a system with pyarrow installed:

```python
>>> pd.options.future.infer_string = True
>>> ser = pd.Series([""a"", ""b""], dtype=""str"")
>>> ser.dtype.storage
'pyarrow'
```

Essentially we interpret the default `string_storage` option setting of `""auto""` differently for the NaN vs NA variant of the string dtype, which you can see in the code here:

https://github.com/pandas-dev/pandas/blob/5f23aced2f97f2ed481deda4eaeeb049d6c7debe/pandas/core/arrays/string_.py#L152-L163

---

__Proposal__: I think it makes sense to also switch to ""pyarrow"" as the default string storage (if installed) for the nullable StringDtype. This is somewhat a breaking change (although mostly for the dtype object itself, because behaviour-wise for string operations, there should be hardly any difference between both backends), so I would keep this for 3.0 and properly document it in the whatsnew notes.
","['API Design', 'Strings', 'NA - MaskedArrays']",2024-11-12 13:43:06,2025-09-08 16:40:21,2,closed
60282,BUG (string dtype): `replace()` value in string column with non-string should cast to object dtype instead of raising an error,"For all other dtypes (I think, just checked with the one below), if the value to replace with in `replace()` doesn't fit into the calling series, then we ""upcast"" to object dtype and then do the replacement anyway.

Simple example with an integer series:

```python
>>> ser = pd.Series([1, 2])
>>> ser.replace(1, ""str"")
0    str
1      2
dtype: object
```

However, for the future string dtype, and then trying to replace a value with a non-string, we do _not_ cast to object dtype currently, but raise instead:

```python
>>> pd.options.future.infer_string = True
>>> ser = pd.Series([""a"", ""b""])
>>> ser.replace(""a"", 1)
...
File ~/scipy/repos/pandas/pandas/core/internals/blocks.py:713, in Block.replace(self, to_replace, value, inplace, mask)
    709 elif self._can_hold_element(value):
    710     # TODO(CoW): Maybe split here as well into columns where mask has True
    711     # and rest?
    712     blk = self._maybe_copy(inplace)
--> 713     putmask_inplace(blk.values, mask, value)
    714     return [blk]
    716 elif self.ndim == 1 or self.shape[0] == 1:
...

File ~/scipy/repos/pandas/pandas/core/arrays/string_.py:746, in __setitem__(self, key, value)
...
TypeError: Invalid value '1' for dtype 'str'. Value should be a string or missing value, got 'int' instead.
```

Making `replace()` strict (preserve dtype) in general is a much bigger topic, so I think for now we should just keep the current behaviour of upcasting to object dtype when needed.","['Bug', 'Strings', 'replace']",2024-11-12 10:18:23,2024-11-12 21:41:47,0,closed
60276,"BUG: `from pandas.core.tools.datetimes import parsing` no longer works since pandas 2.2.0, not documented","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas
from pandas.core.tools.datetimes import parsing  # since pandas 2.2.0 raises ImportError: cannot import name 'parsing' from 'pandas.core.tools.datetimes'
```


### Issue Description

Since pandas v2.2.0, `from pandas.core.tools.datetimes import parsing` raises `ImportError: cannot import name 'parsing' from 'pandas.core.tools.datetimes'`

This is not documented in the changelog, so maybe it is an accident?

### Expected Behavior

I can import successfully

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457
python                : 3.12.7.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 24.1.0
Version               : Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.2.0
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0
setuptools            : 75.3.0
pip                   : 24.3.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Internals', 'Needs Triage', 'Closing Candidate']",2024-11-10 17:52:08,2024-11-10 19:53:08,2,closed
60275,"DOC: Document merge_cells=""columns"" in to_excel","https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_excel.html

The `merge_cells` argument can also take `""columns""` due to #35384. This should be added to the docstring.","['Docs', 'IO Excel', 'good first issue']",2024-11-10 16:03:58,2024-11-13 21:07:35,2,closed
60274,API: to_excel with merge_cells=False treats index and columns differently,"```python
df = pd.DataFrame({""a"": [1, 1], ""b"": [2, 3], ""c"": 4, ""d"": 5}).set_index([""a"", ""b""])
df.columns = pd.MultiIndex.from_tuples([(""x"", ""y""), (""x"", ""z"")])
df.to_excel(""test.xlsx"", merge_cells=False)
```

![image](https://github.com/user-attachments/assets/7d766e73-6869-4af5-9abe-e66e1f9dcaf6)

In the above the multi-level nature of the index is maintained, however the columns are collapsed to a single row in the form `x.y, x.z`. It seems to me that these should be treated the same. The output should be multiple rows of non-merged cells.","['Bug', 'IO Excel', 'MultiIndex']",2024-11-10 15:43:32,2024-11-13 21:02:28,3,closed
60273,BUG: period[h] + column splicing is not work,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
pr = pd.period_range('2024-01-01 00:00:00', '2024-01-01 02:00:00', freq='h')
df = pd.DataFrame(index=pr)
df['date'] = df.index.to_timestamp().floor('D')
df['hour'] = df.index.hour
df.index.name = 'value'
df = df.reset_index()
df = df.pivot(index='date', columns='hour', values='value')

print(df)
# hour                       0                 1                 2
# date                                                            
# 2024-01-01  2024-01-01 00:00  2024-01-01 01:00  2024-01-01 02:00

print(df[[0,1,2]])
# hour                       0                 1                 2
# date                                                            
# 2024-01-01  2024-01-01 00:00  2024-01-01 00:00  2024-01-01 00:00
```


### Issue Description

when the datatype is period[h], the slicing will not produce the correct results.
if the datatype is changed to object. result is correct

### Expected Behavior

The expected behavior is to slice the column properly based on hour [0, 1, 2].
However the above results give all [0] column for all of [0, 1, 2]

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.85+
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.8.2
setuptools            : 75.1.0
pip                   : 24.1.2
Cython                : 3.0.11
pytest                : 7.4.4
hypothesis            : None
sphinx                : 5.0.2
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.3.0
html5lib              : 1.1
pymysql               : None
psycopg2              : 2.9.10
jinja2                : 3.1.4
IPython               : 7.34.0
pandas_datareader     : 0.10.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
gcsfs                 : 2024.10.0
matplotlib            : 3.8.0
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.24.0
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.36
tables                : 3.8.0
tabulate              : 0.9.0
xarray                : 2024.10.0
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Indexing', 'Period']",2024-11-10 03:53:44,2025-03-17 16:55:32,7,closed
60272,"ENH: When running json_normalize over a pandas dataframe, be able to insert/keep an index/Id series/column","### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Currently I am running a pandas Json_normalize in a dataframe where one of its columns is a nested json. The output dataframe is a unnested json with all fields and values.

After facing my needed, I´ve questioned about it on [stackoverflow](https://stackoverflow.com/questions/79170754/unnesting-a-pandas-json-column-and-keeping-an-id-column), but I´ve realized that the solution os not really ""user friendly"".



### Feature Description

So, I would like to suggest that when running a json_normalize over a dataframe, I could set anoter column/series to be kept with the resulting dataframe.

### Alternative Solutions

Concatenating commands is a solution, but not really ""easy"" (and I don´t know about performance impact using this)

```
import pandas as pd
import json

data = {
            ""id de transação"": [1, 2, 3, 4, 5],
            ""nome"": [""Alice"", ""Bob"", ""Charlie"", ""David"", ""Eve""],
            ""dados"": [
                {""data"": ""2024-01-01"", ""local"": ""São Paulo"", ""valor"": 100.50},
                {""data"": ""2024-01-02"", ""local"": ""Rio de Janeiro"", ""valor"": 200.75},
                {""data"": ""2024-01-03"", ""local"": ""Belo Horizonte"", ""valor"": 300.00},
                {""data"": ""2024-01-04"", ""local"": ""Curitiba"", ""valor"": 400.25},
                {""data"": ""2024-01-05"", ""local"": ""Porto Alegre"", ""valor"": 500.50}
            ]
        }
df = pd.DataFrame(data)

out = (df[['id de transação', 'nome']]
       .join(pd.json_normalize(data=df['dados'], record_path=None)
               .set_axis(df.index)
            )
      )
```

### Additional Context

_No response_","['Enhancement', 'IO JSON', 'Closing Candidate']",2024-11-09 22:40:41,2024-11-10 16:08:10,5,closed
60258,REF: Replace maybe_infer_to_datimelike with maybe_convert_objects,"`maybe_infer_to_datimelike` has an interesting side effect of changing object dtypes to string dtypes, which is surprising. The original conversation can be seen at https://github.com/pandas-dev/pandas/pull/60255/files#r1835048393

I think this is something that can be refactored when we are doing with the 2.3 series",['Clean'],2024-11-08 22:34:50,2025-08-28 17:46:45,0,closed
60244,BUG: dropna throws error with arguments axis=1 and subset not None,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({""name"": ['Alfred', 'Batman', 'Catwoman'], ""toy"": [np.nan, 'Batmobile', 'Bullwhip'], ""born"": [pd.NaT, pd.NaT,pd.NaT]})
df.dropna(axis=1, subset=['born'])
```


### Issue Description

Traceback (most recent call last):
  File ""/home/vscode/.local/lib/python3.10/site-packages/pandas/core/frame.py"", line 6670, in dropna
    raise KeyError(np.array(subset)[check].tolist())
KeyError: ['born']

### Expected Behavior

dropna should return the dataframe without the `born` column.

### Installed Versions

<details>

commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.15
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.5
Version               : #1-NixOS SMP PREEMPT_DYNAMIC Tue Oct 22 13:51:37 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 23.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Closing Candidate']",2024-11-08 14:21:54,2024-11-08 21:48:49,2,closed
60237,"BUG: `.convert_dtypes(dtype_backend=""pyarrow"")` strips timezone from tz-aware pyarrow timestamp Series","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> import pandas as pd
>>> s = pd.Series(pd.to_datetime(range(5), utc=True, unit=""h""), dtype=""timestamp[ns, tz=UTC][pyarrow]"")
>>> s
0    1970-01-01 00:00:00+00:00
1    1970-01-01 01:00:00+00:00
2    1970-01-01 02:00:00+00:00
3    1970-01-01 03:00:00+00:00
4    1970-01-01 04:00:00+00:00
dtype: timestamp[ns, tz=UTC][pyarrow]
>>> s.convert_dtypes(dtype_backend=""pyarrow"")
0    1970-01-01 00:00:00
1    1970-01-01 01:00:00
2    1970-01-01 02:00:00
3    1970-01-01 03:00:00
4    1970-01-01 04:00:00
dtype: timestamp[ns][pyarrow]
```


### Issue Description

Calling `.convert_dtypes(dtype_backend=""pyarrow"")` on a Series that is already a timezone aware pyarrow timestamp dtype strips the timezone information.

Testing on older versions, this seems to be a regression introduced sometime between versions 2.0.3 and 2.1.0rc0

### Expected Behavior

No change should be made to the dtype

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 3f7bc81ae6839803ecc0da073fe83e9194759550
python                : 3.12.2
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 3.0.0.dev0+1654.g3f7bc81ae6
numpy                 : 2.1.3
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Dtype Conversions', 'good first issue', 'Arrow', 'Localization']",2024-11-08 06:29:30,2025-02-18 20:36:00,10,closed
60236,BUG: .duplicated() ignoring duplicates for MultiIndex,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
#!/usr/bin/env python3

import pandas as pd

midx = pd.MultiIndex.from_product([(0, 1), (0, 1)], names=('x', 'y'))
midx = midx.set_levels([0] * len(midx), level='x', verify_integrity=False)

print(midx)
print('Duplicated: ', midx.duplicated())
print('Unique: ',midx.is_unique)
```


### Issue Description

Pandas does not detect multiindex duplicates that were created using `set_levels()`.
MRE outputs:
```
MultiIndex([(0, 0),
            (0, 1),
            (0, 0),
            (0, 1)],
           names=['x', 'y'])
Duplicated:  [False False False False]
Unique:  True
```

Python debugger cuts out in `multi.py::duplicated()` and I think the final error is somewhere in autogenerated cython hashtable bindings [here](https://github.com/pandas-dev/pandas/blob/3f7bc81ae6839803ecc0da073fe83e9194759550/pandas/_libs/hashtable_func_helper.pxi.in#L125)? I'm not sure what to do to debug from multi.py onward.

I found https://github.com/pandas-dev/pandas/issues/27035#issuecomment-505446429, that mentions missing preconditions check that might be related, but this is pure speculation on my part. Besides, tuples should be hashable.

### Expected Behavior

Detect duplicates/non-uniqueness. MRE outputs:
```
MultiIndex([(0, 0),
            (0, 1),
            (0, 0),
            (0, 1)],
           names=['x', 'y'])
Duplicated:  [False False True True]
Unique:  False
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 3f7bc81ae6839803ecc0da073fe83e9194759550
python                : 3.12.7
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.4-gentoo
Version               : #1 SMP PREEMPT_DYNAMIC Tue Oct 22 20:38:14 CEST 2024
machine               : x86_64
processor             : AMD Ryzen 5 4500 6-Core Processor
byteorder             : little
LC_ALL                : None
LANG                  : en_IE.utf8
LOCALE                : en_IE.UTF-8

pandas                : 3.0.0.dev0+1654.g3f7bc81ae
numpy                 : 2.1.3
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'MultiIndex', 'Closing Candidate']",2024-11-08 04:12:29,2024-11-29 13:28:13,6,closed
60234,BUG (string dtype): logical operation with bool and string failing,"We do allow using logical operators like `|` to be used with non-boolean data (at which point the non-bool series would be cast to bool, I assume). For example:

```python
>>> ser1 = pd.Series([False, False])
>>> ser2 = pd.Series([0.0, 0.1])
>>> ser1 | ser2
0    False
1     True
dtype: bool
```

This also worked with strings in object dtype:

```python
>>> ser2 = pd.Series(["""", ""b""], dtype=object)
>>>  ser1 | ser2
0    False
1     True
dtype: bool
```

but currently fails with the pyarrow-backed string dtype:

```python
>>> pd.options.future.infer_string = True
>>> ser2 = pd.Series(["""", ""b""])
>>> ser1 | ser2
...

File ~/scipy/repos/pandas/pandas/core/arrays/arrow/array.py:833, in ArrowExtensionArray._logical_method(self, other, op)
    831     return self._evaluate_op_method(other, op, ARROW_BIT_WISE_FUNCS)
    832 else:
--> 833     return self._evaluate_op_method(other, op, ARROW_LOGICAL_FUNCS)

File ~/scipy/repos/pandas/pandas/core/arrays/arrow/array.py:824, in ArrowExtensionArray._evaluate_op_method(self, other, op, arrow_funcs)
    822     result = pc_func(self._pa_array, other)
    823 except pa.ArrowNotImplementedError as err:
--> 824     raise TypeError(self._op_method_error_message(other_original, op)) from err
    825 return type(self)(result)

TypeError: operation 'ror_' not supported for dtype 'str' with dtype 'bool'
```","['Bug', 'Numeric Operations', 'Strings']",2024-11-07 21:39:53,2025-07-29 20:52:05,8,closed
60229,BUG/API: sum of a string column with all-NaN or empty,"We decided to allow the `sum` operation for the future string dtype (PR in https://github.com/pandas-dev/pandas/pull/59853/, based on discussion in https://github.com/pandas-dev/pandas/issues/59328). 

But I ran into a strange case in groupby where the end result contains `""0""` in case of an empty or all-NaN group.

Reproducible example:

```python
df = pd.DataFrame(
    {
        ""key"": [1, 2, 2, 3, 3, 3],
        ""col1"": [np.nan, 2, np.nan, 4, 5, 6],
        ""col2"": [np.nan, ""b"", np.nan, ""d"", ""e"", ""f""],
    }
)
result = df.groupby(""key"").sum()
```

Currently, you get this:

```python
>>> result
     col1 col2
key           
1     0.0    0
2     2.0    b
3    15.0  def

>>> result[""col2""].values
array([0, 'b', 'def'], dtype=object)
```

So the ""sum"" operation has introduced a `0`. Not very useful I think in context of strings, but at least it is object dtype and can contain anything.

However, with `pd.options.future.infer_string = True` enabled and starting from a proper string dtype, the result is seemingly the same (the repr looks the same), but the values in the column are now strings:

```python
>>> result[""col2""].values
<ArrowStringArrayNumpySemantics>
['0', 'b', 'def']
Length: 3, dtype: str
```

So the integer `0` has been converted to a string.

I think we certainly should not introduce this `""0""` string, and returning object dtype with `0` is also quite useless I think (but at least not inventing a new string in your data). 
But if we have to return something, the empty string is probably the equivalent of 0 in case of string or the ""sum"" operation?

cc @rhshadrach @WillAyd @Dr-Irv 

","['Bug', 'Groupby', 'Strings']",2024-11-07 15:55:19,2025-03-10 16:06:55,20,closed
60228,BUG (string dtype): comparison of string column to mixed object column fails,"At the moment you can freely compare with mixed object dtype column:

```python
>>> ser_string = pd.Series([""a"", ""b""])
>>> ser_mixed = pd.Series([1, ""b""])
>>> ser_string == ser_mixed
0    False
1     True
dtype: bool
```

But with the string dtype enabled (using pyarrow), this now raises an error:

```python
>>> pd.options.future.infer_string = True
>>> ser_string = pd.Series([""a"", ""b""])
>>> ser_mixed = pd.Series([1, ""b""])
>>> ser_string == ser_mixed
...
File ~/scipy/repos/pandas/pandas/core/arrays/arrow/array.py:510, in ArrowExtensionArray._box_pa_array(cls, value, pa_type, copy)
...
--> 510     pa_array = pa.array(value, from_pandas=True)
...
ArrowInvalid: Could not convert 'b' with type str: tried to convert to int64
```

This happens because the ArrowEA tries to convert the `other` operand to Arrow as well, which fails for mixed types.

In general, I think our rule is that `==` comparison never fails, but then just gives False for when values are not comparable. ","['Bug', 'Numeric Operations', 'Strings']",2024-11-07 15:29:13,2025-09-29 11:36:07,2,closed
60227,"DOC: Improve documentation df.interpolate() for methods   ‘time’, ‘index’ and ‘values’","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html

### Documentation problem

It is not possible to understand what exactly the method `interpolate` does from reading the documentation. See e.g. this SE post for more details

https://stackoverflow.com/questions/65511992/pandas-interpolation-type-when-method-index

### Suggested fix for documentation

Rewrite doctstring and documentation page for the method","['Docs', 'Missing-data']",2024-11-07 13:46:35,2024-11-12 21:46:06,2,closed
60224,BUG: Cannot import pandas2.2.3 in free-threading python3.13.0,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.
- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.
- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
python3.13t -m venv myenv
myenv\Scripts\activate

(myenv)pip install pandas==2.2.3
(myenv)python
(myenv)>>> import pandas
```


### Issue Description

According to [release note](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v2.2.3.html), pandas2.2.3 can be used in free-threading python. But when importing pandas(2.2.3) in python3.13t on windows11, python would exit without output or error message:

```
E:\>python3.13t -m venv myenv
E:\>myenv\Scripts\activate
(myenv) E:\>pip install pandas==2.2.3
Collecting pandas==2.2.3
  Using cached pandas-2.2.3-cp313-cp313t-win_amd64.whl
Collecting numpy>=1.26.0 (from pandas==2.2.3)
  Using cached numpy-2.1.3-cp313-cp313t-win_amd64.whl.metadata (60 kB)
Collecting python-dateutil>=2.8.2 (from pandas==2.2.3)
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas==2.2.3)
  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas==2.2.3)
  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.3)
  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)
Using cached numpy-2.1.3-cp313-cp313t-win_amd64.whl (12.6 MB)
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)
Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)
Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas
Successfully installed numpy-2.1.3 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2024.2 six-1.16.0 tzdata-2024.2

[notice] A new release of pip is available: 24.2 -> 24.3.1
[notice] To update, run: python.exe -m pip install --upgrade pip

(myenv) E:\>python
Python 3.13.0 experimental free-threading build (tags/v3.13.0:60403a5, Oct  7 2024, 09:53:29) [MSC v.1941 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas

(myenv) E:\>
```

Then I built cpython with debug flag and tried again, the output is as follows:
```
(debugenv) E:\cpython\cpython-3.13.0\PCbuild\amd64>python
Python 3.13.0 experimental free-threading build (main, Nov  4 2024, 19:14:19) [MSC v.1940 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas
Traceback (most recent call last):
  File ""<python-input-0>"", line 1, in <module>
    import pandas
  File ""E:\cpython\cpython-3.13.0\PCbuild\amd64\debugenv\Lib\site-packages\pandas\__init__.py"", line 49, in <module>
    from pandas.core.api import (
    ...<62 lines>...
    )
  File ""E:\cpython\cpython-3.13.0\PCbuild\amd64\debugenv\Lib\site-packages\pandas\core\api.py"", line 1, in <module>
    from pandas._libs import (
    ...<4 lines>...
    )
  File ""E:\cpython\cpython-3.13.0\PCbuild\amd64\debugenv\Lib\site-packages\pandas\_libs\__init__.py"", line 16, in <module>
    import pandas._libs.pandas_parser  # isort: skip # type: ignore[reportUnusedImport]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ImportError: DLL load failed while importing pandas_parser: The specified module could not be found.
```

### Expected Behavior

Pandas should be correctly imported.

### Installed Versions

I can't use pandas in python3.13t, so it's a little hard to execute `pd.show_versions()`.","['Bug', 'Build', 'Needs Triage']",2024-11-07 02:53:49,2024-11-08 01:53:40,6,closed
60213,PERF: index.unique much slower than get_level_values.drop_duplicates,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

It is not very important but still quite surprising. unique should be the method to use and faster but is twice slower.

df=pd.DataFrame({""M"": [""M1"",""M2""], ""P"": [""P1"", ""P2""], ""V"": [1.,2.]})
i = df.set_index(['M','P']).index

In [6]: %timeit i.unique(""M"")
30.9 µs ± 958 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [7]: %timeit i.get_level_values('M').drop_duplicates()
16.1 µs ± 84 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.7
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.5-200.fc40.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Tue Oct 22 19:13:11 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.3
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 23.3.2
Cython                : 3.0.9
sphinx                : None
IPython               : 8.23.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
html5lib              : 1.1
hypothesis            : None
gcsfs                 : 2023.6.0+1.g7cc53d9
jinja2                : 3.1.4
lxml.etree            : 5.1.0
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : 7.4.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.3
sqlalchemy            : 2.0.36
tables                : N/A
tabulate              : 0.9.0
xarray                : N/A
xlrd                  : 2.0.1
xlsxwriter            : 3.1.9
zstandard             : 0.22.0
tzdata                : 2024.2
qtpy                  : 2.4.1
pyqt5                 : None


</details>


### Prior Performance

_No response_","['Performance', 'Algos', 'Closing Candidate']",2024-11-06 09:49:06,2025-08-05 16:51:36,1,closed
60210,BUG: rowspan in read_html failed,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# html string
s = '<table>\n<caption>Đơn vị tính: Việt Nam đồng</caption>\n<tr>\n<th rowspan=""2"">Tổng giới hạn bồi thường / năm / người</th>\n<th>Chương trình 1</th>\n<th>Chương trình 2</th>\n<th>Chương trình 3</th>\n</tr>\n<tr>\n<td>50,000,000</td>\n<td>100,000,000</td>\n<td>200,000,000</td>\n</tr>\n<tr>\n<td>Tử vong do tai nạn</td>\n<td>50,000,000</td>\n<td>100,000,000</td>\n<td>200,000,000</td>\n</tr>\n<tr>\n<td>Thương tật toàn bộ vĩnh viễn/ năm</td>\n<td>50,000,000</td>\n<td>100,000,000</td>\n<td>200,000,000</td>\n</tr>\n<tr>\n<td>Thương tật bộ phận vĩnh viễn/ năm</td>\n<td>50,000,000</td>\n<td>100,000,000</td>\n<td>200,000,000</td>\n</tr>\n<tr>\n<td>Chi phí y tế điều trị thương tật do tai nạn/ năm</td>\n<td>10,000,000</td>\n<td>20,000,000</td>\n<td>30,000,000</td>\n</tr>\n</table>'


print(pd.read_html(s))
```


### Issue Description

I have a HTML string (with rowspan attribute), here is how it look when rendering
![image](https://github.com/user-attachments/assets/68387883-6cd1-4ff3-bdb6-890c62cb3bf0)

Then I convert to dataframe by using `pd.read_html` command (pandas 2.2.3), here is how it looks:
![image](https://github.com/user-attachments/assets/9bd58d7e-2d29-4e5c-925f-75a25d7e3453)

Totally wrong, the row of `50000000`, `100000000` and `200000000` should be aligned to the right.


### Expected Behavior

the row of `50000000`, `100000000` and `200000000` should be aligned to the right

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.9
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-124-generic
Version               : #134~20.04.1-Ubuntu SMP Tue Oct 1 15:27:33 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2022.7
dateutil              : 2.8.2
pip                   : 24.3.1
Cython                : None
sphinx                : 5.0.2
IPython               : 8.10.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.2
lxml.etree            : 4.9.1
matplotlib            : None
numba                 : N/A
numexpr               : 2.8.4
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.10.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.19.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None","['Bug', 'IO HTML']",2024-11-06 04:12:26,2024-12-03 00:16:34,8,closed
60203,PERF: selection inside a row using a list of labels 30 slower than selection by the same individual labels,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

The code below accesses two columns either together or separately:
```
import pandas as pd
import numpy as np
import timeit

# Create large DataFrame
n_rows = 100_000
df = pd.DataFrame({
    'a': np.random.random(n_rows),
    'b': np.random.random(n_rows)
})

# Method 1: Access columns inside function
def sum_inside(row):
    return row[0] + row[1]

# Method 2: Pass values directly
def sum_outside(a, b):
    return a + b

# Time Method 1
t1 = timeit.timeit(lambda: df.apply(lambda row: sum_inside(row[['a','b']].values), axis=1), number=1)

# Time Method 2  
t2 = timeit.timeit(lambda: df.apply(lambda row: sum_outside(row['a'], row['b']), axis=1), number=1)

print(f""Method 1 (access inside): {t1:.4f} seconds"")
print(f""Method 2 (pass values): {t2:.4f} seconds"")
```
Output:
```
Method 1 (access inside): 15.6217 seconds
Method 2 (pass values): 0.5135 seconds
```

Using `iloc` does not suffer the same issue. Replace:
```
t1 = timeit.timeit(lambda: df.apply(lambda row: sum_inside(row.iloc[0:2].values), axis=1), number=1)
t2 = timeit.timeit(lambda: df.apply(lambda row: sum_outside(row.iloc[0], row.iloc[1]), axis=1), number=1)
```
Output:
```
Method 1 (access inside): 1.5293 seconds
Method 2 (pass values): 0.8811 seconds
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.7
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.26100
machine               : AMD64
processor             : AMD64 Family 23 Model 49 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_Germany.1252

pandas                : 2.2.3
numpy                 : 1.26.3
pytz                  : 2024.1
dateutil              : 2.8.2
pip                   : 24.0
Cython                : None
sphinx                : 8.1.3
IPython               : 8.22.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.2.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.2
lxml.etree            : 4.9.4
matplotlib            : 3.8.3
numba                 : 0.59.0
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 14.0.2
pyreadstat            : None
pytest                : 8.2.0
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>


### Prior Performance

_No response_","['Indexing', 'Performance', 'Closing Candidate']",2024-11-05 21:14:25,2024-11-12 22:06:48,3,closed
60199,ENH: To add an `inplace` option for pd.DataFrame.reindex,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could update the index of a dataframe inplace.

### Feature Description

Functions `pd.DataFrame.set_index()` and `pd.DataFrame.reset_index()` both have an option of `inplace=True`. I hope function `pd.DataFrame.reindex()` add it as well. This might lead to adding another option of `drop=True`.

### Alternative Solutions

N/A

### Additional Context

_No response_",['Enhancement'],2024-11-05 18:35:21,2024-11-12 21:25:39,2,closed
60194,BUG: Index.equals is not commutative for string and category dtypes,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

index1=pd.Index([""apple"",""mango"",""orange"",""pear""],dtype=""string"")
index2=pd.Index([""apple"",""mango"",""orange"",""pear""],dtype=""category"")

assert index1.equals(index2)==index2.equals(index1)
```

``` 
Traceback (most recent call last):
  File ""/home/pandas/Draft/rough1.py"", line 63, in <module>
    assert index1.equals(index2)==index2.equals(index1)
AssertionError
```


### Issue Description

pandas.Index.equals method is giving different results for string and category dtypes based on order

assert index2.equals(index1) gives True whereas
assert index1.equals(index2) gives False which makes these operations non-commutative

The first statement returns True because index 2 is pandas.core.indexes.category.CategoricalIndex which calls the .equals method in the sub class that overrides the Base Index class and implements the correct logic however

The second statement returns False because it enters this block
https://github.com/pandas-dev/pandas/blob/bdc79c146c2e32f2cab629be240f01658cfb6cc2/pandas/core/indexes/base.py#L5637-L5643

which converts the the string and categorical indexes into an ExtensionArray subclass and calls .equals again on those inputs but now it calls .equals method from the ExtensionArray class which does not ignore data types.

I can make a pull request and supplement with a few tests to resolve this issue.

### Expected Behavior

assert index1.equals(index2)==index2.equals(index1) should pass
assert index1.equals(index2) should pass

### Installed Versions

<details>

commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.153.1-microsoft-standard-WSL2
Version               : #1 SMP Fri Mar 29 23:14:13 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : 8.0.2
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.0
dataframe-api-compat  : None
fastparquet           : 2024.5.0
fsspec                : 2024.9.0
html5lib              : 1.1
hypothesis            : 6.112.1
gcsfs                 : 2024.9.0post1
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 17.0.0
pyreadstat            : 1.2.7
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.9.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.35
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Discussion', 'ExtensionArray']",2024-11-05 14:28:50,2024-11-05 16:56:01,7,closed
60190,"BUG: `pprint_thing(..., quote_strings=True)` fails for strings with embedded single-quotes","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas.io.formats.printing import pprint_thing

print(pprint_thing(""'510"", quote_strings=True))
```


### Issue Description

The code above produces the incorrect `''510'` as output, instead of `""'510""` as python does, or even `'\'510'`.

In the [current master](https://github.com/pandas-dev/pandas/blob/main/pandas/io/formats/printing.py#L240C1-L242C1) the cause is clear enough:
```python
    elif isinstance(thing, str) and quote_strings:
        result = f""'{as_escaped_string(thing)}'""
```
Either python's string `repr` could be used (assuming the rest of its behavior is fit for the purpose), or single-quotes could be explicitly backslash-escaped.

### Expected Behavior

As above.

### Installed Versions

As above, but is present in GitHub master as of right now. ","['Bug', 'Output-Formatting']",2024-11-05 00:05:31,2024-11-29 13:51:46,3,closed
60184,BUG: TypeError: Cannot convert numpy.ndarray to numpy.ndarray,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
data = {""ID"": [1, 2, 4], ""Names"" : ['k', 'X', 'y']}

df = pd.DataFrame(data)
df.head()
```


### Issue Description

It was working fine when I was running it about 7 Hour ago but now it won't work mean while it give me this error
TypeError: Cannot convert numpy.ndarray to numpy.ndarray

### Expected Behavior

TypeError: Cannot convert numpy.ndarray to numpy.ndarray

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.85+
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.8.2
setuptools            : 75.1.0
pip                   : 24.1.2
Cython                : 3.0.11
pytest                : 7.4.4
hypothesis            : None
sphinx                : 5.0.2
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.3.0
html5lib              : 1.1
pymysql               : None
psycopg2              : 2.9.10
jinja2                : 3.1.4
IPython               : 7.34.0
pandas_datareader     : 0.10.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
gcsfs                 : 2024.10.0
matplotlib            : 3.8.0
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.24.0
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.36
tables                : 3.8.0
tabulate              : 0.9.0
xarray                : 2024.10.0
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
",['Build'],2024-11-04 15:13:25,2025-04-13 17:20:29,14,closed
60181,DOC: Distinguish between Series.round and Series.dt.round,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round

### Documentation problem

When using Series.round, it does not work on date data.

### Suggested fix for documentation

Adding Series.dt.round in the ""See also"" section would make it more convenient for users to find the relevant documentation.
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.round.html","['Docs', 'Datetime']",2024-11-04 12:51:07,2024-11-04 18:12:39,2,closed
60169,BUG: build_table_schema (AttributeError: 'datetime.timezone' object has no attribute 'zone'),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
from datetime import datetime
from pandas.io.json._table_schema import build_table_schema

df = pd.DataFrame({
    'date': pd.date_range(start=datetime.now().astimezone(), periods=5, freq='D')
})
build_table_schema(df)
```


### Issue Description

this is fixed in the main branch per this [commit](https://github.com/pandas-dev/pandas/commit/96d732e3ccaf5b10373747664392fa231afd3c3a) 3 months ago, but the release version 2.2.3 is still broken. is there a workaround or can this fix be released 

### Expected Behavior

return the schema without errors



### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-1057-aws
Version               : #63~20.04.1-Ubuntu SMP Mon Mar 25 10:28:36 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.10.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 18.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.10.0
scipy                 : 1.14.1
sqlalchemy            : None
tables                : 3.10.1
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
None

</details>
","['Bug', 'Timezones']",2024-11-01 18:22:03,2024-11-02 12:09:24,3,closed
60164,BUG:  Groupby ignores unobserved categories when passing more than one column,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""c1"": [""r"", ""r"", ""r""], ""c2"" : pd.Categorical([""a"", ""a"", ""b""], categories=[""a"", ""b"", ""c""])})

assert(len(df.groupby([""c1"", ""c2""], observed=False)) == len(df.groupby([""c2""], observed=False)))
```


### Issue Description

From the documentation, I would expect no difference in the number of groups between statements.

### Expected Behavior

Both `groupby` statement should return 3 groups (and include unobserved categories).

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.2.0
Version               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:55:06 PST 2023; root:xnu-10002.61.3~2/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 68.1.2
pip                   : 23.3.1
Cython                : None
pytest                : 8.2.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.10.0
gcsfs                 : None
matplotlib            : 3.8.2
numba                 : 0.58.1
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 10.0.1
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.2
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : 0.19.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-11-01 10:59:00,2024-11-01 20:58:30,2,closed
60163,QST: Why Doesn't Pandas Have an Insert Index Function or Method?,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/69995045/inserting-values-at-an-index-position-in-pandas-df

### Question about pandas

I encountered a problem while trying to insert an index with Pandas. I couldn’t find any built-in function or method in Pandas for inserting an index. In most articles, I only found a few methods listed below, which I find to be cumbersome and potentially problematic in terms of performance. Is there a related function or method in Pandas that allows for direct index insertion?",['Usage Question'],2024-11-01 04:08:53,2024-11-02 13:59:52,1,closed
60149,BUG: Docs won't build (Unexpected exception in `doc\source\user_guide\enhancingperf.rst`),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
git clone https://github.com/pandas-dev/pandas.git
cd pandas
conda env create --file environment.yml
conda activate pandas-dev
python -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true
cd doc

python make.py html
python make.py clean
python make.py --single user_guide/enhancingperf.rst
```


### Issue Description

I was following the steps to build the documentation as given [here](https://pandas.pydata.org/docs/dev/development/contributing_documentation.html), but while running `python make.py html` it failed when building `user_guide/enhancingperf.rst` :


```
WARNING: ources... [ 95%] user_guide/enhancingperf
>>>-------------------------------------------------------------------------
Exception in C:\Users\91942\Documents\GitHub\pandas\doc\source\user_guide\enhancingperf.rst at block ending on line 101
Specify :okexcept: as an option in the ipython:: block to suppress this message
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[6], line 1
----> 1 get_ipython().run_line_magic('prun', '-l 4 df.apply(lambda x: integrate_f(x[""a""], x[""b""], x[""N""]), axis=1)  # noqa E999')

File ~\anaconda3\envs\pandas-dev\lib\site-packages\IPython\core\interactiveshell.py:2480, in InteractiveShell.run_line_magic(self, magic_name, line, _stack_depth)
   2478     kwargs['local_ns'] = self.get_local_scope(stack_depth)
   2479 with self.builtin_trap:
-> 2480     result = fn(*args, **kwargs)
   2482 # The code below prevents the output from being displayed
   2483 # when using magics with decorator @output_can_be_silenced
   2484 # when the last Python token in the expression is a ';'.
   2485 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):

File ~\anaconda3\envs\pandas-dev\lib\site-packages\IPython\core\magics\execution.py:317, in ExecutionMagics.prun(self, parameter_s, cell)
    315     arg_str += '\n' + cell
    316 arg_str = self.shell.transform_cell(arg_str)
--> 317 return self._run_with_profiler(arg_str, opts, self.shell.user_ns)

File ~\anaconda3\envs\pandas-dev\lib\site-packages\IPython\core\magics\execution.py:339, in ExecutionMagics._run_with_profiler(self, code, opts, namespace)
    337 prof = profile.Profile()
    338 try:
--> 339     prof = prof.runctx(code, namespace, namespace)
    340     sys_exit = ''
    341 except SystemExit:

File ~\anaconda3\envs\pandas-dev\lib\cProfile.py:101, in Profile.runctx(self, cmd, globals, locals)
     99 self.enable()
    100 try:
--> 101     exec(cmd, globals, locals)
    102 finally:
    103     self.disable()

File <string>:1

File ~\Documents\GitHub\pandas\pandas\core\frame.py:10398, in DataFrame.apply(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)
  10384 from pandas.core.apply import frame_apply
  10386 op = frame_apply(
  10387     self,
  10388     func=func,
   (...)
  10396     kwargs=kwargs,
  10397 )
> 10398 return op.apply().__finalize__(self, method=""apply"")

File ~\Documents\GitHub\pandas\pandas\core\apply.py:903, in FrameApply.apply(self)
    900 elif self.raw:
    901     return self.apply_raw(engine=self.engine, engine_kwargs=self.engine_kwargs)
--> 903 return self.apply_standard()

File ~\Documents\GitHub\pandas\pandas\core\apply.py:1054, in FrameApply.apply_standard(self)
   1052 def apply_standard(self):
   1053     if self.engine == ""python"":
-> 1054         results, res_index = self.apply_series_generator()
   1055     else:
   1056         results, res_index = self.apply_series_numba()

File ~\Documents\GitHub\pandas\pandas\core\apply.py:1070, in FrameApply.apply_series_generator(self)
   1067 results = {}
   1069 for i, v in enumerate(series_gen):
-> 1070     results[i] = self.func(v, *self.args, **self.kwargs)
   1071     if isinstance(results[i], ABCSeries):
   1072         # If we have a view on v, we need to make a copy because
   1073         #  series_generator will swap out the underlying data
   1074         results[i] = results[i].copy(deep=False)

File <string>:1, in <lambda>(x)

File ~\Documents\GitHub\pandas\pandas\core\series.py:969, in Series.__getitem__(self, key)
    966     key = np.asarray(key, dtype=bool)
    967     return self._get_rows_with_mask(key)
--> 969 return self._get_with(key)

File ~\Documents\GitHub\pandas\pandas\core\series.py:981, in Series._get_with(self, key)
    978 elif isinstance(key, tuple):
    979     return self._get_values_tuple(key)
--> 981 return self.loc[key]

File ~\Documents\GitHub\pandas\pandas\core\indexing.py:1195, in _LocationIndexer.__getitem__(self, key)
   1193 maybe_callable = com.apply_if_callable(key, self.obj)
   1194 maybe_callable = self._raise_callable_usage(key, maybe_callable)
-> 1195 return self._getitem_axis(maybe_callable, axis=axis)

File ~\Documents\GitHub\pandas\pandas\core\indexing.py:1424, in _LocIndexer._getitem_axis(self, key, axis)
   1421     if hasattr(key, ""ndim"") and key.ndim > 1:
   1422         raise ValueError(""Cannot index with multidimensional key"")
-> 1424     return self._getitem_iterable(key, axis=axis)
   1426 # nested tuple slicing
   1427 if is_nested_tuple(key, labels):

File ~\Documents\GitHub\pandas\pandas\core\indexing.py:1364, in _LocIndexer._getitem_iterable(self, key, axis)
   1361 self._validate_key(key, axis)
   1363 # A collection of keys
-> 1364 keyarr, indexer = self._get_listlike_indexer(key, axis)
   1365 return self.obj._reindex_with_indexers(
   1366     {axis: [keyarr, indexer]}, allow_dups=True
   1367 )

File ~\Documents\GitHub\pandas\pandas\core\indexing.py:1562, in _LocIndexer._get_listlike_indexer(self, key, axis)
   1559 ax = self.obj._get_axis(axis)
   1560 axis_name = self.obj._get_axis_name(axis)
-> 1562 keyarr, indexer = ax._get_indexer_strict(key, axis_name)
   1564 return keyarr, indexer

File ~\Documents\GitHub\pandas\pandas\core\indexes\base.py:6056, in Index._get_indexer_strict(self, key, axis_name)
   6053 else:
   6054     keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)
-> 6056 self._raise_if_missing(keyarr, indexer, axis_name)
   6058 keyarr = self.take(indexer)
   6059 if isinstance(key, Index):
   6060     # GH 42790 - Preserve name from an Index

File ~\Documents\GitHub\pandas\pandas\core\indexes\base.py:6108, in Index._raise_if_missing(self, key, indexer, axis_name)
   6105     raise KeyError(f""None of [{key}] are in the [{axis_name}]"")
   6107 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())
-> 6108 raise KeyError(f""{not_found} not in index"")

KeyError: ""['c'] not in index""

<<<-------------------------------------------------------------------------

Exception occurred:
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\IPython\sphinxext\ipython_directive.py"", line 584, in process_input
    raise RuntimeError(
RuntimeError: Unexpected exception in `C:\Users\91942\Documents\GitHub\pandas\doc\source\user_guide\enhancingperf.rst` line 101
The full traceback has been saved in C:\Users\91942\AppData\Local\Temp\sphinx-err-5kwabx28.log, if you want to report the issue to the developers.
Please also report this if it was a user error, so that a better error message can be provided next time.
A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!
```

The same error occurs while running `python make.py --single user_guide/enhancingperf.rst`. 

Full traceback given in `sphinx-err-5kwabx28.log` is:

```
# Platform:         win32; (Windows-10-10.0.22631-SP0)
# Sphinx version:   8.1.3
# Python version:   3.10.15 (CPython)
# Docutils version: 0.21.2
# Jinja2 version:   3.1.4
# Pygments version: 2.18.0

# Last messages:
#   user_guide/dsintro
#   
#   
#   reading sources... [ 95%]
#   user_guide/duplicates
#   
#   
#   reading sources... [ 95%]
#   user_guide/enhancingperf
#   

# Loaded extensions:
#   sphinx.ext.mathjax (8.1.3)
#   alabaster (1.0.0)
#   sphinxcontrib.applehelp (2.0.0)
#   sphinxcontrib.devhelp (2.0.0)
#   sphinxcontrib.htmlhelp (2.1.0)
#   sphinxcontrib.serializinghtml (1.1.10)
#   sphinxcontrib.qthelp (2.0.0)
#   contributors (0.1)
#   IPython.sphinxext.ipython_directive (unknown version)
#   IPython.sphinxext.ipython_console_highlighting (unknown version)
#   matplotlib.sphinxext.plot_directive (3.9.2)
#   sphinx.ext.autodoc.preserve_defaults (8.1.3)
#   sphinx.ext.autodoc.type_comment (8.1.3)
#   sphinx.ext.autodoc.typehints (8.1.3)
#   sphinx.ext.autodoc (8.1.3)
#   sphinx.ext.autosummary (8.1.3)
#   numpydoc (1.8.0)
#   sphinx_copybutton (0.5.2)
#   sphinx_design (0.6.1)
#   sphinx.ext.coverage (8.1.3)
#   sphinx.ext.doctest (8.1.3)
#   sphinx.ext.extlinks (8.1.3)
#   sphinx.ext.ifconfig (8.1.3)
#   sphinx.ext.intersphinx (8.1.3)
#   sphinx.ext.linkcode (8.1.3)
#   sphinx.ext.todo (8.1.3)
#   nbsphinx (0.9.5)
#   pydata_sphinx_theme (unknown version)

# Traceback:
Traceback (most recent call last):
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\cmd\build.py"", line 514, in build_main
    app.build(args.force_all, args.filenames)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\application.py"", line 381, in build
    self.builder.build_update()
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\builders\__init__.py"", line 358, in build_update
    self.build(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\builders\__init__.py"", line 385, in build
    updated_docnames = set(self.read())
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\builders\__init__.py"", line 502, in read
    self._read_serial(docnames)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\builders\__init__.py"", line 567, in _read_serial
    self.read_doc(docname)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\builders\__init__.py"", line 630, in read_doc
    publisher.publish()
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\core.py"", line 234, in publish
    self.document = self.reader.read(self.source, self.parser,
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\io.py"", line 106, in read
    self.parse()
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\readers\__init__.py"", line 76, in parse
    self.parser.parse(self.input, document)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\sphinx\parsers.py"", line 85, in parse
    self.statemachine.run(inputlines, document, inliner=self.inliner)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 169, in run
    results = StateMachineWS.run(self, input_lines, input_offset,
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\statemachine.py"", line 233, in run
    context, next_state, result = self.check_line(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\statemachine.py"", line 445, in check_line
    return method(match, context, next_state)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 3034, in text
    self.section(title.lstrip(), source, style, lineno + 1, messages)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 325, in section
    self.new_subsection(title, lineno, messages)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 391, in new_subsection
    newabsoffset = self.nested_parse(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 279, in nested_parse
    state_machine.run(block, input_offset, memo=self.memo,
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 195, in run
    results = StateMachineWS.run(self, input_lines, input_offset)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\statemachine.py"", line 233, in run
    context, next_state, result = self.check_line(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\statemachine.py"", line 445, in check_line
    return method(match, context, next_state)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 2790, in underline
    self.section(title, source, style, lineno - 1, messages)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 325, in section
    self.new_subsection(title, lineno, messages)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 391, in new_subsection
    newabsoffset = self.nested_parse(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 279, in nested_parse
    state_machine.run(block, input_offset, memo=self.memo,
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 195, in run
    results = StateMachineWS.run(self, input_lines, input_offset)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\statemachine.py"", line 233, in run
    context, next_state, result = self.check_line(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\statemachine.py"", line 445, in check_line
    return method(match, context, next_state)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 2790, in underline
    self.section(title, source, style, lineno - 1, messages)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 325, in section
    self.new_subsection(title, lineno, messages)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 391, in new_subsection
    newabsoffset = self.nested_parse(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 279, in nested_parse
    state_machine.run(block, input_offset, memo=self.memo,
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 195, in run
    results = StateMachineWS.run(self, input_lines, input_offset)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\statemachine.py"", line 233, in run
    context, next_state, result = self.check_line(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\statemachine.py"", line 445, in check_line
    return method(match, context, next_state)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 2357, in explicit_markup
    nodelist, blank_finish = self.explicit_construct(match)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 2369, in explicit_construct
    return method(self, expmatch)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 2106, in directive
    return self.run_directive(
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\docutils\parsers\rst\states.py"", line 2156, in run_directive
    result = directive_instance.run()
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\IPython\sphinxext\ipython_directive.py"", line 1033, in run
    rows, figure = self.shell.process_block(block)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\IPython\sphinxext\ipython_directive.py"", line 732, in process_block
    self.process_input(data, input_prompt, lineno)
  File ""C:\Users\91942\anaconda3\envs\pandas-dev\lib\site-packages\IPython\sphinxext\ipython_directive.py"", line 584, in process_input
    raise RuntimeError(
RuntimeError: Unexpected exception in `C:\Users\91942\Documents\GitHub\pandas\doc\source\user_guide\enhancingperf.rst` line 101
```

The issue appears to be with this line:

https://github.com/pandas-dev/pandas/blob/9cd4a281c42838cd32261b92a55aed830ebeae03/doc/source/user_guide/enhancingperf.rst?plain=1#L101

Should I add `:okexcept:` to the block to suppress this message?

### Expected Behavior

HTML doc should be successfully built.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 7bd594c81acb5f6428e9ef54ba5a9da1f2860a89
python                : 3.10.15
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_India.1252

pandas                : 3.0.0.dev0+1600.g7bd594c81a
numpy                 : 1.26.4
dateutil              : 2.9.0
pip                   : 24.3.1
Cython                : 3.0.11
sphinx                : 8.1.3
IPython               : 8.29.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.2
fastparquet           : 2024.5.0
fsspec                : 2024.10.0
html5lib              : 1.1
hypothesis            : 6.115.6
gcsfs                 : 2024.10.0
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 18.0.0
pyreadstat            : 1.2.8
pytest                : 8.3.3
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : 1.0.10
s3fs                  : 2024.10.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Docs', 'Windows']",2024-10-30 22:34:39,2025-06-22 17:34:51,3,closed
60148,DOC: pandas.DataFrame.to_html additional description for the border parameter,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_html.html

### Documentation problem

`pandas.DataFrame.to_html` parameter `border` is missing description for the behaviour when the `border=0` or `border=False` is passed. 

This means that documenation should be extended, as if the `border=0` or `False `is passed, the border is not present in the output `<table>` tag. Documentation indicates that `border` will always be included but this is lie. This behaviour was introduced in the pandas 1.5.0: https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html#:~:text=to_html()%20now%20excludes%20the%20border%20attribute%20from%20%3Ctable%3E%20elements%20when%20border%20keyword%20is%20set%20to%20False.

### Suggested fix for documentation

Improved description for the `border` parameter:

**border: int or bool**
When an integer value is provided, it sets the border attribute in the opening <table> tag, specifying the thickness of the border.
If False or 0 (zero) is passed, the border attribute will not be present in the `<table>`tag.

The default value for this parameter is governed by pd.options.display.html.border.
","['Docs', 'IO HTML']",2024-10-30 20:16:19,2025-02-04 17:50:50,1,closed
60139,QST: Is pandas.to_sql considered an ORM?,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/staging-ground/79139173

### Question about pandas

I'm working with a Pandas DataFrame and need to insert data into a SQL database. I'm using pandas.to_sql to accomplish this, but I am a bit confused about whether this method can be classified as an ORM tool or not.

From my understanding, ORMs like SQLAlchemy handle not only data insertion but also allow you to interact with the database tables as Python objects, providing features like lazy loading, data relationships, and change tracking.

Can anyone clarify if pandas.to_sql fits the definition of an ORM, or is it simply a convenience function for inserting data?","['Usage Question', 'Needs Triage']",2024-10-30 12:47:47,2024-10-30 18:49:00,1,closed
60129,BUG: allow complex type in convert_dtypes,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

convert_dtypes should be able to do this type of coercion from string complex looking

```python
print(pd.DataFrame(['1.0+5j', '1.5-3j']).astype(complex).convert_dtypes())
```

Related to closed https://github.com/pandas-dev/pandas/issues/4895

### Feature Description

-

### Alternative Solutions

-

### Additional Context

_No response_","['Bug', 'Dtype Conversions', 'Complex']",2024-10-30 07:30:04,2025-11-18 03:10:37,3,closed
60126,VOTE: Voting issue for PDEP-17: Backwards compatibility and deprecation policy,"### Locked issue

- [X] I locked this voting issue so that only voting members are able to cast their votes or comment on this issue.


### PDEP number and title

PDEP-17: Backwards compatibility and deprecation policy

### Pull request with discussion

https://github.com/pandas-dev/pandas/pull/59125

### Rendered PDEP for easy reading

https://github.com/Aloqeely/pandas/blob/pdep-17/web/pandas/pdeps/0017-backwards-compatibility-and-deprecation-policy.md

### Discussion participants

_No response_

### Voting will close in 15 days.

on November 13

### Vote

Cast your vote in a comment below.
* +1: approve.
* 0: abstain.
    * Reason: A one sentence reason is required.
* -1: disapprove
    * Reason: A one sentence reason is required.
A disapprove vote requires prior participation in the linked discussion PR.",['Vote'],2024-10-29 21:30:43,2024-12-17 19:43:39,14,closed
60124,BUG: 'Engine' object has no attribute 'cursor',"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
@task
    def push_into_postgres(df , table_name):
        
        user = 'postgres'  # Ensure this is correct
        password = 'postgres'  # Ensure this is correct
        host = 'astro-airflow_c0c605-postgres-1'
        port = 5432
        database_name = 'postgres'

        db_uri = f'postgresql://{user}:{password}@{host}:{port}/{database_name}'
        engine = create_engine(db_uri)

        try:
            df.to_sql(table_name, con=engine, if_exists='append', index=False)
            print(f""Data successfully pushed to {table_name}"")
```


### Issue Description

i had all the import properly with all the versions 

### Expected Behavior

what is going wrong 
the dataframe which is getting used its proper i had printed it 


### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'IO SQL', 'Needs Info']",2024-10-29 15:52:00,2025-07-23 09:38:16,2,closed
60123,BUG: Converting NumPy-nullable dtypes to str,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.DataFrame(
    {
        ""a"": [1, 2, None]
    },
    dtype=""Int64""
).astype(str).astype(""Int64"")

```



### Issue Description

When casting an Int64 NA-Value to a string it returns `<NA>` which is not well interpretable as a missing value for anything. It even fails in converting it back to an Int64 value.

### Expected Behavior

Int64 NA-Values should be casted to `None` when casted to String, as this is the equivalent represantation in a string-column, similar to being convertet to a float NaN for float, which is also shown by the following example, which works:

```
import pandas as pd

pd.DataFrame(
    {
        ""a"": [1, 2, None]
    },
    dtype=""Int64""
).astype(str).replace(""<NA>"", None).astype(""Int64"")
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-47-generic
Version               : #47~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Oct  2 16:16:55 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.1
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.0.3
pip                   : 24.0
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.22.2
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 15.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.28
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
",['Strings'],2024-10-29 12:20:18,2024-10-30 23:06:54,10,closed
60113,BUG: .replace() won't work on Series if a NaN is found on new string dtype,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Use new string type string[pyarrow_numpy]
pd.options.future.infer_string = True

df = pd.Series([""VALUE1"", None, ""VALUE3""], name=""COLUMN1"")
replace_dict = {""VALUE1"": ""REPLACED1"", ""VALUE3"": ""REPLACED3""}
df = df.replace(replace_dict)
```


### Issue Description

The replace method is expected to replace all values in the df Series that match the keys in replace_dict with their corresponding values. However, the replacement stops once a NaN (""string"", pd.options.future.infer_string = True) value is encountered, and the subsequent values are not replaced.

If the column is of type object, that works fine. (Check expected behaviour code)

### Expected Behavior

Value VALUE3 should be replaced by REPLACED3.

```
import pandas as pd

# String dtype will be object
pd.options.future.infer_string = False

df = pd.Series([""VALUE1"", None, ""VALUE3""], name=""COLUMN1"")
replace_dict = {""VALUE1"": ""REPLACED1"", ""VALUE3"": ""REPLACED3""}
df = df.replace(replace_dict)
```

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 25 Model 80 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : 1.4.2
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : 1.2.7
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.36
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None","['Bug', 'Needs Info']",2024-10-28 07:09:09,2024-10-29 15:16:59,5,closed
60111,ENH: A .chi2() method on the DataFrame and Series class that will resemble the .corr() methods,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

**Problem Description**

Currently, Pandas does not offer a method for calculating pairwise chi-square tests between columns in `DataFrame` or between two `Series` objects. Chi-square tests are useful for understanding associations between categorical variables. While correlation methods like `.corr()` serve to evaluate relationships among continuous data, there is no equivalent method for categorical data.

Researchers and data analysts who work with categorical data currently need to rely on external libraries or custom code to perform chi-square tests across columns in a `DataFrame` or between two `Series`. 

**Potential Benefits May Include**

1. **Swifter Categorical Data Analysis**: Enable exploration of associations within categorical data directly within Pandas.
2. **Consistent API**: By mimicking the structure and options of `.corr()`, the `.chi2()` method will feel intuitive.
3. **Enhanced Efficiency**: Avoids the need to transfer data between Pandas and other libraries.
4. **Optimized for Large Datasets**: Uses Cython to improve performance, making it feasible to compute pairwise chi-square tests even on large datasets.

**Potential Use Cases + Target Users**

- **Data Scientists** and **Statisticians** performing exploratory analysis of categorical data.
- **Researchers** in fields where categorical variables are prevalent.
- **Data Analysts** in biz domains where categorical variables are prevalent.

Using the Titanic data ideal model output could be as follows:

```Python
import pandas as pd
import numpy as np
import seaborn as sns

df = sns.load_dataset('titanic')
df.chi2()

              sex  embarked    class      who     deck
sex       0.00000   0.00126  0.00021  0.00000  0.00774
embarked  0.00126   0.00000  0.00000  0.00440  0.05592
class     0.00021   0.00000  0.00000  0.00000  0.00000
who       0.00000   0.00440  0.00000  0.00000  0.00003
deck      0.00774   0.05592  0.00000  0.00003  0.00000
```



### Feature Description

**Solution**

a `.chi2()` method for both `DataFrame` and `Series` classes would provide efficient and consistent code options that will perform these so-called pairwise chi-square tests (and produce a correlation-matrix-like output we could call or think of ass a so-called chi2-matrix):

- **`DataFrame.chi2()`**: To perform pairwise chi-square tests for all categorical or integer columns within a `DataFrame`, returning a symmetric matrix similar to the `DataFrame.corr()` method. Users can choose to output either p-values or chi-square statistics, and an adjustable `max_categories` parameter limits the inclusion of columns with too many unique values.
  
- **`Series.chi2(other_series)`**: Performs a chi-square test between two `Series` objects. It returns either the p-value or chi-square statistic.

Both would have optional `verbose` modes to include degrees of freedom values in the output.

**Potential Code `pandas/core/frame.py`**

```Python
from pandas._libs.algos import nanchi2
import numpy as np
import pandas as pd

class DataFrame:
    # Other methods ...

    def chi2(
        self,
        output: str = ""p-value"",
        max_categories: int = 40,
        verbose: bool = False
    ) -> pd.DataFrame:
        """"""
        Compute pairwise chi-square analysis of categorical columns, excluding NA/null values.

        Parameters
        ----------
        output : {'p-value', 'chi2stat'}, default 'p-value'
            Determines output format:
            * 'p-value': returns a matrix of p-values from chi-square tests.
            * 'chi2stat': returns a matrix of chi-square statistics. If `verbose=True`,
              each entry is a tuple (chi2_statistic, degrees_of_freedom, p-value).
        max_categories : int, default 40
            Maximum number of unique values allowed for `object` and `int` data types to be included
            in the chi-square calculations. Columns with more than `max_categories` unique values are excluded.
        verbose : bool, default False
            If True and `output=""chi2stat""`, each entry in the matrix contains (chi2_statistic, degrees_of_freedom, p-value).

        Returns
        -------
        DataFrame
            Chi-square matrix with pairwise comparisons between columns.

        Raises
        ------
        ValueError
            If the DataFrame contains no columns meeting the criteria for chi-square analysis.

        Notes
        -----
        Only categorical data and `int`/`object` data types with fewer than `max_categories` unique
        values are used. Identical columns return p-value=1.0 and chi2stat=0.0 for optimization.

        Examples
        --------
        >>> import pandas as pd
        >>> df = pd.DataFrame({
        ...     ""A"": [""dog"", ""dog"", ""cat"", ""dog""],
        ...     ""B"": [""apple"", ""orange"", ""apple"", ""orange""],
        ...     ""C"": [1, 2, 1, 2]
        ... })
        >>> df.chi2(output=""p-value"")
        """"""

        # Filter columns by dtype and unique values
        valid_columns = [
            col for col in self.columns
            if (self[col].dtype == 'object' or self[col].dtype == 'int' or pd.api.types.is_categorical_dtype(self[col]))
            and self[col].nunique(dropna=True) <= max_categories
        ]
        if not valid_columns:
            raise ValueError(
                ""No columns meet the criteria for chi-square analysis. ""
                ""Ensure categorical, `int`, or `object` columns with fewer than ""
                f""{max_categories} unique values are present.""
            )

        # Prepare data array with valid columns
        data = self[valid_columns].to_numpy(dtype=float, na_value=np.nan)

        # Use the nanchi2 function from _libs.algos for efficient chi-square calculation
        chi2_matrix = nanchi2(data, max_categories=max_categories, output=output)

        # Handle verbose output for chi2stat
        if output == ""chi2stat"" and verbose:
            result = pd.DataFrame(index=valid_columns, columns=valid_columns)
            for i, col1 in enumerate(valid_columns):
                for j, col2 in enumerate(valid_columns):
                    if i == j:
                        result.loc[col1, col2] = (0.0, 0, 1.0)  # Identical columns
                    else:
                        chi2_stat = chi2_matrix[i, j]
                        dof = (self[col1].nunique() - 1) * (self[col2].nunique() - 1)
                        p_val = chi2_matrix[i, j]
                        result.loc[col1, col2] = (chi2_stat, dof, p_val)
            return result

        # Convert result to DataFrame for standard output
        result = pd.DataFrame(chi2_matrix, index=valid_columns, columns=valid_columns)
        return result
```
**Potential Code `pandas/core/series.py`**
```Python
from pandas._libs.algos import nanchi2
import numpy as np
import pandas as pd

class Series:
    # Other methods ...

    def chi2(
        self,
        other: pd.Series,
        output: str = ""p-value"",
        max_categories: int = 40,
        verbose: bool = False
    ) -> float:
        """"""
        Compute chi-square association between this Series and another Series, excluding NA/null values.

        Parameters
        ----------
        other : Series
            The other Series with which to compute the chi-square statistic.
        output : {'p-value', 'chi2stat'}, default 'p-value'
            Determines output format:
            * 'p-value': returns the p-value from the chi-square test.
            * 'chi2stat': returns the chi-square statistic.
        max_categories : int, default 40
            Maximum number of unique values allowed for `object` and `int` data types to be included
            in the chi-square calculations. Series with more than `max_categories` unique values are excluded.
        verbose : bool, default False
            If True, returns a tuple with (chi2_statistic, degrees_of_freedom, p-value). 
            Ignored if `output` is 'p-value'.

        Returns
        -------
        float or tuple
            Chi-square test result. If `output=""p-value""`, returns the p-value. 
            If `output=""chi2stat""`, returns the chi-square statistic. If `verbose=True`, 
            returns a tuple with (chi2_statistic, degrees_of_freedom, p-value).

        Raises
        ------
        ValueError
            If the Series have incompatible lengths, unsupported data types, or excessive unique values.

        Notes
        -----
        Only categorical data and `int`/`object` data types with fewer than `max_categories` unique
        values are used. Identical Series return p-value=1.0 and chi2stat=0.0 for optimization.

        Examples
        --------
        >>> s1 = pd.Series([""dog"", ""dog"", ""cat"", ""dog""])
        >>> s2 = pd.Series([""apple"", ""orange"", ""apple"", ""orange""])
        >>> s1.chi2(s2, output=""p-value"")
        """"""

        # Ensure the other input is a Series and has compatible length
        if not isinstance(other, pd.Series):
            raise TypeError(""`other` must be a Series."")
        if len(self) != len(other):
            raise ValueError(""Both Series must have the same length."")

        # Check if both Series meet unique value criteria and have supported dtypes
        if (self.nunique(dropna=True) > max_categories or other.nunique(dropna=True) > max_categories):
            raise ValueError(
                ""Both Series must have fewer than `max_categories` unique values for chi-square analysis.""
            )
        if not (
            pd.api.types.is_categorical_dtype(self) or
            pd.api.types.is_integer_dtype(self) or
            pd.api.types.is_object_dtype(self)
        ):
            raise ValueError(""Series must be of type 'int', 'object', or 'category'."")

        if not (
            pd.api.types.is_categorical_dtype(other) or
            pd.api.types.is_integer_dtype(other) or
            pd.api.types.is_object_dtype(other)
        ):
            raise ValueError(""`other` must be of type 'int', 'object', or 'category'."")

        # Check if the Series are identical and optimize by returning expected values
        if self.equals(other):
            return 1.0 if output == ""p-value"" else 0.0

        # Prepare the data as a 2D array for nanchi2 function
        data = np.vstack([self.fillna(np.nan), other.fillna(np.nan)]).T
        chi2_matrix = nanchi2(data, max_categories=max_categories, output=output)

        # Retrieve the appropriate output format
        if output == ""p-value"":
            return chi2_matrix[0, 1]
        else:
            chi2_stat = chi2_matrix[0, 1]
            dof = (self.nunique() - 1) * (other.nunique() - 1)
            p_val = chi2_matrix[0, 1] if verbose else None

            return (chi2_stat, dof, p_val) if verbose else chi2_stat
```
**Potential Code `doc/source/reference/api/pandas.DataFrame.chi2.rst`**
```Python
.. _pandas.DataFrame.chi2:

pandas.DataFrame.chi2
=====================

DataFrame.chi2(output='p-value', max_categories=40) -> DataFrame

Compute pairwise chi-square analysis of categorical columns, excluding NA/null values.

This method calculates the chi-square association between pairs of columns in a DataFrame, comparing categorical columns or those with a limited number of unique values (default: 40). The output can either be a matrix of p-values or chi-square statistics.

Parameters
----------
output : {'p-value', 'chi2stat'}, default 'p-value'
    Determines output format:
    * 'p-value': returns a matrix of p-values from chi-square tests.
    * 'chi2stat': returns a matrix of chi-square statistics with degrees of freedom.

max_categories : int, default 40
    Maximum number of unique values allowed for `object` and `int` data types to be included
    in the chi-square calculations. Columns with more than `max_categories` unique values are excluded.

Returns
-------
DataFrame
    Symmetric chi-square matrix with pairwise comparisons between columns.

Raises
------
ValueError
    If the DataFrame contains no columns meeting the criteria for chi-square analysis.

Notes
-----
Only categorical data and `int`/`object` data types with fewer than `max_categories` unique
values are included. Identical columns return p-value=1.0 and chi2stat=0.0 for optimization.

Examples
--------
>>> import pandas as pd
>>> df = pd.DataFrame({
...     ""A"": [""dog"", ""dog"", ""cat"", ""dog""],
...     ""B"": [""apple"", ""orange"", ""apple"", ""orange""],
...     ""C"": [1, 2, 1, 2]
... })
>>> df.chi2(output=""p-value"")
          A         B         C
A  1.000000  0.300000  0.200000
B  0.300000  1.000000  0.150000
C  0.200000  0.150000  1.000000

See Also
--------
pandas.DataFrame.corr : Compute pairwise correlation of columns.
pandas.DataFrame.corrwith : Compute pairwise correlation with another DataFrame or Series.
pandas.Series.chi2 : Compute chi-square association with another Series.

```
**Potential Code `doc/source/reference/api/pandas.Series.chi2.rst`**
```Python
.. _pandas.Series.chi2:

pandas.Series.chi2
==================

Series.chi2(other, output='p-value', max_categories=40, verbose=False) -> float or tuple

Compute chi-square association between this Series and another Series, excluding NA/null values.

This method calculates the chi-square association between two Series, comparing categorical values or those with a limited number of unique values (default: 40). The output can be the p-value, the chi-square statistic, or additional details if `verbose=True`.

Parameters
----------
other : Series
    The other Series with which to compute the chi-square statistic.
output : {'p-value', 'chi2stat'}, default 'p-value'
    Determines output format:
    * 'p-value': returns the p-value from the chi-square test.
    * 'chi2stat': returns the chi-square statistic.
max_categories : int, default 40
    Maximum number of unique values allowed for `object` and `int` data types to be included
    in the chi-square calculations. Series with more than `max_categories` unique values are excluded.
verbose : bool, default False
    If True and `output=""chi2stat""`, returns a tuple with (chi2_statistic, degrees_of_freedom, p-value).

Returns
-------
float or tuple
    Chi-square test result. If `output=""p-value""`, returns the p-value. 
    If `output=""chi2stat""`, returns the chi-square statistic. If `verbose=True` 
    with `output=""chi2stat""`, returns a tuple (chi2_statistic, degrees_of_freedom, p-value).

Raises
------
ValueError
    If the Series have incompatible lengths, unsupported data types, or excessive unique values.

Notes
-----
Only categorical data and `int`/`object` data types with fewer than `max_categories` unique
values are used. Identical Series return p-value=1.0 and chi2stat=0.0 for optimization.

Examples
--------
>>> import pandas as pd
>>> s1 = pd.Series([""dog"", ""dog"", ""cat"", ""dog""])
>>> s2 = pd.Series([""apple"", ""orange"", ""apple"", ""orange""])
>>> s1.chi2(s2, output=""p-value"")
0.300000

See Also
--------
pandas.Series.corr : Compute correlation with another Series.
pandas.DataFrame.chi2 : Compute pairwise chi-square association between columns of a DataFrame.
pandas.Series.chi2 : Compute chi-square association with another Series.

```
**Potential Code `pandas/tests/frame/methods/test_chi2.py`**
```Python
import numpy as np
import pytest
import pandas as pd
from pandas import DataFrame
import pandas._testing as tm

class TestDataFrameChi2:
    def test_chi2_basic(self):
        # Test basic functionality with categorical data
        df = DataFrame({
            ""A"": [""dog"", ""dog"", ""cat"", ""dog""],
            ""B"": [""apple"", ""orange"", ""apple"", ""orange""],
            ""C"": [1, 2, 1, 2]
        })
        result = df.chi2()
        assert result.shape == (3, 3)
        assert result.index.equals(df.columns)
        assert result.columns.equals(df.columns)
    
    def test_chi2_output_p_value(self):
        # Test output=""p-value""
        df = DataFrame({
            ""A"": [""yes"", ""no"", ""yes"", ""yes""],
            ""B"": [""high"", ""low"", ""medium"", ""medium""],
            ""C"": [1, 2, 1, 3]
        })
        result = df.chi2(output=""p-value"")
        assert result.shape == (3, 3)
        assert result.loc[""A"", ""B""] >= 0  # p-value range check

    def test_chi2_output_chi2stat(self):
        # Test output=""chi2stat""
        df = DataFrame({
            ""A"": [""up"", ""down"", ""up"", ""down""],
            ""B"": [""high"", ""medium"", ""medium"", ""low""],
            ""C"": [1, 2, 2, 1]
        })
        result = df.chi2(output=""chi2stat"")
        assert result.shape == (3, 3)
        assert isinstance(result.loc[""A"", ""B""], float)  # Check statistic is a float

    def test_chi2_max_categories(self):
        # Test max_categories threshold
        df = DataFrame({
            ""A"": [""cat"" + str(i) for i in range(50)],  # Exceeds default max_categories of 40
            ""B"": [""type"" + str(i % 3) for i in range(50)]
        })
        with pytest.raises(ValueError, match=""No columns meet the criteria for chi-square analysis""):
            df.chi2()

    def test_chi2_na_handling(self):
        # Test handling of NaNs
        df = DataFrame({
            ""A"": [""yes"", ""no"", np.nan, ""yes""],
            ""B"": [""high"", np.nan, ""medium"", ""medium""],
            ""C"": [1, 2, 1, np.nan]
        })
        result = df.chi2(output=""p-value"")
        assert result.loc[""A"", ""B""] >= 0  # p-value should be non-negative
        assert np.isnan(result.loc[""A"", ""C""])  # Row with NaNs should yield NaN

    def test_chi2_identical_columns(self):
        # Test optimization for identical columns
        df = DataFrame({
            ""A"": [""dog"", ""dog"", ""cat"", ""dog""],
            ""B"": [""dog"", ""dog"", ""cat"", ""dog""],
            ""C"": [1, 2, 1, 2]
        })
        result = df.chi2(output=""p-value"")
        assert result.loc[""A"", ""B""] == 1.0  # Identical columns should return p-value=1.0

    def test_chi2_non_categorical_data(self):
        # Test error handling for non-categorical data
        df = DataFrame({
            ""A"": [1.5, 2.5, 3.5, 4.5],  # Continuous numeric data
            ""B"": [""apple"", ""orange"", ""apple"", ""orange""],
            ""C"": [""yes"", ""no"", ""yes"", ""yes""]
        })
        with pytest.raises(ValueError, match=""must be of type 'int', 'object', or 'category'""):
            df.chi2()
    
    def test_chi2_single_column(self):
        # Test single column DataFrame
        df = DataFrame({
            ""A"": [""dog"", ""dog"", ""cat"", ""dog""]
        })
        result = df.chi2()
        assert result.shape == (1, 1)
        assert result.loc[""A"", ""A""] == 1.0  # Single column should return p-value=1.0

```
**Potential Code `pandas/tests/frame/methods/test_chi2.py`**
```Python
import numpy as np
import pytest
import pandas as pd
from pandas import Series
import pandas._testing as tm

class TestSeriesChi2:
    def test_chi2_basic(self):
        # Basic functionality with categorical data
        s1 = Series([""dog"", ""dog"", ""cat"", ""dog""])
        s2 = Series([""apple"", ""orange"", ""apple"", ""orange""])
        result = s1.chi2(s2)
        assert isinstance(result, float)  # Expecting a single p-value

    def test_chi2_output_p_value(self):
        # Test output=""p-value"" explicitly
        s1 = Series([""yes"", ""no"", ""yes"", ""yes""])
        s2 = Series([""high"", ""low"", ""medium"", ""medium""])
        result = s1.chi2(s2, output=""p-value"")
        assert 0 <= result <= 1  # p-value should be within this range

    def test_chi2_output_chi2stat(self):
        # Test output=""chi2stat""
        s1 = Series([""up"", ""down"", ""up"", ""down""])
        s2 = Series([""high"", ""medium"", ""medium"", ""low""])
        result = s1.chi2(s2, output=""chi2stat"")
        assert isinstance(result, float)  # Expecting chi-square statistic as a float

    def test_chi2_verbose_output(self):
        # Test verbose output for chi2stat
        s1 = Series([""yes"", ""no"", ""yes"", ""yes""])
        s2 = Series([""high"", ""low"", ""medium"", ""medium""])
        result = s1.chi2(s2, output=""chi2stat"", verbose=True)
        assert isinstance(result, tuple)  # Should return tuple in verbose mode
        assert len(result) == 3  # Tuple should contain (chi2_statistic, degrees_of_freedom, p-value)

    def test_chi2_max_categories(self):
        # Test max_categories threshold
        s1 = Series([""cat"" + str(i) for i in range(50)])  # Exceeds default max_categories of 40
        s2 = Series([""type"" + str(i % 3) for i in range(50)])
        with pytest.raises(ValueError, match=""must have fewer than `max_categories` unique values""):
            s1.chi2(s2)

    def test_chi2_na_handling(self):
        # Test handling of NaNs
        s1 = Series([""yes"", ""no"", np.nan, ""yes""])
        s2 = Series([""high"", np.nan, ""medium"", ""medium""])
        result = s1.chi2(s2, output=""p-value"")
        assert 0 <= result <= 1 or np.isnan(result)  # Allow p-value or NaN

    def test_chi2_identical_series(self):
        # Test optimization for identical Series
        s1 = Series([""dog"", ""dog"", ""cat"", ""dog""])
        s2 = s1.copy()  # Identical Series
        result = s1.chi2(s2, output=""p-value"")
        assert result == 1.0  # Identical series should return p-value=1.0

    def test_chi2_non_categorical_data(self):
        # Test error handling for non-categorical data
        s1 = Series([1.5, 2.5, 3.5, 4.5])  # Continuous numeric data
        s2 = Series([""apple"", ""orange"", ""apple"", ""orange""])
        with pytest.raises(ValueError, match=""must be of type 'int', 'object', or 'category'""):
            s1.chi2(s2)

    def test_chi2_mismatched_lengths(self):
        # Test error handling for mismatched Series lengths
        s1 = Series([""dog"", ""dog"", ""cat"", ""dog""])
        s2 = Series([""apple"", ""orange"", ""apple""])  # Mismatched length
        with pytest.raises(ValueError, match=""Both Series must have the same length""):
            s1.chi2(s2)

```

### Alternative Solutions

Currently, to perform chi-square tests on pairs of categorical columns in a `DataFrame`, users can rely on a combination of the following libraries and approaches:

**Using Scipy’s `chi2_contingency` Function**
   - Import `chi2_contingency` from `scipy.stats` and compute chi-square values using a contingency or `pd.crosstab()` for each pair of categorical columns.
   - **Example**:
     ```python
     import pandas as pd
     from scipy.stats import chi2_contingency
     import seaborn as sns

     # Load a dataset and calculate chi-square for a pair of columns
     df = sns.load_dataset('titanic')
     chi2_result = chi2_contingency(pd.crosstab(df['pclass'], df['embark_town']))
     ```
   - This approach requires manually constructing `pd.crosstab` tables for each pair of columns (or doing so in a loop), making it cumbersome for pairwise analysis across multiple columns. It also lacks an optimized and integrated way to produce pairwise matrices directly within Pandas.

**Other Third-Party Libraries**:
   - There are other libraries like `seaborn` or `statsmodels` facilitate chi-square tests and visualizations which may weigh against implementing this in Pandas. However the same can be said for correlation, which is available in many other libraries.

Fuilt-in functionality would streamline categorical data analysis within Pandas, aligning with the goal of being a comprehensive tool for data manipulation and analysis.

### Additional Context

Searched for related issues, found none. However I may have missed them. Thanks to all in the world of Pandas for consideration, review, and efforts.","['Enhancement', 'Needs Discussion', 'Closing Candidate']",2024-10-26 21:01:20,2025-08-05 16:35:31,5,closed
60107,BUG: Stata value label limits incorrect,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Here's an example writing a Stata file with value labels that raises an error: 

import string
import random
import pandas as pd
df = pd.DataFrame(range(65534), columns = [""col""])
value_labels = {""col"": {i: ''.join(random.choices(string.ascii_letters, k=21)) for i in range(65534)}}
df.to_stata(""label_test.dta"", value_labels=value_labels)

```python-traceback
>>> df.to_stata(""label_test.dta"", value_labels=value_labels)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".venv/lib/python3.12/site-packages/pandas/core/frame.py"", line 2893, in to_stata
    writer = statawriter(
             ^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/pandas/io/stata.py"", line 2365, in __init__
    self._prepare_pandas(data)
  File "".venv/lib/python3.12/site-packages/pandas/io/stata.py"", line 2616, in _prepare_pandas
    non_cat_value_labels = self._prepare_non_cat_value_labels(data)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/pandas/io/stata.py"", line 2415, in _prepare_non_cat_value_labels
    svl = StataNonCatValueLabel(colname, labels, self._encoding)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/pandas/io/stata.py"", line 820, in __init__
    self._prepare_value_labels()
  File "".venv/lib/python3.12/site-packages/pandas/io/stata.py"", line 727, in _prepare_value_labels
    raise ValueError(
ValueError: Stata value labels for a single variable must have a combined length less than 32,000 characters.
```

Here's an example in Stata that constructs the same value label (65,534 entries with 21 characters each) but does not produce an error:
```stata
clear all
set obs 65534
gen x = _n

* Set up label
label define test_lbl 0 """"

* Loop over rows and create unique label for each value
forvalues i = 1/`=_N' {
	* Add a unique length-21 string label
	label define test_lbl `i' ""`: di %20.19f runiform()'"", add
}

label values x test_lbl

save ""label_test.dta"", replace

clear all

use ""label_test.dta"", clear
forvalues i = 1/65534 {
	assert strlen(""`: label test_lbl `i', strict'"") == 21
}
```
```


### Issue Description

pandas requires that the _combined_ length of the value label not exceed 32,000 characters, but the limit in Stata is actually that _each_ value label can be up to 32,000 characters, and you can have up to 65,536 value labels (see the ""label"" section on this page: https://www.stata.com/products/detailed-size-limits/#label). 

### Expected Behavior

The expected behavior is for pandas to only raise an error if an individual label exceeds 32,000 characters, or if there are more than 65,536 labels. If neither of these limits are exceeded, then pandas should write out the value label, even if the combined length is more than 32,000 characters. 

### Installed Versions

```
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.0.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 21.6.0
Version               : Darwin Kernel Version 21.6.0: Wed Apr 24 06:02:02 PDT 2024; root:xnu-8020.240.18.708.4~1/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 75.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.32
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None
```","['Bug', 'IO Stata', 'Needs Triage']",2024-10-25 20:31:09,2024-10-31 15:56:56,3,closed
60102,BUG: Frequency shift on empty DataFrame doesn't shift index,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
dates = pd.date_range('2020-01-01', '2020-01-03')
df = pd.DataFrame(index=dates, columns=[])
day = pd.offsets.Day()
shifted_dates = df.shift(freq=day).index
assert (shifted_dates == dates + day).all()
```


### Issue Description

Running `DataFrame.shift` with `freq` on a `DataFrame` with empty columns does not shift the index.

### Expected Behavior

I would expect `DataFrame.shift` to shift the index similarly to how it would if the `DataFrame` had columns.  Particularly with the above example, I would expect `df.reindex(['A'], axis=1).shift(freq=day).reindex([], axis=1)` to be equivalent to `df.shift(freq=day)`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit           : 91111fd99898d9dcaa6bf6bedb662db4108da6e6
python           : 3.10.13.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.15.102.hrtdev
Version          : #1 SMP Tue Mar 14 13:23:09 EDT 2023
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           :
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.5.1
numpy            : 1.23.2
pytz             : 2022.1
dateutil         : 2.8.2
setuptools       : 62.1.0
pip              : 24.2
Cython           : 0.29.30
pytest           : 8.2.1
hypothesis       : 6.108.2
sphinx           : 5.0.2
blosc            : 1.10.6
feather          : None
xlsxwriter       : None
lxml.etree       : 4.9.1
html5lib         : 1.1
pymysql          : 1.0.2
psycopg2         : 2.9.3
jinja2           : 3.1.2
IPython          : 8.10.0
pandas_datareader: None
bs4              : 4.11.1
bottleneck       : 1.3.5
brotli           : 1.0.9
fastparquet      : 0.8.1
fsspec           : 2023.10.0
gcsfs            : None
matplotlib       : 3.5.2+hrt1
numba            : 0.59.0
numexpr          : 2.8.3
odfpy            : None
openpyxl         : 3.0.10
pandas_gbq       : None
pyarrow          : 12.2.2.dev14+g99c7fc95e
pyreadstat       : None
pyxlsb           : 1.0.9
s3fs             : None
scipy            : 1.9.1
snappy           : None
sqlalchemy       : 1.4.49
tables           : 3.7.0
tabulate         : 0.8.10
xarray           : 2022.6.0
xlrd             : 2.0.1
xlwt             : None
zstandard        : 0.20.0
tzdata           : 2022.1

</details>
","['Bug', 'Datetime', 'good first issue', 'Transformations']",2024-10-24 18:49:30,2024-11-05 18:41:40,2,closed
60101,BUG: unable to insert 64-bit integers into dataframe.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df1 = pd.DataFrame(index=range(1), columns=['foo','bar'], dtype='Int64')
df2 = df1.copy()

# insert using a dict
df1.loc[0] = {'foo': 1729509695129311113}

# insert normally
df2.loc[0, 'foo'] = 1729509695129311113

assert df1.equals(df2), 'oops'
```


### Issue Description

when trying to insert an int into a (nullable) `Int64` column, depending on the method used, the integer is first coereced to a `float64` and therefore loses some of its precision.

### Expected Behavior

both methods should work without losing precsion, i.e., like `df2`.

### Installed Versions


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.3
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
","['Bug', 'Dtype Conversions', 'Closing Candidate']",2024-10-24 15:36:46,2024-10-29 06:56:48,2,closed
60099,BUG: Inconsistent output type in Excel for PeriodIndex in Index or MultiIndex,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

period_index = pd.period_range(start=""2006-10-06"", end=""2006-10-07"", freq=""D"")
pi_index = pd.Series(0, index=period_index)
pi_multi = pd.Series(0, index=pd.MultiIndex.from_arrays([period_index, [0, 0]]))

pi_index.to_excel(""index_as_date_openpyxl.xlsx"", engine=""openpyxl"", index=True)
pi_multi.to_excel(""index_as_str_openpyxl.xlsx"", engine=""openpyxl"", index=True)

pi_index.to_excel(""index_as_date_xlsxwriter.xlsx"", engine=""xlsxwriter"",  index=True)
pi_multi.to_excel(""index_as_str_xlsxwriter.xlsx"", engine=""xlsxwriter"", index=True)
```


### Issue Description

When exporting a Series/DataFrame to excel when the index contains a PeriodIndex the format of the output depends on the index being an Index or a MultiIndex.

When the PeriodIndex is in an Index, the output format in Excel is a date.
When the PeriodIndex is a level from a MultiIndex, the output format is a string.

### Expected Behavior

The expected behaviour is that the format of the period would not be dependent on it being in an Index or in a MultiIndex. Being either a string or a date in excel is both acceptable as long as it is consistent in both outputs.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.4
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.100+
Version               : #1 SMP PREEMPT_DYNAMIC Sat Oct  5 14:28:44 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.25.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : 8.2.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.31
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO Excel', 'Period', 'MultiIndex']",2024-10-24 13:30:21,2024-11-22 20:20:40,2,closed
60098,PERF: Slowdowns with .isin() on columns typed as np.uint64,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

```py
import pandas as pd, numpy as np
data = pd.DataFrame({
    ""uints"": np.random.randint(10000, size=300000, dtype=np.uint64),
    ""ints"": np.random.randint(10000, size=300000, dtype=np.int64),
})

%timeit data[""ints""].isin([1, 2]) # 3ms
%timeit data[""ints""].isin(np.array([1, 2], dtype=np.int64)) # 3ms
%timeit data[""ints""].isin(np.array([1, 2], dtype=np.uint64)) # 5ms
%timeit data[""ints""].isin([np.int64(1), np.int64(2)]) # 3ms
%timeit data[""ints""].isin([np.uint64(1), np.uint64(2)]) # 5ms

%timeit data[""uints""].isin([1, 2]) # 14ms (!)
%timeit data[""uints""].isin(np.array([1, 2], dtype=np.int64)) # 5ms
%timeit data[""uints""].isin(np.array([1, 2], dtype=np.uint64)) # 3ms
%timeit data[""uints""].isin([np.int64(1), np.int64(2)])  # 17ms (!)
%timeit data[""uints""].isin([np.uint64(1), np.uint64(2)]) # 17ms (!)
```

The last line, with older numpy==1.26.4 (last version <2.0), is even worse: ~200ms.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-27-generic
Version               : #28~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 15 10:51:06 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.2

</details>


### Prior Performance

With pandas 1.4.4 and numpy 1.26.4, all the benchmarks above show 3-5ms (3ms on signedness match, 5ms on signedness mismatch). So despite updating numpy mitigating the worst 200ms regression, this still looks like a 5x performance regression on pandas side since 1.4.4.

I'm guessing the regression could be related to PR https://github.com/pandas-dev/pandas/pull/46693 , which happened on the 1.5.0 release.","['Performance', 'Regression', 'isin']",2024-10-24 11:48:50,2025-05-19 16:14:48,0,closed
60093,How to implement pandas-style df.loc[] in Polars for custom library?,"[Polars issue](https://github.com/pola-rs/polars/issues/19414)
## My goals
> Create the ultimate bridge between the powerful performance of Polars and the intuitive, user-friendly API of Pandas.

[My QA to chatgpt o1](https://github.com/milisp/panars/issues/1#issue-2610075826)

## Question
### polars to pandas style, create a DataFrame, must use df.loc[], not df.loc()
```python
df = DataFrame(
    {""ref"": list('abcd'), 'x': [1,2,3,4], 'z': [7,8,9,6]},
    index=list('cfgh')
)
```

### pass test
```python
df.loc[df.ref == 'c']
df.loc[df.ref.eq('c')]
df.loc[(df.ref == 'c') & (df.x > 1)]
df.loc[(df.ref == 'c') | (df.x > 1)]
df.loc[df.ref.isin(['a','b'])]
df.loc[df.x.between(2,4)]
df.loc['c']
df.loc[:, 'x']
df.loc[:, ['x', 'z']]
df.loc[:, 'x':'z']
df.loc['c'][1]
```",['Closing Candidate'],2024-10-24 01:10:17,2024-10-24 13:49:14,3,closed
60092,BUG: select_dtypes does not work properly with numpy.ndarray,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

table_df = pd.DataFrame({""id"": [1, 2, 3], ""dest_id"": [""var-foo-1"", ""var-foo-2"", ""baz-taz-1""], ""arr_col"": [np.array([1, 2]), np.array([3, 4]), np.array([4, 5])]})

print(type(table_df[""dest_id""][0]), type(table_df[""arr_col""][0]))

print(table_df.select_dtypes(include=[np.ndarray]).columns)
```


### Issue Description

I work with several dataframes, which occasionally have array columns. I was using `select_dtypes` to search for those columns containing Array type to manipulate them, but the function also returns the string columns, and my code crashes when it tries to apply the array function to the string column.

I was working with pandas 2.2.2 / numpy 1.26.2 when this happened, but I made a new environment and upgraded to the latest versions and it still happened.

### Expected Behavior

This is the current output:
```
>>> print(type(table_df[""dest_id""][0]), type(table_df[""arr_col""][0]))
<class 'str'> <class 'numpy.ndarray'>
>>> 
>>> print(table_df.select_dtypes(include=[np.ndarray]).columns)
Index(['dest_id', 'arr_col'], dtype='object')
```
I would expect that only `arr_col` is returned with select_dtypes when using `include=[np.ndarray]`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 68d9dcab5b543adb3bfe5b83563c61a9b8afae77
python                : 3.11.6
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-44-generic
Version               : #44-Ubuntu SMP PREEMPT_DYNAMIC Fri Jun  7 15:10:09 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1580.g68d9dcab5b
numpy                 : 2.1.2
dateutil              : 2.9.0.post0
pip                   : 23.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : 5.2.2
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : 2024.1
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
",['Bug'],2024-10-23 22:30:41,2024-11-02 13:48:26,8,closed
60091,BUG: Merging on two non-monotonic `np.intc` arrays fails,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({
    ""join_key"": pd.Series([0, 2, 1], dtype=np.intc)
})
df_details = pd.DataFrame({
    ""join_key"": pd.Series([0, 1, 2], dtype=np.intc),
    ""value"": [""a"", ""b"", ""c""]
})

merged = pd.merge(df, df_details, on=""join_key"", how=""left"")
```


### Issue Description

When merging on two `np.intc` arrays (where at least one is non-monotonic), pandas throws a ""Buffer dtype mismatch"" error. It only seems to happen when both arrays are `np.intc` type. I encountered this issue because that is the dtype returned from `pd.DatetimeIndex.weekday`.

```
Traceback (most recent call last):
  File ""merge_bug_test.py"", line 26, in <module>
    merged_1 = pd.merge(df, df_details, on=""join_key"", how=""left"")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""path\to\env\Lib\site-packages\pandas\core\reshape\merge.py"", line 184, in merge
    return op.get_result(copy=copy)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""path\to\env\Lib\site-packages\pandas\core\reshape\merge.py"", line 886, in get_result
    join_index, left_indexer, right_indexer = self._get_join_info()
                                              ^^^^^^^^^^^^^^^^^^^^^
  File ""path\to\env\Lib\site-packages\pandas\core\reshape\merge.py"", line 1151, in _get_join_info
    (left_indexer, right_indexer) = self._get_join_indexers()
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""path\to\env\Lib\site-packages\pandas\core\reshape\merge.py"", line 1125, in _get_join_indexers
    return get_join_indexers(
           ^^^^^^^^^^^^^^^^^^
  File ""path\to\env\Lib\site-packages\pandas\core\reshape\merge.py"", line 1759, in get_join_indexers
    lidx, ridx = get_join_indexers_non_unique(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""path\to\env\Lib\site-packages\pandas\core\reshape\merge.py"", line 1793, in get_join_indexers_non_unique
    lkey, rkey, count = _factorize_keys(left, right, sort=sort)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""path\to\env\Lib\site-packages\pandas\core\reshape\merge.py"", line 2561, in _factorize_keys
    llab = rizer.factorize(lk)  # type: ignore[arg-type]
           ^^^^^^^^^^^^^^^^^^^
  File ""pandas\\_libs\\hashtable_class_helper.pxi"", line 3045, in pandas._libs.hashtable.Int64Factorizer.factorize
ValueError: Buffer dtype mismatch, expected 'const int64_t' but got 'int'
```

It seems this issue was raised in #52451, the solution that was implemented involves checking `if np.intc is not np.int32`. This check returns True, meaning that `np.intc` is mapped to `Int64Factorizer`. It appears that this check no longer works as intended, or it is not robust enough.

A similar issue is raised in #58713 for `uintc`. In the proposed solution (#58727), it was suggested to use `.itemsize` as a more robust check. I think that suggestion might get to the root cause, as my `np.intc(0).itemsize` is 4, so `np.intc` should be mapped to `Int32Factorizer`



### Expected Behavior

Merging should succeed if dtypes for both keys are the same.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.5
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : ...
machine               : AMD64
processor             : Intel64 Family 6 ...
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 2.0.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : 1.4.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : 1.2.7
pytest                : None
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.9.0
scipy                 : None
sqlalchemy            : 2.0.35
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.2
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Reshaping', 'good first issue']",2024-10-23 20:14:20,2024-10-27 13:32:55,4,closed
60087,QST: Why is it necessary to not allow `include_groups=True` in future versions for groupby(...).apply(...) operations?,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/77969964/deprecation-warning-with-groupby-apply

### Question about pandas

I find it strange that in the docs it is stated that the include_groups option won't allow True as an option starting in 3.0. I use `groupby(...).apply(...)` all the time working with tabular data and fully expect the group by columns to be included in the result. I feel like I have to be mis-understanding something about the warning because I can't fathom not having to select all the columns I need from the group by every time I use the operation. This is a very common data pattern I use:

```python
summarize_df = (
  df.
  groupby(['user_id']).
  apply(lambda df: pd.Series(dict(
    num_orders = len(df),
    aov = np.mean(df['amount']),
    first_order = np.min(df['order_date'])
  ))).
  reset_index()
)
```

Which will give me another dataframe that I can use and has `user_id` in the column. The stack overflow question I linked seems to suggest the only way to get the user_id as a column in the future will be to 1.) add it to the index first or 2.) selecting the group by column after the apply call, but I would have to copy and re-select every column I created in the apply statement as well. The documentation for DataFrameGroupBy.apply doesn't make it clear how you'd keep the group by column. 

Is this the intended behavior of this change? Is it assumed every user who is doing what I am doing is using `agg` instead of `apply`? Thanks!","['Groupby', 'Usage Question', 'Needs Info', 'Apply']",2024-10-23 13:34:28,2024-10-28 13:34:10,7,closed
60086,BUG: DataFrame Dict not callabe,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

d = {'col1': [1,3], 'col2': [4,5]}
df = pd.DataFrame(data=d)
print(df)


  File ~\anaconda3\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
    exec(code, globals, locals)

  File c:\dennis\savedennis\f\pythonstocks\code\untitled3.py:10
    df = pd.DataFrame(data=d)

TypeError: 'dict' object is not callable
```


### Issue Description

Everything worked fine until a couple of days ago

### Expected Behavior

This code is exactly copied from your examples

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.11.7.final.0
python-bits         : 64
OS                  : Windows
OS-release          : 10
Version             : 10.0.22631
machine             : AMD64
processor           : Intel64 Family 6 Model 140 Stepping 2, GenuineIntel
byteorder           : little
LC_ALL              : None
LANG                : en
LOCALE              : English_United States.1252

pandas              : 2.1.4
numpy               : 1.26.4
pytz                : 2023.3.post1
dateutil            : 2.8.2
setuptools          : 68.2.2
pip                 : 23.3.1
Cython              : None
pytest              : 7.4.0
hypothesis          : None
sphinx              : 5.0.2
blosc               : None
feather             : None
xlsxwriter          : None
lxml.etree          : 4.9.3
html5lib            : 1.1
pymysql             : None
psycopg2            : None
jinja2              : 3.1.3
IPython             : 8.20.0
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : 1.3.7
dataframe-api-compat: None
fastparquet         : None
fsspec              : 2023.10.0
gcsfs               : None
matplotlib          : 3.8.0
numba               : 0.59.0
numexpr             : 2.8.7
odfpy               : None
openpyxl            : 3.0.10
pandas_gbq          : None
pyarrow             : 14.0.2
pyreadstat          : None
pyxlsb              : None
s3fs                : 2023.10.0
scipy               : 1.11.4
sqlalchemy          : 2.0.25
tables              : 3.9.2
tabulate            : 0.9.0
xarray              : 2023.6.0
xlrd                : None
zstandard           : 0.19.0
tzdata              : 2023.3
qtpy                : 2.4.1
pyqt5               : None
Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Info']",2024-10-23 12:07:53,2025-08-05 17:00:34,2,closed
60085,BUG: Type Annotation Inconsistency in read_sql_* Functions ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd 
import sqlite3


date_params = {""date_col"": {""utc"": True}}

with sqlite3.connect(""blah"") as con:
    # Fails type check.
    df = pd.read_sql_query(""SELECT * FROM tablename"", con, parse_dates=date_params)
    print(df)
```


### Issue Description

The pandas type annotations for the `parse_dates` argument in `read_sql_table()` and `read_sql_query()` is overly restrictive. It incorrectly causes type checkers to complain when using the `parse_dates` argument to pass keyword arguments to `to_datetime()` as documented [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_query.html).

To solve this problem, the annotated type for `parse_date` just needs to be switched from `parse_dates: list[str] | dict[str, str] | None` to `list[str] | dict[str, str] | dict[str, dict[str, Any]] | None`.

This problem is not always visible because the corresponding `pandas-stubs` already does this. The inconsistency appears however in some type checkers when additional stubs are not available or configured though.

To illustrate, take the provided (valid) example and run `pyright` on it (with no arguments). It will output the following.

```
(bug_venv)$ pyright example.py
/home/user/Code/pandas_bug/example.py
/home/user/Code/pandas_bug/example.py:8:10 - error: No overloads for ""read_sql_query"" match the provided arguments (reportCallIssue)
/home/user/Code/pandas_bug/example.py:8:72 - error: Argument of type ""dict[str, dict[str, bool]]"" cannot be assigned to parameter ""parse_dates"" of type ""list[str] |dict[str, str] | None"" in function ""read_sql_query""
Type ""dict[str, dict[str, bool]]"" is not assignable to type ""list[str] | dict[str, str] | None""
""dict[str, dict[str, bool]]"" is not assignable to ""list[str]""
""dict[str, dict[str, bool]]"" is not assignable to ""dict[str, str]""
Type parameter ""_VT@dict"" is invariant, but ""dict[str, bool]"" is not the same as ""str""
Consider switching from ""dict"" to ""Mapping"" which is covariant in the value type
""dict[str, dict[str, bool]]"" is not assignable to ""None"" (reportArgumentType)
2 errors, 0 warnings, 0 informations
```

I am more than happy to submit a pull request for this is desired, but thought it best to put in this issue first in case I am missing something.

### Expected Behavior

import pandas as pd 
import sqlite3


date_params = {""date_col"": {""utc"": True}}

with sqlite3.connect(""blah"") as con:
    # Type checks correctly
    df = pd.read_sql_query(""SELECT * FROM tablename"", con, parse_dates=date_params)
    print(df) 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Linux
OS-release            : 6.11.2-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Fri, 04 Oct 2024 21:51:11 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>","['IO SQL', 'Typing']",2024-10-23 01:34:05,2024-12-15 20:45:43,2,closed
60084,BUG: arithmetic can break equality-hash invariant for Timestamp during DST transition,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# this is immediately following a DST transition
dt_str = ""2023-11-05 01:00-08:00""
tz_str = ""America/Los_Angeles""

ts1 = pd.Timestamp(dt_str, tz=tz_str)

# This is the shortest way I know of to reproduce--more complex math or `pd.date_range` can also
# create this behavior.
ts2 = ts1 + pd.Timedelta(hours=0)
                                                                                                                                                                                                                               
assert ts1 == ts2  # True
assert hash(ts1) == hash(ts2)  # False
```


### Issue Description

Datetime arithmetic on a `Timestamp` instance on a DST transition can create two `Timestamps` that compare equal but have different hash values. Inspecting the example, the cause seems to be that `ts1.fold == 0` and `ts2.fold == 1`, despite otherwise being equal and representing the same instant in time. This breaks the invariant documented in [`object.__hash__`](https://docs.python.org/3/reference/datamodel.html#object.__hash__):

> object.\_\_hash\_\_(self)
> ... The only required property is that objects which compare equal have the same hash value; ...

Converting to UTC or using `pytz`'s normalization as `ts.tz.normalize(ts)` both serve as workarounds for this issue.

### Expected Behavior

I expected `Timestamp` instances that compare equal to have the same hash value.

### Installed Versions

<details>
                                                                                                                                                                                                                                           
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.153.1-microsoft-standard-WSL2
Version               : #1 SMP Fri Mar 29 23:14:13 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8
                                                                                                                                                                                                                                           
pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.1.2
Cython                : None
sphinx                : None
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'good first issue', 'Needs Tests', 'hashing', 'Timestamp']",2024-10-22 21:13:08,2024-11-17 13:52:52,15,closed
60083,DOC: Negative values ​​of n in pandas.DataFrame.head,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/generic.py#L5818-L5893)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html

### Documentation problem

When you explain negative values ​​of n you indicate an incorrect equivalence of the indexing of the DataFrame.
It says:  ""For negative values of n, this function returns all rows except the last |n| rows, equivalent to df[:n].""


### Suggested fix for documentation

And it should say:
""..., equivalent to __df[:-n]__.""","['Docs', 'Closing Candidate']",2024-10-22 20:23:03,2024-10-23 20:20:07,2,closed
60082,"BUG: reading long csv with high numeric values displays a ""mixed types"" message","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# Load csv attached to this issue
import pandas as pd
df = pd.read_csv(""pandas-bug-reproducer.csv"", header=0, index_col=False)
```


### Issue Description

The `read_csv` command results in the following message (this is ipython output, but it also happens non-interactively)
```
<ipython-input-61-2957767dea3a>:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.
```

Column 7 is then imported as strings, not floats.

I can work around this by using the methods in the hint, but this smells like a bug, as if I remove **any** line in the CSV, the issue disappears. If I replace the last line by a copy-paste of the one before, the bug also goes away.

It is quite tricky to create a small reproducer, so I am attaching the file here.
Replacing all text with ""a"" and values with ""1"" kept the issue, while making the data anonymous and very compressible: 
[pandas-bug-reproducer.zip](https://github.com/user-attachments/files/17477355/pandas-bug-reproducer.zip)


### Expected Behavior

This message should not appear, and the data in column 7 should be imported as floating point values.

Moreover, changing the input csv by adding or removing random lines should not affect pandas's behavior.

### Installed Versions

<details>

<summary>First version I tried</summary>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.4.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-553.16.1.el8_10.x86_64
Version               : #1 SMP Thu Aug 1 04:16:12 EDT 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.2.0
pip                   : 24.2
Cython                : 3.0.2
pytest                : 8.2.2
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : 3.1.9
lxml.etree            : 4.9.3
html5lib              : None
pymysql               : 1.0.2
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.4.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
gcsfs                 : None
matplotlib            : 3.7.3
numba                 : 0.60.0
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.15
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>

<details>

<summary>Second version I tried</summary>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.2
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-553.16.1.el8_10.x86_64
Version               : #1 SMP Thu Aug 1 04:16:12 EDT 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : 8.1.3
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV', 'Closing Candidate']",2024-10-22 16:18:11,2025-08-05 16:51:20,3,closed
60080,BUG: Timezone upon DatetimeIndex union is changed to UTC,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

i1 = pd.DatetimeIndex(['2020-01-01 10:00:00+05:00']).astype('datetime64[us, UTC+05:00]')
i2 = pd.DatetimeIndex(['2020-01-01 10:00:00+05:00']).astype('datetime64[ns, UTC+05:00]')
i1.union(i2) # returns DatetimeIndex(['2020-01-01 05:00:00+00:00'], dtype='datetime64[ns, UTC]', freq=None)
```


### Issue Description

@lukemanley @mroeschke looks like test case from https://github.com/pandas-dev/pandas/issues/55238 could be extended to different timezones, currently if taking the union of two `DatetimeIndex` with the same timezone, but different `unit`, then it results in an UTC timezone.

### Expected Behavior

The result should be the higher resolution unit and the original timezone

### Installed Versions

<details>
python : 3.10.14
pandas : 2.2.2
</details>
","['Bug', 'Dtype Conversions', 'Timezones']",2024-10-21 20:39:27,2025-05-30 18:21:54,2,closed
60078,BUG: timedeltas.pyx.c varies between builds,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
I extracted this reproducer:


cd ~/rpmbuild/BUILD/pandas-2.2.2 && for i in $(seq 30) ; do setarch -R cython -3 ./pandas/../pandas/../pandas/../pandas/_libs/tslibs/timedeltas.pyx -o pyx.c ; m=$(md5sum pyx.c|cut -c1-20); cp -a pyx.c pyx.c.$m ; echo $m; done | sort | uniq -c
```


### Issue Description

While working on [reproducible builds](https://reproducible-builds.org/) for [openSUSE](https://en.opensuse.org/openSUSE:Reproducible_Builds) (sponsored by the NLnet NGI0 fund), I found that
our python-pandas 2.2.2 and 2.2.3 (also seen in pandas-2.0.2, possibly also earlier versions) vary
in the generated `pandas/_libs/tslibs/timedeltas.pyx.c` file.

The reproducer only produces 30 times the same hash with the `setarch -R` to disable ASLR.
Without it, you get a random chance for two different results.
The probability also seems to depend on the length of the path string.

```
      5 7653942dcdd2ad6fd8a5
     25 af0a5fefad3a417a7420
```

Using
```bash
diff -u9 pyx.c.[7a]*
```
the diff of those files is
```diff
--- pyx.c.7653942dcdd2ad6fd8a5 2024-10-21 03:51:25.407942993 +0000
+++ pyx.c.af0a5fefad3a417a7420 2024-10-21 03:52:02.744128704 +0000
@@ -45871,19 +45871,19 @@
 
 static PyObject *__pyx_pf_6pandas_5_libs_6tslibs_10timedeltas_9Timedelta___new__(CYTHON_UNUSED PyObject *__pyx_self, PyObject *__pyx_v_cls, PyObject *__pyx_v_value, PyObject *__pyx_v_unit, PyObject *__pyx_v_kwargs) {
   PyObject *__pyx_v_unsupported_kwargs = NULL;
   PyObject *__pyx_v_seconds = NULL;
   PyObject *__pyx_v_ns = NULL;
   PyObject *__pyx_v_us = NULL;
   PyObject *__pyx_v_ms = NULL;
   PyObject *__pyx_v_err = NULL;
   PyObject *__pyx_v_msg = NULL;
-  npy_timedelta __pyx_v_new_value;
+  __pyx_t_5numpy_int64_t __pyx_v_new_value;
   NPY_DATETIMEUNIT __pyx_v_reso;
   NPY_DATETIMEUNIT __pyx_v_new_reso;
   PyObject *__pyx_8genexpr2__pyx_v_key = NULL;
   PyObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_t_2;
   Py_ssize_t __pyx_t_3;
   PyObject *__pyx_t_4 = NULL;
```

### Expected Behavior

Build results should be deterministic.

### Installed Versions

openSUSE Tumbleweed 20241018
Cython-3.0.11","['Bug', 'Upstream issue']",2024-10-21 04:14:59,2025-04-02 21:35:53,7,closed
60074,BUG: Pandas group by aggregate sum returning nan for a big dataset with mostly float64 values and a very few null values,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Code:
neighborhood_stats = taxi_with_neighborhood.groupby('ntaname',dropna=True).agg({
    'cost_per_mile': ['sum','count']
}).reset_index()

Output:
ntaname	cost_per_mile
sum	count
0	Airport	NaN	470286
1	Allerton-Pelham Gardens	NaN	30
2	Arden Heights	NaN	3
3	Astoria	NaN	28449
4	Auburndale	NaN	40
```


### Issue Description


![Screenshot 2024-10-19 021337](https://github.com/user-attachments/assets/b887b073-24fa-438f-b719-b11adc6a5454)
![Screenshot 2024-10-19 021405](https://github.com/user-attachments/assets/2a2b6206-ff8c-4ec3-b86a-1c14cdecfa66)
`from azureml.opendatasets import NycTlcYellow
from datetime import datetime
from dateutil import parser
import geopandas as gpd
from shapely.geometry import Point
import pandas as pd

start_date = parser.parse('2015-01-01')
end_date = parser.parse('2015-01-31')

nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)
taxi_data = nyc_tlc.to_pandas_dataframe()

taxi_data.info()

neighborhoods = gpd.read_file('nyc/geo_export_efe01be6-5681-4d19-b441-eefd6f6bb3fa.shp')

geometry = [Point(xy) for xy in zip(taxi_data['startLon'], taxi_data['startLat'])]
taxi_gdf = gpd.GeoDataFrame(taxi_data, geometry=geometry, crs=""EPSG:4326"")

taxi_with_neighborhood = gpd.sjoin(taxi_gdf, neighborhoods, how=""left"", predicate=""within"")

taxi_with_neighborhood['cost_per_mile'] = taxi_with_neighborhood['totalAmount'] / taxi_with_neighborhood['tripDistance']

taxi_with_neighborhood.reset_index(drop=True, inplace=True)

neighborhood_stats = taxi_with_neighborhood.groupby('ntaname', dropna=True).agg({
    'cost_per_mile': ['sum', 'count']
}).reset_index()

neighborhood_stats.head()

taxi_with_neighborhood['cost_per_mile'][0]

cost_per_mile_sum = taxi_with_neighborhood.groupby('ntaname')['cost_per_mile'].sum()

print(cost_per_mile_sum)

taxi_with_neighborhood_pdf = pd.DataFrame(taxi_with_neighborhood)

neighborhood_stats = taxi_with_neighborhood.groupby('ntaname', dropna=True).agg({
    'cost_per_mile': ['sum', 'count']
}).reset_index()

neighborhood_stats.head()

pd.show_versions()
`

### Expected Behavior

should return float values for sum

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit           : 0f437949513225922d851e9581723d82120684a6
python           : 3.10.15.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.10.0-32-cloud-amd64
Version          : #1 SMP Debian 5.10.223-1 (2024-08-10)
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 2.0.3
numpy            : 1.24.4
pytz             : 2024.2
dateutil         : 2.9.0
setuptools       : 74.1.2
pip              : 24.2
Cython           : 3.0.11
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 3.1.4
IPython          : 8.21.0
pandas_datareader: None
bs4              : 4.12.3
bottleneck       : None
brotli           : 1.1.0
fastparquet      : None
fsspec           : 2024.9.0
gcsfs            : 2024.9.0post1
matplotlib       : 3.7.3
numba            : 0.58.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 9.0.0
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : 1.11.4
snappy           : None
sqlalchemy       : 2.0.35
tables           : None
tabulate         : 0.9.0
xarray           : None
xlrd             : None
zstandard        : 0.23.0
tzdata           : 2024.1
qtpy             : None
pyqt5            : None

</details>
","['Bug', 'Needs Triage']",2024-10-19 15:46:52,2024-10-20 16:31:27,0,closed
60071,BUG: chaining groupby.ffill().bfill() fills beyond group ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# dataframe setup (has one group with all missing values)
df = pd.DataFrame({
    'key': ['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'd'],
    'val': ['x', None, 'x', 'y', None, None, None, None, None, None, None, 'z']
})

# compare the output of these three:
df.update(df.groupby('key').ffill())
df.update(df.groupby('key').bfill())
df.update(df.groupby('key').ffill().bfill())
```


### Issue Description

In the real world, I'm reading in versions of a file, stacking them, and then need to fill in missing values for subsequent operations so I can retain the full metadata we know.

I wanted to groupby and fill both ways to fill in any holes and ran into a bizarre result where a value was replicated thousands of times. #47693 is the closest but almost the ""reverse""; I have no NA values in my groupby variable, but some fill columns have all missing values. I verified that `dropna` has no effect.

Using my repro `df`, `ffill()` seems to do the right thing, leaving group `c` as all NA.
```
df.update(df.groupby('key').ffill())
df

print(df)
   key   val
0    a     x
1    a     x
2    a     x
3    b     y
4    b     y
5    b     y
6    c  None
7    c  None
8    c  None
9    d  None
10   d  None
11   d     z
```

Using `bfill()` also does the right thing.

```
df.update(df.groupby('key').bfill())
print(df)

   key   val
0    a     x
1    a     x
2    a     x
3    b     y
4    b  None
5    b  None
6    c  None
7    c  None
8    c  None
9    d     z
10   d     z
11   d     z
```

However, chaining these together results in backfilling from `d` into `c`:
```
df.update(df.groupby('key').ffill().bfill())
print(df)

   key val
0    a   x
1    a   x
2    a   x
3    b   y
4    b   y
5    b   y
6    c   z
7    c   z
8    c   z
9    d   z
10   d   z
11   d   z
```

On a whim, I wondered if somehow the scope of `groupby()` is only the first operation and tried `df.groupby('key').ffill().groupby('key').bfill()` but got a key error.

Last potentially relevant comment: pandas has been making it harder and harder to keep the grouping columns in the resultant data. I don't really know what the best way is, and SO posts feel clunky to me. That said, [this felt the cleanest](https://stackoverflow.com/questions/58181262/groupby-with-ffill-deletes-group-and-does-not-put-group-in-index) and is why I'm doing the `update()` approach. If that's the cause of this, wanted to mention it.

### Expected Behavior

I would expect the following behavior:

- `groupby().ffill()`: would forward fill within a group; any missing values following non-missing values would be replaced with the preceding non-missing value [matches expectation]
- `groupby().bfill()`: would backward fill within a group; any missing values preceding a non-missing value would be replaced with the next non-missing value [matches expectation]
- `groupby().ffill().bfill()`: combo of the above, however if the group has _no_ values, it would stay this way, and no inherit values from outside of the group


### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Wed Jul 31 20:48:52 PDT 2024; root:xnu-10063.141.1.700.5~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.1
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.2.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.1
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.22.2
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : 2.0.28
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-10-18 18:50:34,2024-10-18 22:52:38,2,closed
60070,PERF: Extreme regression in 2.2 when aggregating a DataFrame with .attrs data,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

Hi, so it seems some interaction between copy-on-write and .attrs data leads to extremely slow performance, at least with custom aggregations. In the below code, the timed aggregations all perform identical in v2.1. But in v2.2, the last one, with custom .attrs data and copy-on-write enabled, is about 10x slower. Using my original dataset, which I cannot share, but which is simply larger in both dimensions, the result was even more extreme, being almost 50x slower (from less than a second to 40s).

``` python
import numpy as np
import pandas as pd
from sklearn import datasets
from pandas import options as pdopt

print(f""{pd.__version__=}"")

X, y = datasets.fetch_covtype(return_X_y=True, as_frame=True)
X[""group""] = np.random.choice(range(20_000), size=len(X))

print(""\nExecution times with and without metadata before setting copy_on_write to 'warn'"")
%timeit -n1 -r1 X.groupby(""group"").Elevation.apply(lambda ser: (ser >= 3000).sum() / len(ser))
X.attrs[""metadata""] = {col: {""hello"": {""world"": ""foobar""}} for col in X.columns}
%timeit -n1 -r1 X.groupby(""group"").Elevation.apply(lambda ser: (ser >= 3000).sum() / len(ser))

pdopt.mode.copy_on_write = True #""warn""

X, y = datasets.fetch_covtype(return_X_y=True, as_frame=True)
X[""group""] = np.random.choice(range(20_000), size=len(X))

print(""\nExecution times with and without metadata after setting copy_on_write to 'warn'"")
%timeit -n1 -r1 X.groupby(""group"").Elevation.apply(lambda ser: (ser >= 3000).sum() / len(ser))
X.attrs[""metadata""] = {col: {""hello"": {""world"": ""foobar""}} for col in X.columns}
%timeit -n1 -r1 X.groupby(""group"").Elevation.apply(lambda ser: (ser >= 3000).sum() / len(ser))
```

The output:
```
pd.__version__='2.2.3'

Execution times with and without metadata before setting copy_on_write to 'warn'
661 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
667 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)

Execution times with and without metadata after setting copy_on_write to 'warn'
671 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
5.22 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
```

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.15
python-bits           : 64
OS                    : Darwin
OS-release            : 24.0.0
Version               : Darwin Kernel Version 24.0.0: Tue Sep 24 23:36:26 PDT 2024; root:xnu-11215.1.12~1/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.2
Cython                : 0.29.37
sphinx                : 8.1.3
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.23.0
psycopg2              : 2.9.10
pymysql               : 1.4.6
pyarrow               : 16.1.0
pyreadstat            : 1.2.7
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.3
sqlalchemy            : 2.0.36
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>


### Prior Performance

```
pd.__version__='2.1.4'

Execution times with and without metadata before setting copy_on_write to 'warn'
703 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
695 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)

Execution times with and without metadata after setting copy_on_write to 'warn'
694 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
691 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
```","['Groupby', 'Performance', 'metadata', 'Closing Candidate']",2024-10-18 17:14:01,2025-08-05 16:32:45,5,closed
60066,"PERF: df.astype(""float64[pyarrow]"") is slow, df.astype(""Float64"") is super slow","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

df = pd.DataFrame(np.zeros((5000, 5000)))
```
```python
%%time
_ = df.astype(""Float64"")
```
CPU times: user 3.87 s, sys: 185 ms, total: 4.05 s
Wall time: 4.05 s

```python
%%timeit
_ = df.astype(""float64[pyarrow]"")
```
566 ms ± 15.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

```python
%%timeit
_ = df.astype(""float64"", copy=True)
```
148 ms ± 984 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)

It takes over 4s for a middle-sized dataframe of 5000 x 5000 to convert to `Float64`, while `float64[pyarrow]` is about 7x faster. The fastest `float64` takes only 150ms, even with `copy` enabled.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.14
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-122-generic
Version               : #132-Ubuntu SMP Thu Aug 29 13:45:52 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.0
Cython                : 3.0.7
sphinx                : 7.3.7
IPython               : 8.25.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : 8.2.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : 2.0.31
tables                : 3.9.2
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None
</details>


### Prior Performance

_No response_","['Performance', 'good first issue', 'ExtensionArray', 'Constructors']",2024-10-18 03:03:51,2024-10-30 19:31:20,4,closed
60064,"BUG: DataFrame(data=[None, 1], dtype='timedelta64[ns]') raises ValueError: Buffer has wrong number of dimensions","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame([None, 1], dtype='timedelta64[ns]')
```


### Issue Description

The error message doesn't tell the user what to do differently. Also, it's possible that there shouldn't be an error at all.

### Expected Behavior

I think this should be allowed, in which case the resulting series would look like

```
                          0
0                       NaT
1 0 days 00:00:00.000000001
```

If it's not allowed, the error message should be clearer.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.9.18
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Wed Jul 31 20:48:52 PDT 2024; root:xnu-10063.141.1.700.5~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
pip                   : 23.3.1
Cython                : None
sphinx                : None
IPython               : 8.18.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Timedelta', 'Constructors']",2024-10-17 17:10:59,2024-11-07 21:30:25,1,closed
60063,BUG: `to_latex()` does not handle braces in new headers gracefully,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""a"": [1, 2]})
result = df.to_latex(header=[r""$\bar{y}$""])
```


### Issue Description

This gives a `KeyError`. I looked into the reason, and it happens because at some point `r""$\bar{y}$"".format(x)` is called, and the braces are interpreted as string interpolation.

### Expected Behavior

I expect one of the following:
1. An error message saying that single braces should be escaped using `{{`
2. Documentation stating the same thing
3. That the single brace is not interpreted as string interpolation

### Installed Versions

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : 2a10e04a099d5f1633abcdfbb2dd9fdf09142f8d
python                : 3.10.15
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-45-generic
Version               : #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1579.g2a10e04a09
numpy                 : 1.26.4
dateutil              : 2.9.0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : 8.1.3
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.1
fastparquet           : 2024.5.0
fsspec                : 2024.9.0
html5lib              : 1.1
hypothesis            : 6.115.2
gcsfs                 : 2024.9.0post1
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 17.0.0
pyreadstat            : 1.2.7
pytest                : 8.3.3
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : 1.0.10
s3fs                  : 2024.9.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.36
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'IO LaTeX', 'Styler']",2024-10-17 09:39:26,2024-10-30 19:23:13,5,closed
60062,BUG: pyxlsb fails to parse datetime-like data,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example
When trying to build some new tests, I found a strange behavior.
- My apologies, but I can't simply have a copy/paste code answer as we can't export to `xlsb `and `xlsm `directly in pandas if I'm not mistaken. 
- Data can be found with test environment in pandas\tests\io\data\excel
- A local copy : [07. Pandas tests.zip](https://github.com/user-attachments/files/17407021/07.Pandas.tests.zip)

```python
import pandas as pd
from pathlib import Path

files = Path(""."").glob(""test1*"")
files = list(files)
files
>>> [WindowsPath('test1.ods'),
>>>  WindowsPath('test1.xls'),
>>> WindowsPath('test1.xlsb'),
>>> WindowsPath('test1.xlsm'),
>>> WindowsPath('test1.xlsx')]

for f in files:
    print(f)
    display(pd.read_excel(f))
```


### Issue Description

<img width=""282"" alt=""image"" src=""https://github.com/user-attachments/assets/fa55a5ec-df5e-4548-b0a6-99fc3479c2ce"">


### Expected Behavior

All dataframes should be read the same

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.6.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19042
machine               : AMD64
processor             : AMD64 Family 23 Model 24 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : Dutch_Belgium.1252

pandas                : 2.2.2
numpy                 : 2.0.2
pytz                  : 2024.2
dateutil              : 2.9.0
setuptools            : 75.1.0
pip                   : 24.2
Cython                : None
pytest                : 8.3.3
hypothesis            : None
sphinx                : 7.4.7
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.3.0
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.21.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : 0.60.0
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'IO Excel', 'Needs Triage']",2024-10-17 06:28:30,2024-10-18 05:41:46,3,closed
60059,BLD: Drop Python 3.10 support,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

NA

### Question about pandas

Per spec0, python 3.10 support should be dropped. Will pandas 3 still support it?","['Build', 'Dependencies', 'Python 3.10']",2024-10-16 13:47:54,2025-08-09 20:00:33,17,closed
60053,"BUG: numerical inconsistency in calculating rolling std, when the same data from different begining","### Pandas version checks

- [ ] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
test = pd.read_csv('std_problem.csv', index_col=0, parse_dates=True)

print(test.rolling(1000).std().iloc[-1])
data    0.0
Name: 2018-01-03 08:45:00, dtype: float64
print(test.iloc[-35785:].rolling(1000).std().iloc[-1])
data    0.0
Name: 2018-01-03 08:45:00, dtype: float64
print(test.iloc[-35784:].rolling(1000).std().iloc[-1])
data    1.230596
print(test.iloc[-35781:].rolling(1000).std().iloc[-1])
data    0.959358
Name: 2018-01-03 08:45:00, dtype: float64
Name: 2018-01-03 08:45:00, dtype: float64
print(test.iloc[-1000:].rolling(1000).std().iloc[-1])
data    0.701844
Name: 2018-01-03 08:45:00, dtype: float64
print(np.std(test.iloc[-1000:], ddof=1))
data    0.701844
dtype: float64
```


### Issue Description

I have a data Series，which has a length of 93230，I want to calculate rolling std，but I got 0 for last one， that’s alomost impossible， so I check the result for only the last 1000 std， it’s the same with numpy.std, I found maybe from a special beging, the rolling std will give different results!

### Expected Behavior

I expect they give the same result: 0.701844,  no matter what the begining is, because the rolling 1000 should only use the latest 1000 numbers for the last std
[std_problem.csv](https://github.com/user-attachments/files/17388072/std_problem.csv)


### Installed Versions

<details>

I test with pandas = 2.1.2 and pandas = 2.2.3, both have the same problem

pd.show_versions()
INSTALLED VERSIONS
------------------
commit              : a60ad39b4a9febdea9a59d602dad44b1538b0ea5
python              : 3.10.13.final.0
python-bits         : 64
OS                  : Linux
OS-release          : 6.8.0-45-generic
Version             : #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024
machine             : x86_64
processor           : x86_64
byteorder           : little
LC_ALL              : None
LANG                : en_US.UTF-8
LOCALE              : en_US.UTF-8
pandas              : 2.1.2
numpy               : 1.26.4
pytz                : 2024.1
dateutil            : 2.9.0
setuptools          : 69.5.1
pip                 : 24.0
lxml.etree          : 5.2.2
jinja2              : 3.1.4
IPython             : 8.20.0
pandas_datareader   : 0.10.0
bs4                 : 4.12.3
bottleneck          : 1.3.7
fsspec              : 2024.6.1
matplotlib          : 3.8.4
numba               : 0.59.1
numexpr             : 2.10.1
pyarrow             : 16.1.0
s3fs                : 2024.6.1
scipy               : 1.12.0
sqlalchemy          : 2.0.31
tables              : 3.9.2
tabulate            : 0.9.0
xarray              : 2024.6.0
xlrd                : 2.0.1
zstandard           : 0.22.0
tzdata              : 2024.1


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-45-generic
Version               : #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
IPython               : 8.28.0
tzdata                : 2024.2

</details>
","['Bug', 'Window']",2024-10-16 02:05:01,2024-10-26 18:47:11,9,closed
60049,BUG: Boolean Series (actually object) with <NA> values breaks ~ negation and reverts to bit-wise operations,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
~pd.Series([True, False]) => [False, True]
~pd.Series([True, False, pd.NA]) => [-2, -1, <NA>]

~pd.Series([True, False, pd.NA], dtype=pd.BooleanDtype()) => [False, True, <NA>]
```


### Issue Description

In the example it is of course possible to work around the issue with forcing the dtype, but in the context of actual dataframes coming from real databases, NULLs are still best handled by simply replacing them with empty strings and hoping for the best, because no NULL-like object seems to work well with ordinary string and filter operations.

As a sidenote, if the pd.NA is instead a None, a TypeError is thrown.

 #59831 is a related issue. 

### Expected Behavior

I'd expect the T/F values to flip and the NAs to remain by default, as they do when the dtype is forced.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2023.3
dateutil              : 2.8.2
setuptools            : 68.2.2
pip                   : 24.2
Cython                : 3.0.5
pytest                : 7.4.2
hypothesis            : 6.87.1
sphinx                : 6.1.3
blosc                 : None
feather               : 0.4.1
xlsxwriter            : 3.1.2
lxml.etree            : 4.9.3
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : 2023.8.0
fsspec                : 2023.9.2
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 13.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : 2.0.22
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2023.11.0
xlrd                  : 2.0.1
zstandard             : 0.22.0
tzdata                : 2022.7
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Duplicate Report', 'Constructors', 'NA - MaskedArrays', 'Closing Candidate']",2024-10-15 12:51:41,2025-06-13 14:22:05,5,closed
60048,ENH: Are there any possibility to add the support for static type hint of the DataFrame and Series ?,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

**New Feature Wanted**

<!-- Enter a clear and concise description of your feature proposal here. -->

We Know that mypy and typing now support dict type hint by
```python
from typing import TypedDict, Optional, Literal
class OverlapsDict(TypedDict):
    id: int
    seq_id: int
    pr_order: int
    pos1: int
    seq: str    
    pos2: int
    seq_len: int
    repeat_info: float # exactly, should be np.nan
    repeat_type: float # exactly, should be np.nan
    item_type: Literal['overlap']
    devmode: str
    update_time: str
```
we can use `OverlapsDict` to restrict dict parameters like 

```python
def myfunc(a:OverlapsDict):
    pass
```

**Can the DataFrame and Series type also support a type hint like this, 
which will check that the `DataFrame.columns` or `Series.index` has the certain columns and their values are in certain dtypes.** 



### Feature Description

is it possible to achieve a new type class `TypedDataFrame`  or make `DataFrame` or `Seires` Generic TypeVar which can be used as the following code?

```python 
# define
class OverlapsDataFrame(TypedDataFrame):
    id: int
    seq_id: int
    pr_order: int
    pos1: int
    seq: str    
    pos2: int
    seq_len: int
    repeat_info: float # exactly, should be np.nan
    repeat_type: float # exactly, should be np.nan
    item_type: Literal['overlap']
    devmode: str
    update_time: str
# usage
def myfunc2(a:OverlapsDataFrame):
    pass
```
and / or 
```python
def myfunc2(a:pd.DataFrame[OverlapsDict]):
    pass
```
After the definition, We can restrict DataFrame parameters with the `OverlapsDataFrame`. The constraint in the example is `a` must be a DataFrame and has columns of certain names with certain types defined by `OverlapsDataFrame` or
`pd.DataFrame[OverlapsDict]`

<!-- Please explain why this feature should be implemented and how it would be used. Add examples, if applicable. -->

### Alternative Solutions

If both forms can be added is the best, either is OK.


### Additional Context

_No response_","['Enhancement', 'Typing']",2024-10-15 12:29:00,2024-10-28 20:36:53,3,closed
60044,DOC: The `value` parameter of `pandas.Timedelta` can also accept float,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.Timedelta.html#pandas.Timedelta

### Documentation problem

It currently states:

https://github.com/pandas-dev/pandas/blob/2a10e04a099d5f1633abcdfbb2dd9fdf09142f8d/pandas/_libs/tslibs/timedeltas.pyx#L1867

There is no `float`.

### Suggested fix for documentation

Add `float, ` to accepted types. 

There already exists a code branch for `float` values so this should not be a problem:
https://github.com/pandas-dev/pandas/blob/2a10e04a099d5f1633abcdfbb2dd9fdf09142f8d/pandas/_libs/tslibs/timedeltas.pyx#L2056","['Docs', 'Timedelta']",2024-10-15 01:43:12,2024-10-29 20:57:27,5,closed
60042,"BUG: resampling empty dataframe under specific circumstance leads to `Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex'`","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
subdf = pd.DataFrame({'My Column 1': []}, index=pd.to_datetime([], unit='s', utc=True))
df = pd.DataFrame()
df = df.join(subdf, how='outer')
df = df.resample(""H"").asfreq()

# output:
'''
<stdin>:1: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/xytxytxyt/.pyenv/versions/temp/lib/python3.11/site-packages/pandas/core/generic.py"", line 9771, in resample
    return get_resampler(
           ^^^^^^^^^^^^^^
  File ""/Users/xytxytxyt/.pyenv/versions/temp/lib/python3.11/site-packages/pandas/core/resample.py"", line 2050, in get_resampler
    return tg._get_resampler(obj, kind=kind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/xytxytxyt/.pyenv/versions/temp/lib/python3.11/site-packages/pandas/core/resample.py"", line 2272, in _get_resampler
    raise TypeError(
TypeError: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex'
'''
```


### Issue Description

see the example

interestingly, this does not happen without the `join()`:
```
import pandas as pd
df = pd.DataFrame({'My Column 1': []}, index=pd.to_datetime([], unit='s', utc=True))
df = df.resample(""H"").asfreq()

# output:
# <stdin>:1: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.
```

### Expected Behavior

no error is raised

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.8
python-bits           : 64
OS                    : Darwin
OS-release            : 22.6.0
Version               : Darwin Kernel Version 22.6.0: Mon Apr 22 20:50:39 PDT 2024; root:xnu-8796.141.3.705.2~1/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.2
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-10-14 20:10:34,2024-10-15 02:53:03,5,closed
60041,DOC: low Contrast issue,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

[https://pandas.pydata.org/docs/getting_started/index.html#intro-to-pandas](url)

### Documentation problem

here in dark mode,
the text has low contrast issue, it's hard to read the test
![image](https://github.com/user-attachments/assets/59298eb6-5750-420a-9eab-fc47a7b8d75d)


### Suggested fix for documentation

```html
<div class=""d-flex flex-row tutorial-card-header-2"" bis_skin_checked=""1"">
<button class=""btn btn-dark btn-sm""></button>
What kind of data does pandas handle?
</div>
```
making the text color ""What kind of data does pandas handle"" white or any other bright color to increase the contrast
my changing the css color to white,
If you accept to fix this issue let me fix it (it will be my first PR)","['Docs', 'Duplicate Report', 'Closing Candidate']",2024-10-14 17:39:09,2024-10-25 12:03:52,2,closed
60039,"BUG: ""python make.py html"" for documentation is giving error","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
python make.py html
```


### Issue Description

While trying to [build documentation](https://pandas.pydata.org/docs/dev/development/contributing_documentation.html#building-the-documentation) for pandas-dev, the build fails with **Sphinx parallel build error**
<img width=""1071"" alt=""Screenshot 2024-10-14 at 19 06 45"" src=""https://github.com/user-attachments/assets/9c1c2fc7-c313-43a3-a65d-f18f4099cba4"">


[Stackoverflow Issue Reference](https://stackoverflow.com/questions/79062269/pandas-dev-local-setup-is-failing-when-building-documentation)



### Expected Behavior

The build should be successful and the docs should be built in local

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2a10e04a099d5f1633abcdfbb2dd9fdf09142f8d
python                : 3.10.15
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Fri Jul  5 17:56:39 PDT 2024; root:xnu-10063.141.1~2/RELEASE_ARM64_T8122
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 0+untagged.35606.g2a10e04
numpy                 : 1.26.4
dateutil              : 2.9.0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : 8.1.1
IPython               : 8.28.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.0
fastparquet           : 2024.5.0
fsspec                : 2024.9.0
html5lib              : 1.1
hypothesis            : 6.114.1
gcsfs                 : 2024.9.0post1
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 17.0.0
pyreadstat            : 1.2.7
pytest                : 8.3.3
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : 1.0.10
s3fs                  : 2024.9.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.35
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Docs', 'Needs Info']",2024-10-14 13:38:44,2025-08-05 17:01:43,9,closed
60028,"BUG: read_csv with chained fsspec TAR file and compression=""infer"" fails with tarfile.ReadError","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import tarfile

with open(""test.csv"", ""wt"") as file:
    file.write(""1;2\n3;4"")
with tarfile.open(""test-csv.tar"", ""w"") as archive:
    archive.add(""test.csv"")

import pandas as pd

data = pd.read_csv(""tar://test.csv::file://test-csv.tar"", compression=None)  # works
print(data)
data = pd.read_csv(""tar://test.csv::file://test-csv.tar"")  # does not work
```


### Issue Description

For chained URLs, the file gets misidentified as TAR, which leads to this backtrace:

```python3
Traceback (most recent call last):
  File ""/projects/ratarmount/worktrees/1/trigger-pandas-bug.py"", line 12, in <module>
    data = pd.read_csv(""tar://test.csv::file://test-csv.tar"", compression=""infer"")  # does not work
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py"", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py"", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py"", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py"", line 1880, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File ""~/.local/lib/python3.12/site-packages/pandas/io/common.py"", line 828, in get_handle
    handle = _BytesTarFile(
             ^^^^^^^^^^^^^^
  File ""~/.local/lib/python3.12/site-packages/pandas/io/common.py"", line 991, in __init__
    self.buffer: tarfile.TarFile = tarfile.TarFile.open(  # type: ignore[assignment]
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.12/tarfile.py"", line 1842, in open
    raise ReadError(f""file could not be opened successfully:\n{error_msgs_summary}"")
tarfile.ReadError: file could not be opened successfully:
- method gz: ReadError('not a gzip file')
- method bz2: ReadError('not a bzip2 file')
- method xz: ReadError('not an lzma file')
- method tar: ReadError('truncated header')
```

I have checked the source code, and the problem seems to be that the full URL is checked for ending with a TAR extension. Instead, only the last part in the chain should be checked, i.e., it should check the extension of `tar://test.csv` not `tar://test.csv::file://test-csv.tar`.

https://github.com/pandas-dev/pandas/blob/2a10e04a099d5f1633abcdfbb2dd9fdf09142f8d/pandas/io/common.py#L593-L594

### Expected Behavior

It should work without an error.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 2a10e04a099d5f1633abcdfbb2dd9fdf09142f8d
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-45-generic
Version               : #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1579.g2a10e04a0
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : None
IPython               : 8.20.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : 2024.9.0
html5lib              : 1.1
hypothesis            : None
gcsfs                 : 2024.9.0post1
jinja2                : 3.1.4
lxml.etree            : 5.2.1
matplotlib            : 3.6.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : 15.0.2
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pytz                  : 2024.1
pyxlsb                : None
s3fs                  : 2024.9.0
scipy                 : 1.11.4
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO CSV']",2024-10-13 19:04:18,2024-11-11 21:16:01,4,closed
60026,BUG: pandas.series.str.replace() fails on compiled regex pattern for `pat`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import regex as re

df = pd.DataFrame({""col"": [""foo_this"", ""bar_this""]})
pattern = re.compile(r""(foo|bar)_.*"")
df.col = df.col.str.replace(pat=pattern, repl=r""\1_that"", regex=True)
```


### Issue Description

pandas.series.str.replace() works fine, if `pat` is a pattern string, but for compiled patterns, a `TypeError` is risen.

### Expected Behavior

col
=====
foo_that
bar_that

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.9
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : de_DE.cp1252

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : 8.0.2
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
html5lib              : 1.1
hypothesis            : 6.112.1
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.8.4
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.35
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Needs Triage', 'Closing Candidate']",2024-10-13 14:09:22,2024-10-13 20:31:16,2,closed
60025,ENH: Explicitly opt out of pytz support,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could explicitly opt out of using `pytz` with `pandas`, even if another library ([APScheduler](https://github.com/agronholm/apscheduler) in my case) uses it as a dependency.

APScheduler is due to also drop `pytz` requirement in future, but I'd like to get ahead now.

### Feature Description

Patch here to 

https://github.com/pandas-dev/pandas/blob/2a10e04a099d5f1633abcdfbb2dd9fdf09142f8d/pandas/_libs/tslibs/timezones.pyx#L25

https://github.com/pandas-dev/pandas/blob/2a10e04a099d5f1633abcdfbb2dd9fdf09142f8d/pandas/_libs/tslibs/timezones.pyx#L41

### Alternative Solutions

N/A

### Additional Context

- https://github.com/pandas-dev/pandas/issues/34916
- https://github.com/pandas-dev/pandas/pull/59089","['Enhancement', 'Timezones']",2024-10-13 00:01:49,2025-05-20 02:42:45,3,closed
60024,DOC: Intro to pandas table not readily visible in dark mode,"https://pandas.pydata.org/docs/getting_started/index.html#intro-to-pandas

This issue exists on 2.2 and the 2.3.x branch but not main. I think this is not worth fixing for 2.2 (would require a release I think), but could be fixed before we release 2.3.",['Docs'],2024-10-12 14:50:45,2025-04-30 16:21:25,6,closed
60023,"BUG: ""Python int too large"" in maybe_convert_objects with numpy 1.26","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.

### Reproducible Example

`pip install numpy==1.26.4 pandas==2.2.3`

```
import numpy, pandas
numpy._set_promotion_state(""weak_and_warn"")
x = pandas.DataFrame({""x"": [1]})
print(x)
```

### Issue Description

If using numpy 1.26, and numpy is set to ""weak"" or ""weak_and_warn"" promotion mode (meant to be compatible with the behavior of numpy 2.x), this causes internal pandas functions to fail.

For example, the above command to print a trivial DataFrame results in:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/core/frame.py"", line 1214, in __repr__
    return self.to_string(**repr_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/util/_decorators.py"", line 333, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/core/frame.py"", line 1394, in to_string
    return fmt.DataFrameRenderer(formatter).to_string(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/io/formats/format.py"", line 962, in to_string
    string = string_formatter.to_string()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/io/formats/string.py"", line 29, in to_string
    text = self._get_string_representation()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/io/formats/string.py"", line 53, in _get_string_representation
    return self._fit_strcols_to_terminal_width(strcols)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/io/formats/string.py"", line 163, in _fit_strcols_to_terminal_width
    col_lens = Series([Series(ele).str.len().max() for ele in strcols])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/core/series.py"", line 584, in __init__
    data = sanitize_array(data, index, dtype, copy)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/core/construction.py"", line 654, in sanitize_array
    subarr = maybe_convert_platform(data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/benjamin/v/lib/python3.11/site-packages/pandas/core/dtypes/cast.py"", line 138, in maybe_convert_platform
    arr = lib.maybe_convert_objects(arr)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""lib.pyx"", line 2602, in pandas._libs.lib.maybe_convert_objects
OverflowError: Python int too large to convert to C long
```

This doesn't happen with numpy 1.26 in its default ""legacy"" mode.  It doesn't happen with numpy 2.x in either ""legacy"" or ""weak"" mode.

More information about numpy 1.x versus 2.x and promotion modes is documented here: https://numpy.org/devdocs/numpy_2_0_migration_guide.html#changes-to-numpy-data-type-promotion

### Expected Behavior

`print(pandas.DataFrame({""x"": [1]}))` should not crash.  It should work properly regardless of the global numpy promotion setting.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.2
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.0-7-amd64
Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.1.20-2 (2023-04-08)
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 23.0.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
```

</details>
",['Closing Candidate'],2024-10-11 21:47:06,2025-08-05 16:35:54,7,closed
60017,QST: how can I save my sparse dataframe with indexes and columns to a format other than .pkl?,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/60956268/save-sparse-pandas-dataframe-to-different-file-types

### Question about pandas

I want have sparse dataframe structure saved in one file (to avoid proliferation of files)

how can I write and read and get the same sparse df? any alternative to pickle?","['Usage Question', 'Sparse', 'Needs Info']",2024-10-10 23:02:46,2024-10-27 14:04:33,5,closed
60010,DOC: Update contributing documentation with code coverage details,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/development/contributing_codebase.html

### Documentation problem

Thanks for the great work on Pandas! I was interested in finding out how Pandas measures code coverage and found that there doesn't seem to be much content yet on how to do this for the project. There's a dead link at https://pandas-coverage-12d2130077bc.herokuapp.com/ for the link mentioned in the content `(tip: you can use the [pandas-coverage app](https://pandas-coverage-12d2130077bc.herokuapp.com/))`. 

I also noticed there's a `codecov.yml` file in the project so I looked this up manually by viewing: https://app.codecov.io/github/pandas-dev/pandas . This looks like a nice way to measure code coverage for the project which has already been configured, but isn't already mentioned within documentation (perhaps in addition to the already mentioned tool, once the link is fixed).

### Suggested fix for documentation

I suggest adding the Codecov link somewhere in the contributing documentation and fixing the broken link for the `pandas-coverage app` (multiple measurements / perspectives could be helpful). ",['Docs'],2024-10-09 17:31:31,2024-10-29 20:46:49,3,closed
60009,DOC: document that DataFrame and Series round() only affect numeric columns,"[DataFrame.round](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.round.html) and `Series.round` seem to ignore non-numeric columns, but their documentation doesn't mention that.",['Docs'],2024-10-09 17:22:06,2024-10-29 20:50:35,2,closed
60003,TYP: More precise NaT stubs,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Amendment to #59996. The current stub has some imprecise typing.


### Feature Description

- inequalities do not cover comparisons of NaTType with itself.
- addition/subtraction does not cover `pd.Period` and numpy types.
- dunder methods should be marked with positional-only parameters, even if the class supports keyword usage.

### Alternative Solutions

N/A

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-10-08 10:30:01,2024-10-08 18:36:42,0,closed
59996,TYP: Add missing methods to `NaTType` stub,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

A few methods supported by `pd.NaT` are missing in the stub.

### Feature Description

- `__pos__` and `__neg__`
- `__mul__`
- `__truediv__` and `__floordiv__`


### Alternative Solutions

N/A

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-10-07 14:44:17,2024-10-07 17:22:28,0,closed
59992,"Potential performance regression with ""API: value_counts to consistently maintain order of input""","PR #59745
@rhshadrach 
![Screenshot 2024-10-07 at 14 39 22](https://github.com/user-attachments/assets/af38975b-8719-4170-ae9b-0029db1ae196)


 ""`groupby.GroupByMethods.time_dtype_as_group` (Python) with application='direct', dtype='int16', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3bb7a472ce800063618142c648...06702dd4b5a8721d800002f7c5a729c4"",
        ""`groupby.GroupByMethods.time_dtype_as_group` (Python) with application='direct', dtype='object', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3bd51c799280002f4c4e9c54cb...06702dd4d44d70ec8000dee1970b99d9"",
        ""`groupby.GroupByMethods.time_dtype_as_group` (Python) with application='direct', dtype='float', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3bcbf8727a80003f7c6cb544eb...06702dd4caca74d68000c0ae0fc0c6ac"",
        ""`groupby.GroupByMethods.time_dtype_as_group` (Python) with application='direct', dtype='int', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3ba2e97361800099085aef8ea1...06702dd4a08e78b68000cea55a0cf474"",
        ""`groupby.GroupByMethods.time_dtype_as_field` (Python) with application='direct', dtype='int16', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3b5d607c988000e105e684bd7e...06702dd457547156800015aefb91cd35"",
        ""`groupby.GroupByMethods.time_dtype_as_field` (Python) with application='direct', dtype='uint', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3b8e5371bf800050742a52feb7...06702dd48b7370c38000dd78883d9f72"",
        ""`groupby.GroupByMethods.time_dtype_as_field` (Python) with application='direct', dtype='int', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3b496474398000f10a224d80c7...06702dd442767a948000a4cd3b23e711"",
        ""`groupby.GroupByMethods.time_dtype_as_field` (Python) with application='direct', dtype='object', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3b7abe7b688000a3c66aab4c6f...06702dd4770876fa8000c307419bd755"",
        ""`groupby.GroupByMethods.time_dtype_as_group` (Python) with application='direct', dtype='uint', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3be8397aec80002970cb77654e...06702dd4e7ce7ca48000147baece32b2"",
        ""`groupby.GroupByMethods.time_dtype_as_field` (Python) with application='direct', dtype='float', method='value_counts', ncols=1, param5='cython'"": ""http://57.128.112.95:5000/compare/benchmarks/06701c3b716970cb80006db835563ce8...06702dd46da4778580009d623b9a289a""","['Groupby', 'Performance', 'Algos']",2024-10-07 12:39:48,2024-10-07 20:14:06,1,closed
59989,#59745 broke groupby().value_counts with all nan column,"              This broke the following case:

```
df = pd.DataFrame({""a"": [1, 2, 1, 2], ""b"": np.nan})
df.groupby(""a"").value_counts()
```

with

```
Traceback (most recent call last):
  File ""/Users/patrick/Library/Application Support/JetBrains/PyCharm2024.1/scratches/scratch_1.py"", line 100, in <module>
    df.groupby(""a"").value_counts()
  File ""/Users/patrick/mambaforge/envs/dask-expr/lib/python3.12/site-packages/pandas/core/groupby/generic.py"", line 2723, in value_counts
    return self._value_counts(subset, normalize, sort, ascending, dropna)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/patrick/mambaforge/envs/dask-expr/lib/python3.12/site-packages/pandas/core/groupby/groupby.py"", line 2535, in _value_counts
    result_series = cast(Series, gb.size())
                                 ^^^^^^^^^
  File ""/Users/patrick/mambaforge/envs/dask-expr/lib/python3.12/site-packages/pandas/core/groupby/groupby.py"", line 2747, in size
    result = self._grouper.size()
             ^^^^^^^^^^^^^^^^^^^^
  File ""/Users/patrick/mambaforge/envs/dask-expr/lib/python3.12/site-packages/pandas/core/groupby/ops.py"", line 696, in size
    ids = self.ids
          ^^^^^^^^
  File ""/Users/patrick/mambaforge/envs/dask-expr/lib/python3.12/site-packages/pandas/core/groupby/ops.py"", line 750, in ids
    return self.result_index_and_ids[1]
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""properties.pyx"", line 36, in pandas._libs.properties.CachedProperty.__get__
  File ""/Users/patrick/mambaforge/envs/dask-expr/lib/python3.12/site-packages/pandas/core/groupby/ops.py"", line 769, in result_index_and_ids
    result_index, ids = self._ob_index_and_ids(
                        ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/patrick/mambaforge/envs/dask-expr/lib/python3.12/site-packages/pandas/core/groupby/ops.py"", line 884, in _ob_index_and_ids
    ob_ids = np.where(ob_ids == -1, -1, index.take(ob_ids))
                                        ^^^^^^^^^^^^^^^^^^
IndexError: cannot do a non-empty take from an empty axes.
```

_Originally posted by @phofl in https://github.com/pandas-dev/pandas/issues/59745#issuecomment-2396534876_
            ","['Bug', 'Groupby', 'Regression', 'Algos']",2024-10-07 10:29:02,2024-10-07 22:30:41,2,closed
59978,"DOC: Pandas 2.2.3 pyproject.toml missing ""Python 3.13"" in classifiers -- but Pandas supports this Python version","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

This does not directly impact Pandas behavior, so Docs is the best fit for issue from what I can see.  On searching docs, this is the closest ref:  [https://pandas.pydata.org/docs/dev/getting_started/install.html](https://pandas.pydata.org/docs/dev/getting_started/install.html)<br>

I also searched:
https://pandas.pydata.org/docs/dev/search.html?q=3.13

### Documentation problem

Pandas 2.2.3 `pyproject.toml` file does not include `Python 3.13` in the list of classifiers.<br>

Although Pandas 2.2.3 is broadly compatible with `Python 3.13`, that fact is not documented in `pyproject.toml` causing Pandas version support to be shown incorrectly on on PyPi and other python sources.<br>

The Pandas release announcement on 20 Sept 2024 [https://pandas.pydata.org/docs/whatsnew/v2.2.3.html](https://pandas.pydata.org/docs/whatsnew/v2.2.3.html) states: *""Pandas 2.2.3 is the first version of pandas that is generally compatible with the upcoming Python 3.13, and both wheels for free-threaded and normal `Python 3.13` will be uploaded for this release.""*  In addition, I have tested Pandas 2.2.3 with `Python 3.13.0rc3` and not found issues.  

No documentation on current active docs reference Python 3.13:  https://pandas.pydata.org/docs/dev/search.html?q=3.13<br>

*Impact -- People do not know Pandas is compatible with Python 3.13**
- on PyPi -- Pandas 2.2.3 does not show Python 3.13 Programming Language Classifier.
- on pyreadiness.org for Python 3.13 - Pandas does not show up as compatible with 3.13 [https://pyreadiness.org/3.13/](https://pyreadiness.org/3.13/)

### Suggested fix for documentation

**Suggested fix:**
- in [pandas main branch](https://github.com/pandas-dev/pandas/tree/main)<br>
- Add the following line to `pandas/pyproject.toml`  in the `classifiers` block: <br> ```'Programming Language :: Python :: 3.13',```<br>

 **Why it's better:**
Adding this change would populate the Programming Language section of the Classifiers on PyPi, which would then flow through to other sites like pyreadiness.org.",['Docs'],2024-10-06 00:11:40,2024-10-09 18:18:55,7,closed
59969,PERF: Possible Memory Leak when Importing Parquet File with PyArrow Engine in Pandas,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

**Description**

We've identified a memory leak when importing Parquet files into Pandas DataFrames using the PyArrow engine. The issue occurs specifically during the conversion from Arrow to Pandas objects, as memory is not released even after deleting the DataFrame and invoking garbage collection.

**Key findings:**

- **No leak with PyArrow alone:** When using PyArrow to read Parquet without converting to Pandas (i.e., no _.to_pandas()_), the memory leak does not occur.
- **Leak with _.to_pandas()_:** The memory leak appears during the conversion from Arrow to Pandas, suggesting the problem is tied to this process.
- **No issue with Fastparquet or Polars:** Fastparquet and Polars (even with PyArrow) do not exhibit this memory issue, reinforcing that the problem is in Pandas’ handling of Arrow data.

**Reproduction Code**

```python
import pandas as pd 
import polars as pl
import gc
import pyarrow.parquet
import ctypes

# To manually trigger memory release
malloc_trim = ctypes.CDLL(""libc.so.6"").malloc_trim

for _ in range(10): 
    df = pd.read_parquet(""/data/to/file.parquet"", engine=""pyarrow"")
    # Also tested with:
    # df = pyarrow.parquet.read_pandas(""/data/to/file.parquet"").to_pandas()
    # df = pl.read_parquet(""/data/to/file.parquet"", use_pyarrow=True)
    
    del df  # Explicitly delete DataFrame
    print(gc.get_count())  # Check object count before garbage collection
    
    for _ in range(3):  # Force garbage collection multiple times
        gc.collect()
    
    print(gc.get_count())  # Check object count after garbage collection

# Calling malloc_trim(0) is the only way we found to release the memory
# malloc_trim(0)
```

**Observations:**

- **Garbage Collection:** Despite invoking the garbage collector multiple times, memory allocated to the Python process keeps increasing when _.to_pandas()_ is used, indicating improper memory release during the conversion.
- **Direct Use of PyArrow:** When we import the data directly using PyArrow (without converting to Pandas), the memory usage remains stable, showing that the problem originates in the Arrow-to-Pandas conversion process.
- **Manual Memory Release (ctypes):** The only reliable way we have found to release the memory is by manually calling _malloc_trim(0)_ via ctypes. However, we believe this is not a proper solution and that memory management should be handled internally by Pandas.

**OS environment**

_Icon name: computer-vm
Chassis: vm
Virtualization: microsoft
Operating System: Red Hat Enterprise Linux 8.10 (Ootpa)
CPE OS Name: cpe:/o:redhat:enterprise_linux:8::baseos
Kernel: Linux 4.18.0-553.16.1.el8_10.x86_64
Architecture: x86-64_

**Affected Versions**

_pandas==2.2.2
pandas==2.2.3
Latest development version (as of writing)_

**Conclusion**

The issue seems to occur during the conversion from Arrow to Pandas, rather than being a problem within PyArrow itself. Given that memory is only released by manually invoking _malloc_trim(0)_, we suspect there is a problem with how Pandas handles memory management when working with Arrow data. This issue does not arise when using Fastparquet or Polars, further indicating that it is specific to the Pandas-Arrow interaction.

We recommend investigating how memory is allocated and released during the conversion from Arrow objects to Pandas DataFrames to resolve this issue.

Please let us know if further details are needed, and we are happy to assist.

**Contributors:**

- @Voltagabbana 
- @okamiRvS 
- @carlonicolini 

We would appreciate any feedback or insights from the maintainers and other contributors on how to improve memory management in this context.


### Installed Versions

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-553.16.1.el8_10.x86_64
Version               : #1 SMP Thu Aug 1 04:16:12 EDT 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : 2024.5.0
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


### Prior Performance

_No response_","['Performance', 'Needs Info']",2024-10-04 15:46:15,2024-10-05 17:15:17,3,closed
59968,DOC: get_dummies behaviour with dummy_na = True is counter-intuitive / incorrect when no NaN values present,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html

### Documentation problem

The [docs](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) currently read for this function:

> Add a column to indicate NaNs, if False NaNs are ignored.

However, when no NaN values are present, a useless constant NaN indicator column is still added:

```python
>>> df = pd.DataFrame(pd.Series([0, 1], dtype=""int64""))
>>> df
   0
0  0
1  1
>>> pd.get_dummies(df, columns=[0], dummy_na=True, drop_first=True)
   0_1.0  0_nan
0  False  False
1   True  False
>>> pd.get_dummies(df, columns=[0], dummy_na=True)
   0_0.0  0_1.0  0_nan
0   True  False  False
1  False   True  False
```

This is arguably quite unexpected behaviour, as constant columns do not contain information except in some very rare cases and for specific custom models. 

I.e. for almost any kind of model this column will be ignored (but then annoyingly clutter e.g. `.feature_importances` variables and also perhaps needlessly increase compute times for algorithms that scale significantly with the number of features, but which may not have methods for ignoring constant input features). For data with a lot of binary features, and pipelines or models which also might do e.g. conversion to floating-point dtypes, all these useless extra constant features could also significantly increase memory requirements (especially in multiprocessing contexts). 

I imagine the intended design decision is that if you do e.g. `df1, df2 = train_test_split(df)`, and it ends up such that `df1` doesn't have any NaN values for some feature `""f""`, but `df2` does, then at least with the current implementation, the user can be ensured the following does not raise an AssertionError:

```python
dummies1 = pd.get_dummies(df1, [""f""], dummy_na=True)
dummies2 = pd.get_dummies(df2, [""f""], dummy_na=True)
assert dummies1.columns.tolist() == dummies2.columns.tolist()
```

But still, that is a pretty strange use-case, as dummification should generally happen right at the beginning on the full data. 

In my opinion the default behaviour should to _**only** add a NaN indicator column if NaN values are actually present..._ . I actually would consider this to be an implementation or design bug, for like 99% of use-cases. But at bare minimum this undesirable and unexpected behaviour should be documented with some reasoning. 

### Suggested fix for documentation

Just change the docs to:

> Add a column to indicate NaNs. If True, A NaN indicator column will be added even if no NaN values are present [insert reasoning here]. If False, NaNs are ignored [actually ""ignored"" is extremely unclear too, as per [this issue](https://github.com/pandas-dev/pandas/issues/15923)].",['Docs'],2024-10-04 14:19:40,2024-10-08 02:35:05,8,closed
59961,BUG: convert_dtypes does not always convert numpy.nan to pd.NA,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

x = pd.Series([np.nan, 0.0, 1.2, pd.NA]).convert_dtypes()
y = pd.Series([np.nan, 0.0, 30.2, 10]).convert_dtypes()
z = pd.Series([np.nan, 0.0, 15.2, 9.2]).convert_dtypes()
(z**2 / (x*y)).convert_dtypes()
```


### Issue Description

`numpy.nan` resulting from an arithmetic operation (e.g., division by zero) is not being converted to `pd.NA`. The above examples outputs:

```
0        <NA>
1         NaN
2    6.375276
3        <NA>
dtype: Float64
```

### Expected Behavior

I expect all `np.nan` present in the Series are converted to `pd.NA`:

```
0        <NA>
1        <NA>
2    6.375276
3        <NA>
dtype: Float64

```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 139def2145b83d40364235c6297e1833eab7bb05
python                : 3.12.3
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-41-generic
Version               : #41-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug  2 20:41:06 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 3.0.0.dev0+1545.g139def2145
numpy                 : 2.2.0.dev0+git20240930.3ee9e6a
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Duplicate Report', 'Closing Candidate', 'PDEP missing values']",2024-10-04 09:44:12,2024-10-05 13:37:42,5,closed
59960,BUG: Plotting bug in Pandas 2.2.2 and 2.2.3,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
wget https://github.com/jlchang/cb-python-intro-lesson-template/raw/refs/heads/main/episodes/files/data.zip
unzip data.zip

import pandas as pd
df_long = pd.read_pickle('data/df_long.pkl')
albany = df_long[df_long['branch'] == 'Albany Park']
albany.plot()
```


### Issue Description

FYI, Pandas 2.2.2 seems to have a plotting bug (this does not seem to be specific to Colab). For [this tutorial](https://broadinstitute.github.io/2024-09-27-python-intro-lesson/data-visualisation.html#plotting-with-pandas), running `albany['circulation'].plot()` renders:
<img width=""585"" alt=""Screenshot 2024-10-04 at 5 12 09 AM"" src=""https://github.com/user-attachments/assets/0ed0f0b1-c6ef-4cfb-aabe-d1b683ff1320"">


### Expected Behavior

The expected plot looks like:
<img width=""597"" alt=""Screenshot 2024-10-04 at 5 14 11 AM"" src=""https://github.com/user-attachments/assets/aa28c8ab-47bb-4499-850a-c789064f3dc1"">
Pandas `2.0.3` generates the expected plot (Pandas `2.2.3` is also problematic)

### Installed Versions


Python 3.10.9 on Mac M1 Max running Sonoma 14.6.1 (23G93)
<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.9
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.23.5
pytz                  : 2022.7.1
dateutil              : 2.8.2
pip                   : 24.0
Cython                : None
sphinx                : None
IPython               : 8.11.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.11.2
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.2
lxml.etree            : None
matplotlib            : 3.6.3
numba                 : 0.56.4
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.10.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : N/A
pyqt5                 : None

</details>

The same issue happens in Google Colab which is running Python 3.10.12
<details>
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.85+
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.2
dateutil              : 2.8.2
pip                   : 24.1.2
Cython                : 3.0.11
sphinx                : 5.0.2
IPython               : 7.34.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
html5lib              : 1.1
hypothesis            : None
gcsfs                 : 2024.6.1
jinja2                : 3.1.4
lxml.etree            : 4.9.4
matplotlib            : 3.7.1
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.23.2
psycopg2              : 2.9.9
pymysql               : None
pyarrow               : 16.1.0
pyreadstat            : None
pytest                : 7.4.4
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.35
tables                : 3.8.0
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>","['Bug', 'Visualization', 'Regression', 'Closing Candidate']",2024-10-04 09:42:55,2024-10-06 11:48:57,5,closed
59957,BUG: Disable Numpy memory allocation while concat,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.concat(df_list,axis=1)
```


### Issue Description

We have sparse data with many null values, and while reading it using Pandas with PyArrow, it doesn't consume much memory because of pandas internal compression logic. However, during concatenation, NumPy allocates memory that isn't actually used, causing our Python script to fail due to memory allocation issues. Can you provide an option to disable NumPy memory allocation when concatenating DataFrames along axis=1?

### Expected Behavior

Numpy should not allocate memory when it is not used

### Installed Versions

<details>

pandas                : 2.2.2
numpy                 : 2.1.0


</details>
","['Bug', 'Needs Triage']",2024-10-04 06:30:31,2024-10-04 17:58:50,1,closed
59956,ENH: Disable Numpy memory allocation while concat,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

We have sparse data with many null values, and while reading it using Pandas with PyArrow, it doesn't consume much memory because of pandas internal compression logic. However, during concatenation, NumPy allocates memory that isn't actually used, causing our Python script to fail due to memory allocation issues. Can you provide an option to disable NumPy memory allocation when concatenating DataFrames along axis=1?

### Feature Description

pd.concat(df_list,axis=1,numpy_allocation=False)

### Alternative Solutions

Atleast can you provide how can we change C++ script internally and use it for our purpose

### Additional Context

Please let me know if i am wrong.","['Enhancement', 'Performance', 'Needs Info']",2024-10-04 06:25:07,2024-10-12 14:26:01,8,closed
59953,ENH: Is there a way to plot a long format data in matplotlib without pivoting the table ? ,"### Feature Type

- [ ] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

### Problem

I am currently reading this book called Hands On Data Analysis using Pandas by Stefie Molin. There are two formats of data, wide and long format data. The author uses pandas and matplotlib to plot a wide format data while uses seaborn package for the long format data. I tried searching in the web and it seems to be the custom. I tried asking gpt as well, and I can plot the long format data without seaborn too but it seems that I have to pivot the dataset. Is there a way around it .

Wide Data Frame Sample

date | TMAX | TMIN | TOBS
-- | -- | -- | --
2018-10-28 | 8.3 | 5.0 | 7.2
2018-10-04 | 22.8 | 11.7 | 11.7
2018-10-20 | 15.0 | -0.6 | 10.6
2018-10-24 | 16.7 | 4.4 | 6.7
2018-10-23 | 15.6 | -1.1 | 10.0

Long Data Frame Sample


date | datatype | value
-- | -- | --
2018-10-01 | TMAX | 21.1
2018-10-01 | TMIN | 8.9
2018-10-01 | TOBS | 13.9
2018-10-02 | TMAX | 23.9
2018-10-02 | TMIN | 13.9
2018-10-02 | TOBS | 17.2


Long Data Frame after pivoting

![image](https://github.com/user-attachments/assets/4eb22ed2-1e4f-4b40-be77-2b817f5573a7)



plot command for wide df
`ax = wide_df.plot(
    x='date',
    y=['TMAX', 'TMIN', 'TOBS'],
    figsize=(15, 5),
    title='Temperature in NYC in October 2018'
)
`
plot command for long df after pivot
`ax=long_df.pivot(index='date',columns='datatype',values='value')`
and apply a similar command as above


plot command for long_df with seaborn 
`ax=sns.lineplot(data=long_df,x='date',y='value',hue='datatype')`

Why isn't there a hue parameter or something similar in pandas for a long data format? My question can also be framed this way,
"" Why is pandas not enough for plotting? Why do I need external packages like matplotlib and seaborn to plot pandas data structure?"" 

Forgive me for my ignorance but I really want to know why cann't the features available in pandas and seaborn be available in pandas.

### Feature Description

Lets start with a hue feature in pandas for a long data format

### Alternative Solutions

we might have to pivot the table if we have to plot without using seaborn if we just need to use pandas

### Additional Context

_No response_","['Visualization', 'Usage Question', 'Needs Info']",2024-10-03 13:59:42,2024-12-28 14:48:45,4,closed
59951,BUG: Handling of Pydantic v2 time zone objects,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from datetime import datetime, timezone
import pandas as pd
from pydantic import BaseModel

class Model(BaseModel):
    dt: datetime

model = Model(dt=""2024-06-01 00:00:00+00:00"")
dt2 = datetime(2024, 6, 10, tzinfo=timezone.utc)

dt_range = pd.date_range(model.dt, dt2)
```


### Issue Description

Calling `pd.date_range` results the following error (full stack trace below): `Inputs must both have the same timezone, UTC != UTC`

The issue arises because:
* `model.dt` has a timezone of type `pydantic_core._pydantic_core.TzInfo`; whereas
* `dt2` has a timezone of type `datetime.timezone`.

This appears to be related to the issue discussed here: https://github.com/pydantic/pydantic/issues/8195 - not sure if this is best tackled in Pandas or Pydantic, but the current situation is very confusing when the error message reports `UTC != UTC`.

For reference my Pydantic versions are:
```
pydantic          2.9.2
pydantic_core     2.23.4
```

```
Traceback (most recent call last):
  File ""/home/tcoleman/miniconda3/envs/tmp-pandas/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py"", line 2646, in _infer_tz_from_endpoints
    inferred_tz = timezones.infer_tzinfo(start, end)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""timezones.pyx"", line 369, in pandas._libs.tslibs.timezones.infer_tzinfo
AssertionError: Inputs must both have the same timezone, UTC != UTC

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tcoleman/miniconda3/envs/tmp-pandas/lib/python3.12/site-packages/pandas/core/indexes/datetimes.py"", line 1008, in date_range
    dtarr = DatetimeArray._generate_range(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tcoleman/miniconda3/envs/tmp-pandas/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py"", line 445, in _generate_range
    tz = _infer_tz_from_endpoints(start, end, tz)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tcoleman/miniconda3/envs/tmp-pandas/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py"", line 2649, in _infer_tz_from_endpoints
    raise TypeError(
TypeError: Start and end cannot both be tz-aware with different timezones
```

### Expected Behavior

Ability to run `pd.date_range` without error when the timezones are the same.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.123.1-microsoft-standard-WSL2
Version               : #1 SMP Mon Aug 7 19:01:48 UTC 2023
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage']",2024-10-03 09:38:36,2024-10-03 17:42:14,1,closed
59950,BUG: DataFrame.query() throws error when df has duplicate column names,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({'a': range(3), 'b': range(3), 'c': range(3)}).rename(columns={'b': 'a'})
print(df.query('c == 1'))
```


### Issue Description

Since pandas 2.2.1 this throws an unexpected error:
`TypeError: dtype 'a    int64
a    int64
dtype: object' not understood`

This is because DataFrame.query() calls DataFrame.eval() which in turn calls DataFrame._get_cleaned_column_resolvers().

The dict comprehension in DataFrame._get_cleaned_column_resolvers() was changed in version 2.2.1.
version 2.2.0
```
return {
       clean_column_name(k): Series(
            v, copy=False, index=self.index, name=k
       ).__finalize__(self)
       for k, v in zip(self.columns, self._iter_column_arrays())
       if not isinstance(k, int)
 }
 ```
 
version 2.2.1
```
return {
            clean_column_name(k): Series(
                v, copy=False, index=self.index, name=k, dtype=self.dtypes[k]
            ).__finalize__(self)
            for k, v in zip(self.columns, self._iter_column_arrays())
            if not isinstance(k, int)
   }
   ```
   
   since the dtypes are now checked when the Series are created, this introduces the error described above, since for a duplicate
   column name self.dtypes[k] returns a Series instead of single value.


### Expected Behavior

1) I would expect either the behavior prior to v2.2.1 where the above example would return:
```
>>> df.query('c == 1')
   a  a  c
1  1  1  1
```
moreover, calling query() on column 'a' also works:
```
>>> df.query('a == 1')
   a  a  c
1  1  1  1
```
or
2) If above behavior is unwanted, I would except better error handling, smt like:
```
>>> df.query('c == 1')
DuplicateColumnError: DataFrame.query() is not supported for DataFrames with duplicate column names
```


### Installed Versions

INSTALLED VERSIONS
------------------
commit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.4.0-165-generic
Version               : #182-Ubuntu SMP Mon Oct 2 19:43:28 UTC 2023
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.1
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.8.2
setuptools            : 69.1.1
pip                   : 23.0
Cython                : None
pytest                : 8.2.0
hypothesis            : 6.100.4
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : 0.19.2
pyarrow               : 15.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.27
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None","['Bug', 'good first issue', 'expressions']",2024-10-03 09:07:39,2024-11-05 19:00:31,12,closed
59949,"ENH: read_excel, read_csv, to_csv, to_excel ... pandas read, write files with progress bar","### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

read_excel, read_csv, to_csv, to_excel ... pandas read, write files with progress bar
we need progress bar for Large data readings

### Feature Description

read_excel, read_csv, to_csv, to_excel ... pandas read, write files with progress bar
we need progress bar for Large data readings

### Alternative Solutions

read_excel, read_csv, to_csv, to_excel ... pandas read, write files with progress bar
we need progress bar for Large data readings

### Additional Context

read_excel, read_csv, to_csv, to_excel ... pandas read, write files with progress bar
we need progress bar for Large data readings","['Enhancement', 'Needs Triage']",2024-10-03 06:27:51,2024-10-03 17:43:44,3,closed
59944,BUG: pd.to_numeric(timedelta_scalar) raises TypeError,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.to_numeric(pd.Timedelta(1))
```


### Issue Description

Getting a `TypeError`

### Expected Behavior

For the example above, I should get the integer `1`. That would match the behavior of `pd.to_numeric(pd.Series(pd.Timedelta(1)))`

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.9.18
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Wed Jul 31 20:48:52 PDT 2024; root:xnu-10063.141.1.700.5~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
pip                   : 23.3.1
Cython                : None
sphinx                : None
IPython               : 8.18.1
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.2
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Dtype Conversions', 'Timedelta', 'Timestamp']",2024-10-02 20:29:14,2024-10-05 17:12:10,2,closed
59938,BUG: Inconsistent treatment of NaNs when `.apply()` function is used on categorical columns,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame(
    {
        ""a"": [4, np.nan, 6],
        ""b"": [""one"", ""two"", np.nan]
    }
)
df[""b""] = df[""b""].astype(""category"")

df[""a'""] = df[""a""].apply(lambda x: pd.notnull(x))  # rows with NaNs are treated
df[""b'""] = df[""b""].apply(lambda x: pd.notnull(x))  # rows with NaNs are skipped
display(df)
```


### Issue Description

There is an inconsistency in how `DataFrame.apply()` function works on columns with categorical data, vs columns with any other type of the data. Generally speaking, `.apply()` function is called for every row of data (assuming `axis=0` here), and then the user-defined function would be called on that row. This happens for all values of data, including NaNs, so if a special behaviour for NaNs is needed, it can be integrated easily. However, if the data column is of type `category`, the rows with NaNs appear to be automatically skipped, so rows with NaNs cannot be processed by the user.

In my opinion, this is a fundamental inconsistency, which I would call a bug. I would understand that in some situations the skipping of the NaN rows might be a preferred behaviour, but then it should probably be controllable via keyword arguments and certainly not datatype-dependent.

### Expected Behavior

```df[""b'""] = df[""b""].astype(""string"").apply(lambda x: pd.notnull(x))  # would work correctly...```

...but I don't quite understand why such a fundamental behavior needs to be dependent on the type of the data.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.3.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22621
machine               : AMD64
processor             : AMD64 Family 25 Model 68 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.0.2
pip                   : 23.3.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : 2024.5.0
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Categorical', 'Apply', 'good first issue']",2024-10-02 10:47:47,2024-10-04 21:19:58,2,closed
59934,BUG: Trying to use convert_dtypes with dtype_backend='pyarrow' specified on empty categorical series results in error or null[pyarrow] dtype,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# example 1
ser1 = pd.Series(pd.Categorical([None] * 5))
converted1 = ser1.convert_dtypes(dtype_backend=""pyarrow"")

# example 2
ser2 = pd.Series(pd.Categorical([None] * 5, categories=[""S1"", ""S2""]))
converted2 = ser2.convert_dtypes(dtype_backend=""pyarrow"")
```


### Issue Description

Trying to convert categorical series with empty categories results in `pyarrow.lib.ArrowNotImplementedError: NumPyConverter doesn't implement <null> conversion. `.
Trying to convert categorical series with non-empty categories returns null[pyarrow] dtype.

This is inconsistent with regular behavior when converting categoricals - if series is not empty, convert_dtypes call returns categorical dtype (essentially ignoring the requested pyarrow backend conversion).

I encountered this issue when trying to read files (like parquet, spss, ...) where some categorical variables are empty...

### Expected Behavior

Returning categorical dtype with propagated categories, and not raising errors.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 00855f81bd84cc6ed9ae42c5f66916b2208dbe04
python                : 3.12.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252
pandas                : 3.0.0.dev0+1536.g00855f81bd
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : 8.0.2
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.0
fastparquet           : 2024.5.0
fsspec                : 2024.9.0
html5lib              : 1.1
hypothesis            : 6.112.2
gcsfs                 : 2024.9.0post1
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 17.0.0
pyreadstat            : 1.2.7
pytest                : 8.3.3
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : 1.0.10
s3fs                  : 2024.9.0
scipy                 : 1.14.1
sqlalchemy            : 2.0.35
tables                : 3.10.1
tabulate              : 0.9.0
xarray                : 2024.9.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Categorical', 'Arrow']",2024-10-02 08:22:10,2025-03-17 19:44:23,0,closed
59933,BUG: locset with Series as column key fails inconsistently,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> import pandas as pd

>>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=list(""ABC""))

>>>  df.loc[:, pd.Series(['A', 'C'])] = pd.Series([10, 20, 30]) # succeeds

>>> df
    A  B   C
0  10  2  20
1  10  5  20

>>> df.loc[:, pd.Series(['A', 'B', 'C'])] = pd.Series([10, 20, 30]) # fails
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1429, in Block.setitem(self, indexer, value, using_cow)
   1428 try:
-> 1429     values[indexer] = casted
   1430 except (TypeError, ValueError) as err:

ValueError: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (2,3)

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
Cell In[5], line 1
----> 1 df.loc[:, pd.Series(['A', 'B',  'C'])] = pd.Series([10, 20, 30])

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/indexing.py:911, in _LocationIndexer.__setitem__(self, key, value)
    908 self._has_valid_setitem_indexer(key)
    910 iloc = self if self.name == ""iloc"" else self.obj.iloc
--> 911 iloc._setitem_with_indexer(indexer, value, self.name)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/indexing.py:1944, in _iLocIndexer._setitem_with_indexer(self, indexer, value, name)
   1942     self._setitem_with_indexer_split_path(indexer, value, name)
   1943 else:
-> 1944     self._setitem_single_block(indexer, value, name)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/indexing.py:2218, in _iLocIndexer._setitem_single_block(self, indexer, value, name)
   2215 self.obj._check_is_chained_assignment_possible()
   2217 # actually do the set
-> 2218 self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)
   2219 self.obj._maybe_update_cacher(clear=True, inplace=True)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/internals/managers.py:415, in BaseBlockManager.setitem(self, indexer, value, warn)
    411     # No need to split if we either set all columns or on a single block
    412     # manager
    413     self = self.copy()
--> 415 return self.apply(""setitem"", indexer=indexer, value=value)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/internals/managers.py:363, in BaseBlockManager.apply(self, f, align_keys, **kwargs)
    361         applied = b.apply(f, **kwargs)
    362     else:
--> 363         applied = getattr(b, f)(**kwargs)
    364     result_blocks = extend_blocks(applied, result_blocks)
    366 out = type(self).from_blocks(result_blocks, self.axes)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1432, in Block.setitem(self, indexer, value, using_cow)
   1430     except (TypeError, ValueError) as err:
   1431         if is_list_like(casted):
-> 1432             raise ValueError(
   1433                 ""setting an array element with a sequence.""
   1434             ) from err
   1435         raise
   1436 return self

ValueError: setting an array element with a sequence.

>>> In [7]: df.loc[:, pd.Series(['A'])] = pd.Series([10, 20, 30]) # Fails
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1429, in Block.setitem(self, indexer, value, using_cow)
   1428 try:
-> 1429     values[indexer] = casted
   1430 except (TypeError, ValueError) as err:

ValueError: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (2,1)

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
Cell In[7], line 1
----> 1 df.loc[:, pd.Series(['A'])] = pd.Series([10, 20, 30])

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/indexing.py:911, in _LocationIndexer.__setitem__(self, key, value)
    908 self._has_valid_setitem_indexer(key)
    910 iloc = self if self.name == ""iloc"" else self.obj.iloc
--> 911 iloc._setitem_with_indexer(indexer, value, self.name)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/indexing.py:1944, in _iLocIndexer._setitem_with_indexer(self, indexer, value, name)
   1942     self._setitem_with_indexer_split_path(indexer, value, name)
   1943 else:
-> 1944     self._setitem_single_block(indexer, value, name)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/indexing.py:2218, in _iLocIndexer._setitem_single_block(self, indexer, value, name)
   2215 self.obj._check_is_chained_assignment_possible()
   2217 # actually do the set
-> 2218 self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)
   2219 self.obj._maybe_update_cacher(clear=True, inplace=True)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/internals/managers.py:415, in BaseBlockManager.setitem(self, indexer, value, warn)
    411     # No need to split if we either set all columns or on a single block
    412     # manager
    413     self = self.copy()
--> 415 return self.apply(""setitem"", indexer=indexer, value=value)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/internals/managers.py:363, in BaseBlockManager.apply(self, f, align_keys, **kwargs)
    361         applied = b.apply(f, **kwargs)
    362     else:
--> 363         applied = getattr(b, f)(**kwargs)
    364     result_blocks = extend_blocks(applied, result_blocks)
    366 out = type(self).from_blocks(result_blocks, self.axes)

File ~/.miniconda3/envs/snowpark/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1432, in Block.setitem(self, indexer, value, using_cow)
   1430     except (TypeError, ValueError) as err:
   1431         if is_list_like(casted):
-> 1432             raise ValueError(
   1433                 ""setting an array element with a sequence.""
   1434             ) from err
   1435         raise
   1436 return self

ValueError: setting an array element with a sequence

>>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list(""ABC""))

>>> df.loc[:, pd.Series(['A', 'B', 'C'])] = pd.Series([10, 20, 30]) # Succeeds

>>> df
    A   B   C
0  10  20  30
1  10  20  30
2  10  20  30
```


### Issue Description

It seems that we can only provide as many column indexers in the Series key for the column as are rows, but all of the above examples should succeed.

### Expected Behavior

Columns that are indexed should be set with the corresponding value (determined by position).

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.1
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : 5.0.2
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.27.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Indexing']",2024-10-01 23:40:51,2025-03-01 19:17:21,6,closed
59932,BUG: np.nan_to_num change pandas DataFrame in place when copy = True ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

x = pd.DataFrame([np.nan])
np.nan_to_num(x, copy=True)
print(x)
```


### Issue Description

Result x is changed to 0.0 even when nan_to_num is called with copy=True.

This issue is originally reported to numpy [here](https://github.com/numpy/numpy/issues/27487). Looks like pandas ignores `copy=True` NumPy passes?

### Expected Behavior

x should not be changed when copy=True.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.8
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.0-smp-1100.466.602.1
Version               : #1 [v5.10.0-1100.466.602.1] SMP @1708156698
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 2.1.1
pytz                  : 2024.1
dateutil              : 2.8.1
pip                   : None
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : 5.3.0
matplotlib            : None
numba                 : None
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : None
zstandard             : None
tzdata                : None
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Compat']",2024-10-01 17:07:38,2024-11-04 20:19:57,4,closed
59927,"Potential performance regression with ""BUG (string): ArrowEA comparisons with mismatched types"" ","PR #59089

@mroeschke 

edit: changed the PR (maybe) causing these. 

 ""`timeseries.DatetimeIndex.time_normalize` (Python) with index_type='tz_aware'"": ""http://57.128.112.95:5000/compare/benchmarks/066f645062ef701180006acc763ff04b...066f672246ca708480008caae7fcae98"",
        ""`timeseries.DatetimeIndex.time_timeseries_is_month_start` (Python) with index_type='tz_aware'"": ""http://57.128.112.95:5000/compare/benchmarks/066f6450656d7a338000261fcdf27aa5...066f672249d0743880006ea312170a5e"",
        ""`timeseries.DatetimeAccessor.time_dt_accessor_normalize` (Python) with t='US/Eastern'"": ""http://57.128.112.95:5000/compare/benchmarks/066f645054697fc68000c1b0f2cf30ef...066f672236e0765f80000d2a5a7495c5"",
        ""`timeseries.DatetimeIndex.time_to_time` (Python) with index_type='tz_aware'"": ""http://57.128.112.95:5000/compare/benchmarks/066f64506d23794c8000b694ac8c3ee8...066f6722555478a380001de21300bc86"",
        ""`timeseries.DatetimeAccessor.time_dt_accessor_time` (Python) with t='US/Eastern'"": ""http://57.128.112.95:5000/compare/benchmarks/066f645056a470f4800002bd73ddfb6f...066f672239377fff8000d46f40374267"",
        ""`indexing.SortedAndUnsortedDatetimeIndexLoc.time_loc_sorted` (Python)"": ""http://57.128.112.95:5000/compare/benchmarks/066f6449250f782680001f3c602cd479...066f671b60217a4f800001cf43cda62c"",
        ""`timeseries.DatetimeAccessor.time_dt_accessor_year` (Python) with t='US/Eastern'"": ""http://57.128.112.95:5000/compare/benchmarks/066f645058b571e9800066b9e2e7653d...066f67223b8179528000f64eb905c640"",
        ""`timeseries.TzLocalize.time_infer_dst` (Python) with t='US/Eastern'"": ""http://57.128.112.95:5000/compare/benchmarks/066f6450803972f08000381401641f77...066f672273de7fb380000854185e3419"",
        ""`timeseries.DatetimeIndex.time_to_date` (Python) with index_type='tz_aware'"": ""http://57.128.112.95:5000/compare/benchmarks/066f645067d178d280002bb4fbffe23a...066f67224f3d761c800064699af09ede"",
        ""`timeseries.DatetimeAccessor.time_dt_accessor_date` (Python) with t='US/Eastern'"": ""http://57.128.112.95:5000/compare/benchmarks/066f64504e167d0480003af47296ee52...066f67222dc274798000cbfdf7925327"",
        ""`timeseries.DatetimeAccessor.time_dt_accessor_month_name` (Python) with t='US/Eastern'"": ""http://57.128.112.95:5000/compare/benchmarks/066f6450526679df800020c024151d73...066f672231ec7dc38000a7d42004efd7"",
        ""`timeseries.DatetimeAccessor.time_dt_accessor_day_name` (Python) with t='US/Eastern'"": ""http://57.128.112.95:5000/compare/benchmarks/066f6450504e7a3b8000bc019968733e...066f67222fc67757800086752033493c""

<img width=""810"" alt=""Screenshot 2024-09-30 at 14 28 07"" src=""https://github.com/user-attachments/assets/b00a794d-800c-47ef-b052-427509026ac4"">
",[],2024-09-30 12:29:02,2024-10-01 17:01:11,1,closed
59923,Please upload built c extensions to PYPI. Llama-index needs installable pandas.,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Linux-6.8.0-45-generic-x86_64-with-glibc2.39

### Installation Method

Other

### pandas Version

2.2.3

### Python Version

3.12.3

### Installation Logs

https://github.com/run-llama/llama_index/issues/16286

","['Build', 'Needs Triage']",2024-09-30 03:50:15,2024-09-30 05:23:52,1,closed
59915,"BUG: ""Columns must be same length as key"" exception when .apply() per row on an empty DataFrame.","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

MAPPING = {""a"": 1, ""b"": 2}

df = pd.DataFrame(columns=['hour', 'value', 'zone'])

df[""updated_hour""] = pd.to_datetime(df[""hour""])
df[""updated_value""] = df[""value""].apply(lambda x: x - x % 100_000)

# This line will fail since there are no rows to apply the lambda on
df[""updated_value""] = df.apply(lambda row: min(row[""updated_value""], MAPPING[row[""zone""]]), axis=1)
```


### Issue Description

Above code raises exception: ""Columns must be same length as key."".

### Expected Behavior

Return and Assign an empty Series without raising errors (if no rows are present).

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.1.0
Version               : Darwin Kernel Version 23.1.0: Mon Oct  9 21:27:24 PDT 2023; root:xnu-10002.41.9~6/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.1.0
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 72.2.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Apply']",2024-09-28 16:06:49,2024-09-29 13:21:21,1,closed
59913,BUG: SparseFrameAccessor.to_dense ignores _constructor,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import scipy.sparse as sp
import pandas as pd

class MF(pd.DataFrame):
    @property
    def _constructor(self):
        return MF
	
arr = pd.core.arrays.SparseArray.from_spmatrix(sp.csr_matrix((3, 1)))
df = MF(arr)
print(type(df.sparse.to_dense()).__name__)
```


### Issue Description

`to_dense` ignores `_constructor` and always returns `DataFrame`.

```python
    def to_dense(self) -> DataFrame:
        from pandas import DataFrame

        data = {k: v.array.to_dense() for k, v in self._parent.items()}
        return DataFrame(data, index=self._parent.index, columns=self._parent.columns)
```

I think the code should look like this:

```python
    def to_dense(self) -> DataFrame:
        data = {k: v.array.to_dense() for k, v in self._parent.items()}
        constr = self._parent._constructor
        return constr(data, index=self._parent.index, columns=self._parent.columns)
```

We use(d) an ugly workaround, but it will stop working in Pandas 3 because of #58733. See our temporary solution in https://github.com/biolab/orange3/pull/6897/commits/98c48e1b3b7c705f10af195a395213114dc2a917. :(

### Expected Behavior

When subclassing `DataFrame` (which, I know, is discouraged, but sometimes difficult to avoid), `to_dense` should observe the constructor returned by `_constructor`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 23c497bb2f7e05af1fda966e7fb04db942453559
python                : 3.11.10
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 3.0.0.dev0+1524.g23c497bb2f
numpy                 : 1.26.4
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : 3.0.11
sphinx                : 4.5.0
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.13.0b2
blosc                 : None
bottleneck            : 1.4.0
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : 5.3.0
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : 8.3.3
python-calamine       : None
pytz                  : 2024.2
pyxlsb                : None
s3fs                  : None
scipy                 : 1.15.0.dev0+git20240926.4c936c8
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : None
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Sparse', 'Subclassing']",2024-09-28 13:54:05,2024-10-04 22:09:53,1,closed
59905,BUILD: Install regression on armv7,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Linux-6.10.4-linuxkit-armv7l-with

### Installation Method

pip install

### pandas Version

pandas-3.0.0.dev0+1524.g23c497bb2f

### Python Version

3.12.5

### Installation Logs

For Home Assistant we have an action to build custom musllinux wheels. Building `pandas` on `armv7` started to fail recently with the following error:

```
  [89/152] Compiling C object pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o
  FAILED: pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o
  cc -Ipandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p -Ipandas/_libs -I../../pandas/_libs -I../../venv-312d/lib/python3.12/site-packages/numpy/core/include -I../../pandas/_libs/include -I/usr/local/include/python3.12 -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -Wextra -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o -MF pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o.d -o pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o -c ../../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c
  In file included from /usr/include/fortify/string.h:26,
                   from ../../pandas/_libs/include/pandas/portable.h:12,
                   from ../../pandas/_libs/include/pandas/vendored/ujson/lib/ultrajson.h:55,
                   from ../../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c:41:
  /usr/include/fortify/stdlib.h:42:1: error: 'realpath' undeclared here (not in a function)
     42 | _FORTIFY_FN(realpath) char *realpath(const char *__p, char *__r)
        | ^~~~~~~~~~~
  In file included from /usr/local/include/python3.12/Python.h:23,
                   from ../../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c:43:
  /usr/include/fortify/stdlib.h: In function 'realpath':
  /usr/include/fortify/stdlib.h:45:2: error: #error PATH_MAX unset. A fortified realpath will not work.
     45 | #error PATH_MAX unset. A fortified realpath will not work.
        |  ^~~~~
```

I bisected the issue to #55150 (cc @WillAyd), in particular the change in `pandas/_libs/src/vendored/ujson/python/JSONtoObj.c`:
```diff
...
 #define PY_ARRAY_UNIQUE_SYMBOL UJSON_NUMPY
 #define NO_IMPORT_ARRAY
 #define PY_SSIZE_T_CLEAN
+#include ""pandas/vendored/ujson/lib/ultrajson.h""
 #include <Python.h>
 #include <numpy/arrayobject.h>
-#include ""pandas/vendored/ujson/lib/ultrajson.h""
...
```

It seems that, at least on the particular platform, the include order is important.

For the current main branch, it seems to be enough to modify this part:
https://github.com/pandas-dev/pandas/blob/23c497bb2f7e05af1fda966e7fb04db942453559/pandas/_libs/src/vendored/ujson/python/JSONtoObj.c#L41-L43
```diff
-#include ""pandas/vendored/ujson/lib/ultrajson.h""
 #define PY_SSIZE_T_CLEAN
+// clang-format off
 #include <Python.h>
+#include ""pandas/vendored/ujson/lib/ultrajson.h""
+// clang-format on
```

<details>

```
/usr/src# pip install -v .
Using pip 24.2 from /usr/src/venv-312d/lib/python3.12/site-packages/pip (python 3.12)
Processing /usr/src
  Running command Preparing metadata (pyproject.toml)
  + meson setup /usr/src /usr/src/.mesonpy-09kmtf13/build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=/usr/src/.mesonpy-09kmtf13/build/meson-python-native-file.ini
  The Meson build system
  Version: 1.2.1
  Source dir: /usr/src
  Build dir: /usr/src/.mesonpy-09kmtf13/build
  Build type: native build
  Project name: pandas
  Project version: 3.0.0.dev0+1524.g23c497bb2f
  C compiler for the host machine: cc (gcc 13.2.1 ""cc (Alpine 13.2.1_git20240309) 13.2.1 20240309"")
  C linker for the host machine: cc ld.bfd 2.42
  C++ compiler for the host machine: c++ (gcc 13.2.1 ""c++ (Alpine 13.2.1_git20240309) 13.2.1 20240309"")
  C++ linker for the host machine: c++ ld.bfd 2.42
  Cython compiler for the host machine: cython (cython 3.0.11)
  Host machine cpu family: arm
  Host machine cpu: armv7l
  Program python found: YES (/usr/src/venv-312d/bin/python)
  Found pkg-config: /usr/bin/pkg-config (2.2.0)
  Run-time dependency python found: YES 3.12
  Build targets in project: 54

  pandas 3.0.0.dev0+1524.g23c497bb2f

    User defined options
      Native files: /usr/src/.mesonpy-09kmtf13/build/meson-python-native-file.ini
      buildtype   : release
      vsenv       : True
      b_ndebug    : if-release
      b_vscrt     : md

  Found ninja-1.11.1.git.kitware.jobserver-1 at /usr/src/venv-312d/bin/ninja

  Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
  /usr/src/venv-312d/bin/meson compile -C .
  + /usr/src/venv-312d/bin/ninja
  [1/152] Generating pandas/_libs/index_class_helper_pxi with a custom command
  [2/152] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command
  [3/152] Generating pandas/_libs/algos_common_helper_pxi with a custom command
  [4/152] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command
  [5/152] Generating pandas/_libs/intervaltree_helper_pxi with a custom command
  [6/152] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command
  [7/152] Generating pandas/_libs/algos_take_helper_pxi with a custom command
  [8/152] Generating pandas/__init__.py with a custom command
  [9/152] Generating pandas/_libs/sparse_op_helper_pxi with a custom command
  [10/152] Generating write_version_file with a custom command
  [11/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/base.pyx
  [12/152] Compiling C object pandas/_libs/tslibs/parsing.cpython-312-arm-linux-musleabihf.so.p/.._src_parser_tokenizer.c.o
  [13/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/ccalendar.pyx
  [14/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/dtypes.pyx
  [15/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/nattype.pyx
  [16/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/np_datetime.pyx
  [17/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/fields.pyx
  [18/152] Compiling C object pandas/_libs/tslibs/base.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o
  [19/152] Linking target pandas/_libs/tslibs/base.cpython-312-arm-linux-musleabihf.so
  [20/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/parsing.pyx
  [21/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/conversion.pyx
  [22/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/strptime.pyx
  [23/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/timezones.pyx
  [24/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/timedeltas.pyx
  [25/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/offsets.pyx
  [26/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/period.pyx
  [27/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/timestamps.pyx
  [28/152] Compiling C object pandas/_libs/tslibs/ccalendar.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o
  [29/152] Linking target pandas/_libs/tslibs/ccalendar.cpython-312-arm-linux-musleabihf.so
  [30/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/tzconversion.pyx
  [31/152] Compiling C object pandas/_libs/tslibs/np_datetime.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_np_datetime.pyx.c.o
  [32/152] Linking target pandas/_libs/tslibs/np_datetime.cpython-312-arm-linux-musleabihf.so
  [33/152] Compiling Cython source /usr/src/pandas/_libs/tslibs/vectorized.pyx
  [34/152] Compiling C object pandas/_libs/tslibs/dtypes.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_dtypes.pyx.c.o
  [35/152] Linking target pandas/_libs/tslibs/dtypes.cpython-312-arm-linux-musleabihf.so
  [36/152] Compiling C object pandas/_libs/tslibs/nattype.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_nattype.pyx.c.o
  [37/152] Linking target pandas/_libs/tslibs/nattype.cpython-312-arm-linux-musleabihf.so
  [38/152] Compiling Cython source /usr/src/pandas/_libs/arrays.pyx
  [39/152] Compiling C object pandas/_libs/tslibs/conversion.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_conversion.pyx.c.o
  [40/152] Linking target pandas/_libs/tslibs/conversion.cpython-312-arm-linux-musleabihf.so
  [41/152] Compiling C object pandas/_libs/tslibs/timezones.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_timezones.pyx.c.o
  [42/152] Linking target pandas/_libs/tslibs/timezones.cpython-312-arm-linux-musleabihf.so
  [43/152] Compiling C object pandas/_libs/tslibs/fields.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_fields.pyx.c.o
  [44/152] Linking target pandas/_libs/tslibs/fields.cpython-312-arm-linux-musleabihf.so
  [45/152] Compiling C object pandas/_libs/tslibs/vectorized.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_vectorized.pyx.c.o
  [46/152] Linking target pandas/_libs/tslibs/vectorized.cpython-312-arm-linux-musleabihf.so
  [47/152] Compiling Cython source /usr/src/pandas/_libs/algos.pyx
  [48/152] Compiling C object pandas/_libs/tslibs/tzconversion.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_tzconversion.pyx.c.o
  [49/152] Compiling Cython source /usr/src/pandas/_libs/hashing.pyx
  [50/152] Linking target pandas/_libs/tslibs/tzconversion.cpython-312-arm-linux-musleabihf.so
  [51/152] Compiling C object pandas/_libs/arrays.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_arrays.pyx.c.o
  [52/152] Linking target pandas/_libs/arrays.cpython-312-arm-linux-musleabihf.so
  [53/152] Compiling C object pandas/_libs/tslibs/strptime.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_strptime.pyx.c.o
  [54/152] Compiling Cython source /usr/src/pandas/_libs/indexing.pyx
  [55/152] Linking target pandas/_libs/tslibs/strptime.cpython-312-arm-linux-musleabihf.so
  [56/152] Compiling C object pandas/_libs/tslibs/parsing.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_parsing.pyx.c.o
  [57/152] Linking target pandas/_libs/tslibs/parsing.cpython-312-arm-linux-musleabihf.so
  [58/152] Compiling C object pandas/_libs/indexing.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_indexing.pyx.c.o
  [59/152] Linking target pandas/_libs/indexing.cpython-312-arm-linux-musleabihf.so
  [60/152] Compiling Cython source /usr/src/pandas/_libs/groupby.pyx
  [61/152] Compiling Cython source /usr/src/pandas/_libs/internals.pyx
  [62/152] Compiling Cython source /usr/src/pandas/_libs/index.pyx
  [63/152] Compiling C object pandas/_libs/lib.cpython-312-arm-linux-musleabihf.so.p/src_parser_tokenizer.c.o
  [64/152] Compiling C object pandas/_libs/tslibs/period.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_period.pyx.c.o
  [65/152] Linking target pandas/_libs/tslibs/period.cpython-312-arm-linux-musleabihf.so
  [66/152] Compiling Cython source /usr/src/pandas/_libs/hashtable.pyx
  [67/152] Compiling Cython source /usr/src/pandas/_libs/missing.pyx
  [68/152] Compiling C object pandas/_libs/pandas_datetime.cpython-312-arm-linux-musleabihf.so.p/src_vendored_numpy_datetime_np_datetime.c.o
  [69/152] Compiling Cython source /usr/src/pandas/_libs/lib.pyx
  [70/152] Compiling Cython source /usr/src/pandas/_libs/join.pyx
  [71/152] Compiling C object pandas/_libs/hashing.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_hashing.pyx.c.o
  [72/152] Linking target pandas/_libs/hashing.cpython-312-arm-linux-musleabihf.so
  [73/152] Compiling C object pandas/_libs/pandas_datetime.cpython-312-arm-linux-musleabihf.so.p/src_vendored_numpy_datetime_np_datetime_strings.c.o
  [74/152] Compiling C object pandas/_libs/pandas_datetime.cpython-312-arm-linux-musleabihf.so.p/src_datetime_date_conversions.c.o
  [75/152] Compiling Cython source /usr/src/pandas/_libs/interval.pyx
  [76/152] Compiling C object pandas/_libs/pandas_datetime.cpython-312-arm-linux-musleabihf.so.p/src_datetime_pd_datetime.c.o
  [77/152] Linking target pandas/_libs/pandas_datetime.cpython-312-arm-linux-musleabihf.so
  [78/152] Compiling C object pandas/_libs/pandas_parser.cpython-312-arm-linux-musleabihf.so.p/src_parser_io.c.o
  [79/152] Compiling C object pandas/_libs/pandas_parser.cpython-312-arm-linux-musleabihf.so.p/src_parser_pd_parser.c.o
  [80/152] Compiling C object pandas/_libs/pandas_parser.cpython-312-arm-linux-musleabihf.so.p/src_parser_tokenizer.c.o
  [81/152] Linking target pandas/_libs/pandas_parser.cpython-312-arm-linux-musleabihf.so
  [82/152] Compiling C object pandas/_libs/parsers.cpython-312-arm-linux-musleabihf.so.p/src_parser_tokenizer.c.o
  [83/152] Compiling C object pandas/_libs/parsers.cpython-312-arm-linux-musleabihf.so.p/src_parser_io.c.o
  [84/152] Compiling C object pandas/_libs/tslibs/timedeltas.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_timedeltas.pyx.c.o
  [85/152] Linking target pandas/_libs/tslibs/timedeltas.cpython-312-arm-linux-musleabihf.so
  [86/152] Compiling C object pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_ujson.c.o
  [87/152] Compiling C object pandas/_libs/tslibs/timestamps.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_timestamps.pyx.c.o
  [88/152] Linking target pandas/_libs/tslibs/timestamps.cpython-312-arm-linux-musleabihf.so
  [89/152] Compiling C object pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o
  FAILED: pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o
  cc -Ipandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p -Ipandas/_libs -I../../pandas/_libs -I../../venv-312d/lib/python3.12/site-packages/numpy/core/include -I../../pandas/_libs/include -I/usr/local/include/python3.12 -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -Wextra -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o -MF pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o.d -o pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_JSONtoObj.c.o -c ../../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c
  In file included from /usr/include/fortify/string.h:26,
                   from ../../pandas/_libs/include/pandas/portable.h:12,
                   from ../../pandas/_libs/include/pandas/vendored/ujson/lib/ultrajson.h:55,
                   from ../../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c:41:
  /usr/include/fortify/stdlib.h:42:1: error: 'realpath' undeclared here (not in a function)
     42 | _FORTIFY_FN(realpath) char *realpath(const char *__p, char *__r)
        | ^~~~~~~~~~~
  In file included from /usr/local/include/python3.12/Python.h:23,
                   from ../../pandas/_libs/src/vendored/ujson/python/JSONtoObj.c:43:
  /usr/include/fortify/stdlib.h: In function 'realpath':
  /usr/include/fortify/stdlib.h:45:2: error: #error PATH_MAX unset. A fortified realpath will not work.
     45 | #error PATH_MAX unset. A fortified realpath will not work.
        |  ^~~~~
  [90/152] Compiling C object pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_lib_ultrajsonenc.c.o
  [91/152] Compiling C object pandas/_libs/json.cpython-312-arm-linux-musleabihf.so.p/src_vendored_ujson_python_objToJSON.c.o
  [92/152] Compiling Cython source /usr/src/pandas/_libs/parsers.pyx
  [93/152] Compiling C object pandas/_libs/missing.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_missing.pyx.c.o
  [94/152] Compiling C object pandas/_libs/internals.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_internals.pyx.c.o
  [95/152] Compiling C object pandas/_libs/tslibs/offsets.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_tslibs_offsets.pyx.c.o
  [96/152] Compiling C object pandas/_libs/index.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_index.pyx.c.o
  [97/152] Compiling C object pandas/_libs/lib.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_lib.pyx.c.o
  [98/152] Compiling C object pandas/_libs/join.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_join.pyx.c.o
  [99/152] Compiling C object pandas/_libs/interval.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_interval.pyx.c.o
  [100/152] Compiling C object pandas/_libs/algos.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_algos.pyx.c.o
  [101/152] Compiling C object pandas/_libs/groupby.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_groupby.pyx.c.o
  [102/152] Compiling C object pandas/_libs/hashtable.cpython-312-arm-linux-musleabihf.so.p/meson-generated_pandas__libs_hashtable.pyx.c.o
  ninja: build stopped: subcommand failed.
  error: subprocess-exited-with-error
  
  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: /usr/src/venv-312d/bin/python /usr/src/venv-312d/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py prepare_metadata_for_build_wheel /tmp/tmp827zleqr
  cwd: /usr/src
  Preparing metadata (pyproject.toml) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```
</details>
","['Bug', 'Build', 'ARM']",2024-09-27 01:33:27,2024-09-30 20:47:42,0,closed
59904,"BUG: pd.read_csv date parsing not working with dtype_backend=""pyarrow"" and missing values","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import io

data = """"""date,id
20/12/2025,a
,b
31/12/2020,c
""""""
df = pd.read_csv(io.StringIO(data), parse_dates=[""date""], dayfirst=True, dtype_backend=""pyarrow"")
df.dtypes
# date        string[pyarrow_numpy]
# id          large_string[pyarrow]
```


### Issue Description

dtype for the date column is not a date dtype as a result of a date parsing failure because of the missing value on the second row

### Expected Behavior

dtype for the date column should be `timestamp[ns][pyarrow]`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.5.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.3.0
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'IO CSV', 'good first issue', 'Needs Tests', 'Arrow']",2024-09-26 13:22:38,2024-11-15 18:20:12,12,closed
59902,docs: update `freq` description for `pd.Timedelta.ceil` to match `DatetimeIndex.ceil`'s `freq` description,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.


- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.

### Reproducible Example

```python
import pandas as pd
td = pd.Timedelta('1001ms')
td.ceil(""s"")
td.ceil(""min"")
td.ceil(""m"")
```


### Issue Description

Calling td.ceil(""m"") causes the error:

```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""timedeltas.pyx"", line 2011, in pandas._libs.tslibs.timedeltas.Timedelta.ceil
  File ""timedeltas.pyx"", line 1934, in pandas._libs.tslibs.timedeltas.Timedelta._round
  File ""timedeltas.pyx"", line 2309, in pandas._libs.tslibs.timedeltas.get_unit_for_round
  File ""offsets.pyx"", line 756, in pandas._libs.tslibs.offsets.BaseOffset.nanos.__get__
ValueError: <MonthEnd> is a non-fixed frequency
```

### Expected Behavior

td.ceil(""m"") should behave the same as td.ceil(""min"")

The documentation for [pandas.Timedelta.ceil](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.ceil.html#pandas.Timedelta.ceil) states that the freq parameter uses the same units as class constructor Timedelta.

The documentation for [pandas.Timedelta](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html#pandas.Timedelta) lists (‘minutes’, ‘minute’, ‘min’, or ‘m’) as one of the possible value lines. This suggests to me that ""min"" and ""m"" are equivalent.

It seems that either the documentation could be clearer or there is a bug?

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.4
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.3
numpy                 : 2.0.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.2
qtpy                  : None
pyqt5                 : None



</details>
","['Docs', 'Timedelta', 'good first issue']",2024-09-26 12:20:49,2024-10-30 18:33:15,14,closed
59899,API: `astype` method fails to raise errors for `category` data type,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

col = pd.Series([""a"", ""b"", ""c""], dtype=str)
cat = pd.api.types.CategoricalDtype(categories=[""a"", ""b""])

col = col.astype(dtype=cat, errors=""raise"")
print(col)

0      a
1      b
2    NaN
dtype: category
Categories (2, object): ['a', 'b']
```


### Issue Description

No error is raised when recasting as a `category`, despite the presence of an undefined value, `c`. Rather, `c` is coerced to `NaN`.

This behavior appears inconsistent with that of other data types, such as `int`.

### Expected Behavior

I believe an error should be raised.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.9.0
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['API Design', 'Categorical', 'Astype']",2024-09-26 07:08:22,2025-09-08 20:58:23,4,closed
59895,REF: make _cast_pointwise_result an EA method,"In the extension tests.extension.base.ops.BaseOpsUtil we have a _cast_pointwise_result method that is used to patch the result of Series.combine to do appropriate dtype inference so that we have e.g.

```
expected = series + other
middle = series.combine(other, operator.add)
result = self._cast_pointwise_result(middle)
tm.assert_series_equal(result, expected)
```

(There is also a test_combine_add method which does not currently use _cast_pointwise_result but could.  Also test_combine_le looks like we implemented an entirely separate pattern for customizing dtype inference in that test.  I'll look into seeing if these can re-use the _cast_pointwise_result pattern.)

This cast_pointwise_result should just be an EA method.  ATM we override it in the arrow, masked, and string tests (I suspect that we _should_ be overriding it in the sparse tests but it looks like we skip a bunch of them instead).

This will effectively replace _from_scalars, which was a mis-feature.  It will also let us de-kludge places where we use maybe_cast_pointwise_result and generally be more robust.
","['Refactor', 'ExtensionArray']",2024-09-25 22:49:29,2025-08-20 16:00:18,1,closed
59892,BUG: Dataframe init constructor fills with NaN when index = MultiIndex,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

series1 = pd.Series(data=[1,2], index=['A','B'])
series2 = pd.Series(data=[3,4], index=['A','B'])

df = pd.DataFrame({'s1': series1, 's2': series2})
index = pd.MultiIndex.from_product([['C'],['A','B']])
df.index = index

# df is
# | |s1|s2|
#C|A| 1| 3|
# |B| 2| 4|


df2 = pd.DataFrame({'s1': series1, 's2': series2}, index=index)

# df2 is
# | | s1| s2|
#C|A|NaN|NaN|
# |B|NaN|NaN|
```


### Issue Description

Dataframe fills with NaN when the Dataframe __init__ / constructor is passed a MultiIndex for the `index` argument (as in `df2`). Only workaround is to replace the index after the dataframe is initialized (as in `df`).

Example: `df` is fine above, but `df2` is filled with NaN.

### Expected Behavior

`df2` and `df` should be the same above.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.10.11.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.3
numpy                 : 1.23.5
pytz                  : 2023.3
dateutil              : 2.8.2
setuptools            : 65.5.0
pip                   : 24.2
Cython                : 0.29.34
pytest                : 7.4.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.3.0
html5lib              : 1.1
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.2
IPython               : 8.12.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : 2023.2.0
fsspec                : 2023.4.0
gcsfs                 : None
matplotlib            : 3.7.1
numba                 : 0.56.4
numexpr               : 2.8.4
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 11.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.10.1
sqlalchemy            : 2.0.9
tables                : 3.8.0
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Constructors']",2024-09-25 18:15:13,2024-09-28 12:07:29,2,closed
59891,BUG: `isna` doesn't detect `pyarrow.NA` produced by 0/0 ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
#%%
df = pd.DataFrame({""A"": [0, 1, 2, pd.NA, 1, 2], ""B"": [0, pd.NA, 2, pd.NA, 2, 4]}, dtype=""float[pyarrow]"")

df[""rate""] = df[""A""]/df[""B""]

df.info()
df[""rate""].isna()

#%%
df = pd.DataFrame([1, 2, np.nan])
df.info()
df.isna()

#%%
pd.Series([pa.NA], dtype=""float[pyarrow]"").isna()
```


### Issue Description

I deal with tables with a lot of missing data, and using `pyarrow` backend is a must. However, I recently noticed that some intermediate results are not processed as they should be: applying `isna`  to the `rate` column above results in `pyarrow.NaN` being see as a regular number, whereas `numpy.nan` is processed like expected. Creating a dataframe from scratch though behaves like it should. 

### Expected Behavior

The documentation says the following:

https://github.com/pandas-dev/pandas/blob/0691c5cf90477d3503834d983f69350f250a6ff7/pandas/core/dtypes/missing.py#L101-L178

and I'd expect `NaN` to be seen the same regardless of the actual dtype; this is a well-defined `IEEE 754` object. There is a hint to what might be causing it here: https://github.com/apache/arrow/issues/35535#issuecomment-1543482341

> That's because pyarrow does not set that the NaN is the missing value indicator, and thus NaNs in the input are preserved.



### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.6
python-bits           : 64
OS                    : Linux
OS-release            : 6.10.10-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Thu, 12 Sep 2024 17:21:02 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.1.2
Cython                : None
sphinx                : None
IPython               : 8.26.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 17.0.0
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'PDEP missing values']",2024-09-25 14:58:10,2024-09-27 03:38:29,2,closed
59890,"DOC: Should `DataFrame.drop()` accept a `set` for the `columns` , `index` and `labels` argument?","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html

### Documentation problem

The arguments `labels`, `index` and `columns` are documented as ""single label or list-like"".  But a set is accepted:
```python
import pandas as pd

df = pd.DataFrame({1: [2], 3: [4]})   # Fix is here
df = df.drop(columns={1})
```

The pandas source declaration in the typing declarations does not allow a set to be passed.

### Suggested fix for documentation

Unclear.

Either we update the docs to say a set is allowed (and update the internal types), OR we add a check to see if a set is passed and raise an exception.

First raised as a `pandas-stubs` issue in https://github.com/pandas-dev/pandas-stubs/issues/1008","['Docs', 'Needs Discussion']",2024-09-25 13:54:25,2025-02-26 21:09:27,6,closed
59886,BUG: df.to_sql() shows a case sensitive behaviour with an error message 'table ... already exists',"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import sqlite3
from sqlite3 import Error
import pandas as pd

#db_file_path given to local SQLite DB
print(db_file_path)
table_grouped = 'groupedData' #Table is named ""GroupedData"" in the SQLite DB

data = [[123, 10.0, '2020-01-01'], [123, 20.0, '2020-02-01'], [123, 15.0, '2020-03-01']]
df_grouped = pd.DataFrame(data, columns=['CustomerID', 'TotalSalesValue', 'SalesDate'])

try:
    db_conn = sqlite3.connect(db_file_path)
    db_conn.row_factory = sqlite3.Row
except Error as e:
    print(e)

#This try block works to show table columns and data in df:
try:
    db_colums = [x.keys() for x in db_conn.cursor().execute(f'select * from {table_grouped};').fetchall()]
    print(db_colums[0])
    print(df_grouped.head(5))
except Error as e:
    print('ERROR Try 1')
    print(e)

#This try block throws: ""table ""groupedData"" already exists""
try: 
    df_grouped.to_sql(table_grouped, db_conn, if_exists='append', index=False) 
    db_conn.commit()
except Error as e:
    print('ERROR Try 2')
    print(e)

#Close db Connection
try:
    db_conn.close()
except Error as e:
    print(e)
```


### Issue Description

Please also see: https://stackoverflow.com/questions/79019484/

Output of code on local machine:
```

[C:\Users\...SQLite\Data.db)
['CustomerID', 'TotalSalesValue', 'SalesDate']
   CustomerID  TotalSalesValue   SalesDate
0         123             10.0  2020-01-01
1         123             20.0  2020-02-01
2         123             15.0  2020-03-01
ERROR Try 2
table ""groupedData"" already exists
```

Using Panadas 2.2.3, sqlite3.version 2.6.0 and python 3.12.5, I get an error ""table ... already exits"" when using to_sql() with if_exists='append'. I just try to append some data from a Pandas df to a SQLite DB table. Using if_exists='replace' produces the same result. In order to make sure that the db connection is active and the columns match, I used some simple print statements in a previous try block and the failing to.sql in the try block with the to_sql command. Also a ""select statement"" from the same table is used before and works. The first block is executed without an exception and the second block throws the message 'table ""groupedData"" already exists'.

It turned out that the problem was case sensitivity. I named the SQLite db table ""GroupedData"" and used ""groupedData"" in Python with Pandas. While SQL table names are usually not case-sensitive and also sqlite3 is not case-sensitive with table names, to_sql() from Pandas shows a case sensitive behaviour with a error message not indicating the problem properly. Using table_grouped='GroupedData', the try block with to_sql() worked for both ""append"" and ""replace"".

### Expected Behavior

Either giving a more specific error message or changing the behaviour of to_sql() to be also non case-sensitive would be my suggestion.

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.12.5
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 170 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : de_DE.cp1252

pandas                : 2.2.3
numpy                 : 2.1.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None","['Bug', 'Needs Triage']",2024-09-25 09:42:40,2024-09-30 10:12:49,3,closed
59884,BUG: Disabling pandas option display.html.use_mathjax has no effect,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.set_option('display.html.use_mathjax', False)
pd.DataFrame([
    ""$2.13 dont_make_this_math $5.24""
])
```


### Issue Description

My pandas data frames contain text columns which contain dollar signs. I need to be represented as raw strings not as a math-formatted picture.  

Pandas provides the option `display.html.use_mathjax` to this end. However, setting the option does not change pandas behavior. Even with the config set the string ""$2.13 dont_make_this_math $5.24"" has its spaces removed and text after `_` subscripted.

I understand this can be managed by explicitly escaping dollar signs in any text column I come across, but this is a lot of overhead.

### Expected Behavior

![Screenshot 2024-09-25 at 10 52 27](https://github.com/user-attachments/assets/c4ada236-1ded-4c99-8f9c-1675f7a49a71)

not as

![Screenshot 2024-09-25 at 10 52 18](https://github.com/user-attachments/assets/014efb65-f851-4b17-9b36-6b21473153bd)

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.9
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 23.3.1
Cython                : None
sphinx                : None
IPython               : 8.27.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : 15.0.2
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Output-Formatting', 'good first issue']",2024-09-25 08:53:52,2024-11-14 17:16:19,6,closed
59881,BUG: `groupby` results in index with duplicate levels even when `group_keys=False`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.
- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.
- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.

### Reproducible Example

```python
from pandas import DataFrame

df = DataFrame(
   data={
      'A': [1, 2, 1]
   },
   index=['one', 'two', 'one']
)

transformed_df = (
   df
   .groupby(level=0,
            group_keys=False)
   .rolling(window=2,
            min_periods=1)
   .sum()
)
print(transformed_df)
```

### Issue Description

The example creates a `DataFrame` with duplicate index levels even though `group_keys=False`:
```
>>> print(transformed_df)
           A
one one  1.0
    one  2.0
two two  2.0
```

### Expected Behavior

`group_keys=False` should produce a `DataFrame` with the same index as the original:
```
     A
one  1.0
one  2.0
two  2.0
```

### Installed Versions

<details>
```
INSTALLED VERSIONS
------------------
commit                : 0691c5cf90477d3503834d983f69350f250a6ff7
python                : 3.11.10
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.3
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
html5lib              : 1.1
hypothesis            : None
gcsfs                 : None
jinja2                : 3.1.4
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```
</details>","['Bug', 'Groupby']",2024-09-23 22:55:12,2024-09-24 01:05:57,4,closed
59879,BUG (string dtype): looking up missing value in the Index fails,"https://github.com/pandas-dev/pandas/issues/59765 is the general issue about consistency in index lookups for missing values, but shorter term, we at least need to fix being able to lookup missing values for the future string dtype.

With object dtype, this works fine:

```python
>>> idx = pd.Index([""a"", ""b"", None])
>>> idx.get_loc(None)
2
```

But when enabling the string dtype, neither None or np.nan work as key:

```python
>>> pd.options.future.infer_string = True
>>> idx = pd.Index([""a"", ""b"", None])
>>> idx.get_loc(None)
...
KeyError: None
>>> idx.get_loc(np.nan)
...
KeyError: nan
```","['Bug', 'Strings', 'Index']",2024-09-23 15:49:30,2025-01-02 19:49:07,5,closed
59876,BUG: `pd.options.display.float_format` did not follow left side or before decimal places format ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Set the global float format
pd.options.display.float_format = '{:6.3f}'.format

# Example DataFrame
df = pd.DataFrame({
    'A': [123.456, 789.1011],
    'B': [2.71828, 3.14159]
})

df
```


### Issue Description

Pandas `pd.options.display.float_format` did not follow left side or before decimal places format.

### Expected Behavior

If also follows the left side or before decimal places format. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.11.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:21 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.1.0
pip                   : 24.0
Cython                : None
pytest                : 8.1.1
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.23.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Output-Formatting', 'good first issue']",2024-09-23 09:28:23,2024-10-04 19:45:31,21,closed
59874,ENH: Droping consecutive duplicates,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Hi! so the function drop_duplicates allows dropping duplicates but doesn't take into consideration consecutive duplicates.
For example, a panda series -
```python
[1,2,2,3,2,4,5,6]
```
Will result in after the drop_duplicates is applied: 
```python
[1,2,3,4,5,6]
```
But I would want to achieve this: 
```python
[1,2,3,2,4,5,6]
```
(There is a lengthy discussion about it [HERE](https://stackoverflow.com/questions/19463985/pandas-drop-consecutive-duplicates)

Do you think the implementation I wrote below will be the best way to apply this change? (of course with many more adjustments)
If so, I could try to add it via a PR

### Feature Description

```python
def drop_duplicates(
    self,
    *,
    keep: DropKeep= ""first"",
    inplace: bool = False,
    ignore_index: bool = False,
    consecutive: bool = False  # New parameter for consecutive duplicates
) -> ""Series"" | None:
    """"""
    Return Series with duplicate values removed.

    Parameters
    ----------
    keep : {'first', 'last', False}, default 'first'
        Method to handle dropping duplicates:
        - 'first': Keep the first occurrence of each duplicate.
        - 'last': Keep the last occurrence of each duplicate.
        - False: Remove all duplicates.
    inplace : bool, default False
        If True, perform operation inplace and return None.
    ignore_index : bool, default False
        If True, the resulting Series will be re-indexed from 0 to n-1.
    consecutive : bool, default False
        If True, only remove consecutive duplicates.

    Returns
    -------
    Series or None
        Series with duplicates dropped, or None if inplace=True.
    """"""


    if consecutive:
        result = self[self != self.shift(1)]
    else:
        # Handle the default drop_duplicates behavior
        result = super().drop_duplicates(keep=keep)

    if ignore_index:
        result.index = pd.RangeIndex(len(result))

    if inplace:
        self._update_inplace(result)
        return None
    else:
        return result
```

### Alternative Solutions

.

### Additional Context

_No response_","['Enhancement', 'Needs Discussion', 'Closing Candidate']",2024-09-22 23:44:50,2024-09-25 22:10:28,3,closed
59851,BUILD: no linux-aarch64 wheels for v2.2.3?,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Linux-5.15.0-104.119.4.2.el8uek.aarch64-aarch64-with-glibc2.28

### Installation Method

pip install

### pandas Version

2.2.3

### Python Version

3.10.13

### Installation Logs

```sh
pip install pandas==2.2.3 --only-binary=:all:
```

<details>

ERROR: Could not find a version that satisfies the requirement pandas==2.2.3 (from versions: 1.3.3, 1.3.4, 1.3.5, 1.4.0rc0, 1.4.0, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.5.0rc0, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 2.0.0rc0, 2.0.0rc1, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0rc0, 2.2.0, 2.2.1, 2.2.2)
ERROR: No matching distribution found for pandas==2.2.3

</details>
","['Build', 'ARM']",2024-09-20 16:11:58,2024-09-20 19:13:17,8,closed
59845,BUG: pd.options.future.no_silent_downcasting is not backward compatible,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
with pd.option_context('future.no_silent_downcasting', True):
  pd.Series([None]).fillna(False)
```


### Issue Description

I was not able to find a code that doesn't throw warnings for pandas >= 2.0. The example will work in pandas 2.2, but will break for earlier versions. Ahem... This is probably the first time in my Python development that a backwards incompatible change has been issued with a backwards incompatible workaround.

### Expected Behavior

Not throw any warning.

i could do something like this
```python
if pandas.__version__ >= '2.2.0':
  with pd.option_context('future.no_silent_downcasting', True):
    pd.Series([None]).fillna(False)
else:
  pd.Series([None]).fillna(False)
```
I suppose this code would work, but it's a bad code.
  
### Installed Versions

<details>

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2024.1
dateutil              : 2.8.1

</details>
","['Bug', 'Needs Info', 'Closing Candidate', 'PDEP6-related']",2024-09-20 04:07:00,2025-10-14 22:21:13,2,closed
59839,ENH: add a comments variable to pandas.DataFrame.to_csv,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

For many years I have been adding comments to my Pandas generated CSV files by pre--generating the header, writing out the comments, and then appending the pandas to_csv output:

```
df = ....
with open('example.csv', mode='w') as f:
    f.write(','.join(df.keys())+'\n')
    f.write('# First comment\n')
    f.write('# Second comment...\n')

df.to_csv('example.csv', mode='a', index=False)
```
see StackOverflow post for examples: https://stackoverflow.com/questions/29233496/write-comments-in-csv-file-with-pandas

I think it would be easy to add a ""comments"" variable to to_csv to write out a list of comments in the header prepended with whatever the comment character is set:

```
my_comments = [""First comment"",
               ""Second comment...""]
df.to_csv('example.csv', comments=my_comments, index=False)
```

and it would produce a CSV similar to:

> Var1, Var2
> \# First comment
> \# Second comment...
> 123, 456

I think this would be an easy add, and I am willing to code it up and make a pull request, but would like to see if there is any buy-in before spending the time.

### Feature Description

It would be nice if pandas to_csv had a comments variable that worked something like:

```
my_comments = [""First comment"",
               ""Second comment...""]
df.to_csv('example.csv', comments=my_comments, index=False)
```

and it would produce a CSV similar to:

> Var1, Var2
> \# First comment
> \# Second comment...
> 123, 456


### Alternative Solutions

For years I have been adding comments to my Pandas generated CSV files by doing something like:

```
df = ....
with open('example.csv', mode='w') as f:
    f.write(','.join(df.keys())+'\n')
    f.write('# First comment\n')
    f.write('# Second comment...\n')

df.to_csv('example.csv', mode='a', index=False)
```
see StackOverflow post for examples: https://stackoverflow.com/questions/29233496/write-comments-in-csv-file-with-pandas


### Additional Context

People have been using workarounds for a decade or more (see: https://stackoverflow.com/questions/29233496/write-comments-in-csv-file-with-pandas ), and it would be much more elegant if Pandas added the ability to inject comments into a CSV header.","['Enhancement', 'IO CSV', 'Needs Discussion', 'Closing Candidate']",2024-09-19 14:58:01,2024-09-25 22:11:32,12,closed
59838,Inconsistent Return Types Between numpy 1.26.4 and numpy 2.1.0 in pandas 2.2.2,"When using numpy 2.1.0, certain methods in pandas (e.g. first_valid_index() and .at[] access) return `numpy.int64` instead of plain Python integers as seen when using numpy 1.26.4. This creates inconsistencies in behavior.

## To reproduce
1. Environment 1: Python 3.10, pandas 2.2.2, numpy 1.26.4 

```
conda create -n pd_np_1 python=3.10 pandas=2.2.2 numpy=1.26.4
```
2. Environment 2: Python 3.10, pandas 2.2.2, numpy 2.1.0  

```
conda create -n pd_np_2 python=3.10 pandas=2.2.2 -c conda-forge
conda activate pd_np_2
pip install numpy==2.1.0
```
3.
```
import pandas as pd
import numpy as np

pd.Series([None, None, 3], index=[1, 2, 3]).first_valid_index()
pd.DataFrame([[0, 1, 2]], index=['a'], columns=['A', 'B', 'C']).at['a', 'A']
```

## Issue
Environment 1 (numpy 1.26.4)

```py
>>> pd.Series([None, None, 3], index=[1, 2, 3]).first_valid_index()
3
>>> pd.DataFrame([[0, 1, 2]], index=['a'], columns=['A', 'B', 'C']).at['a', 'A']
0
```

Environment 2 (numpy 2.1.0)

```py
>>> pd.Series([None, None, 3], index=[1, 2, 3]).first_valid_index()
np.int64(3)
>>> pd.DataFrame([[0, 1, 2]], index=['a'], columns=['A', 'B', 'C']).at['a', 'A']
np.int64(0)
```

## Discusion
Is this intended behavior, or is it a compatibility issue between pandas 2.2.2 and numpy 2.1.0?",['Closing Candidate'],2024-09-19 02:54:20,2024-09-20 01:56:16,2,closed
59833,BUG: Timestamp.tz and DatetimeIndex.tz are inconsistent when pytz 2024.2 is installed,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
t0 = pd.Timestamp(""01-01-2000"")
print(repr(t0.tz_localize(""CET"").tz))
print(repr(pd.DatetimeIndex([t0]).tz_localize(""CET"").tz))
```


### Issue Description

If `pytz = ""==2024.2""` is installed the example prints

```
<DstTzInfo 'CET' CET+1:00:00 STD>
<DstTzInfo 'CET' LMT+0:18:00 STD>
```

Downgrading `pytz`to `2024.1` resolves this issue.

### Expected Behavior

The `.tz` property should produce the same result regardless if the object is a `Timestamp` or `DatetimeIndex`. Hence, I expect this example to print

```
<DstTzInfo 'CET' CET+1:00:00 STD>
<DstTzInfo 'CET' CET+1:00:00 STD>
```


### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.10.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-40-generic
Version               : #40~22.04.3-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.1.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
setuptools            : 69.2.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Timezones', 'Compat', 'Dependencies', 'Closing Candidate']",2024-09-18 14:06:30,2025-08-05 16:36:18,2,closed
59832,BUG: `None` values are not processed when applying `pd.isnull` to a Series with dtype `category`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Create a Series with dtype category which contains a None value
my_series = pd.Series([1, None], dtype=""category"")

my_series.apply(pd.isnull)
# This will output
# 0    False
# 1      NaN   <------------------ I would expect this to be True
# dtype: category
# Categories (1, bool): [False]
```


### Issue Description

When applying `pd.isnull` to a Series of dtype `category` which contains `None` like values (`None`, `np.nan`, `pd.NA`, etc.), the output doesn't contain a boolean but a float `nan` instead.

### Expected Behavior

The output should contain only boolean values and all `None`, `pd.NA` etc. values should be evaluated as `True`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.3.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:21 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 1.24.3
pytz                  : 2023.3
dateutil              : 2.8.2
setuptools            : 67.6.1
pip                   : 23.0.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.13.2
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 12.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'Categorical', 'Apply']",2024-09-18 12:16:10,2024-10-06 11:40:44,6,closed
59831,ENH: Restore the functionality of `.fillna`,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

The currently very useful behaviour of `.fillna` is being deprecated.

```python
a = pd.Series([True, False, None])
a
# 0     True
# 1    False
# 2     None
# dtype: object
```

Using `a.fillna` raises a warning:
```python
a.fillna(True)
# [/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/3077193745.py:1](https://file+.vscode-resource.vscode-cdn.net/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/3077193745.py:1): FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
#   a.fillna(True)
```

Full message of the warning is:
> FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`

The proposed solutions don't work:
```python
a.fillna(True).infer_objects(copy=False)
# [/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/1224563968.py:1](https://file+.vscode-resource.vscode-cdn.net/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/1224563968.py:1): FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
#   a.fillna(True).infer_objects(copy=False)
```

maybe I misunderstood the Warning message?
```python
a.infer_objects(copy=False).fillna(True)
# [/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/2319989247.py:1](https://file+.vscode-resource.vscode-cdn.net/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/2319989247.py:1): FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
#   a.infer_objects(copy=False).fillna(True)    # maybe I misunderstood the message?
```

Let's try to opt-in...
```python
with pd.option_context(""future.no_silent_downcasting"", True):
  r = a.fillna(True)
r
# 0     True
# 1    False
# 2     True
# dtype: object
```
No, it's no longer a `bool` Series...

Some online resources suggest first casting to `bool`...
```python
a.astype(bool)
# 0     True
# 1    False
# 2    False
# dtype: bool
```
Looks like this is a potential replacement for `.fillna(False)` but not for `.fillna(True)`...

Wait, there's a `downcast` parameter for `.fillna`!
```python
a.fillna(True, downcast=True)
# [/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/4257582953.py:1](https://file+.vscode-resource.vscode-cdn.net/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/4257582953.py:1): FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.
#   a.fillna(True, downcast=True)
# [/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/4257582953.py:1](https://file+.vscode-resource.vscode-cdn.net/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/ipykernel_85835/4257582953.py:1): FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
#   a.fillna(True, downcast=True)
```
Oh no, it's _deprecated_ as well, not I got _TWO_ warnings...

### Feature Description

Restore the functionality of `.fillna` _WITHOUT_ the Warning.

```python
a = pd.Series([True, False, None])
a.fillna(True)
# 0     True
# 1    False
# 2     True
# dtype: bool
```



### Alternative Solutions

Currently, the only ""correct"" option is to use _nullable Boolean type_.

```python
a.astype('boolean').fillna(True).astype(bool)
# or, if we're happy keeping the type `boolean`...
a.astype('boolean').fillna(True)
```

This is overly verbose, but would be acceptable if `boolean` was inferred automatically for `[True, False, None]` (or `[True, False, np.nan]`), but currently it's not...

(The additional confusion is that _integer_ nullable types are distinguished by uppercase (`int64 -> Int64`) but _boolean_ nullable type isn't (`bool -> boolean`)... so it took me a very long time to even find this solution!)

### Additional Context

_No response_","['Enhancement', 'Missing-data', 'Dtype Conversions', 'Needs Discussion', 'Needs Info', 'Closing Candidate']",2024-09-18 10:13:33,2025-08-05 17:00:14,6,closed
59829,BUG: df.MultiIndex.levels continues to return the index of the original df after mutation,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame([[1, 'a', 'foo'], [2, 'b', 'bar']]).set_index([0, 1])
print(df.index.levels[0])  # returns Index([1, 2], dtype='int64', name=0)

d1 = df.head(1)
print(d1.index.levels[0])  # returns Index([1, 2], dtype='int64', name=0)

d2 = df.drop((1, 'a'))
print(d2.index.levels)  # returns FrozenList([[1, 2], ['a', 'b']])
```


### Issue Description

`df.MultiIndex.levels` continues to return the index of the original df, even after mutation. 

In the examples above, rows are removed but `.index.levels` continues to return the index of the original dataframe.

### Expected Behavior
As `d1` and `d2` have a single row, expected output of: 
* d1: `d1.index.levels[0]` should be `Index([1], dtype='int64', name=0)`.
* d2: `d2.index.levels` should be `FrozenList([[2], ['b']])`

I was able to get the expected output by 
```
d3 = df.head(1).reset_index().set_index([0, 1])
print(d3)
print(d3.index.levels[0])  # returns [1] as expected
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Australia.1252

pandas                : 2.2.2
numpy                 : 2.1.1
pytz                  : 2024.2
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 23.2.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'MultiIndex']",2024-09-18 00:21:46,2024-09-18 00:29:22,1,closed
59825,BUG: Index union with datetime64[us] dtype and frequency,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

left = pd.DatetimeIndex(['2021-12-11 01:05:00'], dtype='datetime64[us]', freq='5min')
right = pd.DatetimeIndex(['2021-12-11 01:05:00', '2021-12-11 01:10:00'], dtype='datetime64[us]', freq='5min')

left.union(right)
```


### Issue Description

Ouput of the example is:

`DatetimeIndex(['2021-12-11 01:05:00', '2021-12-14 12:25:00'], dtype='datetime64[us]', freq='5min')`

'2021-12-14 12:25:00' doesn't exists in input indices. 

The bug doesn't appear if nanoseconds are used as dtype or if at least one frequency is not specified.

### Expected Behavior

output should be:

`DatetimeIndex(['2021-12-11 01:05:00', '2021-12-11 01:10:00'], dtype='datetime64[us]', freq='5min')`

or 

`DatetimeIndex(['2021-12-11 01:05:00', '2021-12-11 01:10:00'], dtype='datetime64[us]', freq=None)`


### Installed Versions

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : None
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'Needs Triage']",2024-09-17 08:56:32,2024-09-18 01:24:52,2,closed
59824,"QST: how to pandas fast nested for loop for ""non numeric"" columns?","### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/78992395/how-to-pandas-fast-nested-for-loop-for-non-numeric-columns

### Question about pandas

""""""
input:

df1: 
     name  rpm    power
0   John   1500   high+
1   Mary   1400   high-
2  Sally   300    low-
3    Doe   700    medium-
4   July   1000   medium+


df2: 
     name  age
0  Peter   77
1  Sally   44
2  Micky   22
3  Sally   34
4  July    50
5   Bob    20


required output is:

 but i want it df2:
     name  age  rpm    power
0  Peter   77   0      NA
1  Sally   44   300    low-
2  Micky   22   0      NA
3  Sally   34   300    low-
4   July   50   1000   medium+
5    Bob   20   0      NA
""""""","['Usage Question', 'Needs Triage']",2024-09-17 06:23:23,2024-09-17 10:15:51,2,closed
59817,No Python 3.13 wheels available in scientific-python-nightly-wheels,"Python 3.13 wheels have been failing for ~5 days see [build logs](https://github.com/pandas-dev/pandas/actions/workflows/wheels.yml?query=branch%3Amain+event%3Aschedule) and because of this, there is no Python 3.13 pandas wheel in [scientific-python-nightly-wheels](https://anaconda.org/scientific-python-nightly-wheels/pandas/files). We noticed this in scikit-learn where we are testing against pandas dev for Python 3.13 free-threaded https://github.com/scikit-learn/scikit-learn/issues/29852#issuecomment-2352255945.

The errors does not seem too hard to fix: `np._get_promotion_state` does not exist in numpy-dev and has been removed in https://github.com/numpy/numpy/pull/27156. Note that numpy 2.1.1 has Python 3.13 wheels and still has `np._get_promotion_state` so one option would be to switch to released numpy.

The error from [build log](https://github.com/pandas-dev/pandas/actions/runs/10877117924/job/30178019629#step:7:3864)
```
==================================== ERRORS ====================================
  ____________ ERROR collecting tests/series/indexing/test_setitem.py ____________
  ../venv/lib/python3.13/site-packages/pandas/tests/series/indexing/test_setitem.py:1449: in <module>
      or (np_version_gte1p24 and np._get_promotion_state() != ""weak"")
  ../venv/lib/python3.13/site-packages/numpy/__init__.py:414: in __getattr__
      raise AttributeError(""module {!r} has no attribute ""
  E   AttributeError: module 'numpy' has no attribute '_get_promotion_state'
  ____________ ERROR collecting tests/series/indexing/test_setitem.py ____________
  ../venv/lib/python3.13/site-packages/pandas/tests/series/indexing/test_setitem.py:1449: in <module>
      or (np_version_gte1p24 and np._get_promotion_state() != ""weak"")
  ../venv/lib/python3.13/site-packages/numpy/__init__.py:414: in __getattr__
      raise AttributeError(""module {!r} has no attribute ""
  E   AttributeError: module 'numpy' has no attribute '_get_promotion_state'
```



",[],2024-09-16 09:21:28,2024-09-16 17:40:34,3,closed
59814,ENH: Add kwargs to `Series.map`,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

In version 2.2, `pandas.DataFrame.map` supports passing additional arguments to functions used when mapping values. However, this functionality is not provided by `pandas.Series.map`. It would be more consistent and useful if both `map` functions had the same signature

### Feature Description

The signature of `Series.map` would be 'Series.map(func, na_action=None, **kwargs)' rather than `Series.map(arg, na_action=None)` with the additional `kwargs` provided to the `func` at the point when mapping is performed

### Alternative Solutions

Users can replace their function `func` with ` functools.partial(func, **kwargs)` to achieve the same functionality - this is what `DataFrame.map` seems to use internally. 

### Additional Context

Presumably the difference is because `Series.map()` supports functions, dicts, or Series instances, whereas `DataFrame.map()` only supports functions? Since their use cases are so similar and they even share a name, it would probably be easiest for both users if both `map` functions had the same feature set, one way or another","['Enhancement', 'Apply', 'API - Consistency']",2024-09-16 01:29:06,2024-09-25 19:18:04,3,closed
59806,"DOC: Separate Examples for String Methods (str.isalnum(), str.isalpha(), etc.) in docs","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.Series.str.isalnum.html

### Documentation problem

In the current documentation for pandas string methods `(e.g., str.isalnum(), str.isalpha(), str.isdigit(), etc.)`, the examples for all string methods are grouped together in a way that makes it difficult to interpret which example corresponds to which method. This can be confusing for users who are looking for examples for a specific method, and it reduces the clarity of the documentation.

### Suggested fix for documentation

I suggest updating the documentation to have distinct examples for each string method. Each method should have its own section with a description and example specific to that method, which would improve clarity and make it easier to navigate.

If this is deemed valid, I would be happy to work on resolving it. Could this issue be assigned to me?","['Docs', 'Strings']",2024-09-15 07:21:32,2024-09-29 18:55:19,2,closed
59790,ENH: to_excel: warning/conversion of text values starting with '=' ,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Excel interprets text values starting with '=' as formula.
It can happen that values in a pandas Series (col of data frame) might start with '=' (for example '=== Attention ===').
If this is faced by excel:
- In case of valid formula, it is interpreted
- In case of invalid formula, the file is corrupted and the values are null after attempt to fix
We should handle this better

### Feature Description

If the value starts with '=', issue a warning and add a single quote prefix to the value:
Before: `=== Attention ===`
After: `'=== Attention ==='`

### Alternative Solutions

For columns with object dtypes, or categories with underlying object dtype, for values which are strings, force the excel format as text.

### Additional Context

- The proposed ' prefix is not 100% correct: when we prefix any text with a ' in excel, it is interpreted as text. Unfortunatety, the proposal will show the ' (see example code below)
- It might be problematic if people wants to clearly use formula, but I don't think that's really meaningful. Maybe an option to avoid that conversion (but issue the warning anyway) can be a good option
- Example code:

```
import pandas as pd

df = pd.DataFrame([
    ""foo"",
    ""bar"",
    ""baz"",
    ""=== Attention ==="",  # Problem
    ""'===Attention ===""   # OK but the single quote will be displayed
])

df.to_excel('demo.xlsx')
```
","['Enhancement', 'IO Excel', 'Closing Candidate']",2024-09-13 12:00:10,2024-09-16 17:29:46,6,closed
59789,DOC: Intro to data structures,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#basics-series

### Documentation problem

From scalar value
If data is a scalar value, an index must be provided. 

### Suggested fix for documentation

If data is a scalar value, an index is not mandatory. 

![image](https://github.com/user-attachments/assets/69e44832-4592-4237-a60a-431800d301a2)
","['Docs', 'Series', 'Constructors']",2024-09-13 11:49:46,2024-09-30 21:50:18,3,closed
59783,"DOC: Questions on Usage of *args, **kwargs, and inplace parameters in pandas.Series.cat methods","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.Series.cat.remove_categories.html#pandas.Series.cat.remove_categories

https://pandas.pydata.org/docs/reference/api/pandas.Series.cat.rename_categories.html#pandas.Series.cat.rename_categories


### Documentation problem

I am currently trying to work on the issue #59592, specifically for the remove_categories and rename_categories methods. Before finalizing my changes, I need some clarification on a few points regarding the documentation:

1. `*args` and `**kwargs` Usage: 
- For remove_categories, the method only seems to use the removals parameter. 
- Similarly, for rename_categories, `*args` and `**kwargs` are mentioned but not utilized in the method implementation. Could you clarify whether the inclusion of `*args` and `**kwargs` in the documentation for methods is by design? 
- I have noticed this pattern across many methods and wanted to confirm if this is intentional or if the documentation should be updated to reflect actual usage.

 2. `inplace` Parameter: The `inplace` parameter for methods is included in the error codes of the validation docstring. However, it is not explicitly mentioned in the current documentation, nor is it included as part of the method definition for both methods.



### Suggested fix for documentation

Verify usage of ` **args`, `**kwargs`, and 'inplace' if not make changes to the `pandas.Series.cat` methods in docs.","['Docs', 'Accessors']",2024-09-12 09:46:33,2024-10-08 18:41:40,2,closed
59781,ENH: Allow custom aggregation functions with multiple return values.,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I want to aggregate multiple columns with bootstrapping (scipy.stats.bootstrap). However, this aggregation produces multiple scalar outputs, e.g., the lower and upper bound of the confidence interval, and the mean (= at least 3 columns). I want to apply this aggregation to multiple columns independently (hence 'apply is not really suitable). However, `agg` can only handle aggregation function with one scalar output. Bootstrapping is a 101 task in data analysis and I don't really understand why this is so complicated to implement in pandas. I know that theoretically I could write one aggregate function for the lower, upper bound and the mean separately. but that would 1. take three times longer and 2. would mathematically be questionable as the bound and the mean might come from different random processes. Fixing the randomness would be one way, but still three times longer is not good...

### Feature Description

Here is some demo code

```python
import pandas as pd
import numpy as np

def custom_aggregate(data):
    # return 1  # Works
    return pd.Series({
        'mean_ci_lower': [0.3], # hardcoded for demo
        'mean_ci_upper': [0.5], # hardcoded for demo
        'real_mean': np.mean(data),
    })

def main():
    data = {
    'acctrain': [0.496070, 0.579231, 0.1, 0.3],
    'acctest':  [0.455256, 0.147513, 0.1, 0.5],
    'experimentname': ['experimentA', 'experimentB', 'experimentA', 'experimentB']
    }

    df = pd.DataFrame(data)
    print(df)
    print(""Aggregated: "")
    df2 = df.groupby([""experimentname""]).agg({
        ""acctrain"": custom_aggregate,
        ""acctest"": custom_aggregate,
    }).reset_index()
    print(df2)

if __name__ == '__main__':
    main()
```

I would expect to get multi-indexed data frame like 
```
                acctrain                                               acctest                                                   
                bs_mean_ci_lower    bs_mean_ci_upper    real_mean      bs_mean_ci_lower    bs_mean_ci_upper    real_mean    
experimentname                                                                                                              
experimentA     0.3                 0.4                 0.298035       0.3                 0.4                 0.277628     
experimentB     0.3                 0.4                 0.439616       0.3                 0.4                 0.323757     
```



### Alternative Solutions

The hacky workaround I use right now is something along the lines of 
```python
def custom_aggregate(data):
    return (1,2)
```
and later
```python
df2[['acctrain_ci_lower', 'acctrain_ci_upper']] = pd.DataFrame(df2['acctrain'].tolist(), index=df2.index)
df2[['acctest_ci_lower', 'acctest_ci_upper']] = pd.DataFrame(df2['acctest'].tolist(), index=df2.index)
```

So I return a tuple and then extract the tuple later into multiple columns. This requires a lot hardcoding...


### Additional Context

_No response_","['Enhancement', 'Groupby', 'Apply', 'Closing Candidate']",2024-09-11 15:33:12,2024-09-16 17:30:31,2,closed
59779,BUG: Ambigious behaviour of multiplication assignment operator for series of type integer,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame(data={""spam"": [1, 2, 3, 4], ""eggs"": [1.0, 2.0, 3.0, 4.0]})
cols2scale = [df.spam, df.eggs]
scale = 2.0
for series in cols2scale:
    series *= scale

print(df)
print(df.spam)
print(df.mean())
```


### Issue Description

When attempting to scale multiple Series in a pandas DataFrame using a multiplication assignment operator (in-place modification), the changes are not reflected in the DataFrame for the series of dtype ""int64"". Looking at the print statements (df.spam) shows the correct values.

In my application i have a class holding multiple pandas dataframes as attributes. I thought looping over the attributes in a list is a good idea. However i am a beginner, so please point out to me if i am to blame for this issue. What confuses me, is that it works for float columns but not for integers.





### Expected Behavior

Both the spam and eggs columns in the DataFrame should be scaled by the specified factor (2.0), and the output should reflect the updated values.

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.5.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : de_DE.cp1252

pandas                : 2.2.2
numpy                 : 2.1.1
pytz                  : 2024.2
dateutil              : 2.9.0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.27.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'Copy / view semantics']",2024-09-11 13:45:31,2024-09-14 14:16:54,1,closed
59777,ENH: Add a clear option to interpret strings as Pandas dtypes specifically.,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

When the pd.read_excel function encounters an empty cell with a numeric dtype, it leads to a conversion error. The solution to this, is to use the Pandas specific nullable numeric types, which work extremely well. The problem is that these datatypes are hard to access via strings. pd.read_excel is especially keen on interpreting strings as numpy datatypes, while pd.api.types.pandas_dtype is capable of returning pandas dtypes, but it's unclear what this is based on.

### Feature Description

Since the pandas dtypes often have several advantages over their numpy counterparts, it would make sense that functions which are capable of interpreting strings as datatypes have a flag, which forces them (if possible) to interpret strings as pandas datatypes and not their numpy counterparts.

### Alternative Solutions

A workaround for now, is to run every datatype through the pd.api.types.pandas_dtype function, with very specific capitalization.
Running:
type(pd.api.types.pandas_dtype(""Int64""))
Returns:
<class 'pandas.core.arrays.integer.Int64Dtype'>
Which is good, but running:
type(pd.api.types.pandas_dtype(""int64""))
Returns:
<class 'numpy.dtypes.Int64DType'>
So while this is technically a solution, trying to use the pandas interpreter for input from a user who doesn't have intricate knowledge of the issue is not possible.

### Additional Context

_No response_","['Enhancement', 'IO Excel', 'Strings', 'Needs Info']",2024-09-11 12:27:43,2025-08-05 16:59:03,2,closed
59776,BUG: assert_frame_equal triggers DeprecationWarning with column with empty lists/arrays,"We are running into this in the pyarrow test suite. Reproducer on current main:

```python
empty_list_array1 = np.empty((3,), dtype=object)
empty_list_array1.fill([])
df1 = pd.DataFrame({'a': empty_list_array1})

empty_list_array2 = np.empty((3,), dtype=object)
empty_list_array2.fill(np.array([], dtype=object))
df2 = pd.DataFrame({'a': empty_list_array2})

import warnings
warnings.simplefilter(""always"")
```

gives:

```
>>> pd.testing.assert_frame_equal(df1, df2)
/pandas/core/dtypes/missing.py:503: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  return lib.array_equivalent_object(left, right)
```

The above is with numpy 1.26 and pandas main, but also see the same with latest numpy and pandas (2.1.1 and 2.2.2).
","['Bug', 'Testing']",2024-09-11 09:29:52,2024-09-12 06:39:38,0,closed
59770,BUG: replace with 3 or more `None` values fails when copy_on_write is enabled,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.DataFrame({'mycol' : ['one','two']}).replace({'mycol': {'<NA>': None, 'null': None, '': None}}) #  all good
pd.options.mode.copy_on_write = True
pd.DataFrame({'mycol' : ['one','two']}).replace({'mycol': {'<NA>': None, 'null': None, '': None}}) # error
```


### Issue Description

When enabling copy_on_write, if you replace dictionary has 3 or more `None` values, it will return an error.

Interestingly, if you use two, it works:

```python
import pandas as pd
pd.DataFrame({'mycol' : ['one','two']}).replace({'mycol': {'<NA>': None, 'null': None, '': None}}) #  all good
pd.options.mode.copy_on_write = True
pd.DataFrame({'mycol' : ['one','two']}).replace({'mycol': {'<NA>': None, 'null': None, '': None}}) # error
pd.DataFrame({'mycol' : ['one','two']}).replace({'mycol': {'<NA>': None, 'null': None}}) # works
```

Also, if you use `pd.NA` instead of `None`, it also works

### Expected Behavior

Normal replacement, just like when `copy_on_write` is not enabled

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-1064-azure
Version               : #73~20.04.1-Ubuntu SMP Mon May 6 09:43:44 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.1.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.1.1
pip                   : 24.0
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.10.0
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.1
sqlalchemy            : 2.0.31
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'replace', 'Copy / view semantics']",2024-09-10 11:50:51,2024-09-15 20:00:27,2,closed
59769,"BUG: to_datetime raises ""AttributeError: 'NoneType' object has no attribute 'total_seconds'"" even with errors='coerce'","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
# next line OK
date = pd.to_datetime(""Wed, 1 Dec 2021 08:00:00 -0600 (CST)"", errors='coerce', utc=True, format='mixed')
# next line raises Exception -> AttributeError: 'NoneType' object has no attribute 'total_seconds'
date = pd.to_datetime(""Sun, 14 Apr 2024 20:00:00 +0200 (CET)"", errors='coerce', utc=True, format='mixed')
```


### Issue Description

I have many dates to parse, some have a TimeZone like ""(CET)"", ""(CST)"" and much others, some not. The format is not predictable, so I cannot pass a predefined format string. The shown examples may be similar here, but this is not the case in real life. After some hours of analysis I finally found one specific date, which actually raises an exception.

`Sun, 14 Apr 2024 20:00:00 +0200 (CET)`

### Expected Behavior

First I would expect that with `errors='coerce'` no error will be raised even the format is completely wrong, it should instead return ""NaT"", as the documentation suggests. 

Second to me there is no ""big"" difference between the working date string  `Wed, 1 Dec 2021 08:00:00 -0600 (CST)` and the one that raises an error `Sun, 14 Apr 2024 20:00:00 +0200 (CET)`. I.e. both have the same format, the biggest difference is the TimeZone abbreviation, which is present in both cases, but different. In fact, if I omit `(CET)`, the string can be parsed correctly. 

As a workaround I could manually check whether a TimeZone abbreviation is present and remove it prior to call `to_datetime`. Especially when the time offset is present as well, this information is kind of redundant, i.e. it should not even be of interest for `to_datetime`. But this workaround should not be necessary in my opinion, as I think this is a bug and should be resolved in ""to_datetime"".

_See also a similar issue here: https://github.com/pandas-dev/pandas/issues/54479. Although I cannot reproduce this in my environment._

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.5.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.10.7-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Thu, 29 Aug 2024 16:48:57 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.1.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : None
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime']",2024-09-10 08:11:55,2025-05-06 02:30:26,12,closed
59763,"BUILD: pandas on conda main repo installs `numexpr`, while on `conda-forge` it does not","### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Windows-11-10.0.22631-SP0

### Installation Method

conda install

### pandas Version

2.2.2

### Python Version

3.12 and 3.11

### Installation Logs

When you do `conda install pandas` and it uses the main repo, `numexpr` is automatically installed, even though we document it as being optional:

```text
conda install pandas
Channels:
 - defaults
 - gurobi
Platform: win-64
Collecting package metadata (repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: C:\Condadirs\envs\tnumexpr

  added / updated specs:
    - pandas


The following NEW packages will be INSTALLED:

  blas               pkgs/main/win-64::blas-1.0-mkl
  bottleneck         pkgs/main/win-64::bottleneck-1.3.7-py311hd7041d2_0
  intel-openmp       pkgs/main/win-64::intel-openmp-2023.1.0-h59b6b97_46320
  mkl                pkgs/main/win-64::mkl-2023.1.0-h6b88ed4_46358
  mkl-service        pkgs/main/win-64::mkl-service-2.4.0-py311h2bbff1b_1
  mkl_fft            pkgs/main/win-64::mkl_fft-1.3.10-py311h827c3e9_0
  mkl_random         pkgs/main/win-64::mkl_random-1.2.7-py311hea22821_0
  numexpr            pkgs/main/win-64::numexpr-2.8.7-py311h1fcbade_0
  numpy              pkgs/main/win-64::numpy-1.26.4-py311hdab7c0b_0
  numpy-base         pkgs/main/win-64::numpy-base-1.26.4-py311hd01c5d8_0
  pandas             pkgs/main/win-64::pandas-2.2.2-py311hea22821_0
  python-dateutil    pkgs/main/win-64::python-dateutil-2.9.0post0-py311haa95532
  python-tzdata      pkgs/main/noarch::python-tzdata-2023.3-pyhd3eb1b0_0
  pytz               pkgs/main/win-64::pytz-2024.1-py311haa95532_0
  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_1
  tbb                pkgs/main/win-64::tbb-2021.8.0-h59b6b97_0


Proceed ([y]/n)? n
```
When you install `pandas` from `conda-forge`, `numexpr` is not included:
```text
conda install pandas -c conda-forge
Channels:
 - conda-forge
 - defaults
 - gurobi
Platform: win-64
Collecting package metadata (repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: C:\Condadirs\envs\tnumexpr

  added / updated specs:
    - pandas


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    intel-openmp-2024.2.1      |    h57928b3_1083         1.8 MB  conda-forge
    libblas-3.9.0              |     23_win64_mkl         5.0 MB  conda-forge
    libcblas-3.9.0             |     23_win64_mkl         5.0 MB  conda-forge
    libexpat-2.6.3             |       he0c23c2_0         136 KB  conda-forge
    libhwloc-2.11.1            |default_h8125262_1000         2.3 MB  conda-fore
    liblapack-3.9.0            |     23_win64_mkl         5.0 MB  conda-forge
    libsqlite-3.46.1           |       h2466b09_0         856 KB  conda-forge
    mkl-2024.1.0               |     h66d3029_694       104.3 MB  conda-forge
    numpy-2.1.1                |  py312h49bc9c5_0         6.7 MB  conda-forge
    pandas-2.2.2               |  py312h72972c8_1        13.5 MB  conda-forge
    pthreads-win32-2.9.1       |       hfa6e2cd_3         141 KB  conda-forge
    python-3.12.5              |h889d299_0_cpython        15.2 MB  conda-forge
    tbb-2021.13.0              |       hc790b64_0         148 KB  conda-forge
    ------------------------------------------------------------
                                           Total:       159.8 MB

The following NEW packages will be INSTALLED:

  intel-openmp       conda-forge/win-64::intel-openmp-2024.2.1-h57928b3_1083
  libblas            conda-forge/win-64::libblas-3.9.0-23_win64_mkl
  libcblas           conda-forge/win-64::libcblas-3.9.0-23_win64_mkl
  libexpat           conda-forge/win-64::libexpat-2.6.3-he0c23c2_0
  libhwloc           conda-forge/win-64::libhwloc-2.11.1-default_h8125262_1000
  libiconv           conda-forge/win-64::libiconv-1.17-hcfcfb64_2
  liblapack          conda-forge/win-64::liblapack-3.9.0-23_win64_mkl
  libsqlite          conda-forge/win-64::libsqlite-3.46.1-h2466b09_0
  libxml2            conda-forge/win-64::libxml2-2.12.7-h0f24e4e_4
  libzlib            conda-forge/win-64::libzlib-1.3.1-h2466b09_1
  mkl                conda-forge/win-64::mkl-2024.1.0-h66d3029_694
  numpy              conda-forge/win-64::numpy-2.1.1-py312h49bc9c5_0
  pandas             conda-forge/win-64::pandas-2.2.2-py312h72972c8_1
  pthreads-win32     conda-forge/win-64::pthreads-win32-2.9.1-hfa6e2cd_3
  python-dateutil    conda-forge/noarch::python-dateutil-2.9.0-pyhd8ed1ab_0
  python-tzdata      conda-forge/noarch::python-tzdata-2024.1-pyhd8ed1ab_0
  python_abi         conda-forge/win-64::python_abi-3.12-5_cp312
  pytz               conda-forge/noarch::pytz-2024.1-pyhd8ed1ab_0
  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0
  tbb                conda-forge/win-64::tbb-2021.13.0-hc790b64_0
  ucrt               conda-forge/win-64::ucrt-10.0.22621.0-h57928b3_0
  vc14_runtime       conda-forge/win-64::vc14_runtime-14.40.33810-hcc2c482_20
```

Are we OK with this difference?  @lithomas1 ","['Docs', 'Upstream issue']",2024-09-09 15:57:06,2024-09-25 19:22:50,7,closed
59761,"QST: How to add docstring to min, max, and resolution defined by MinMaxReso()","### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

None

### Question about pandas

I've been trying to resolve the doc issues mentioned in #59698, but I couldn't find a way to add docstring for the following three attributes, present in [timedeltas.pyx](https://github.com/pandas-dev/pandas/blob/main/pandas/_libs/tslibs/timedeltas.pyx):

```
    i ""pandas.Timedelta.resolution PR02"" \
    i ""pandas.Timedelta.min PR02"" \
    i ""pandas.Timedelta.resolution PR02"" \
``` 

Unlike other python functions, these three attributes don't have their own def func area. Instead, they're being defined as:

```
cdef class _Timedelta(timedelta):
    # cdef readonly:
    #    int64_t value      # nanoseconds
    #    bint _is_populated  # are my components populated
    #    int64_t _d, _h, _m, _s, _ms, _us, _ns
    #    NPY_DATETIMEUNIT _reso

    # higher than np.ndarray and np.matrix
    __array_priority__ = 100
    min = MinMaxReso(""min"")
    max = MinMaxReso(""max"")
    resolution = MinMaxReso(""resolution"")
``` 

Here's the MinMaxReso function (placed a few lines above this cdef class):

```
class MinMaxReso:
    """"""
    We need to define min/max/resolution on both the Timedelta _instance_
    and Timedelta class.  On an instance, these depend on the object's _reso.
    On the class, we default to the values we would get with nanosecond _reso.
    """"""
    def __init__(self, name):
        self._name = name

    def __get__(self, obj, type=None):
        if self._name == ""min"":
            val = np.iinfo(np.int64).min + 1
        elif self._name == ""max"":
            val = np.iinfo(np.int64).max
        else:
            assert self._name == ""resolution""
            val = 1

        if obj is None:
            # i.e. this is on the class, default to nanos
            return Timedelta(val)
        else:
            return Timedelta._from_value_and_reso(val, obj._creso)

    def __set__(self, obj, value):
        raise AttributeError(f""{self._name} is not settable."")

```
I found these same attributes in a couple other files as well, and none of those had documentation, either. So, I would to know how to document what they offer to a pandas user. 
","['Docs', 'Timedelta']",2024-09-09 12:57:28,2024-09-15 15:00:37,1,closed
59752,BUG: Large dataset exception on arrow string operations,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.read_parquet(""mypath/myfile.parquet"", engine=""pyarrow"")
df = df.convert_dtypes(dtype_backend=""pyarrow"")
len(df[df.full_path.str.endswith(""90_WW"") ])
```


### Issue Description

On large datasets (24 million rows) getting ""pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays error"" when performing string operations. with Python 3.11 and pandas 2.2.2. Seems to be related to this pervious issue: https://github.com/pandas-dev/pandas/issues/55606

```python
Traceback (most recent call last):
  File ""C:\Users\corey\Analytics_Software\pycharm\PyCharm 2024.1.1\plugins\python\helpers\pydev\pydevconsole.py"", line 364, in runcode
    coro = func()
           ^^^^^^
  File ""<input>"", line 1, in <module>
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\frame.py"", line 4093, in __getitem__
    return self._getitem_bool_array(key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\frame.py"", line 4155, in _getitem_bool_array
    return self._take_with_is_copy(indexer, axis=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\generic.py"", line 4153, in _take_with_is_copy
    result = self.take(indices=indices, axis=axis)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\generic.py"", line 4133, in take
    new_data = self._mgr.take(
               ^^^^^^^^^^^^^^^
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\internals\managers.py"", line 894, in take
    return self.reindex_indexer(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\internals\managers.py"", line 687, in reindex_indexer
    new_blocks = [
                 ^
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\internals\managers.py"", line 688, in <listcomp>
    blk.take_nd(
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\internals\blocks.py"", line 1307, in take_nd
    new_values = algos.take_nd(
                 ^^^^^^^^^^^^^^
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\array_algos\take.py"", line 114, in take_nd
    return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pandas\core\arrays\arrow\array.py"", line 1309, in take
    return type(self)(self._pa_array.take(indices))
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow\\table.pxi"", line 1052, in pyarrow.lib.ChunkedArray.take
  File ""C:\Users\corey\Miniconda3\envs\analytics311\Lib\site-packages\pyarrow\compute.py"", line 487, in take
    return call_function('take', [data, indices], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow\\_compute.pyx"", line 590, in pyarrow._compute.call_function
  File ""pyarrow\\_compute.pyx"", line 385, in pyarrow._compute.Function.call
  File ""pyarrow\\error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow\\error.pxi"", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays
```

### Expected Behavior

string operations work with arrow backend and large datasets

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22621
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252
pandas                : 2.2.2
numpy                 : 2.1.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'Needs Info', 'Arrow']",2024-09-08 18:36:18,2024-09-16 13:02:20,3,closed
59740,BUG: Inconsistent behavior of `min()` when `NaT` is present,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
print(f""{pd. __version__=}"")

L = []

L.append({""date"": ""2024-01-01""})
L.append({""date"": ""2025-01-01""})
L.append({""date"": """"})

dfA = pd.DataFrame(L)
dfA[""date""] = pd.to_datetime(dfA[""date""])

print(""dfA:"")
print(dfA)
print(f""{min(dfA['date'])=}"")
print(f""{dfA['date'].min()=}"")

print()
print(""dfB:"")
dfB = dfA.iloc[::-1]
print(dfB)
print(f""{min(dfB['date'])=}"")
print(f""{dfB['date'].min()=}"")
```


### Issue Description

The result of the Python `min()` function depends on the order of items in the DataFrame.

### Expected Behavior

`min(dfB[""date""])` should behave the same as `dfB[""date""].min()`

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.2.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-101-generic
Version               : #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.1.2
Cython                : None
pytest                : 8.1.1
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.3
IPython               : 8.23.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : 2024.2.0
fsspec                : 2024.2.0
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : 0.59.0
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 15.0.1
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : 2024.2.0
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Closing Candidate']",2024-09-06 21:35:16,2024-09-06 23:31:37,3,closed
59739,BUG: FutureWarning for Boolean sparse dtypes in pd.DataFrame.sparse.from_spmatrix(),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import scipy
import pandas as pd

coo = scipy.sparse.coo_matrix([[False,True],[True,False]])
pd.DataFrame.sparse.from_spmatrix(coo)  # results in FutureWarning


coo = scipy.sparse.coo_matrix([[0,1],[1,0]])
pd.DataFrame.sparse.from_spmatrix(coo) # no warnings
```


### Issue Description

Attempting to use from_spmatrix() with a boolean-type scipy.sparse matrix raises a warning about arbitrary scalar fill_value:

> FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.
  pd.DataFrame.sparse.from_spmatrix(coo)

but using sparse integer dtype for the scipy matrix does not. I don't understand why this occurs, but it seems like from_spmatrix should be able to handle both of these scenarios. Also, there is no argument to from_spmatrix to specify a type, so it is unclear what the user should do about this future warning if anything. 

### Expected Behavior

No warning, uses dtype matching input

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.13.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 21.6.0
Version               : Darwin Kernel Version 21.6.0: Mon Aug 22 20:19:52 PDT 2022; root:xnu-8020.140.49~2/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.1.0
pip                   : 22.1.2
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.3
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Sparse', 'Warnings']",2024-09-06 16:09:21,2025-06-02 17:22:50,3,closed
59726,BUG: idxmax and idxmin with skipna=False raise NotImplementedError,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pytest

with pytest.raises(NotImplementedError):
  pd.DataFrame([pd.Timedelta(1), None]).idxmax(skipna=False, axis=0)

with pytest.raises(NotImplementedError):
  pd.DataFrame([pd.Timedelta(1), None]).idxmax(skipna=False, axis=1)

with pytest.raises(NotImplementedError):
  pd.DataFrame([pd.Timedelta(1), None]).idxmin(skipna=False, axis=0)

with pytest.raises(NotImplementedError):
  pd.DataFrame([pd.Timedelta(1), None]).idxmin(skipna=False, axis=1)
```


### Issue Description

idxmax and idxmin with skipna=False raise NotImplementedError

### Expected Behavior

should compute idxmin and idxmax without raising error

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.18.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 68.2.2
pip                   : 23.3.1
Cython                : None
pytest                : 8.3.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None
```

</details>
",['Docs'],2024-09-05 23:33:56,2024-09-06 18:43:01,2,closed
59721,BUG: `pd.read_html(...)`: Unable to handle malformed `colspan` /  `rowspan`  ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
tables = pd.read_html(""https://en.wikipedia.org/wiki/Kyrgyz_alphabets"")
```


### Issue Description

The problem arises because of a malformed `colspan`  found in a Wikipedia table having a value of `2;` instead of `2`. 
![image](https://github.com/user-attachments/assets/2f271efc-586e-4344-a8e2-a2113258512a)

The issue stems from this line: 
https://github.com/pandas-dev/pandas/blob/85be99eac9b78afcf98955cd85c60d75c5726242/pandas/io/html.py#L514

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[16], line 1
----> 1 tables = pd.read_html(""https://en.wikipedia.org/wiki/Kyrgyz_alphabets"", converters=defaultdict(lambda: str))

File ~/.local/lib/python3.12/site-packages/pandas/io/html.py:1240, in read_html(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)
   1224 if isinstance(io, str) and not any(
   1225     [
   1226         is_file_like(io),
   (...)
   1230     ]
   1231 ):
   1232     warnings.warn(
   1233         ""Passing literal html to 'read_html' is deprecated and ""
   1234         ""will be removed in a future version. To read from a ""
   (...)
   1237         stacklevel=find_stack_level(),
   1238     )
-> 1240 return _parse(
   1241     flavor=flavor,
   1242     io=io,
   1243     match=match,
   1244     header=header,
   1245     index_col=index_col,
   1246     skiprows=skiprows,
   1247     parse_dates=parse_dates,
   1248     thousands=thousands,
   1249     attrs=attrs,
   1250     encoding=encoding,
   1251     decimal=decimal,
   1252     converters=converters,
   1253     na_values=na_values,
   1254     keep_default_na=keep_default_na,
   1255     displayed_only=displayed_only,
   1256     extract_links=extract_links,
   1257     dtype_backend=dtype_backend,
   1258     storage_options=storage_options,
   1259 )

File ~/.local/lib/python3.12/site-packages/pandas/io/html.py:1006, in _parse(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)
   1003     raise retained
   1005 ret = []
-> 1006 for table in tables:
   1007     try:
   1008         df = _data_to_frame(data=table, **kwargs)

File ~/.local/lib/python3.12/site-packages/pandas/io/html.py:250, in <genexpr>(.0)
    242 """"""
    243 Parse and return all tables from the DOM.
    244 
   (...)
    247 list of parsed (header, body, footer) tuples from tables.
    248 """"""
    249 tables = self._parse_tables(self._build_doc(), self.match, self.attrs)
--> 250 return (self._parse_thead_tbody_tfoot(table) for table in tables)

File ~/.local/lib/python3.12/site-packages/pandas/io/html.py:465, in _HtmlFrameParser._parse_thead_tbody_tfoot(self, table_html)
    462         header_rows.append(body_rows.pop(0))
    464 header = self._expand_colspan_rowspan(header_rows, section=""header"")
--> 465 body = self._expand_colspan_rowspan(body_rows, section=""body"")
    466 footer = self._expand_colspan_rowspan(footer_rows, section=""footer"")
    468 return header, body, footer

File ~/.local/lib/python3.12/site-packages/pandas/io/html.py:521, in _HtmlFrameParser._expand_colspan_rowspan(self, rows, section)
    519     text = (text, href)
    520 rowspan = int(self._attr_getter(td, ""rowspan"") or 1)
--> 521 colspan = int(self._attr_getter(td, ""colspan"") or 1)
    523 for _ in range(colspan):
    524     texts.append(text)

ValueError: invalid literal for int() with base 10: '2;'

```

### Expected Behavior

Expected behavior is to sanitize the malformed  colspan value  proceed with `2`.  

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.5.final.0
python-bits           : 64
OS                    : Linux
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 69.5.1
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.1
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.24.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'IO HTML', 'Closing Candidate']",2024-09-05 17:46:34,2024-11-06 17:05:16,7,closed
59717,BUG: DataFrame.from_records()'s columns argument doesn't work on Numpy's structured array,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

data = np.array([
    ('John', 25, 'New York', 50000),
    ('Jane', 30, 'San Francisco', 75000),
    ('Bob', 35, 'Chicago', 65000),
    ('Alice', 28, 'Los Angeles', 60000)
], dtype=[('name', 'U10'), ('age', 'i4'), ('city', 'U15'), ('salary', 'i4')])

df = pd.DataFrame.from_records(data, columns=['salary', 'city', 'age', 'name'])

print(df)
```


### Issue Description

The `columns` parameter doesn't appear to reorder the columns when the input data is a structured array, even though the documentation says it should. (It does do this for other data types.) In addition, the argument doesn't filter the columns like it does for other input types like dict (a behavior which isn't documented yet, but which will be -- see #59670).



### Expected Behavior

According to the discussion in #15319 and #59670, DataFrame.from_records()'s columns argument should allow the users to reorder columns (and include only specific columns) from the Numpy's structured array the way it works for data passed as a dictionary. 

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.10.12.final.0
python-bits         : 64
OS                  : Linux
OS-release          : 6.1.85+
Version             : #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024
machine             : x86_64
processor           : x86_64
byteorder           : little
LC_ALL              : en_US.UTF-8
LANG                : en_US.UTF-8
LOCALE              : en_US.UTF-8

pandas              : 2.1.4
numpy               : 1.26.4
pytz                : 2024.1
dateutil            : 2.8.2
setuptools          : 71.0.4
pip                 : 24.1.2
Cython              : 3.0.11
pytest              : 7.4.4
hypothesis          : None
sphinx              : 5.0.2
blosc               : None
feather             : None
xlsxwriter          : None
lxml.etree          : 4.9.4
html5lib            : 1.1
pymysql             : None
psycopg2            : 2.9.9
jinja2              : 3.1.4
IPython             : 7.34.0
pandas_datareader   : 0.10.0
bs4                 : 4.12.3
bottleneck          : None
dataframe-api-compat: None
fastparquet         : None
fsspec              : 2024.6.1
gcsfs               : 2024.6.1
matplotlib          : 3.7.1
numba               : 0.60.0
numexpr             : 2.10.1
odfpy               : None
openpyxl            : 3.1.5
pandas_gbq          : 0.23.1
pyarrow             : 14.0.2
pyreadstat          : None
pyxlsb              : None
s3fs                : None
scipy               : 1.13.1
sqlalchemy          : 2.0.32
tables              : 3.8.0
tabulate            : 0.9.0
xarray              : 2024.6.0
xlrd                : 2.0.1
zstandard           : None
tzdata              : 2024.1
qtpy                : None
pyqt5               : None



</details>
","['Bug', 'IO Data']",2024-09-05 09:53:46,2024-09-25 18:17:58,5,closed
59712,BUG: groupby().any() returns True instead of False for groups where timedelta column is all null,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.DataFrame([pd.Timedelta(1), pd.NaT]).groupby([0, 1]).any()
```


### Issue Description

For other dtypes, like integers and strings, groupby().any() returns `True` for groups where all the values are null, e.g.

```python
pd.DataFrame([1, None]).groupby([0, 1]).any()

pd.DataFrame([""a"", None]).groupby([0, 1]).any()
```

### Expected Behavior

groupby().any() should return False for groups where all the timedelta values are null. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.18.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 68.2.2
pip                   : 23.3.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'good first issue', 'Reduction Operations']",2024-09-04 23:21:23,2024-10-01 20:33:43,9,closed
59710,API/BUG: obj.str.slice(start:None:-1),"```python
import pandas as pd
import pyarrow as pa

ser = pd.Series([""aafootwo"", ""aabartwo"", pd.NA, ""aabazqux""], dtype=""string[pyarrow]"")
ser2 = ser.astype(pd.ArrowDtype(pa.string()))

>>> ser.str.slice(None, None, -1)
0    owtoofaa
1    owtrabaa
2        <NA>
3    xuqzabaa
dtype: string

>>> ser2.str.slice(None, None, -1)
0       a
1       a
2    <NA>
3       a
dtype: string[pyarrow]
```

This looks to me like the ArrowEA version is wrong, can @mroeschke or @jorisvandenbossche confirm this is not intentional?

The ArrowEA version's only test is in `test_str_slice` in tests/extension/test_arrow.py, has no test cases with either stop=None or step<0.  The StringDtype version has a `test_slice` in tests/strings/test_strings.py that has a bit better coverage.","['Bug', 'Strings', 'Arrow']",2024-09-04 22:49:07,2024-09-10 08:18:30,2,closed
59704,BUG: Excel corruption when reading twice the same file,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

reference_filepath=""test.xlsx""

df = pd.read_excel(reference_filepath)
writer = pd.ExcelWriter(reference_filepath)
```


### Issue Description

Note: I can't check that this bug exists on the latest version of panda since it is not supported by my system. However, I have 2.0.3 and don't see this bug fixed in the ""bug fix"" sections of 2.2 and 2.1.

When opening the same .xlsx file once with read_excel and once with ExcelWriter, the .xlsx file gets corrupted and become impossible to read by any mean.


### Expected Behavior

I would expect a lock when one of the functions is being used that prevents the other function from interacting with the file. The surprising bit is that, in my understanding, read_excel closes the file after reading.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit           : 0f437949513225922d851e9581723d82120684a6
python           : 3.8.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-19041-Microsoft
Version          : #4355-Microsoft Thu Apr 12 17:37:00 PST 2024
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 2.0.3
numpy            : 1.24.4
pytz             : 2024.1
dateutil         : 2.9.0.post0
setuptools       : 45.2.0
pip              : 24.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 3.2.0
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
brotli           : None
fastparquet      : None
fsspec           : None
gcsfs            : None
matplotlib       : None
numba            : None
numexpr          : None
odfpy            : None
openpyxl         : 3.1.5
pandas_gbq       : None
pyarrow          : None
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : None
snappy           : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 2.0.1
zstandard        : None
tzdata           : 2024.1
qtpy             : None
pyqt5            : None

</details>
","['Bug', 'IO Excel']",2024-09-04 13:30:42,2024-09-04 17:33:00,4,closed
59698,DOC: fix docstring validation errors for pandas.Timedelta/pandas.TimedeltaIndex,"follow up on issues https://github.com/pandas-dev/pandas/issues/56804, https://github.com/pandas-dev/pandas/issues/59458 and https://github.com/pandas-dev/pandas/issues/58063
pandas has a script for validating docstrings:

https://github.com/pandas-dev/pandas/blob/2244402942dbd30bdf367ceae49937c179e42bcb/ci/code_checks.sh#L112-L128


Currently, some methods fail docstring validation check.
The task here is:
* take 2-4 methods
* run: `scripts/validate_docstrings.py <method-name>`
* run `pytest pandas/tests/scalar/test_nat.py::test_nat_doc_strings`
*  fix the docstrings according to whatever error is reported
* remove those methods from `code_checks.sh` script
* commit, push, open pull request

Example:
```
 scripts/validate_docstrings.py pandas.Timedelta.ceil
```
pandas.Timedelta.ceil fails with the  SA01 and ES01  errors
```
################################################################################
################################## Validation ##################################
################################################################################

2 Errors found for `pandas.Timedelta.ceil`:
        ES01    No extended summary found
        SA01    See Also section not found
```

Please don't comment `take` as multiple people can work on this issue. You also don't need to ask for permission to work on this, just comment on which methods are you going to work.

If you're new contributor, please check the [contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)
","['Docs', 'Code Style', 'good first issue']",2024-09-03 18:44:37,2025-03-17 16:54:40,31,closed
59695,DOC: methods in see also section in the pandas.core.groupby.DataFrameGroupBy.agg are not hyperlinks,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

[pandas.core.groupby.DataFrameGroupBy.agg](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html#pandas.core.groupby.DataFrameGroupBy.agg)

### Documentation problem

In the see also section the `DataFrame.groupby.apply`, `DataFrame.groupby.transform` and DataFrame..aggregate` methods though listed, they are not hyperlinks and thus the reader cannot navigate with ease but has to look for them instead 

### Suggested fix for documentation

I am not very familiar with the `sphinx` setup in `pandas` and thus cannot suggest an approach for fixing this. Perhaps there is a good reason that these methods are not hyperlinked, note sure","['Docs', 'Groupby', 'good first issue']",2024-09-03 14:22:57,2024-09-04 16:39:44,4,closed
59694,BUG: memory leak when using read_csv with python engine,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.read_csv(filename, sep=""\t\t|\t"", header=5, engine='python') # file is about 250 MB on disk
## memory used: 3.5GB
df.memory_usage() ## 14294152bytes x 24 columns == 327MB
from copy import deepcopy
df2 = deepcopy(df)
## memory 3.9GB
del df
## memory 3.5GB
del df2
## memory 0.1GB
```


### Issue Description

Somehow there is a memory usage of 3.5GB when loading this 250MB file. I understand that pandas keeps metadata but somehow this does not seem reasonable.

When doing pd.read_csv(filename, sep=""\t"", header=5) (only \t and not engine='python') the memory usage is 416.764KB) which I  think is reasonable.

### Expected Behavior

Memory usage is the same when using both backends. Deepcopy of df  and deleting df should remove data from memory that is not relevant to the values in the table.

### Installed Versions

D:\miniforge3\lib\site-packages\_distutils_hack\__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22621
machine               : AMD64
processor             : AMD64 Family 23 Model 96 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_Germany.1252

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : None
pytest                : 8.3.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None","['Bug', 'IO CSV', 'Needs Info']",2024-09-03 14:11:44,2025-08-05 16:57:44,2,closed
59693,DOC: broken link for governance in the team web page,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/about/team.html

### Documentation problem

The link to the governance page seems broken : `[project governance page]({{ base_url }}governance.html)` resulting in `https://pandas.pydata.org/governance.html` yielding a 404.  The proper url seems to be  `https://pandas.pydata.org/about/governance.html`. 

### Suggested fix for documentation

Changing  `[project governance page]({{ base_url }}governance.html)` to  `[project governance page]({{ base_url }}about/governance.html)` in the `https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/team.md`","['Docs', 'Web']",2024-09-03 12:08:24,2024-10-30 18:51:53,7,closed
59691,ENH: ,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I believe adding a new `similarity_merge` method to the Pandas DataFrame class to merge DataFrames based on similar, but not necessarily identical, string values. This feature would be useful for merging data where exact matches are impractical due to typographical errors or variations in string formatting. I encountered this problem several times, i developed a function to solve it and i said to myself maybe it should become a feature in pandas, so here i am :)

### Feature Description

# Similarity Merge Feature for pandas

## Key Components

1. Similarity Metric: The function will use a string similarity metric to compare values. Initially, we'll implement Levenshtein distance, but the function will be designed to allow for other metrics in the future.

2. Threshold: Users can specify a similarity threshold to determine when strings are considered a match.

3. Multiple Matches: The function will handle cases where multiple potential matches exceed the threshold.

4. Performance Optimization: To improve performance on large datasets, we'll implement some optimization strategies.

## Pseudocode

```python
def similarity_merge(left_df, right_df, left_on, right_on, threshold=0.8, method='levenshtein'):
    """"""
    Merge two DataFrames based on string similarity.
    
    Parameters:
    - left_df, right_df: DataFrames to merge
    - left_on, right_on: Columns to join on
    - threshold: Similarity threshold (0 to 1)
    - method: Similarity method ('levenshtein', 'jaccard', etc.)
    
    Returns:
    - Merged DataFrame
    """"""
    
    # Initialize similarity metric
    similarity_func = get_similarity_function(method)
    
    # Create dictionaries for faster lookup
    left_dict = create_lookup_dict(left_df, left_on)
    right_dict = create_lookup_dict(right_df, right_on)
    
    # Initialize result DataFrame
    result = []
    
    # Iterate through left DataFrame
    for left_key, left_rows in left_dict.items():
        best_matches = find_best_matches(left_key, right_dict, similarity_func, threshold)
        
        for right_key, similarity in best_matches:
            for left_row in left_rows:
                for right_row in right_dict[right_key]:
                    merged_row = merge_rows(left_row, right_row)
                    merged_row['similarity'] = similarity
                    result.append(merged_row)
    
    return pd.DataFrame(result)

def get_similarity_function(method):
    if method == 'levenshtein':
        return levenshtein_distance
    elif method == 'jaccard':
        return jaccard_similarity
    # Add more similarity methods as needed

def create_lookup_dict(df, column):
    return df.groupby(column).apply(lambda x: x.to_dict('records')).to_dict()

def find_best_matches(key, lookup_dict, similarity_func, threshold):
    matches = []
    for other_key in lookup_dict:
        similarity = similarity_func(key, other_key)
        if similarity >= threshold:
            matches.append((other_key, similarity))
    return sorted(matches, key=lambda x: x[1], reverse=True)

def merge_rows(left_row, right_row):
    # Implement row merging logic
    pass

def levenshtein_distance(s1, s2):
    # Implement Levenshtein distance calculation
    pass

def jaccard_similarity(s1, s2):
    # Implement Jaccard similarity calculation
    pass
```

## Usage Example

```python
import pandas as pd

# Sample DataFrames
df1 = pd.DataFrame({'name': ['John Doe', 'Jane Smith', 'Bob Johnson'], 'value': [1, 2, 3]})
df2 = pd.DataFrame({'name': ['John Doe', 'Jane Smth', 'Bobby Johnson'], 'score': [A, B, C]})

# Perform similarity merge
result = similarity_merge(df1, df2, left_on='name', right_on='name', threshold=0.8)

print(result)
```

### Alternative Solutions

# Alternative Solutions and Benefits of pandas Implementation

## Alternative Solutions

There are several existing solutions that partially address the need for similarity-based merging:

1. **Third-party packages:**
   - `fuzzymatcher`: Provides fuzzy matching capabilities for pandas DataFrames.
   - `recordlinkage`: Offers various methods for record linkage, including string similarity.
   - `pandas-dedupe`: Uses machine learning for deduplication and entity resolution.

2. **Custom functions:**
   - It's possible to create a custom function using libraries like `Levenshtein` to perform similarity-based merging. Here's an example implementation:

```python
import pandas as pd
from Levenshtein import ratio

def similarity_merge(left_df, right_df, left_on, right_on, threshold=0.8):
    """"""
    Merge two DataFrames based on string similarity.
    
    Parameters:
    - left_df, right_df: DataFrames to merge
    - left_on, right_on: Columns to join on
    - threshold: Similarity threshold (0 to 1)
    
    Returns:
    - Merged DataFrame
    """"""
    
    def find_matches(value, candidates, threshold):
        similarities = [ratio(value, candidate) for candidate in candidates]
        matches = [i for i, sim in enumerate(similarities) if sim >= threshold]
        return matches, [similarities[i] for i in matches]
    
    # Dictionaries for faster lookup
    left_dict = left_df.set_index(left_on).to_dict('index')
    right_dict = right_df.set_index(right_on).to_dict('index')
    
    right_keys = list(right_dict.keys())
    
    results = []
    
    for left_key, left_row in left_dict.items():
        matches, similarities = find_matches(left_key, right_keys, threshold)
        
        for match, similarity in zip(matches, similarities):
            right_key = right_keys[match]
            right_row = right_dict[right_key]
            
            merged_row = {**left_row, **right_row, 'similarity': similarity}
            merged_row[f'{left_on}_left'] = left_key
            merged_row[f'{right_on}_right'] = right_key
            
            results.append(merged_row)
    
    if not results:
        return pd.DataFrame()
    
    return pd.DataFrame(results)
```

This custom function provides a basic implementation of similarity-based merging using the Levenshtein ratio for string comparison.

3. **SQL-based solutions:**
   - Some databases (e.g., PostgreSQL with pg_trgm) offer fuzzy matching capabilities that could be used in conjunction with pandas.

## Benefits of Implementing in pandas

While these alternatives exist, implementing a `similarity_merge` function directly in pandas would offer several advantages:

1. **Native Integration:** As a built-in pandas function, it would seamlessly integrate with existing pandas workflows, maintaining consistency in API and performance optimizations.

2. **Wider Adoption:** Being part of the core pandas library would make it more accessible to users, encouraging broader adoption and community support.

3. **Comprehensive Documentation:** Official pandas documentation would ensure clear, standardized usage guidelines and examples.

4. **Ongoing Maintenance:** The pandas core team would maintain and improve the feature over time, ensuring its reliability and performance.

5. **Enhanced Functionality:** A pandas implementation could handle multiple scenarios not covered by the current custom function:

   a. **Multiple column matching:** Allow similarity comparison across multiple columns simultaneously.
   
   b. **Customizable similarity metrics:** Support various similarity metrics (Levenshtein, Jaccard, cosine similarity, etc.) and allow users to provide custom metrics.
   
   c. **Handling of non-string data:** Extend similarity matching to numerical or categorical data with appropriate metrics.
   
   d. **Asymmetric thresholds:** Allow different thresholds for left and right DataFrames or even row-specific thresholds.
   
   e. **Parallelization:** Implement parallel processing for improved performance on large datasets.
   
   f. **Memory efficiency:** Optimize for memory usage, crucial for very large DataFrames.
   
   g. **Handling of multi-index DataFrames:** Extend functionality to work with multi-index DataFrames.
   
   h. **Incremental merging:** Allow for incremental updates to merged results as new data comes in.

THANK YOU FOR YOUR TIME !!

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-09-03 10:06:38,2024-09-03 16:39:32,1,closed
59689,DOC: Windows build instructions,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/development/contributing_environment.html#step-1-install-a-c-compiler

### Documentation problem

Installing VS Build Tools 2022 with default settings gives me the following error when I build with mason:

```

(pandas-dev) C:\Users\andre\repos\pandas>python -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true
Using pip 24.2 from C:\Users\andre\mambaforge\envs\pandas-dev\lib\site-packages\pip (python 3.10)
Obtaining file:///C:/Users/andre/repos/pandas
  Running command Checking if build backend supports build_editable
  Checking if build backend supports build_editable ... done
  Running command Preparing editable metadata (pyproject.toml)
  + meson setup C:\Users\andre\repos\pandas C:\Users\andre\repos\pandas\build\cp310 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=C:\Users\andre\repos\pandas\build\cp310\meson-python-native-file.ini
  The Meson build system
  Version: 1.2.1
  Source dir: C:\Users\andre\repos\pandas
  Build dir: C:\Users\andre\repos\pandas\build\cp310
  Build type: native build
  Project name: pandas
  Project version: 3.0.0.dev0+1427.ge07453e24d

  ..\..\meson.build:2:0: ERROR: Unknown compiler(s): [['cl.exe']]
  The following exception(s) were encountered:
  Running `cl.exe /?` gave ""[WinError 2] The system cannot find the file specified""

  A full log can be found at C:\Users\andre\repos\pandas\build\cp310\meson-logs\meson-log.txt
  error: subprocess-exited-with-error

  × Preparing editable metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> See above for output.

  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: 'C:\Users\andre\mambaforge\envs\pandas-dev\python.exe' 'C:\Users\andre\mambaforge\envs\pandas-dev\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py' prepare_metadata_for_build_editable 'C:\Users\andre\AppData\Local\Temp\tmp_6e6694l'
  cwd: C:\Users\andre\repos\pandas
  Preparing editable metadata (pyproject.toml) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

```

### Suggested fix for documentation

Include instructions to select the optional MSVC v142 - VS 2019 C++ x64/x86 build tools on the right of the window:

![image](https://github.com/user-attachments/assets/19dbc141-6397-40b2-ac2b-083dacb76996)

after installing this it compiled.
","['Build', 'Docs', 'Windows']",2024-09-02 21:37:57,2024-11-02 17:06:36,9,closed
59670,"DOC: Document that DataFrame.from_records()'s columns argument also acts as ""include""","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_records.html

### Documentation problem

Currently, it's not clear from the `DataFrame.from_records()` docs that the `columns` argument to `from_records()` also has the effect of what an `include` argument (or `usecols`) would do. Indeed, the current wording once led someone to file a feature request asking for an `include` argument to be added: https://github.com/pandas-dev/pandas/issues/15319
However, the request was later closed when the maintainers realized the `columns` argument already does this (but it's not documented, hence this issue).

### Suggested fix for documentation

Add a sentence or phrase to the documentation of the `columns` argument that the  argument also has the effect of limiting the DataFrame to including only the columns specified. This isn't implied by the current wording, or it's at least a little ambiguous.","['Docs', 'IO Data', 'good first issue']",2024-08-30 19:34:56,2024-09-06 18:14:10,5,closed
59668,BUG: Pandas incompatibility with Numpy 2.1.0,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy
import pandas
```


### Issue Description

<img width=""1470"" alt=""Screenshot 2024-08-30 at 12 31 42"" src=""https://github.com/user-attachments/assets/ace331c8-1f6d-434b-a258-508f90a72102"">

Note: Some sensitive information has been censored

### Expected Behavior

>>> import pandas

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-117-generic
Version               : #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : 8.0.2
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.2
numba                 : None
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : 3.10.1
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'Needs Triage']",2024-08-30 16:49:43,2024-08-30 18:04:34,1,closed
59667,BUG: `df.groupby().aggregate()` doesn't preserve subclass type,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
pandas as pd

class MySeries(pd.Series):
    pass

class MyDataFrame(pd.DataFrame):
    @property
    def _constructor(self):
        return MyDataFrame
    _constructor_sliced = MySeries

MySeries._constructor_expanddim = MyDataFrame

df = MyDataFrame({""a"": reversed(range(10)), ""b"": list('aaaabbbccc')})

assert isinstance(df.groupby(""b"").agg({""a"": ""sum""}), MyDataFrame)
```

### Traceback

<details>

```---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-7-e355635bac0a> in <cell line: 14>()
     12 df = MyDataFrame({""a"": reversed(range(10)), ""b"": list('aaaabbbccc')})
     13 
---> 14 assert isinstance(df.groupby(""b"").agg({""a"": ""sum""}), MyDataFrame)

AssertionError: 
```

</details>


### Issue Description

The `df.groupby().agg()` operation does not preserve subclass type. See [this StackOverflow question](https://stackoverflow.com/questions/78889486/preserving-dataframe-subclass-type-during-pandas-groupby-aggregate) for more details. 

Other relevant issues:
- [groupby and resample methods do not preserve subclassed data structures #28330](https://github.com/pandas-dev/pandas/issues/28330)
- [BUG: DataFrameSubClass.groupby() doesn't use methods of subclass #51757](https://github.com/pandas-dev/pandas/issues/51757)

### Expected Behavior

Expected behavior is that `df.groupby().agg()` would be the same class type as `df`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.85+
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.8.2
setuptools            : 71.0.4
pip                   : 24.1.2
Cython                : 3.0.11
pytest                : 7.4.4
hypothesis            : None
sphinx                : 5.0.2
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.4
html5lib              : 1.1
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : 7.34.0
pandas_datareader     : 0.10.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : 2024.6.1
matplotlib            : 3.7.1
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.23.1
pyarrow               : 14.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.32
tables                : 3.8.0
tabulate              : 0.9.0
xarray                : 2024.6.0
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'Apply', 'Closing Candidate', 'Subclassing']",2024-08-30 16:26:33,2024-09-02 18:00:01,3,closed
59666,BUG: rolling with method='table' and apply sorts columns alphabetically,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

datetime_column = ""datetime""

def sum_of_subtraction(values):
    col_1 = values[:, 0]
    col_2 = values[:, 1]
    return np.mean(col_1 - col_2)


cols_order_1 = [""a"", ""b""]
cols_order_2 = [""b"", ""a""]

df = pd.DataFrame(
    {
        datetime_column: pd.date_range(""2020-01-01"", periods=6),
        ""a"": [1, 2, 3, 4, 5, 6],
        ""b"": [6, 7, 8, 5, 6, 7],
    }
)
order_1_df = (
    df
    .rolling('3D', on=datetime_column, method='table')[cols_order_1]
    .apply(sum_of_subtraction, engine=""numba"", raw=True)
)

order_2_df = (
    df
    .rolling('3D', on=datetime_column, method='table')[cols_order_2]
    .apply(sum_of_subtraction, engine=""numba"", raw=True)
)
```


### Issue Description

With the code above, the function `sum_of_subtraction` returns a different results if the columns of the input argument `values` are swapped (cf the example below).

```python
x = np.array([[1, 2],[3, 4]])
y = x[:, [1, 0]]  # columns are swapped
print(sum_of_subtraction(x))  # returns -1.0
print(sum_of_subtraction(y))  # returns 1.0
```

Yet, using `rolling` with `method='table'` followed with `apply` returns same results, no matter or the columns ordering. Even though the columns were given with a specific ordering, when calling `apply` with `engine='numba'`, the columns are sorted alphabetically. With the code above, here are the results:

`order_1_df`: 
|    |        a |        b | datetime            |
|---:|---------:|---------:|:--------------------|
|  0 | -5       | -5       | 2020-01-01 00:00:00 |
|  1 | -5       | -5       | 2020-01-02 00:00:00 |
|  2 | -5       | -5       | 2020-01-03 00:00:00 |
|  3 | -3.66667 | -3.66667 | 2020-01-04 00:00:00 |
|  4 | -2.33333 | -2.33333 | 2020-01-05 00:00:00 |
|  5 | -1       | -1       | 2020-01-06 00:00:00 |

`order_2_df`:
|    |        a |        b | datetime            |
|---:|---------:|---------:|:--------------------|
|  0 | -5       | -5       | 2020-01-01 00:00:00 |
|  1 | -5       | -5       | 2020-01-02 00:00:00 |
|  2 | -5       | -5       | 2020-01-03 00:00:00 |
|  3 | -3.66667 | -3.66667 | 2020-01-04 00:00:00 |
|  4 | -2.33333 | -2.33333 | 2020-01-05 00:00:00 |
|  5 | -1       | -1       | 2020-01-06 00:00:00 |

### Expected Behavior

The results when the columns are ordering as such `[""b"", ""a""]` must be like that (`order_2_df` dataframe):

|    |       b |       a | datetime            |
|---:|--------:|--------:|:--------------------|
|  0 | 5       | 5       | 2020-01-01 00:00:00 |
|  1 | 5       | 5       | 2020-01-02 00:00:00 |
|  2 | 5       | 5       | 2020-01-03 00:00:00 |
|  3 | 3.66667 | 3.66667 | 2020-01-04 00:00:00 |
|  4 | 2.33333 | 2.33333 | 2020-01-05 00:00:00 |
|  5 | 1       | 1       | 2020-01-06 00:00:00 |

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : e4956ab403846387a435cd7b3a8f36828c23c0c7
python                : 3.10.14
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-1066-azure
Version               : # 75-Ubuntu SMP Thu May 30 14:29:45 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1413.ge4956ab403
numpy                 : 2.0.2
dateutil              : 2.9.0
pip                   : 24.2
Cython                : None
sphinx                : None
IPython               : 8.26.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pytz                  : 2024.1
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Apply', 'Window']",2024-08-30 14:01:48,2024-12-03 00:15:59,2,closed
59664,RLS: 2.3,"Release Date: ???

Features:

- [ ] Non-breaking PDEP-14 implementation https://github.com/pandas-dev/pandas/pull/58551:
  - [ ] https://github.com/pandas-dev/pandas/issues/54792
- [ ] groupby.apply DeprecationWarning -> FutureWarning
- [x] ~Publish Python 3.13 wheels~ (done in 2.2.3)
- [x] ~Numpy 2.1 compatibility (may also require dropping Python 3.9)~ (done in 2.2.3)

cc @pandas-dev/pandas-core

",['Release'],2024-08-30 12:00:31,2025-06-08 17:50:26,28,closed
59662,DOC: Remove unnecessary `:func:` & `:meth:` parentheses,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

use regex pattern <code>:(func|meth):\`[\^\`\]+\(\)\`</code> to search for the occurrences

### Documentation problem

The func role's parentheses are unnecessary as Sphinx will add them to the rendered output.

```rst
.. bad
:func:`read_csv()`

.. good
:func:`read_csv`
```

We're introducing a new rule to the sphinx-lint ([issue](https://github.com/sphinx-contrib/sphinx-lint/issues/114) & [PR](https://github.com/sphinx-contrib/sphinx-lint/pull/115)) and, since sphinx-lint utilizes pandas doc in the CI, we'd like to also fix the issue in pandas.


### Suggested fix for documentation

remove unnecessary `:func:` & `:meth:` parentheses in reST content","['Docs', 'Needs Triage']",2024-08-30 09:54:51,2024-08-30 18:09:50,0,closed
59659,ENH: increase verbosity on error,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

On a specific type error the error causing type is not logged in the error

### Feature Description

By replacing:
`raise AttributeError(""Can only use .str accessor with string values!"")`
with
`raise AttributeError(f""Can only use .str accessor with string values! Inferred dType: {inferred_dtype}"")`
the problem would be solved

Details can be found in [here](https://github.com/pandas-dev/pandas/pull/59649/files)

### Alternative Solutions

Use a debugger and set a break point

### Additional Context

_No response_","['Enhancement', 'Error Reporting', 'Strings']",2024-08-30 08:16:31,2024-10-28 21:05:20,2,closed
59657,PERF: Melt 2x slower when future.infer_string option enabled,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

```
import pandas as pd
import numpy as np

# This configuration option makes this code slow
pd.options.future.infer_string = True

# Define dimensions
n_rows = 10000
n_cols = 10000

# Generate random IDs for the rows
ids = [f""string_id_{i}"" for i in range(1, n_rows + 1)]

# Generate a random sparse matrix with 10% non-NaN values
data = np.random.choice([np.nan, 1], size=(n_rows, n_cols), p=[0.9, 0.1])

# Create a DataFrame from the sparse matrix and add the 'Id' column
df = pd.DataFrame(data, columns=[f""column_name_{i}"" for i in range(1, n_cols + 1)])
df.insert(0, 'Id', ids)

# Melt the DataFrame
df_melted = df.melt(id_vars=['Id'], var_name='Column', value_name='Value')

# Display the first few rows of the melted DataFrame
df_melted.head()
```

### Installed Versions

```
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.5.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : pl_PL.UTF-8
LOCALE                : pl_PL.UTF-8

pandas                : 2.2.2
numpy                 : 2.1.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 73.0.1
pip                   : 24.1.2
Cython                : None
pytest                : 8.3.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.6.1
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```


### Prior Performance

This code with `pd.options.future.infer_string = False` runs in:
`5.23 s ± 1.35 s per loop (mean ± std. dev. of 7 runs, 1 loop each)`
Memory consumption is around 14 GB.

Enabling `pd.options.future.infer_string = True` makes it 2 times slower:
`10.6 s ± 40.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)`
Also memory consumption is bigger with peak around 25GB.","['Performance', 'Reshaping', 'Strings']",2024-08-30 07:27:56,2024-12-03 18:34:26,4,closed
59653,BUG: some cases of binary arithmetic between string and timedelta raise TypeError,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.Series(pd.Timedelta(1)) + pd.Series(['1'])
```


### Issue Description

The example above should produce `pd.Series(pd.Timedelta(2))`, but it raises a `TypeError` instead.

There are some other cases that don't work either:
- `pd.Series(['1']) + pd.Series(pd.Timedelta(1))`
- ` pd.Timedelta(1) + pd.Series(['1'])`
- `pd.Series(pd.Timedelta(1)) + '1'`
- `'1' + pd.Series(pd.Timedelta(1))`
- `pd.Timedelta(1) + pd.Series('1')`

But some cases that do:

- `pd.DataFrame(['1']) + pd.DataFrame([pd.Timedelta(1)])`
- `pd.DataFrame([pd.Timedelta(1)]) + pd.DataFrame(['1'])`
- `pd.Series(['1']) + pd.Timedelta(1)`
- `pd.Series('1') + pd.Timedelta(1)`

### Expected Behavior

Given that the `Timedelta` itself seems to support arithmetic with scalar strings,  it seems that dataframes and series of timedelta should support arithmetic with strings or data structures containing strings.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.18.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 68.2.2
pip                   : 23.3.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Numeric Operations', 'Timedelta']",2024-08-29 22:14:39,2025-11-06 17:10:45,8,closed
59650,BUG: diff method incorrect on datetime64[s],"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""date"": ['2022-01-01', '2023-01-01']}).astype({""date"": ""datetime64[ns]""})
print(df.date.diff())


df = pd.DataFrame({""date"": ['2022-01-01', '2023-01-01']}).astype({""date"": ""datetime64[s]""})
print(df.date.diff())
```


### Issue Description

I'm expecting the difference to be 365 days on both, however on the second example the difference ends up coming in nanoseconds as opposed to seconds.

### Expected Behavior

Expecting the diff to be 365 days on both

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Datetime', 'Needs Info', 'Closing Candidate', 'Transformations']",2024-08-29 14:15:49,2025-08-05 16:54:27,2,closed
59645,ENH: read_md() and to_md(),"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could read from markdown (md) files and write to md files. I use Obsidian (a md based note-taking app) to create tables that document a process that I will follow, but I want to be able to automate the rest of the process by reading those tables directly from md into a pandas data frame.

### Feature Description

Add new functions for reading data frames from md and writing data frames to md.

### Alternative Solutions

Since md is a superset of html, I have an add-on in Obsidian that allows me to export a md file to an html file. Every time I edit the md file, I re-export it to html then I use the read_html() function to read the tables in. It's totally a functionality of convenience. It takes out a manual step that I have to do every time. 

I'm sure I could find an automatic way to turn md into html as well, but I it would definitely be more seamless if I could read the tables straight from md. I don't know a ton about how an md file would be read into python, but seeing as it's a superset of html, I'm assuming it shouldn't be that different from the read_html() parser that already exists.

I could make my own function to parse a markdown file and turn it into a data frame, but at this point, it's easier for me to press a button to convert to html than to write that whole python function.

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-08-28 18:37:03,2024-08-29 15:54:12,2,closed
59641,ENH: add from_edges to IntervalIndex,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could use pandas to create an IntervalIndex like [0.0, 0.1), [0.1, 0.2), ..., [0.9, 1.0]. 

Currently, I successfully cut my data frame using the following code:
```py
bin_edges = [i / 10.0 for i in range(0, 11)]  # Create bins from 0.0 to 1.0 with a size of 0.1
df['bin'] = pd.cut(df['ratio'], bins=bin_edges, include_lowest=True)

print(df['bin'].unique())
# Output:
# [(-0.001, 0.1], (0.9, 1.0], (0.4, 0.5], (0.8, 0.9], (0.3, 0.4], ..., (0.5, 0.6], (0.2, 0.3], (0.1, 0.2], (0.6, 0.7], NaN]
# Length: 11
# Categories (10, interval[float64, right]): [(-0.001, 0.1] < (0.1, 0.2] < (0.2, 0.3] < (0.3, 0.4] ... (0.6, 0.7] < (0.7, 0.8] < (0.8, 0.9] < (0.9, 1.0]]
```

Then, I want to reindexing to add back the missing missing bins and fills them with zero counts using `fillna(0)` with the following code
```py
bin_counts = df['bin'].value_counts().reindex(pd.IntervalIndex.from_breaks(bin_edges)).fillna(0)
```
However, I found the `(-0.001, 0.1]` with a non-zero count is replaced by `(0, 0.1]` with a zero count.

I ask ChatGPT, and it recommends to use `bin_counts = df['bin'].value_counts().reindex(pd.IntervalIndex.from_edges(bin_edges)).fillna(0)` which lead to `AttributeError: type object 'IntervalIndex' has no attribute 'from_edges'`

### Feature Description

Add a new function to IntervalIndex, from_edges, to return IntervalIndex based on bin edges.
```
 def from_breaks(cls, bins) -> IntervalIndex
```

### Alternative Solutions

I haven't found one yet

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-08-28 16:02:46,2024-08-28 16:59:02,2,closed
59640,typos,"left for someone else to determine replacement strings

`ethnicsn` and `explicing`

string `ethnicsn` also present in pandas/tests/io/data/stata/stata15.dta

```
$ sed -n '1591,1592p' pandas/pandas/tests/io/test_stata.py
Value labels for column ethnicsn are not unique. These cannot be converted to
pandas categoricals.
$ sed -n '111,113p' pandas/pandas/tests/plotting/test_hist_method.py
        # _check_plot_works adds an `ax` kwarg to the method call
        # so we get a warning about an axis being cleared, even
        # though we don't explicing pass one, see GH #13188
$
```

don't know if there is any purpose in fixing these

`salaraies` and `thumnail` 

```
$ sed -n '39,42p' pandas/pandas/tests/io/parser/test_network.py
    # test reading compressed urls with various engines and
    # extension inference
    if compression_only == ""tar"":
        pytest.skip(""TODO: Add tar salaraies.csv to pandas/io/parsers/data"")
$ sed -n '52p' pandas/web/pandas/index.html
                                            <img class=""img-fluid img-thumnail py-5 mx-auto"" alt=""{{ company.name }}"" src=""{{ base_url }}{{ company.logo }}""/>
$
```

the shell script is authorised for use and/or modification by the repository maintainers

```
$ cat typos.sh
#!/bin/sh

sed -i ""s/Exceptionn/Exception/g"" pandas/pandas/errors/__init__.py
sed -i ""s/PERMISSOIN/PERMISSION/g"" pandas/pandas/tests/io/xml/test_to_xml.py
sed -i ""s/Pre-emptively/Preemptively/g"" pandas/pandas/core/internals/construction.py
sed -i ""s/accientally/accidentally/g"" pandas/pandas/core/dtypes/cast.py
sed -i ""s/anonther/another/g"" pandas/pandas/tests/extension/base/dtype.py
sed -i ""s/behaviorof/behavior of/g"" pandas/pandas/core/arraylike.py
sed -i ""s/behavivor/behavior/g"" pandas/pandas/core/internals/blocks.py
sed -i ""s/belows/below/g"" pandas/asv_bench/benchmarks/indexing_engines.py
sed -i ""s/concatanated/concatenated/g"" pandas/pandas/io/formats/style.py
sed -i ""s/concatentation/concatenation/g"" pandas/pandas/core/reshape/concat.py
sed -i ""s/determinint/determining/g"" pandas/pandas/core/internals/managers.py
sed -i ""s/elswhere/elsewhere/g"" pandas/pandas/_libs/tslibs/np_datetime.pxd
sed -i ""s/enforrced/enforced/g"" pandas/pandas/tests/indexes/datetimes/test_constructors.py
sed -i ""s/explicily/explicitly/g"" pandas/pandas/core/arrays/string_arrow.py
sed -i ""s/githubs/github's/g"" pandas/pandas/_version.py
sed -i ""s/herely/here/g"" pandas/pandas/_libs/tslibs/timestamps.pyx
sed -i ""s/horrendeous/horrendous/g"" pandas/web/pandas/pdeps/0010-required-pyarrow-dependency.md
sed -i ""s/increaes/increases/g"" pandas/pandas/tests/frame/test_api.py
sed -i ""s/indxed/indexed/g"" pandas/pandas/tests/apply/test_numba.py
sed -i ""s/inherrently/inherently/g"" pandas/pandas/io/formats/style_render.py
sed -i ""s/interwined/intertwined/g"" pandas/pandas/tests/frame/methods/test_rank.py
sed -i ""s/lauout/layout/g"" pandas/pandas/tests/plotting/frame/test_frame_subplots.py
sed -i ""s/notibly/notably/g"" pandas/web/pandas/pdeps/0010-required-pyarrow-dependency.md
sed -i ""s/maintaine/maintain/g"" pandas/pandas/_typing.py
sed -i ""s/mangel/mangle/g"" pandas/pandas/tests/test_aggregation.py
sed -i ""s/mediam/median/g"" pandas/pandas/core/frame.py
sed -i ""s/multiplpy/multiply/g"" pandas/pandas/tests/indexing/test_indexing.py
sed -i ""s/nsmalles/nsmallest/g"" pandas/pandas/_typing.py
sed -i ""s/n_largest/nlargest/g"" pandas/pandas/_typing.py
sed -i ""s/permutated/permuted/g"" pandas/pandas/io/pytables.py
sed -i ""s/pickleable/picklable/g"" pandas/pandas/_libs/tslibs/offsets.pyx
sed -i ""s/pre-emptive/preemptive/g"" pandas/pandas/core/frame.py
sed -i ""s/pre-empts/preempts/g"" pandas/pandas/tests/extension/base/io.py
sed -i ""s/prescibes/prescribes/g"" pandas/web/pandas/community/ecosystem.md
sed -i ""s/punctuations/punctuation/g"" pandas/pandas/core/frame.py
sed -i ""s/recognied/recognized/g"" pandas/pandas/tests/dtypes/test_inference.py
sed -i ""s/reflectd/reflected/g"" pandas/pandas/core/arrays/base.py
sed -i ""s/rentention/retention/g"" pandas/pandas/tests/indexes/datetimes/test_arithmetic.py
sed -i ""s/representaton/representation/g"" pandas/pandas/_libs/tslibs/timestamps.pyx
sed -i ""s/representaton/representation/g"" pandas/pandas/_libs/tslibs/nattype.pyx
sed -i ""s/requireds/required/g"" pandas/pandas/tests/io/parser/test_header.py
sed -i ""s/repondents/respondents/g"" pandas/web/pandas/community/blog/2019-user-survey.md
sed -i ""s/responsibelf or/responsible for/g"" pandas/pandas/core/indexes/base.py
sed -i ""s/revrse/reverse/g"" pandas/pandas/tests/io/formats/style/test_matplotlib.py
sed -i ""s/setpember/september/g"" pandas/web/pandas/pdeps/0012-compact-and-reversible-JSON-interface.md
sed -i ""s/signaure/signature/g"" pandas/pandas/core/generic.py
sed -i ""s/simultaneouly/simultaneously/g"" pandas/pandas/io/formats/style_render.py
$ 
```

these can be added according to maintainer discretion

```
sed -i ""s/Jorurnals/Journals/g"" pandas/doc/source/user_guide/io.rst
sed -i ""s/experimential/experimental/g"" pandas/doc/source/user_guide/io.rst
sed -i ""s/extremeties/extremities/g"" pandas/doc/source/user_guide/style.ipynb
sed -i ""s/intrday/intraday/g"" pandas/doc/source/user_guide/cookbook.rst
sed -i ""s/locallly/locally/g"" pandas/doc/source/development/debugging_extensions.rst
sed -i ""s/offsetes/offsets/g"" pandas/doc/source/whatsnew/v2.0.0.rst
sed -i ""s/passinig/passing/g"" pandas/doc/source/whatsnew/v1.0.0.rst
sed -i ""s/pickleable/picklable/g"" pandas/doc/source/whatsnew/v0.21.1.rst
sed -i ""s/retension/retention/g"" pandas/doc/source/development/contributing_codebase.rst
sed -i ""s/transferrable/transferable/g"" pandas/doc/source/getting_started/index.rst
sed -i ""s/work-arounds/workarounds/g"" pandas/doc/source/whatsnew/v0.25.0.rst
```

<hr>

_cf_: `#59651`

```
$ grep ethnicsn pandas/pandas/tests/io/test_stata.py
Value labels for column ethnicsn are not unique. These cannot be converted to
$ 
```

> @<!-- -->asishm
> 
> As the test failures show, you can't really change the error message without changing the columns in the corresponding stata file.

```
$ grep ETHNICSN pandas/pandas/tests/io/data/stata/stata15.dta
grep: pandas/pandas/tests/io/data/stata/stata15.dta: binary file matches
$ grep ethnicsn pandas/pandas/tests/io/data/stata/stata15.dta
grep: pandas/pandas/tests/io/data/stata/stata15.dta: binary file matches

```

**République du Sénégal**

Code ISO 3166-1
SEN, SN

Domaine Internet
.sn",[],2024-08-28 14:37:11,2024-08-29 20:28:23,4,closed
59639,ENH: df.corr().top_correlated_features(N) A function to return from a dataframe the feature pairs ordered by strongest correlation (top N),"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

When I get a df.corr() matrix, I wish there was a function to return only top n pair of features with strongest correlation. This is doable with a few lines of code but having a one-stop shop function might be warranted due to how frequently it will be used. 

### Feature Description

Add a new method to df.corr() that returns top N pairs of features sorted by correlation strength. Something like : 
df.corr().top_correlated_features(top=5) 

Results: 
colA , colC, 0.952
colB , colE, 0.921
.
.


### Alternative Solutions

thanks to [this post on stackoverflow](https://stackoverflow.com/a/43073761), here is an easy solution that can be implemented in a method with a top_N arg: 
corr_matrix = df.corr().abs()
corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(bool)).stack().sort_values(ascending=False).head(N)

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-08-28 12:43:22,2024-08-28 16:54:06,1,closed
59637,DOC: fix docstring validation errors for pandas.PeriodIndex,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/ef3368a8046f3c2e98c773be179f0a49a51d4bdc/ci/code_checks.sh#L85-L104

### Documentation problem

pandas has GL08, PR07, RT03, and SA01 docstring validation issues in the pandas.PeriodIndex class.

### Suggested fix for documentation

I am working on this issue and will have a fix out soon - opened here for clarity.","['Docs', 'Needs Triage']",2024-08-28 03:43:57,2024-08-28 17:06:44,1,closed
59635,ENH: pd.DataFrame.groupby().apply: parallel,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I have computationnaly heavy treatments on `pandas.core.groupby.generic.DataFrameGroupBy` or `pandas.core.groupby.generic.SeriesGroupBy`.

I would like to be able to do that in parallel, using multiple CPU.



### Feature Description

Add new `n_jobs: int=1` parameter to `pandas.core.groupby.generic.SeriesGroupBy.apply` and `pandas.core.groupby.generic.DataFrameGroupBy.apply`

- Value 1: Keep the current behavior
- Value > 1 or value == -1: Apply using `joblib.Parallel`, with the same `n_jobs` parameter
- Else: Raise `ValueError`

Please see the alternative solutions section of this issue for implementation in my current situation, without using `apply`

### Alternative Solutions

The code below is an example of the workaround I'm currently using. It's an adaptation of my real code, and has not been tested...

    import joblib as jl

    def my_function(*args, n_jobs=1):
        def callback(x):
          return ""This is the result""

        if n_jobs == 1:
            return tuple(callback(x) for x in args)
        elif n_jobs > 1 or n_jobs == -1:
            jl.Parallel(n_jobs=n_jobs)(jl.delayed(callback)(x) for x in args)
        else:
            raise ValueError(f""{n_jobs=}"")

    gg_id = []
    gg_data = []
    group_by_cols = ['foo', 'bar', 'baz']
    select_col = ['foo']
    for g_id, g_data in data.groupby(group_by_cols)[select_col]:
        gg_id.append(g_id)
        gg_data.append(g_data.values)
    results = my_function(*gg_data, parallel=True)
    results = pd.DataFrame(dict(zip(gg_id, results))).T
    results.index.names = group_by_cols


### Additional Context

- Please also see the documentation of `joblib`
- Maybe solutions independent of joblib are possible, but I think that lots of people are already using joblib without knowing it, so I don't really think that it would be a big deal to add it to pandas dependencies","['Enhancement', 'Needs Triage']",2024-08-27 23:37:18,2024-08-27 23:42:49,2,closed
59614,BUG: `numpy.ma.fix_invalid` makes changes in-place in numpy 2.1.0 even with `copy=True`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> import pandas as pd, numpy as np
>>> pd.__version__; np.__version__
'2.2.2'
'2.1.0'
>>> my_series = pd.Series([1.0, 2.0, np.nan, 0.0, 1.0])
>>> my_series
0    1.0
1    2.0
2    NaN
3    0.0
4    1.0
dtype: float64
>>> np.ma.fix_invalid(my_series)
masked_array(data=[1.0, 2.0, --, 0.0, 1.0],
             mask=[False, False,  True, False, False],
       fill_value=1e+20)
>>> my_series
0    1.000000e+00
1    2.000000e+00
2    1.000000e+20
3    0.000000e+00
4    1.000000e+00
dtype: float64
```


### Issue Description

Copying the description of: https://github.com/numpy/numpy/issues/27253

`numpy.ma.fix_invalid` behaves differently between NumPy 2.1.0 and NumPy 2.0.0. Specifically, when passing a pandas Series containing a `numpy.nan` value, `numpy.ma.fix_invalid` now makes changes in-place, even if the `copy` argument is set to its default value of `True`. This issue occurs only with pandas Series, not with NumPy arrays, for example.


### Expected Behavior

```
>>> pd.__version__; np.__version__
'2.2.2'
'2.0.0'
>>> 
>>> my_series = pd.Series([1.0, 2.0, np.nan, 0.0, 1.0])
>>> my_series
0    1.0
1    2.0
2    NaN
3    0.0
4    1.0
dtype: float64
>>> np.ma.fix_invalid(my_series)
masked_array(data=[1.0, 2.0, --, 0.0, 1.0],
             mask=[False, False,  True, False, False],
       fill_value=1e+20)
>>> my_series
0    1.0
1    2.0
2    NaN
3    0.0
4    1.0
dtype: float64
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:13:18 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6030
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 2.1.0
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 8.3.1
hypothesis            : 6.108.4
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
",['Bug'],2024-08-26 15:33:50,2024-11-16 19:34:32,6,closed
59613,QST: CVE-2024-42992 response,"The Mend security scanner started [flagging Pandas](https://www.mend.io/vulnerability-database/CVE-2024-42992) for [CVE-2024-42992](https://nvd.nist.gov/vuln/detail/CVE-2024-42992). On the NVD page, I don't see a score, just that it is ""awaiting analysis"". Looking at what I think is [the original report](https://github.com/juwenyi/CVE-2024-42992), it looks like a non-issue -- `read_csv()` can read arbitrary files, just like `pathlib.Path` or the builtin `open()` function. I don't see why that should be given a CVE, though one should not pass unvalidated user input to the file path argument (maybe there is more to the issue that I am missing). Can the Pandas developers comment on the CVE at this time? Currently, searching for ""CVE-2024-42992"" turns up a lot of reposting of the CVE report but no response to it that I saw.",[],2024-08-26 15:32:19,2024-08-26 15:49:40,2,closed
59612,2.3 release dates,"I see that there is a 2.3 release that will happen. From the items currently milestoned with 2.3, it appears they are exclusively for PDEP-14 string dtype implementation. Is that the scope for the 2.3 release?

There are many bug-fixes that are currently milestoned 3.0. I'm guessing backporting them to the 2.3 branch will be a big task. It might be helpful to get some dates (just estimates are probably ok) for the 2.3 (and any plans for patch versions) and 3.0 release dates",[],2024-08-26 15:09:33,2024-08-30 20:47:40,1,closed
59606,DOC: fix docstring validation errors for pandas.io.formats.style.Styler,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/360597c349f4309364af0d5ac3bab158fd83d9fa/ci/code_checks.sh#L308-L335

### Documentation problem

Following up on issues https://github.com/pandas-dev/pandas/issues/56804, https://github.com/pandas-dev/pandas/issues/59458, https://github.com/pandas-dev/pandas/issues/58063, and https://github.com/pandas-dev/pandas/issues/59592: pandas has docstring validation issues in the `pandas.io.formats.style.Styler` class.

The numpydoc issues are of type PR01, PR07, RT03, and SA01. These issues can be resolved via detailed parameter definitions/return type specification/additions to the ""See Also"" section for the respective docstring.

### Suggested fix for documentation

I am working on this issue and will have a fix out asap - opened here for clarity.","['Docs', 'Needs Triage']",2024-08-26 04:22:50,2024-08-27 00:35:10,2,closed
59603,BUG: Importing pandas interferes with mutexes in unrelated modules.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
#include <iostream>
#include <pybind11/pybind11.h>
#include <mutex>

namespace py = pybind11;

void create_mutex() {
    py::gil_scoped_release release;
    std::cout << ""Creating mutex\n"";
    std::mutex local_mutex;
    std::cout << ""Mutex created\n"";
    {
        std::cout << ""Attempting to lock mutex\n"";
        std::lock_guard<std::mutex> lock(local_mutex);
        std::cout << ""Mutex locked and unlocked\n"";
    }
    std::cout << ""Exiting create_mutex\n"";
}

PYBIND11_MODULE(TestModule, m) {
    m.doc() = ""Test Module"";
    m.def(""create_mutex"", &create_mutex, ""Create mutex"");
}
```


### Issue Description

Creating a python module with the above code, then running it in python:
```
import pandas as pd
import TestModule as tm
tm.create_mutex()
```
crashes without error message after outputting:
```
Creating mutex
Mutex created
Attempting to lock mutex
```

The issue occurs whether the gil is released or not. (using `pybind11::gil_scoped_acquire acquire;`)

### Expected Behavior

Should output this without crashing.
```
Creating mutex
Mutex created
Attempting to lock mutex
Mutex locked and unlocked
Exiting create_mutex
```

Removing the pandas import from the python allows it to do this.


### Installed Versions


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.5.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 23 Model 49 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 2.1.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.1.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
pybind11:             : 2.13.5","['Bug', 'Build']",2024-08-25 15:33:38,2024-08-29 18:54:31,2,closed
59593,ENH: Drop list menu lost when reading xlsx files from pandas,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

> import pandas as pd
> import numpy as np
> from openpyxl.worksheet.datavalidation import DataValidation
> from openpyxl import load_workbook
> 
> ### Create a table by pandas
> month = [7, 7, 7]
> date = [27, 27, 27]
> amount = [1.2, 1.99, 60.8]
> kind = ['Meat', 'Meat', 'Meat']
> quarter = ['3', '3', '3']
> df = pd.DataFrame({
>     'Month': month,
>     'Day': date,
>     'Cost': amount,
>     'Category': kind,
>     'Quarter': quarter
>     })
> df.info()
> print(df.head(10))
> print(len(df))
> df.to_excel('~/Downloads/food_1.xlsx', index = False) #save manipulated table to a Excel file
> 
> ### Open the table by openpyxl and add modifications (drop-down list menu)
> wb = load_workbook('/Users/username/Downloads/food_1.xlsx')
> ws = wb.active
> dv1 = DataValidation(type = ""list"", formula1 = '""Care, Computer, Convenience, Fruit, Housework, Meat, Others, Restaurant, Veg""', allow_blank = True)
> dv2 = DataValidation(type = ""list"", formula1 = '""1, 2, 3, 4""', allow_blank = True)
> ws.add_data_validation(dv1)
> ws.add_data_validation(dv2)
> dv1.add('d2:d51')
> dv2.add('e2:e51')
> wb.save('/Users/username/Downloads/food_2.xlsx')

I write data into a new xlsx table via pandas. Later, I add drop list menu function by openpyxl. But, if I want to add more rows later by pandas again, like this,

> import pandas as pd
> df = pd.read_excel('~/Downloads/Predict.xlsx')

The pre-defined drop list menu function is lost, after the edited xlsx file is saved by pandas.

### Feature Description

Maybe it is a reasonable function to add pandas's self features to support drop-down list menu.

### Alternative Solutions

Or, maybe it is reasonable to enable pandas's support for the drop-down list menu function of openpyxl

### Additional Context

Thanks!","['Enhancement', 'IO Excel', 'Closing Candidate']",2024-08-24 10:38:19,2024-08-25 18:12:01,3,closed
59592,DOC: fix docstring validation errors for pandas.Series,"follow up on issues #56804, #59458 and #58063
pandas has a script for validating docstrings: 

https://github.com/pandas-dev/pandas/blob/0cdc6a48302ba1592b8825868de403ff9b0ea2a5/ci/code_checks.sh#L155-L187

Currently, some methods fail docstring validation check.
The task here is:
* take 2-4 methods
* run: `scripts/validate_docstrings.py <method-name>`
*  fix the docstrings according to whatever error is reported
* remove those methods from `code_checks.sh` script
* commit, push, open pull request

Example:
```
scripts/validate_docstrings.py pandas.Series.prod
```
pandas.Series.prod fails with the ES01 and RT03 errors
```
################################################################################
################################## Validation ##################################
################################################################################

2 Errors found for `pandas.Series.prod`:
        ES01    No extended summary found
        RT03    Return value has no description
```

Please don't comment `take` as multiple people can work on this issue. You also don't need to ask for permission to work on this, just comment on which methods are you going to work.

If you're new contributor, please check the [contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)
","['Docs', 'Code Style', 'good first issue']",2024-08-24 09:41:47,2025-02-25 17:47:59,37,closed
59588,BUG: `to_markdown` raises `ValueError` when values are `np.array`s with more than one element,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> s = pd.Series([np.array([1, 2])])
>>> s
0    [1, 2]
dtype: object 
>>> s.to_markdown()

Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""/home/amu4ca/repo/venvs/sentinela-studies-main/lib/python3.10/site-packages/pandas/core/series.py"", line 1970, in to_markdown
  return self.to_frame().to_markdown(
File ""/home/amu4ca/repo/venvs/sentinela-studies-main/lib/python3.10/site-packages/pandas/util/_decorators.py"", line 333, in wrapper
  return func(*args, **kwargs)
File ""/home/amu4ca/repo/venvs/sentinela-studies-main/lib/python3.10/site-packages/pandas/core/frame.py"", line 2984, in to_markdown
  result = tabulate.tabulate(self, **kwargs)
File ""/home/amu4ca/repo/venvs/sentinela-studies-main/lib/python3.10/site-packages/tabulate/__init__.py"", line 2048, in tabulate
  list_of_lists, headers = _normalize_tabular_data(
File ""/home/amu4ca/repo/venvs/sentinela-studies-main/lib/python3.10/site-packages/tabulate/__init__.py"", line 1471, in _normalize_tabular_data
  rows = list(map(lambda r: r if _is_separating_line(r) else list(r), rows))
File ""/home/amu4ca/repo/venvs/sentinela-studies-main/lib/python3.10/site-packages/tabulate/__init__.py"", line 1471, in <lambda>
  rows = list(map(lambda r: r if _is_separating_line(r) else list(r), rows))
File ""/home/amu4ca/repo/venvs/sentinela-studies-main/lib/python3.10/site-packages/tabulate/__init__.py"", line 107, in _is_separating_line
  (len(row) >= 1 and row[0] == SEPARATING_LINE)
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() 
```


### Issue Description

When trying to convert a `Series` or a `DataFrame` to markdown with `np.ndarray`s as values an error is raised.


### Expected Behavior

The method should print something similar to `list`s behavior:
```python
>>> pd.Series([[1, 2]]).to_markdown()  # All Good!
'|    | 0      |\n|---:|:-------|\n|  0 | [1, 2] |' 
```

Perhaps any iterable should be converted to a list before sending to `tabulate`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.146.1-microsoft-standard-WSL2
Version               : #1 SMP Thu Jan 11 04:09:03 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.2
numpy                 : 2.0.1
tabulate              : 0.9.0 

</details>
","['Bug', 'IO Data', 'Closing Candidate']",2024-08-23 17:52:29,2024-08-25 18:13:00,4,closed
59575,BUG: StringDtype conversion to bool changes False to True,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas
from pandas import StringDtype

df1 = pandas.DataFrame({
    ""field"": [True, False]
}, dtype=StringDtype())
df1
df1.dtypes
df1[""field""] = df1[""field""].astype(""bool"")
df1
df1.dtypes

df2 = pandas.DataFrame({
    ""field"": [True, False]
})
df2
df2.dtypes
df2[""field""] = df2[""field""].astype(""bool"")
df2
df2.dtypes
```


### Issue Description

StringDtype with ""False"" values convert to True values when casted to bool, object converts ""False"" to False.

It's possible we want empty string = False and populated/non-empty string = True, while object operates under different logic. My current understanding is the pandas (numpy) uses objects for strings which would mean the logic for StringDtype -> bool differs from object -> bool, while both represent string -> bool conversions.

### Expected Behavior

when converting ""False"" with a StringDtype type with astype to bool results in a False

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.1.0
Version               : Darwin Kernel Version 23.1.0: Mon Oct  9 21:32:11 PDT 2023; root:xnu-10002.41.9~7/RELEASE_ARM64_T6030
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Usage Question', 'Strings']",2024-08-21 14:06:05,2024-08-21 15:13:53,3,closed
59572,BUG: Wrong result of Kurtosis,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
a = pd.Series([0,0,0,0,0.00005])

a.kurtosis()
Out[205]: np.float64(0.0)

scipy.stats.kurtosis(a,bias=False)
Out[206]: np.float64(4.999999999999997)
```


### Issue Description

Pandas can't give a right result of kurtosis. But the `scipy.stats.kurtosis` can.

### Expected Behavior

see the code

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2024-08-21 05:08:33,2024-08-21 17:49:53,2,closed
59571,BUG: provide better error message for pd.Timedelta - pd.Series[Timestamp],"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

pd.Timedelta(10) - pd.Series([pd.Timestamp(1)])
```


### Issue Description

You can do the same subtraction with integers (`1 - pd.Series(10)`), and you can also do `pd.Timedelta + pd.Series[Timestamp] ` (`pd.Timedelta(10) + pd.Series([pd.Timestamp(1)])`), so pandas should be consistent and allow `pd.Timedelta - pd.Series[Timestamp] `

#25497 is an older, closed issue about the error message here, but this scenario shouldn't cause an error at all.

### Expected Behavior

Should subtract the scalar from each element of the series/dataframe/index

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.18.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.6.0
Version               : Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 68.2.2
pip                   : 23.3.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None

```

</details>
","['Datetime', 'Error Reporting']",2024-08-21 01:01:25,2025-07-16 21:30:17,10,closed
59561,BUG: .str.contains `na` validation,"```python
import pandas as pd
import pyarrow as pa

ser = pd.Series(['a', 'b', None], dtype=pd.StringDtype(storage=""pyarrow""))
ser2 = ser.astype(pd.ArrowDtype(pa.string()))

ser.str.contains(""foo"", na=""bar"")  # <- casts ""bar"" to True
ser2.str.contains(""foo"", na=""bar"")  # <- raises
```

There's a small difference in the _str_contains methods in ArrowExtensionArray vs ArrowStringArray.  The latter uses `bool(na)` when filling null entries, the former uses `na` directly.

I prefer the no-casting behavior, but mainly think we should be consistent.

<b>update</b> Looks like `pandas/tests/strings/test_find_replace.py::test_contains_nan` specifically tests `na=""foo""`","['Bug', 'Strings', 'API - Consistency']",2024-08-20 16:06:32,2024-08-31 16:46:28,4,closed
59559,ENH: Breakpoint method for dataframes,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Pandas chaining is very popular, it's often handy, especially when debugging, to add breakpoints mid-chain.

This simple-ish to do manually:

```python
def trace_chain(df: pd.DataFrame) -> pd.DataFrame:
    breakpoint()
    return df

some_chained_df = (
    pd.read_csv(""cool-csv.csv"")
    .dropna(subset=[""col1"", ""col2""])
    .assign(col3=lambda df: df[""col1""] * df[""col2""])
    .pipe(trace_chain)
    .astype({""col1"": ""float""})
)
```

Not sure if I'm alone, but I find this super helpful and think it'd be awesome (and easy to maintain) if pandas supported this natively!

### Feature Description

Essentially, add in a `breakpoint` method that's something like this:

```python
class DataFrame:
    ...
    def breakpoint(self) -> self:
        breakpoint()  # noqa
        return self
```

That's make the above example something like this:

```python
some_chained_df = (
    pd.read_csv(""cool-csv.csv"")
    .dropna(subset=[""col1"", ""col2""])
    .assign(col3=lambda df: df[""col1""] * df[""col2""])
    .breakpoint()
    .astype({""col1"": ""float""})
)
```

### Alternative Solutions

I think the alternative is just not to include it natively. It is easy enough to do as a user defined function passed in to `pipe`, so would just be a handy convenience (especially for beginners) if it was included.

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-08-20 11:26:11,2024-08-20 17:06:20,1,closed
59557,DOC: read_csv: date_format,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.read_csv.html

### Documentation problem

When using `read_csv`, I came across a `UserWarning: Could not infer format...`. I tried to resolve it by providing a `date_format`, but the warning wouldn't go away. Then I realized that my problem was that I was providing specifically the *date format* but not the *time format*. 

The documentation for this parameter is confusing. It starts with what appear to be two alternate and contradictory definitions of the parameter:
* ""Format to use for parsing dates when used in conjunction with `parse_dates`.""
* ""The `strftime` to parse time, e.g. `""%d/%m/%Y""`.""

### Suggested fix for documentation

Say something like this:

""Format to use for parsing dates and/or times. Used in conjunction with `parse_dates`. See `strftime` documentation for more information on choices...""","['Docs', 'IO CSV']",2024-08-20 00:01:28,2024-08-21 19:33:03,2,closed
59548,"ENH: In pandas.testing.assert_frame_equal, support per-column configuration","### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Our internal validation tool's tolerance needs to depend on compared metrics. For example, when obtaining results from an analytical database from a query like

```sql
SELECT count(distinct device_id) as device_count, avg(score) as score GROUP BY ...
```
We expect `device_count` to always be accurate, but `score` is expected to have random numerical floating point inaccuracies.

My old code ran `assert_frame_equal` several times on different subsets of columns, which is cumbersome and doesn't express the intent well.
I recently refactored it by extracting `assert_frame_equal`'s implementation and just adding the extra arguments to support per-column customizable `rtol` and `atol`.
It would be nice if such an ability was built into Pandas.

Note that this overlaps a bit with feature request https://github.com/pandas-dev/pandas/issues/54861 .

### Feature Description

One way is to add extra arguments to `assert_frame_equal`, usable like so:

```py
assert_frame_equal(
    left,
    right,
    rtol=1e-5,
    atol=1e-8,
    rtols={'device_count': 0, 'score': 1e-6},
    atols={'device_count': 0}, # for unspecified columns, the rtol/atol argument is used as default
)
```
Or the entire comparison configuration (`check_exact`, `check_datetimelike_compat` etc) could be overridden per-series, for example
```py
assert_frame_equal(
    left,
    right,
    overrides={
        'device_count': {'check_exact': True},
        'score': {'rtol': 1e-6},
    }
)
```

### Alternative Solutions

The current way to do it with public APIs is to do something like
```py
for column_names, rtol in [([""device_count"", ...], 0.0), ([""score"", ...], 1e-6), ...]:
    left = # extract index and columns from left
    right = # extract index and columns from right
    assert_frame_equal(left, right, rtol=rtol)
```","['Enhancement', 'Testing', 'Needs Discussion', 'Closing Candidate']",2024-08-19 13:49:40,2025-08-05 16:49:28,6,closed
59543,ENH: Support non-categorical values for pandas bar plots when x axis is datetime values,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

When the x-axis is all dates, and a user tries to plot a bar plot, pandas treats the dates as categorical values.

Take a simple pandas dataframe with date time values and plot it using bar plots:

```python
import pandas as pd

df = pd.DataFrame(
    dict(
        date=pd.date_range(start=""2020-01-01"", end=""2020-12-31"", freq=""MS""),
        data=[1,2,3,4,5,6,7,8,9,10,11,12]
    ), 
)

import matplotlib as mpl
import matplotlib.dates as mdates
import matplotlib.pyplot as plt

fig = mpl.figure.Figure(constrained_layout=True)
axs = fig.subplot_mosaic(""a"")

ax = axs[""a""]

df.plot.bar(x=""date"", y=""data"", ax=ax, legend=False) # incorrect year -> 1970 instead of 2020

formatter = mdates.DateFormatter(""%Y - %b"")
ax.xaxis.set_major_formatter(formatter)

fig
```

You'll get this:

![image](https://github.com/user-attachments/assets/0c10c2b3-d972-41f5-878f-11d48eec30df)

There's unfortunately no way to bypass this. 

Using `x_compat` doesn't do anything:

```python
with pd.plotting.plot_params.use(""x_compat"", True):
    df.plot.bar()
```

And throws an error if you try to use it directly:

```python
df.plot.bar(x_compat=True)
```

![image](https://github.com/user-attachments/assets/2f09568a-8aa9-4314-b4f0-46582f198d42)

If I change the `df` to this (i.e. more data points):

```python
import pandas as pd
import numpy as np

date = pd.date_range(start=""2020-01-01"", end=""2050-12-31"", freq=""MS"")

df = pd.DataFrame(
    dict(
        date=date,
        data=[i for i, x in enumerate(date)]
    ), 
)
```

I get this:

![image](https://github.com/user-attachments/assets/d12f997d-36ea-4691-9ee4-96eb25d1f58b)

imho, this is a bad user experience. 

1. It takes significantly longer to plot because pandas is generating text labels for every data point
2. the plot labels are not useful to a user
3. users have no way to modify this plot to ""fix"" it because the x axis's data interval is categorical, i.e. 0 - N where N represents an integer corresponding to the last time period

    <img width=""360"" alt=""image"" src=""https://github.com/user-attachments/assets/24afb473-1212-4c8c-8397-4c465127194d"">

4. users cannot annotate labels on this plot easily because the x position is now a categorical axis instead of datetime values.

---

fwiw, matplotlib does the right thing when the x axis are all dates:

<img width=""740"" alt=""image"" src=""https://github.com/user-attachments/assets/0499147e-c747-48ca-8964-25324f47a87f"">

---

### Feature Description

Add a new option to `df.plot.bar(...)` that skips treating datetime values as categorical data. `df.plot(...)` already has `use_index=False` and `x_compat=True`. The former option is not useful imo but adding the latter option for bar plots would be great.

### Alternative Solutions

Alternatively, consider passing datetime values to matplotlib always without considering them as categorical data. 

This may be slightly breaking though? 

### Additional Context

This is currently the source of quite a bit of confusion when plotting bar plots with timeseries and line plots on the same `ax`.

e.g.: https://stackoverflow.com/q/39560099

Suggestions include 

1. using `ax.twinx()` and setting the bar plot's `ax` to invisible:

- https://stackoverflow.com/a/33458026

This is currently the best solution to this problem but imo is a little bit of a hack.

2. using `use_index=False` for the line plot:

- https://stackoverflow.com/a/65636540
- https://stackoverflow.com/a/65648326

This makes the line plot difficult to further annotate (x axis values are still 0 - N, and a user cannot use the datetime to place annotation text) and the user still will run into issues with large number of categorical datetime labels.

For context, this enhancement proposal was because I didn't understand that bar plots always use categorical values in pandas and posted this question on stackoverflow: https://stackoverflow.com/q/78882352/5451769","['Enhancement', 'Needs Triage']",2024-08-17 20:58:52,2024-08-24 19:50:31,3,closed
59542,ENH: Checking gaps for time series ,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Add built-in schema validation capabilities to pandas, similar to those provided by pandera. It should check time series gaps based on datestamp gaps.

### Feature Description

Time series data typically requires continuous timestamps without gaps. Currently, users have to implement their own custom solutions to check for missing periods. A built-in method would be useful to verify the continuity of timestamps. For example, if you have a time series with daily data from 2010-01-01 to 2020-01-01, there shouldn't be any gaps, such as a missing entry for 2019-01-02 or any other date. This method should work for various frequencies, not just daily data.

### Alternative Solutions

When gaps are detected, the function should automatically fill them using various interpolation methods or give an error.

### Additional Context

_No response_","['Enhancement', 'Datetime', 'Closing Candidate']",2024-08-17 20:09:46,2024-08-25 18:13:58,2,closed
59532,DOC: Wrong bug number in what's new v3.0.0,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/whatsnew/v3.0.0.html

### Documentation problem

There is a sentence in the `Bug Fixes > Datetimelike` section:

""Bug in date_range() where using a negative frequency value would not include all points between the start and end values (GH 56382)""

The ""56382"" is a wrong number which is not related to the fixed issue.

The correct PR is #56832.
The correct issue is #56147.

### Suggested fix for documentation

Fix ""56382"" to ""56147"" in the mentioned line in what's new v3.0.0.","['Docs', 'good first issue']",2024-08-16 18:25:52,2024-08-19 19:59:35,3,closed
59531,BUG: OverflowError: value too large to convert to int when manipulating very large dataframes,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

test = pd.DataFrame({'count' : np.random.randint(0,100,size=(4261028590))}, index=(pd.DatetimeIndex(np.empty(4261028590))))

stripped = test[test['count'] > 0]
```


### Issue Description

When working with a very large data frame (4261028590) rows I get "".... in pandas._libs.lib.maybe_indices_to_slice
OverflowError: value too large to convert to int:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py"", line 4093, in __getitem__
    return self._getitem_bool_array(key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py"", line 4155, in _getitem_bool_array
    return self._take_with_is_copy(indexer, axis=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py"", line 4153, in _take_with_is_copy
    result = self.take(indices=indices, axis=axis)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py"", line 4133, in take
    new_data = self._mgr.take(
               ^^^^^^^^^^^^^^^
  File ""/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py"", line 893, in take
    new_labels = self.axes[axis].take(indexer)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/datetimelike.py"", line 839, in take
    maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""lib.pyx"", line 522, in pandas._libs.lib.maybe_indices_to_slice
OverflowError: value too large to convert to int
```

While similar to other reports, this occurs in 'pandas._libs.lib.maybe_indices_to_slice'.

Other manipulations on the df also fail.  Perhaps rows are represented internally as int32s ?

### Expected Behavior

Return array where 'count' is greater than zero allowing for df to be filtered down further.

### Installed Versions

pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : 3.0.11
pytest                : 7.4.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.1
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 14.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.3.1
scipy                 : 1.13.1
sqlalchemy            : 2.0.30
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2023.6.0
xlrd                  : 2.0.1
zstandard             : 0.22.0
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None","['Bug', 'Indexing']",2024-08-16 17:51:17,2025-03-07 23:30:56,6,closed
59530,BUG: read_csv's usecols type hint isn't match with list of strings,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.read_csv(""test.csv"", usecols=[""a"", ""b""]) # this line causes type conflict
```


### Issue Description

Pyright gives the following diagnostic message, but according to the pandas docs usecols can be a list of strings. I guess the type annotation of `usecols` is not accurate.

```
Diagnostics:
1. No overloads for ""read_csv"" match the provided arguments [reportCallIssue]
2. Argument of type ""list[str]"" cannot be assigned to parameter ""usecols"" of type ""UsecolsArgType[Unknown]"" in function ""read_csv""
     Type ""list[str]"" is incompatible with type ""UsecolsArgType[Unknown]""
       ""list[str]"" is incompatible with protocol ""SequenceNotStr[Hashable]""
         ""index"" is an incompatible type
           Type ""(value: str, start: SupportsIndex = 0, stop: SupportsIndex = sys.maxsize, /) -> int"" is incompatible with type ""(value: Any, /, start: int = 0, stop: int = ...) -> int""
             Missing keyword parameter ""start""
             Missing keyword parameter ""stop""
       ""list[str]"" is incompatible with ""range""
       ""list[str]"" is incompatible with ""ExtensionArray""
     ... [reportArgumentType]
```

### Expected Behavior

The type annotation of `usecols` should accept list of strings.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 22.6.0
Version               : Darwin Kernel Version 22.6.0: Mon Jun 24 01:25:37 PDT 2024; root:xnu-8796.141.3.706.2~1/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.1.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.31
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Typing', 'Closing Candidate']",2024-08-16 17:19:47,2024-08-18 13:05:04,2,closed
59527,BUG: import has error ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
---> 62 from pandas.core.api import (
     63     # dtype
     64     ArrowDtype,
     65     Int8Dtype,
     66     Int16Dtype,
     67     Int32Dtype,
     68     Int64Dtype,
     69     UInt8Dtype,
     70     UInt16Dtype,
     71     UInt32Dtype,
     72     UInt64Dtype,
     73     Float32Dtype,
     74     Float64Dtype,
     75     CategoricalDtype,
     76     PeriodDtype,
     77     IntervalDtype,
...
File hashtable.pyx:1, in init pandas._libs.hashtable()

File missing.pyx:40, in init pandas._libs.missing()

AttributeError: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)
```


### Issue Description

AttributeError                            Traceback (most recent call last)
Cell In[2], line 1
----> 1 import pandas as pd

### Expected Behavior

AttributeError                            Traceback (most recent call last)
Cell In[2], line 1
----> 1 import pandas as pd

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Build', 'Needs Info']",2024-08-16 15:45:43,2025-08-05 16:56:05,6,closed
59523,"BUG: ""Bad CRC-32 for file'docProps/core.xml"" for Read large Excel file ","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
........
df = pd.read_excel(file_path, sheet_name=sheet_name)
........
```


### Issue Description

When I open an Excel file(.xlsx) with 300000 rows using pd.read_Excel, an error message appears: Bad CRC-32 for file'docProps/core.xml. But if the table has less than 1000 rows, it is normal to open the table with pd.read_excel. 
pd.ExcelFile(file_path) has the same problem.
Why?

### Expected Behavior

df = pd.read_excel(file_path, sheet_name=sheet_name)

### Installed Versions

pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : 0f437949513225922d851e9581723d82120684a6
python           : 3.8.10.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.22621
machine          : AMD64
processor        : Intel64 Family 6 Model 186 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : Chinese (Simplified)_China.936
pandas           : 2.0.3
numpy            : 1.24.4
pytz             : 2024.1
dateutil         : 2.9.0.post0
setuptools       : 56.0.0
pip              : 21.1.1
Cython           : None
pytest           : 8.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
brotli           : None
fastparquet      : None
fsspec           : None
gcsfs            : None
matplotlib       : None
numba            : None
numexpr          : None
odfpy            : None
openpyxl         : 3.1.2
pandas_gbq       : None
pyarrow          : None
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : None
snappy           : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
zstandard        : None
tzdata           : 2024.1
qtpy             : None
pyqt5            : None

","['Bug', 'IO Excel', 'Needs Info']",2024-08-15 08:50:39,2025-01-04 21:15:22,3,closed
59522,BUG: ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
idx1 = pd.DataFrame(columns=None).columns.dtype
idx2 = pd.DataFrame(columns=[]).columns.dtype
assert idx1 == idx2
```


### Issue Description

when starting an empty df with columns as None it is different the setting columns as empty list.

### Expected Behavior

I would expect the columns dtypes to the same, and preferably object (str). Most users will expect Index on columns to be strings.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.8.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-1066-aws
Version               : #72~20.04.1-Ubuntu SMP Thu Jul 18 10:41:27 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.0
pytz                  : 2024.1
dateutil              : 2.8.2
setuptools            : 65.5.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.22.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.2.0
gcsfs                 : None
matplotlib            : 3.8.3
numba                 : 0.58.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : 2.0.29
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
Click to add a cell.

</details>
","['Bug', 'Needs Triage']",2024-08-15 08:03:00,2024-08-15 16:28:43,3,closed
59516,BUG: read_json silently ignores the dtype when engine=pyarrow,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import json

with open(""test.json"", ""w"") as f:
    json.dump({""Col1"": 123}, f)

dtype = {""Col1"": ""int32[pyarrow]""}

df = pd.read_json(""test.json"", dtype=dtype, lines=True, engine=""pyarrow"", dtype_backend=""pyarrow"")

print(df.dtypes.to_dict())  # prints {'Col1': int64[pyarrow]}
```


### Issue Description

[The call to `pyarrow_json.read_json`](https://github.com/pandas-dev/pandas/blob/main/pandas/io/json/_json.py#L942) doesn't pass any [`ParseOptions`](https://arrow.apache.org/docs/python/generated/pyarrow.json.ParseOptions.html#pyarrow.json.ParseOptions). This means that determining the dtype is always left up to the pyarrow json parser's dtype inference system, even when we explicitly passed a requested dtype into `pd.read_json`.

### Expected Behavior

`read_json`'s pyarrow parser should respect the requested `dtype`.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 5, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 72.1.0
pip                   : None
Cython                : None
pytest                : 8.3.2
hypothesis            : None
sphinx                : 7.4.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Dtype Conversions', 'IO JSON']",2024-08-14 17:35:19,2025-03-19 18:12:53,5,closed
59511,BUG: Spurious `FutureWarning` when using `pd.read_json()`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import io
s = '{""A"":{""0"":""X"",""Y"":""Y""}}'
pd.read_json(io.StringIO(s), typ='frame', orient='records')
```


### Issue Description

`read_json()` works correctly but generates three spurious and annoying warnings:

```
FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.
  pd.read_json(io.StringIO(s), typ='frame', orient='records')
FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.
  pd.read_json(io.StringIO(s), typ='frame', orient='records')
FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.
  pd.read_json(io.StringIO(s), typ='frame', orient='records')
```

These warnings are spurious because the usage of `to_datetime` is not controlled by user. The warnings are annoying because there are so many of them.

This was previously raised on Stack Overflow here 3 months ago and it accumulated almost 400 visits: https://stackoverflow.com/questions/78454457/pandas-read-json-future-warning-the-behavior-of-to-datetime-with-unit-when

### Expected Behavior

The warnings are suppressed.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.10.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-44-generic
Version               : #44-Ubuntu SMP PREEMPT_DYNAMIC Fri Jun  7 15:10:09 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 22.3.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'IO JSON', 'Warnings']",2024-08-14 12:58:05,2025-06-02 17:21:51,9,closed
59510,DOC: Incorrect Documentation: pd.Dataframe.dropna,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html

### Documentation problem

![image](https://github.com/user-attachments/assets/b3a93dff-dca7-466f-9544-b36fdf6211b8)

 Keyword Argument 'how' is defined as ""_NoDefault.no_default"" but it has a default value 'any'

### Suggested fix for documentation



Change the value of the keyword arg 'how' to 'any'
","['Docs', 'Closing Candidate']",2024-08-14 08:40:59,2024-08-25 18:14:21,2,closed
59504,BUILD: Python Docker Build Issues,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Linux-6.6.32-linuxkit-aarch64-with-glibc2.36, Linux-6.6.32-linuxkit-amd64-with-glibc2.36

### Installation Method

pip install

### pandas Version

1.1.5

### Python Version

python 3.11

### Installation Logs

<details>
FROM python:3.11

WORKDIR /usr/src/app

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    gcc \
    && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir pandas==1.1.5

RUN python -c ""import pandas as pd; print(pd.__version__)""

CMD [""python3""]

31.85       set build/lib.linux-aarch64-cpython-311/pandas/_version.py to '1.1.5'
31.85       running build_ext
31.85       building 'pandas._libs.algos' extension
31.85       creating build/temp.linux-aarch64-cpython-311
31.85       creating build/temp.linux-aarch64-cpython-311/pandas
31.85       creating build/temp.linux-aarch64-cpython-311/pandas/_libs
31.85       gcc -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -DNPY_NO_DEPRECATED_API=0 -I./pandas/_libs -Ipandas/_libs/src/klib -I/tmp/pip-build-env-8lb37r86/overlay/lib/python3.11/site-packages/numpy/core/include -I/usr/local/include/python3.11 -c pandas/_libs/algos.c -o build/temp.linux-aarch64-cpython-311/pandas/_libs/algos.o
31.85       pandas/_libs/algos.c:785:10: fatal error: numpy/arrayobject.h: No such file or directory
31.85         785 | #include ""numpy/arrayobject.h""
31.85             |          ^~~~~~~~~~~~~~~~~~~~~
31.85       compilation terminated.
31.85       error: command '/usr/bin/gcc' failed with exit code 1
31.85       [end of output]
31.85   
31.85   note: This error originates from a subprocess, and is likely not a problem with pip.
31.85   ERROR: Failed building wheel for pandas
31.85 Failed to build pandas
31.85 ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects




</details>
","['Build', 'Needs Info']",2024-08-13 20:38:33,2024-08-14 16:03:05,4,closed
59502,DEPR: future.no_silent_downcasting option,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
NA
```


### Issue Description

The option no longer does anything in 3.0, so can be deprecated.  The few places where we use it in the code can be removed.

### Expected Behavior

NA

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Dtype Conversions', 'Deprecate']",2024-08-13 17:47:39,2025-10-14 18:49:24,1,closed
59499,BUG: Replace fails after NaN in a Series of `string`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.options.future.infer_string = True
df = pd.Series([""toto"", pd.NA, ""titi"", ""tata""])
df.replace({""tata"": ""tutu""})
```


### Issue Description

Regex replace fails after the NaN value in a Series of `string`.
Result :
```
0    toto
1     NaN
2    titi
3    tata # Expecting: ""tutu""
```

Note :
1. if NaN is before the value to be replaced like in `pd.Series([""toto"", ""titi"", ""tata"", pd.NA])`, it works
2. `df.dropna().replace({""tata"": ""tutu""})` works as expected

### Expected Behavior

```
0    toto
1     NaN
2    titi
3    tutu
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 69.0.3
pip                   : 23.3.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.3
IPython               : 8.20.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : 2.0.25
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None

</details>","['Bug', 'Needs Triage']",2024-08-13 15:57:34,2024-08-13 17:42:02,2,closed
59498,ENH: Adding `skipna:bool` to Rolling.sum,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

There is an inconsistency between the APIs of `pd.DataFrame.sum`/`pd.Series.sum` and the API of `pandas.core.window.rolling.Rolling.sum`. The former two functions support `skipna` to let the user decide whether to silently ignore NaNs or let the presence of NaNs cause the result to be NaN. The second function lacks this parameter. I happen to have a use case where I may not ignore NaNs, so currently I have to essentially reimplement rolling windows in my own high-level code, which makes my code slow to a crawl.

### Feature Description

I suggest adding a `skipna:bool = True` to the signature of Rolling.sum:

```
    def sum(
        self,
        skipna: bool = True,
        numeric_only: bool = False,
        engine: Literal[""cython"", ""numba""] | None = None,
        engine_kwargs: dict[str, bool] | None = None,
    ):
    ...
```

### Alternative Solutions

My current very slow approach is to create a sequence of Index objects, each representing a window. I then iterate on the sequence of index objects, extracting the series corresponding to that window, and I call `window_as_series.sum(skipna=False)`. This works, but repeated indexing to extract a series is inordinately slow compared to a bona fide pandas windowed sum.

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-08-13 15:51:41,2024-08-13 17:56:19,2,closed
59497,BUG: ArrowNotImplementedError: Unsupported cast from int64 to null using function cast_null,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
# Reading multiple .parquet file from directory 
df = pd.read_parquet(""directory"")
```


### Issue Description

assume we are going read multiple file from directory and format is .parquet , when try it 
If first file has column with None value error reported :
""ArrowNotImplementedError: Unsupported cast from int64 to null using function cast_null""
I change first file and replace it with file without None value column and this Error removed.

### Expected Behavior

Not crashing

### Installed Versions

<details>

[e:\Anaconda](file:///E:/Anaconda) File\Lib\site-packages\_distutils_hack\__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.11.7.final.0
python-bits         : 64
OS                  : Windows
OS-release          : 10
Version             : 10.0.22000
machine             : AMD64
processor           : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder           : little
LC_ALL              : None
LANG                : None
LOCALE              : English_United States.1252

pandas              : 2.1.4
numpy               : 1.26.4
pytz                : 2023.3.post1
dateutil            : 2.8.2
setuptools          : 68.2.2
pip                 : 23.3.1
Cython              : None
pytest              : 7.4.0
hypothesis          : None
sphinx              : 5.0.2
blosc               : None
feather             : None
xlsxwriter          : None
lxml.etree          : 4.9.3
html5lib            : None
pymysql             : None
psycopg2            : None
jinja2              : 3.1.3
IPython             : 8.20.0
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : 1.3.7
dataframe-api-compat: None
fastparquet         : None
fsspec              : 2023.10.0
gcsfs               : None
matplotlib          : 3.8.0
numba               : 0.59.0
numexpr             : 2.8.7
odfpy               : None
openpyxl            : 3.0.10
pandas_gbq          : None
pyarrow             : 14.0.2
pyreadstat          : None
pyxlsb              : None
s3fs                : 2023.10.0
scipy               : 1.11.4
sqlalchemy          : 2.0.25
tables              : 3.9.2
tabulate            : 0.9.0
xarray              : 2023.6.0
xlrd                : None
zstandard           : 0.19.0
tzdata              : 2023.3
qtpy                : 2.4.1
pyqt5               : None

</details>
","['Bug', 'Needs Info']",2024-08-13 13:22:26,2025-08-05 16:56:43,4,closed
59494,PERF: Excessive memory consumption in pd.read_parquet,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

During read a parquet file (Only 30M), I got a very high memory consumption, when checking memory usage with memory_profiler

`3690.5 MiB   3401.4 MiB           1       df = pd.read_parquet(file_path)`

how can I solve it

thx!!

### Installed Versions

<details>


pandas                : 2.2.2
numpy                 : 1.23.1
pytz                  : 2023.3
dateutil              : 2.8.2
setuptools            : 50.3.2
pip                   : 23.1.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.14.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.5.0
gcsfs                 : None
matplotlib            : None
numba                 : 0.57.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
None

</details>


### Prior Performance

_No response_","['Performance', 'Needs Info', 'IO Parquet']",2024-08-13 07:19:14,2024-08-19 10:44:04,3,closed
59477, Bug in Chunk Processing: Non-NULL IDs Become NULL During IterationBUG: ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

# Sample DataFrame
data = {
    'race_id': [1, 1, 2, 2, 3, 3],
    'value': [10, 20, 30, 40, 50, 60]
}
df = pd.DataFrame(data)

# Chunk processing
chunks = np.array_split(df, 2)
for chunk in chunks:
    print(chunk)
```


### Issue Description

Description: I have encountered a critical bug in the chunk processing functionality of pandas. When performing iteration on specific non-NULL IDs, the chunk processing results in those IDs becoming NULL. This issue also causes related values to be recognized as NULL within the DataFrame, leading to incorrect processing

### Expected Behavior

Steps to Reproduce:

Perform iteration on a DataFrame with specific non-NULL IDs.
Observe that during chunk processing, those IDs become NULL.
Related values within the DataFrame are also recognized as NULL, causing incorrect processing.
Expected Behavior: The IDs should remain non-NULL during chunk processing, and related values should be processed correctly.

Actual Behavior: The IDs become NULL during chunk processing, and related values are recognized as NULL, leading to incorrect processing.

Additional Context: This bug significantly impacts the reliability of chunk processing in pandas, making it difficult to perform accurate data analysis. A prompt resolution would be greatly appreciated.

### Installed Versions

<details>
Environment:

pandas version: [Version: 2.2.2]
Python version: [3.11.9 64bit]
Operating System: [windows10 64bit]
Replace this line with the output of pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : Japanese_Japan.932

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 24.2
Cython                : 3.0.10
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.1
html5lib              : 1.1
pymysql               : 1.4.6
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.23.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : 2.0.30
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None","['Bug', 'Needs Info']",2024-08-11 05:30:48,2025-08-05 16:57:05,3,closed
59466,Title: Feature Request: Improve diff Function to Support Forward and Backward CompletionENH: ,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Description: Hello pandas development team,

I would like to propose an enhancement to the diff function in the pandas library. While the current implementation of diff is useful for calculating differences between consecutive rows, it lacks the ability to handle forward and backward completion in a seamless manner. This limitation makes it challenging to use diff for certain types of data processing, especially when dealing with large datasets.

Problem Statement: The current diff function calculates the difference between consecutive rows, but it does not provide a way to handle forward and backward completion. This results in incomplete or inaccurate calculations when trying to determine differences across a dataset with specific requirements. For example, in race data analysis, calculating the time differences between horses requires precise handling of forward and backward completion to ensure accurate results.

### Feature Description

Proposed Solution: I propose enhancing the diff function to include options for forward and backward completion. This would allow users to specify whether they want to calculate differences in a forward, backward, or both directions. Additionally, providing options to handle edge cases, such as the first and last rows, would greatly improve the usability of the diff function for complex data processing tasks.

Benefits:

Improved accuracy and completeness in difference calculations.
Enhanced usability for complex data processing tasks.
Reduced need for custom implementations, leading to more efficient code.

### Alternative Solutions

Alternative Solutions: One alternative solution is to implement custom functions to handle forward and backward completion manually. However, this approach can be time-consuming and error-prone, especially when dealing with large datasets. Another alternative is to use other libraries or tools that may offer similar functionality, but integrating them with pandas may introduce additional complexity.

Example: Here is an example of how the enhanced diff function could be used:

import pandas as pd

# Sample DataFrame
df = pd.DataFrame({
    'race_id': [1, 1, 1, 2, 2, 2],
    'time': [100, 102, 104, 200, 202, 204]
})

# Calculate differences with forward and backward completion
df['time_diff'] = df['time'].diff(completion='both')

print(df)


### Additional Context

Additional Context: The provided example demonstrates how the enhanced diff function could be used to calculate differences with forward and backward completion. This feature would be particularly useful in scenarios where precise difference calculations are required, such as in race data analysis.

Thank you for considering this enhancement. I believe it would greatly benefit the pandas community and improve the overall functionality of the library.

Best regards, [Your Name]","['Enhancement', 'Needs Triage']",2024-08-10 02:33:30,2024-08-10 08:31:43,2,closed
59465,Title: Feature Request: Improve diff Function to Support Forward and Backward CompletionENH: ,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Description: Hello pandas development team,

I would like to propose an enhancement to the diff function in the pandas library. While the current implementation of diff is useful for calculating differences between consecutive rows, it lacks the ability to handle forward and backward completion in a seamless manner. This limitation makes it challenging to use diff for certain types of data processing, especially when dealing with large datasets.

Problem Statement: The current diff function calculates the difference between consecutive rows, but it does not provide a way to handle forward and backward completion. This results in incomplete or inaccurate calculations when trying to determine differences across a dataset with specific requirements. For example, in race data analysis, calculating the time differences between horses requires precise handling of forward and backward completion to ensure accurate results.

### Feature Description

Proposed Solution: I propose enhancing the diff function to include options for forward and backward completion. This would allow users to specify whether they want to calculate differences in a forward, backward, or both directions. Additionally, providing options to handle edge cases, such as the first and last rows, would greatly improve the usability of the diff function for complex data processing tasks.

Benefits:

Improved accuracy and completeness in difference calculations.
Enhanced usability for complex data processing tasks.
Reduced need for custom implementations, leading to more efficient code.

### Alternative Solutions

Alternative Solutions: One alternative solution is to implement custom functions to handle forward and backward completion manually. However, this approach can be time-consuming and error-prone, especially when dealing with large datasets. Another alternative is to use other libraries or tools that may offer similar functionality, but integrating them with pandas may introduce additional complexity.

Example: Here is an example of how the enhanced diff function could be used:

import pandas as pd

# Sample DataFrame
df = pd.DataFrame({
    'race_id': [1, 1, 1, 2, 2, 2],
    'time': [100, 102, 104, 200, 202, 204]
})

# Calculate differences with forward and backward completion
df['time_diff'] = df['time'].diff(completion='both')

print(df)


### Additional Context

Additional Context: The provided example demonstrates how the enhanced diff function could be used to calculate differences with forward and backward completion. This feature would be particularly useful in scenarios where precise difference calculations are required, such as in race data analysis.

Thank you for considering this enhancement. I believe it would greatly benefit the pandas community and improve the overall functionality of the library.

Best regards, [Your Name]","['Enhancement', 'Needs Info', 'Transformations']",2024-08-10 02:33:29,2025-08-05 16:55:44,3,closed
59461,BUG: Ability to set both color and style in pandas plotting,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df = pd.DataFrame({
'pig': [20, 18, 489, 675, 1776],
 'horse': [4, 25, 281, 600, 1900]
}, index=[1990, 1997, 2003, 2009, 2014])

color={'pig':'black','horse':'brown'}
style={'pig':'solid','horse':'dashed'}
df.plot.line(color=color,style=style)

plt.show()
```


### Issue Description

In above code we can pass color or style but not both.

### Expected Behavior

We should be able to change both color and style.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.6.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_CA.UTF-8
LOCALE                : en_CA.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 58.0.4
pip                   : 21.2.4
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.1.1
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 15.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Visualization']",2024-08-09 15:23:04,2024-08-22 15:19:38,3,closed
59459,DOC: Development on Gitpod have problems,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/development/contributing_gitpod.html

### Documentation problem

Aiming for a quick-start, I wanted to use GitPod to setup my environment.

After a couple of minor hiccups I try to tackle in https://github.com/pandas-dev/pandas/pull/59456, I stepped on a more serious roadblock:

```
$ python -m pytest pandas

+ /usr/local/bin/ninja
[1/1] Generating write_version_file with a custom command
ImportError while loading conftest '/workspace/pandas/pandas/conftest.py'.
pandas/conftest.py:715: in <module>
    idx = Index(pd.array([f""pandas_{i}"" for i in range(10)], dtype=""string[pyarrow]""))
pandas/core/construction.py:321: in array
    return cls._from_sequence(data, dtype=dtype, copy=copy)
pandas/core/arrays/string_arrow.py:203: in _from_sequence
    return cls(pa.array(result, type=pa.large_string(), from_pandas=True))
pyarrow/array.pxi:281: in pyarrow.lib.array
    ???
pyarrow/array.pxi:4519: in pyarrow.lib.get_values
    ???
pyarrow/pandas-shim.pxi:228: in pyarrow.lib._PandasAPIShim.is_series
    ???
pyarrow/pandas-shim.pxi:124: in pyarrow.lib._PandasAPIShim._have_pandas_internal
    ???
pyarrow/pandas-shim.pxi:103: in pyarrow.lib._PandasAPIShim._check_import
    ???
pyarrow/pandas-shim.pxi:106: in pyarrow.lib._PandasAPIShim._check_import
    ???
pyarrow/pandas-shim.pxi:74: in pyarrow.lib._PandasAPIShim._import_pandas
    ???
E   UserWarning: pyarrow requires pandas 1.0.0 or above, pandas 0+untagged.1.gd5bbd97 is installed. Therefore, pandas-specific integration is not used.
```

This comes from a fresh workspace built on `main`, and it is confirmed by `pandas.__version__`:

```
$ python
Python 3.10.8 (main, Dec  6 2022, 14:13:21) [GCC 10.2.1 20210110] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas
+ /usr/local/bin/ninja
[1/1] Generating write_version_file with a custom command
>>> pandas.__version__
'0+untagged.1.gd5bbd97'
```

Note that in the initialization steps, `pandas 2.2.0.dev0...` got built (output below).

<details>

<summary>Output from the initialization task in .gitpod.yml</summary>

```
$  HISTFILE=/workspace/.gitpod/cmd-0 history -r; {
> mkdir -p .vscode
> cp gitpod/settings.json .vscode/settings.json
> git fetch --tags
> python -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true
> pre-commit install --install-hooks
> 
> } && {
> python -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true
> echo ""✨ Pre-build complete! You can close this terminal ✨ ""
> 
> }
Using pip 24.2 from /usr/local/lib/python3.10/site-packages/pip (python 3.10)
Defaulting to user installation because normal site-packages is not writeable
Obtaining file:///workspace/pandas
  Running command Checking if build backend supports build_editable
  Checking if build backend supports build_editable ... done
  Running command Preparing editable metadata (pyproject.toml)
  + meson setup /workspace/pandas /workspace/pandas/build/cp310 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=/workspace/pandas/build/cp310/meson-python-native-file.ini
  The Meson build system
  Version: 1.2.1
  Source dir: /workspace/pandas
  Build dir: /workspace/pandas/build/cp310
  Build type: native build
  Project name: pandas
  Project version: 0+untagged.1.gd5bbd97
  C compiler for the host machine: cc (gcc 10.2.1 ""cc (Debian 10.2.1-6) 10.2.1 20210110"")
  C linker for the host machine: cc ld.bfd 2.35.2
  C++ compiler for the host machine: c++ (gcc 10.2.1 ""c++ (Debian 10.2.1-6) 10.2.1 20210110"")
  C++ linker for the host machine: c++ ld.bfd 2.35.2
  Cython compiler for the host machine: cython (cython 3.0.11)
  Host machine cpu family: x86_64
  Host machine cpu: x86_64
  Program python found: YES (/usr/local/bin/python)
  Found pkg-config: /usr/bin/pkg-config (0.29.2)
  Run-time dependency python found: YES 3.10
  Build targets in project: 54

  pandas 0+untagged.1.gd5bbd97

    User defined options
      Native files: /workspace/pandas/build/cp310/meson-python-native-file.ini
      buildtype   : release
      vsenv       : True
      b_ndebug    : if-release
      b_vscrt     : md

  Found ninja-1.11.1.git.kitware.jobserver-1 at /usr/local/bin/ninja

  Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
  /usr/local/bin/meson compile -C .
  + /usr/local/bin/ninja
  [1/152] Generating pandas/_libs/algos_common_helper_pxi with a custom command
  [2/152] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command
  [3/152] Generating pandas/_libs/index_class_helper_pxi with a custom command
  [4/152] Generating pandas/_libs/algos_take_helper_pxi with a custom command
  [5/152] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command
  [6/152] Generating pandas/__init__.py with a custom command
  [7/152] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command
  [8/152] Generating pandas/_libs/intervaltree_helper_pxi with a custom command
  [9/152] Generating pandas/_libs/sparse_op_helper_pxi with a custom command
  [10/152] Generating write_version_file with a custom command
  [11/152] Compiling C object pandas/_libs/tslibs/parsing.cpython-310-x86_64-linux-gnu.so.p/.._src_parser_tokenizer.c.o
  [12/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/base.pyx
  [13/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/ccalendar.pyx
  [14/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/dtypes.pyx
  [15/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/nattype.pyx
  [16/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/np_datetime.pyx
  [17/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/vectorized.pyx
  [18/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/timezones.pyx
  [19/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/tzconversion.pyx
  [20/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/fields.pyx
  [21/152] Compiling C object pandas/_libs/tslibs/base.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o
  [22/152] Linking target pandas/_libs/tslibs/base.cpython-310-x86_64-linux-gnu.so
  [23/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/strptime.pyx
  [24/152] Compiling Cython source /workspace/pandas/pandas/_libs/arrays.pyx
  [25/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/conversion.pyx
  [26/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/parsing.pyx
  [27/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/timestamps.pyx
  [28/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/timedeltas.pyx
  [29/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/offsets.pyx
  [30/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslibs/period.pyx
  [31/152] Compiling C object pandas/_libs/tslibs/ccalendar.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o
  [32/152] Linking target pandas/_libs/tslibs/ccalendar.cpython-310-x86_64-linux-gnu.so
  [33/152] Compiling Cython source /workspace/pandas/pandas/_libs/hashing.pyx
  [34/152] Compiling C object pandas/_libs/arrays.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_arrays.pyx.c.o
  [35/152] Linking target pandas/_libs/arrays.cpython-310-x86_64-linux-gnu.so
  [36/152] Compiling C object pandas/_libs/tslibs/np_datetime.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_np_datetime.pyx.c.o
  [37/152] Linking target pandas/_libs/tslibs/np_datetime.cpython-310-x86_64-linux-gnu.so
  [38/152] Compiling C object pandas/_libs/tslibs/dtypes.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_dtypes.pyx.c.o
  [39/152] Linking target pandas/_libs/tslibs/dtypes.cpython-310-x86_64-linux-gnu.so
  [40/152] Compiling C object pandas/_libs/tslibs/vectorized.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_vectorized.pyx.c.o
  [41/152] Linking target pandas/_libs/tslibs/vectorized.cpython-310-x86_64-linux-gnu.so
  [42/152] Compiling C object pandas/_libs/tslibs/nattype.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_nattype.pyx.c.o
  [43/152] Linking target pandas/_libs/tslibs/nattype.cpython-310-x86_64-linux-gnu.so
  [44/152] Compiling Cython source /workspace/pandas/pandas/_libs/indexing.pyx
  [45/152] Compiling C object pandas/_libs/tslibs/timezones.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_timezones.pyx.c.o
  [46/152] Linking target pandas/_libs/tslibs/timezones.cpython-310-x86_64-linux-gnu.so
  [47/152] Compiling Cython source /workspace/pandas/pandas/_libs/algos.pyx
  [48/152] Compiling C object pandas/_libs/indexing.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_indexing.pyx.c.o
  [49/152] Linking target pandas/_libs/indexing.cpython-310-x86_64-linux-gnu.so
  [50/152] Compiling C object pandas/_libs/tslibs/conversion.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_conversion.pyx.c.o
  [51/152] Linking target pandas/_libs/tslibs/conversion.cpython-310-x86_64-linux-gnu.so
  [52/152] Compiling Cython source /workspace/pandas/pandas/_libs/index.pyx
  [53/152] Compiling Cython source /workspace/pandas/pandas/_libs/internals.pyx
  [54/152] Compiling C object pandas/_libs/hashing.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_hashing.pyx.c.o
  [55/152] Linking target pandas/_libs/hashing.cpython-310-x86_64-linux-gnu.so
  [56/152] Compiling C object pandas/_libs/tslibs/fields.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_fields.pyx.c.o
  [57/152] Linking target pandas/_libs/tslibs/fields.cpython-310-x86_64-linux-gnu.so
  [58/152] Compiling C object pandas/_libs/lib.cpython-310-x86_64-linux-gnu.so.p/src_parser_tokenizer.c.o
  [59/152] Compiling C object pandas/_libs/tslibs/tzconversion.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_tzconversion.pyx.c.o
  [60/152] Linking target pandas/_libs/tslibs/tzconversion.cpython-310-x86_64-linux-gnu.so
  [61/152] Compiling C object pandas/_libs/pandas_datetime.cpython-310-x86_64-linux-gnu.so.p/src_vendored_numpy_datetime_np_datetime.c.o
  [62/152] Compiling C object pandas/_libs/pandas_datetime.cpython-310-x86_64-linux-gnu.so.p/src_datetime_date_conversions.c.o
  [63/152] Compiling C object pandas/_libs/pandas_datetime.cpython-310-x86_64-linux-gnu.so.p/src_datetime_pd_datetime.c.o
  [64/152] Compiling C object pandas/_libs/pandas_datetime.cpython-310-x86_64-linux-gnu.so.p/src_vendored_numpy_datetime_np_datetime_strings.c.o
  [65/152] Compiling Cython source /workspace/pandas/pandas/_libs/groupby.pyx
  [66/152] Compiling C object pandas/_libs/pandas_parser.cpython-310-x86_64-linux-gnu.so.p/src_parser_io.c.o
  [67/152] Linking target pandas/_libs/pandas_datetime.cpython-310-x86_64-linux-gnu.so
  [68/152] Compiling C object pandas/_libs/pandas_parser.cpython-310-x86_64-linux-gnu.so.p/src_parser_pd_parser.c.o
  [69/152] Compiling C object pandas/_libs/pandas_parser.cpython-310-x86_64-linux-gnu.so.p/src_parser_tokenizer.c.o
  [70/152] Linking target pandas/_libs/pandas_parser.cpython-310-x86_64-linux-gnu.so
  [71/152] Compiling C object pandas/_libs/parsers.cpython-310-x86_64-linux-gnu.so.p/src_parser_io.c.o
  [72/152] Compiling Cython source /workspace/pandas/pandas/_libs/missing.pyx
  [73/152] Compiling C object pandas/_libs/parsers.cpython-310-x86_64-linux-gnu.so.p/src_parser_tokenizer.c.o
  [74/152] Compiling C object pandas/_libs/json.cpython-310-x86_64-linux-gnu.so.p/src_vendored_ujson_python_ujson.c.o
  [75/152] Compiling C object pandas/_libs/json.cpython-310-x86_64-linux-gnu.so.p/src_vendored_ujson_python_JSONtoObj.c.o
  [76/152] Compiling C object pandas/_libs/json.cpython-310-x86_64-linux-gnu.so.p/src_vendored_ujson_python_objToJSON.c.o
  [77/152] Compiling C object pandas/_libs/json.cpython-310-x86_64-linux-gnu.so.p/src_vendored_ujson_lib_ultrajsonenc.c.o
  [78/152] Compiling C object pandas/_libs/json.cpython-310-x86_64-linux-gnu.so.p/src_vendored_ujson_lib_ultrajsondec.c.o
  [79/152] Linking target pandas/_libs/json.cpython-310-x86_64-linux-gnu.so
  [80/152] Compiling Cython source /workspace/pandas/pandas/_libs/interval.pyx
  [81/152] Compiling Cython source /workspace/pandas/pandas/_libs/lib.pyx
  [82/152] Compiling C object pandas/_libs/tslibs/strptime.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_strptime.pyx.c.o
  [83/152] Linking target pandas/_libs/tslibs/strptime.cpython-310-x86_64-linux-gnu.so
  [84/152] Compiling Cython source /workspace/pandas/pandas/_libs/parsers.pyx
  [85/152] Compiling Cython source /workspace/pandas/pandas/_libs/hashtable.pyx
  [86/152] Compiling Cython source /workspace/pandas/pandas/_libs/ops_dispatch.pyx
  [87/152] Compiling C object pandas/_libs/tslibs/parsing.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_parsing.pyx.c.o
  [88/152] Linking target pandas/_libs/tslibs/parsing.cpython-310-x86_64-linux-gnu.so
  [89/152] Compiling Cython source /workspace/pandas/pandas/_libs/join.pyx
  [90/152] Compiling Cython source /workspace/pandas/pandas/_libs/properties.pyx
  [91/152] Compiling Cython source /workspace/pandas/pandas/_libs/ops.pyx
  [92/152] Compiling C object pandas/_libs/ops_dispatch.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_ops_dispatch.pyx.c.o
  [93/152] Linking target pandas/_libs/ops_dispatch.cpython-310-x86_64-linux-gnu.so
  [94/152] Compiling C object pandas/_libs/tslibs/period.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_period.pyx.c.o
  [95/152] Linking target pandas/_libs/tslibs/period.cpython-310-x86_64-linux-gnu.so
  [96/152] Compiling C object pandas/_libs/properties.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_properties.pyx.c.o
  [97/152] Compiling Cython source /workspace/pandas/pandas/_libs/reshape.pyx
  [98/152] Linking target pandas/_libs/properties.cpython-310-x86_64-linux-gnu.so
  [99/152] Compiling Cython source /workspace/pandas/pandas/_libs/byteswap.pyx
  [100/152] Compiling Cython source /workspace/pandas/pandas/_libs/sas.pyx
  [101/152] Compiling C object pandas/_libs/missing.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_missing.pyx.c.o
  [102/152] Compiling C object pandas/_libs/byteswap.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_byteswap.pyx.c.o
  [103/152] Linking target pandas/_libs/missing.cpython-310-x86_64-linux-gnu.so
  [104/152] Linking target pandas/_libs/byteswap.cpython-310-x86_64-linux-gnu.so
  [105/152] Compiling Cython source /workspace/pandas/pandas/_libs/testing.pyx
  [106/152] Compiling C object pandas/_libs/tslibs/timestamps.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_timestamps.pyx.c.o
  [107/152] Linking target pandas/_libs/tslibs/timestamps.cpython-310-x86_64-linux-gnu.so
  [108/152] Compiling C object pandas/_libs/internals.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_internals.pyx.c.o
  [109/152] Linking target pandas/_libs/internals.cpython-310-x86_64-linux-gnu.so
  [110/152] Compiling Cython source /workspace/pandas/pandas/_libs/tslib.pyx
  [111/152] Compiling C object pandas/_libs/tslibs/timedeltas.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_timedeltas.pyx.c.o
  [112/152] Linking target pandas/_libs/tslibs/timedeltas.cpython-310-x86_64-linux-gnu.so
  [113/152] Compiling Cython source /workspace/pandas/pandas/_libs/sparse.pyx
  [114/152] Compiling C object pandas/_libs/ops.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_ops.pyx.c.o
  [115/152] Compiling Cython source /workspace/pandas/pandas/_libs/writers.pyx
  [116/152] Linking target pandas/_libs/ops.cpython-310-x86_64-linux-gnu.so
  [117/152] Compiling Cython source /workspace/pandas/pandas/_libs/window/indexers.pyx
  [118/152] Compiling C object pandas/_libs/testing.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_testing.pyx.c.o
  [119/152] Linking target pandas/_libs/testing.cpython-310-x86_64-linux-gnu.so
  [120/152] Compiling Cython source /workspace/pandas/pandas/_libs/window/aggregations.pyx
  [121/152] Compiling C object pandas/_libs/reshape.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_reshape.pyx.c.o
  [122/152] Linking target pandas/_libs/reshape.cpython-310-x86_64-linux-gnu.so
  [123/152] Compiling C object pandas/_libs/sas.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_sas.pyx.c.o
  [124/152] Linking target pandas/_libs/sas.cpython-310-x86_64-linux-gnu.so
  [125/152] Compiling C object pandas/_libs/window/indexers.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_window_indexers.pyx.c.o
  [126/152] Linking target pandas/_libs/window/indexers.cpython-310-x86_64-linux-gnu.so
  [127/152] Compiling C object pandas/_libs/writers.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_writers.pyx.c.o
  [128/152] Linking target pandas/_libs/writers.cpython-310-x86_64-linux-gnu.so
  [129/152] Compiling C object pandas/_libs/tslib.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslib.pyx.c.o
  [130/152] Linking target pandas/_libs/tslib.cpython-310-x86_64-linux-gnu.so
  [131/152] Compiling C object pandas/_libs/parsers.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_parsers.pyx.c.o
  [132/152] Linking target pandas/_libs/parsers.cpython-310-x86_64-linux-gnu.so
  [133/152] Compiling C++ object pandas/_libs/window/aggregations.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_window_aggregations.pyx.cpp.o
  [134/152] Linking target pandas/_libs/window/aggregations.cpython-310-x86_64-linux-gnu.so
  [135/152] Compiling C object pandas/_libs/index.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_index.pyx.c.o
  [136/152] Linking target pandas/_libs/index.cpython-310-x86_64-linux-gnu.so
  [137/152] Compiling C object pandas/_libs/tslibs/offsets.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_offsets.pyx.c.o
  [138/152] Linking target pandas/_libs/tslibs/offsets.cpython-310-x86_64-linux-gnu.so
  [139/152] Compiling C object pandas/_libs/lib.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_lib.pyx.c.o
  [140/152] Linking target pandas/_libs/lib.cpython-310-x86_64-linux-gnu.so
  [141/152] Compiling C object pandas/_libs/sparse.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_sparse.pyx.c.o
  [142/152] Linking target pandas/_libs/sparse.cpython-310-x86_64-linux-gnu.so
  [143/152] Compiling C object pandas/_libs/interval.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_interval.pyx.c.o
  [144/152] Linking target pandas/_libs/interval.cpython-310-x86_64-linux-gnu.so
  [145/152] Compiling C object pandas/_libs/join.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_join.pyx.c.o
  [146/152] Linking target pandas/_libs/join.cpython-310-x86_64-linux-gnu.so
  [147/152] Compiling C object pandas/_libs/algos.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_algos.pyx.c.o
  [148/152] Linking target pandas/_libs/algos.cpython-310-x86_64-linux-gnu.so
  [149/152] Compiling C object pandas/_libs/groupby.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_groupby.pyx.c.o
  [150/152] Linking target pandas/_libs/groupby.cpython-310-x86_64-linux-gnu.so
  [151/152] Compiling C object pandas/_libs/hashtable.cpython-310-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_hashtable.pyx.c.o
  [152/152] Linking target pandas/_libs/hashtable.cpython-310-x86_64-linux-gnu.so
  Preparing editable metadata (pyproject.toml) ... done
Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/site-packages (from pandas==0+untagged.1.gd5bbd97) (1.26.4)
Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas==0+untagged.1.gd5bbd97) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas==0+untagged.1.gd5bbd97) (2024.1)
Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas==0+untagged.1.gd5bbd97) (2024.1)
Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==0+untagged.1.gd5bbd97) (1.16.0)
Building wheels for collected packages: pandas
  Running command Building editable for pandas (pyproject.toml)
  Building editable for pandas (pyproject.toml) ... done
  Created wheel for pandas: filename=pandas-0+untagged.1.gd5bbd97-cp310-cp310-linux_x86_64.whl size=33543 sha256=79eb7f15ae85d6c0e578a26bf5b074cb095a89e3875b24dfebe9d646c9483d49
  Stored in directory: /tmp/pip-ephem-wheel-cache-8vso3s_8/wheels/39/77/57/1c321003879eca324f847754963bcb179286b0b84b53b1dde9
Successfully built pandas
Installing collected packages: pandas
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
fastparquet 2024.5.0 requires pandas>=1.5.0, but you have pandas 0+untagged.1.gd5bbd97 which is incompatible.
pyreadstat 1.2.7 requires pandas>=1.2.0, but you have pandas 0+untagged.1.gd5bbd97 which is incompatible.
seaborn 0.13.2 requires pandas>=1.2, but you have pandas 0+untagged.1.gd5bbd97 which is incompatible.
xarray 2024.7.0 requires pandas>=2.0, but you have pandas 0+untagged.1.gd5bbd97 which is incompatible.
Successfully installed pandas-0+untagged.1.gd5bbd97
pre-commit installed at .git/hooks/pre-commit
Using pip 24.2 from /usr/local/lib/python3.10/site-packages/pip (python 3.10)
Defaulting to user installation because normal site-packages is not writeable
Obtaining file:///workspace/pandas
  Running command Checking if build backend supports build_editable
  Checking if build backend supports build_editable ... done
  Running command Preparing editable metadata (pyproject.toml)
  + meson setup --reconfigure /workspace/pandas /workspace/pandas/build/cp310 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=/workspace/pandas/build/cp310/meson-python-native-file.ini
  Cleaning... 0 files.
  The Meson build system
  Version: 1.2.1
  Source dir: /workspace/pandas
  Build dir: /workspace/pandas/build/cp310
  Build type: native build
  Project name: pandas
  Project version: 2.2.0.dev0+2274.gd5bbd97345
  C compiler for the host machine: cc (gcc 10.2.1 ""cc (Debian 10.2.1-6) 10.2.1 20210110"")
  C linker for the host machine: cc ld.bfd 2.35.2
  C++ compiler for the host machine: c++ (gcc 10.2.1 ""c++ (Debian 10.2.1-6) 10.2.1 20210110"")
  C++ linker for the host machine: c++ ld.bfd 2.35.2
  Cython compiler for the host machine: cython (cython 3.0.11)
  Host machine cpu family: x86_64
  Host machine cpu: x86_64
  Program python found: YES (/usr/local/bin/python)
  Build targets in project: 54

  pandas 2.2.0.dev0+2274.gd5bbd97345

    User defined options
      Native files: /workspace/pandas/build/cp310/meson-python-native-file.ini
      buildtype   : release
      vsenv       : True
      b_ndebug    : if-release
      b_vscrt     : md

  Found ninja-1.11.1.git.kitware.jobserver-1 at /usr/local/bin/ninja

  Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
  /usr/local/bin/meson compile -C .
  + /usr/local/bin/ninja
  [1/1] Generating write_version_file with a custom command
  Preparing editable metadata (pyproject.toml) ... done
Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/site-packages (from pandas==2.2.0.dev0+2274.gd5bbd97345) (1.26.4)
Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas==2.2.0.dev0+2274.gd5bbd97345) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas==2.2.0.dev0+2274.gd5bbd97345) (2024.1)
Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas==2.2.0.dev0+2274.gd5bbd97345) (2024.1)
Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.0.dev0+2274.gd5bbd97345) (1.16.0)
Building wheels for collected packages: pandas
  Running command Building editable for pandas (pyproject.toml)
  Building editable for pandas (pyproject.toml) ... done
  Created wheel for pandas: filename=pandas-2.2.0.dev0+2274.gd5bbd97345-cp310-cp310-linux_x86_64.whl size=33657 sha256=daf4cb77e643e000d8a22b50e7d228127e739772ec5c687040f2a9fee9b0b81b
  Stored in directory: /tmp/pip-ephem-wheel-cache-va_pbd3a/wheels/39/77/57/1c321003879eca324f847754963bcb179286b0b84b53b1dde9
Successfully built pandas
Installing collected packages: pandas
  Attempting uninstall: pandas
    Found existing installation: pandas 0+untagged.1.gd5bbd97
    Uninstalling pandas-0+untagged.1.gd5bbd97:
      Removing file or directory /home/gitpod/.local/lib/python3.10/site-packages/
      Successfully uninstalled pandas-0+untagged.1.gd5bbd97
Successfully installed pandas-2.2.0.dev0+2274.gd5bbd97345
✨ Pre-build complete! You can close this terminal ✨ 
```

</details>


Reading around, seems like it could be a shallow clone/missing tags problem, but `git fetch --unshallow` happily reports that the repo is indeed not shallow, and `git tag` reports a lot of tags:

```
$ git fetch --unshallow 
fatal: --unshallow on a complete repository does not make sense

$ git tag | wc -l
157
```

### Suggested fix for documentation

By digging a bit, I feel that the Gitpod files has been touched after the documentation has been written, and the screenshots/information are not really up-to-date.
Also, after https://github.com/pandas-dev/pandas/pull/54046 I think that most of the `gitpod` directory is now unused and might be deleted(?)

I'm willing to help with the housekeeping if you have any advices... And if you have ideas on how to make `pytest` run, I'm interested!
","['Build', 'Docs']",2024-08-09 13:07:45,2025-11-07 17:53:32,6,closed
59458,DOC: fix docstring validation errors for `pandas.Timestamp`,"follow up on issues #56804 and #58063
pandas has a script for validating docstrings:

https://github.com/pandas-dev/pandas/blob/0cdc6a48302ba1592b8825868de403ff9b0ea2a5/ci/code_checks.sh#L206-L244

Currently, some methods fail docstring validation check.
The task here is:
* take 2-4 methods
* run: `scripts/validate_docstrings.py <method-name>`
*  fix the docstrings according to whatever error is reported
* remove those methods from `code_checks.sh` script
* commit, push, open pull request

Example:
```
scripts/validate_docstrings.py pandas.Timestamp.tz_localize
```
pandas.Timestamp.tz_localize fails with the  SA01  error
```
################################################################################
################################## Validation ##################################
################################################################################

1 Errors found for `pandas.Timestamp.tz_localize`:
        SA01    See Also section not found
```

Please don't comment `take` as multiple people can work on this issue. You also don't need to ask for permission to work on this, just comment on which methods are you going to work.

If you're new contributor, please check the [contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)
","['Docs', 'Code Style', 'good first issue']",2024-08-09 11:59:24,2025-04-07 16:57:54,29,closed
59454,"BUG: escapechar=',' Causes Double Commas in Output in Pandas 2.2.2","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Example DataFrame
df = pd.DataFrame({'column': ['value1, with comma', 'value2, with another comma']})

# Exporting DataFrame to CSV with escapechar=','
df.to_csv('output.csv', escapechar=',', index=False)
```


### Issue Description

When using escapechar=',' in Pandas version 2.2.2, the resulting output includes double commas ,, in places where the data originally contained a comma. This behavior is inconsistent with Pandas version 1.4.3, where the escapechar did not cause this issue.

Steps to Reproduce:

1. Create a DataFrame with a column containing commas in its values.
2. Export the DataFrame to a CSV file using escapechar=','.
3. Observe the output file and note the double commas ,, where there was originally a single comma.

### Expected Behavior

The output CSV should properly escape commas without doubling them. The behavior should be consistent with previous Pandas versions where the escape character was used correctly.

### Installed Versions

<details>

2.2.2

</details>
","['Bug', 'IO CSV', 'Upstream issue']",2024-08-09 01:24:25,2024-08-18 13:02:58,5,closed
59452,"BUG: Series.replace(dict-like, dict-like) raises uninformative AttributeError","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [2]: pd.Series([1, 2]).replace(pd.Series([1]), pd.Series([2]))
AttributeError: 'Series' object has no attribute '_replace_columnwise'
```


### Issue Description

It appears `.replace(dict-like, dict-like)` is only valid if the original object is a `DataFrame` so this should probably raise a `ValueError` instead of raising an `AttributeError`

### Expected Behavior

Raise a `ValueError`

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Enhancement', 'Error Reporting', 'replace']",2024-08-08 19:51:56,2024-08-21 19:30:10,1,closed
59445,QST: How to use 'numba' for group by sum,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/78845935/qst-how-to-use-numba-for-group-by-sum

### Question about pandas


```
import numba

@numba.njit
def sum_func(index, values):
    return pd.Series(np.sum(values), index=index)

df_long_control[['user_id', 'reward_type', 'reward']].groupby(['user_id', 'reward_type'], as_index=False).apply(sum_func, engine='numba')
```
I'm trying to group by `user_id` and `reward_type` and apply sum function using numba, but got an error
```
TypeError: sum_func() got an unexpected keyword argument 'engine'
```","['Usage Question', 'Needs Triage']",2024-08-07 22:44:38,2024-08-07 23:15:44,1,closed
59440,BUG: assert_frame_equal does not include the obj parameter in error when a MultiIndex is different,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas import DataFrame
from pandas.testing import assert_frame_equal

df_1 = DataFrame(
    {
        ""name"": [""John"", ""Anna"", ""Peter"", ""Linda""],
        ""surname"": [""Smith"", ""Jones"", ""Brown"", ""Wilson""],
        ""age"": [23, 36, 33, 26],
        ""city"": [""New York"", ""Paris"", ""Berlin"", ""London""],
    }
).set_index([""name"", ""surname""])


df_surnames = (
    df_1.reset_index()
    .assign(surname=[""A"", ""B"", ""C"", ""D""])
    .set_index([""name"", ""surname""])
)
try:
    assert_frame_equal(df_1, df_surnames, obj=""df_surnames"")
except AssertionError as e:
    print(f""AssertionError: {e}"")
    # AssertionError: MultiIndex level [1] are different <== the obj parameter is ignored!

# I prepared a repo for this so here's the link: https://github.com/serl/pandas-assert-index-bug/blob/main/main.py
```


### Issue Description

`assert_frame_equal` has the `obj` argument which we use and abuse to understand where's the problem in our test suite (more info: we have dozens of dataframes in a ""data store"" object, and we implemented a little function to compare two stores; the `obj` is very very useful to point to the faulty dataframe).

So for example, in the example above, if we had a difference in the `cities` column, `assert_frame_equal(df_1, df_cities, obj=""df_cities"")` would say `df_cities.iloc[:, 1] (column name=""city"") are different`.
Problem is: for MultiIndex, the `obj` is not passed down and we have `MultiIndex level [1] are different`.


### Expected Behavior

Having `obj` somewhere in the exception message. Maybe `df_surnames.index[1] are different`?

The problem seems to be [here](https://github.com/pandas-dev/pandas/blob/main/pandas/_testing/asserters.py#L286). I would love to prepare a little PR to tackle this if we agree on a solution

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:16:51 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Enhancement', 'Testing']",2024-08-07 16:30:27,2024-09-03 21:53:02,4,closed
59438,"BUG: pandas.DataFrame.plot crashes, if subplots argument receives a touple.","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

df = pd.DataFrame(
  np.random.rand(10, 3),
  columns=['A', 'B', 'C']
)

# Works:
df.plot(
  subplots=[
    ['A', 'B'],
    ['C']
  ]
)

# Crashes:
df.plot(
  subplots=[
    ('A', 'B'),
    ('C')
  ]
)
```


### Issue Description

The df.plot() command will crash, if it receives tuples in a list as arguments, but the documentation states: 

> sequence of iterables of column labels: Create a subplot for each group of columns. For example **[(‘a’, ‘c’), (‘b’, ‘d’)]** will create (...)


### Expected Behavior

do not crash.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : DE
LOCALE                : German_Switzerland.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : 2024.2.0
fsspec                : 2024.3.1
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.60.0
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 14.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : 2024.7.0
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-08-07 08:59:00,2024-08-07 18:16:18,3,closed
59432,BUG: dropna in pivot_table is not affecting the outcome,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
What is the use of dropna parameter if both True and False yield same result?

import pandas as pd
import seaborn as sns

# Load the dataset
df = sns.load_dataset(""penguins"")

df=df.melt( id_vars=['species','island','sex'],
         value_vars=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'], 
         var_name='metric', 
         value_name='value'
       )

k1 = df.pivot_table(index='island', columns='metric', values='value',
                                   aggfunc=['mean', 'count','size'], dropna=False,observed=False).reset_index()
k2 = df.pivot_table(index='island', columns='metric', values='value',
                                   aggfunc=['mean', 'count','size'], dropna=True,observed=False).reset_index()

print(""Pivot table with dropna=False:\n"")
print(k1)
print(""\n\nPivot table with dropna=True:\n"")
print(k2)
```


### Issue Description

Regardless of whether dropna parameter is set as True or False the output remains the same.

```
Pivot table with dropna=False:

           island          mean                                                \
metric            bill_depth_mm bill_length_mm  body_mass_g flipper_length_mm   
0          Biscoe     15.874850      45.257485  4716.017964        209.706587   
1           Dream     18.344355      44.167742  3712.903226        193.072581   
2       Torgersen     18.429412      38.950980  3706.372549        191.196078   

               count                                               \
metric bill_depth_mm bill_length_mm body_mass_g flipper_length_mm   
0                167            167         167               167   
1                124            124         124               124   
2                 51             51          51                51   

                size                                               
metric bill_depth_mm bill_length_mm body_mass_g flipper_length_mm  
0                168            168         168               168  
1                124            124         124               124  
2                 52             52          52                52  


Pivot table with dropna=True:

           island          mean                                                \
metric            bill_depth_mm bill_length_mm  body_mass_g flipper_length_mm   
0          Biscoe     15.874850      45.257485  4716.017964        209.706587   
1           Dream     18.344355      44.167742  3712.903226        193.072581   
2       Torgersen     18.429412      38.950980  3706.372549        191.196078   

               count                                               \
metric bill_depth_mm bill_length_mm body_mass_g flipper_length_mm   
0                167            167         167               167   
1                124            124         124               124   
2                 51             51          51                51   

                size                                               
metric bill_depth_mm bill_length_mm body_mass_g flipper_length_mm  
0                168            168         168               168  
1                124            124         124               124  
2                 52             52          52                52  
```

### Expected Behavior

Expected behaviour is that result should be different.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.6.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_CA.UTF-8
LOCALE                : en_CA.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 58.0.4
pip                   : 21.2.4
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.1.1
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 15.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-08-06 23:09:18,2024-08-06 23:21:20,0,closed
59429,BUG: stacked bar graphs show invalid label position due to invalid rectangle bottom when data is 0,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import matplotlib.pyplot as plt
permutations = [(a,b,c) for a in range(2) for b in range(2) for c in range(3)]
data = [
    {'i': i, 'a':a, 'b':b, 'c':c, 't': a+b+c}
    for i, (a,b,c) in enumerate(permutations)
]
df = pd.DataFrame.from_dict(data)
ax = df[['a','b', 'c']].plot.bar(stacked=True)
bl = ax.bar_label(ax.containers[-1], df['t'])
plt.show()
```


### Issue Description

if the top part of the stacked plot has data value 0, the bar-label does not appear on top, but at the bottom of the bar.
![grafik](https://github.com/user-attachments/assets/2c839adf-44e6-445e-bba5-0af33eef04af)

Further debugging shows that all bars with data = 0 have their y position set to 0.0. They should have the top of the bar below as their bottom = y.

### Expected Behavior

Bar-Labels should be positioned on top for all stacks.

![grafik](https://github.com/user-attachments/assets/58c5bc3d-1e21-41bc-8804-f2f5516ae593)

this behaviour can be produced by correcting the y positions of the defective bars

```
def correct_stack(container, info=False):
    """""" correct the y positions of stacked bars with 0 height

    This is needed because the y position is calculated wrongly when data value is 0 on stacked bars created by Pandas plot.bar.
    """"""
    # Attention, since we start at row 1, r shows to the row below - which we need
    for r, row in enumerate(container[1:]):
        for b, bar in enumerate(row):
            (my_x, my_y), my_height = bar.xy, bar.get_height()
            # note that r show to the bar below the current bar, and c is the stack
            support = container[r][b]    # this is the bar we are resting on top of
            (s_x, s_y), s_height = support.xy, support.get_height()
            if info:
                print(f""bar at row: {r+1}, col: {b}: ({my_x}, {my_y}) - {my_height} resting on top of ({s_x, s_y}) - {s_height}"")
            if my_y < s_y + s_height:
                print(f""bar at row: {r+1}, col: {b}: {my_y = } is lower than expected {s_y + s_height}"")
                bar.xy = (my_x, s_y + s_height)

ax2 = df[['a','b','c']].plot.bar(stacked=True)
correct_stack(ax2.containers)
bl = ax2.bar_label(ax2.containers[-1], df['t'])
plt.show()
```


### Installed Versions

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : de_DE.cp1252

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'Visualization']",2024-08-06 20:37:42,2024-11-07 21:32:56,11,closed
59428,BUG: Inconsistent behavior between `.any` and `.all` with missing values,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> import numpy as np
>>> import pandas as pd

>>> x = pd.DataFrame({""a"": [None], ""b"": [np.nan], ""c"": [0]})
>>> x
      a   b  c
0  None NaN  0

# DataFrame.any returns all False values
>>> x.any(axis=""index"")
a    False
b    False
c    False
dtype: bool

# However, DataFrame.all returns True for columns ""a"" and ""b"".
>>> x.all(axis=""index"")
a     True
b     True
c    False
```


### Issue Description

When a column contains all missing values, `DataFrame.any` and `DataFrame.all` can yield inconsistent results.  Intuitively, if `all` returns `True`, then presumably `any` should as well.

### Expected Behavior

`DataFrame.any` should be `True` wherever `DataFrame.all` returns `True`.  However, it's not clear if in the above examples `all` should be returning `True`.

### Installed Versions

<details>

```
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.10.2-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Sat, 27 Jul 2024 16:49:55 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 71.1.0
pip                   : 23.2.1
Cython                : None
pytest                : 8.3.1
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```
</details>
","['Bug', 'Needs Triage']",2024-08-06 17:07:04,2024-08-06 17:40:07,1,closed
59426,Python 3.13 wheel builds are failing,"When testing the wheels, about 80 of these types of errors happen, on Python 3.13 only:
      
```
  >       assert isinstance(left, ExtensionArray), ""left is not an ExtensionArray""
  E       AssertionError: left is not an ExtensionArray
```
See any [recent build on the main branch](https://github.com/pandas-dev/pandas/actions/workflows/wheels.yml?query=branch%3Amain) (like [this one](https://github.com/pandas-dev/pandas/actions/runs/10259795788/job/28384829134), full [raw log](https://productionresultssa3.blob.core.windows.net/actions-results/df87c058-0456-4e3f-9980-f00d7633a7e9/workflow-job-run-02e0c0ba-cd95-5520-7c99-3c9504ffc697/logs/job/job-logs.txt?rsct=text%2Fplain&se=2024-08-06T07%3A20%3A17Z&sig=N5PpHUCBAn%2BmLmrOwCcI4i0MV0%2BWqnBgel8jiiXZXb8%3D&ske=2024-08-06T17%3A46%3A09Z&skoid=ca7593d4-ee42-46cd-af88-8b886a2f84eb&sks=b&skt=2024-08-06T05%3A46%3A09Z&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skv=2024-05-04&sp=r&spr=https&sr=b&st=2024-08-06T07%3A10%3A12Z&sv=2024-05-04)) for more details.

It might be something to do with the `ExtensionArray`, since that seems to be the common theme in the errors.


It seems this was the [last passing wheel build](https://github.com/pandas-dev/pandas/actions/runs/10105253109), which ran on July 26th. It did include successful Python 3.13 wheel builds.",[],2024-08-06 07:12:06,2024-08-06 07:59:30,2,closed
59422,BUG: UnboundLocalError when full outer merging two dataframes,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from datetime import datetime

data_A = list()
data_B = list()

for i in range(10):
    data_A.append({
        ""id"": i,
        ""created_date"": datetime.today(),
        ""created_at"": datetime.now(),
    })

    data_B.append({
        ""id"": i if i % 2 == 0 else 3*i,
        ""created_date"": datetime.today(),
        ""created_at"": datetime.now(),
    })

df_A = pd.DataFrame.from_dict(data_A)

df_B = pd.DataFrame.from_dict(data_B)

df_A.merge(df_B, how=""full"", on=""id"")
```

```
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
Cell In[59], line 23
     19 df_A = pd.DataFrame.from_dict(data_A)
     21 df_B = pd.DataFrame.from_dict(data_B)
---> 23 df_A.merge(df_B, how=""full"", on=""id"")

File ~/Documents/adhoc/.local_lab/lib/python3.12/site-packages/pandas/core/frame.py:10832, in DataFrame.merge(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
  10813 @Substitution("""")
  10814 @Appender(_merge_doc, indents=2)
  10815 def merge(
   (...)
  10828     validate: MergeValidate | None = None,
  10829 ) -> DataFrame:
  10830     from pandas.core.reshape.merge import merge
> 10832     return merge(
  10833         self,
  10834         right,
  10835         how=how,
  10836         on=on,
  10837         left_on=left_on,
  10838         right_on=right_on,
  10839         left_index=left_index,
  10840         right_index=right_index,
  10841         sort=sort,
  10842         suffixes=suffixes,
  10843         copy=copy,
  10844         indicator=indicator,
  10845         validate=validate,
  10846     )

File ~/Documents/adhoc/.local_lab/lib/python3.12/site-packages/pandas/core/reshape/merge.py:184, in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
    169 else:
    170     op = _MergeOperation(
    171         left_df,
    172         right_df,
   (...)
    182         validate=validate,
    183     )
--> 184     return op.get_result(copy=copy)

File ~/Documents/adhoc/.local_lab/lib/python3.12/site-packages/pandas/core/reshape/merge.py:886, in _MergeOperation.get_result(self, copy)
    883 if self.indicator:
    884     self.left, self.right = self._indicator_pre_merge(self.left, self.right)
--> 886 join_index, left_indexer, right_indexer = self._get_join_info()
    888 result = self._reindex_and_concat(
    889     join_index, left_indexer, right_indexer, copy=copy
    890 )
    891 result = result.__finalize__(self, method=self._merge_type)

File ~/Documents/adhoc/.local_lab/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1151, in _MergeOperation._get_join_info(self)
   1147     join_index, right_indexer, left_indexer = _left_join_on_index(
   1148         right_ax, left_ax, self.right_join_keys, sort=self.sort
   1149     )
   1150 else:
-> 1151     (left_indexer, right_indexer) = self._get_join_indexers()
   1153     if self.right_index:
   1154         if len(self.left) > 0:

File ~/Documents/adhoc/.local_lab/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1125, in _MergeOperation._get_join_indexers(self)
   1123 # make mypy happy
   1124 assert self.how != ""asof""
-> 1125 return get_join_indexers(
   1126     self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how
   1127 )

File ~/Documents/adhoc/.local_lab/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1759, in get_join_indexers(left_keys, right_keys, sort, how)
   1757     _, lidx, ridx = left.join(right, how=how, return_indexers=True, sort=sort)
   1758 else:
-> 1759     lidx, ridx = get_join_indexers_non_unique(
   1760         left._values, right._values, sort, how
   1761     )
   1763 if lidx is not None and is_range_indexer(lidx, len(left)):
   1764     lidx = None

File ~/Documents/adhoc/.local_lab/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1802, in get_join_indexers_non_unique(left, right, sort, how)
   1800 elif how == ""outer"":
   1801     lidx, ridx = libjoin.full_outer_join(lkey, rkey, count)
-> 1802 return lidx, ridx

UnboundLocalError: cannot access local variable 'lidx' where it is not associated with a value
```


### Issue Description

I wrote a full outer join between 2 dataframes and it resulted on UnboundLocalError.

### Expected Behavior

Have a joined dataframe of the two dataframes, where half of the rows would match and the other half wouldn't.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-45-generic
Version               : #45~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Jul 15 16:40:02 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 71.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.31
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Enhancement', 'Reshaping', 'Error Reporting']",2024-08-05 18:09:58,2024-08-22 15:24:06,3,closed
59421,BUG: merging DataFrames on a column containing just NaN values triggers address violation in `safe_sort`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

df1 = pd.DataFrame(
    {'x': [1, 2, 3], 'y': [np.nan, np.nan, np.nan], 'z': [4, 5, 6]}
)
df2 = pd.DataFrame(
    {'x': [1, 2, 3], 'y': [np.nan, np.nan, np.nan], 'zz': [4, 5, 6]}
)
df1.merge(df2, on=['x', 'y'], how='outer')
```

### Issue Description

Related to #55984

Merging DataFrames on a column containing all NaN values results in a 
This was not present in 2.1.4 and I think was introduced in #55984 (which fixed other address violations).

Found using asan, can also seen by enabling bounds_checking on `take_1d_*` in [algos_take_helper.pxi.in](https://github.com/pandas-dev/pandas/blob/main/pandas/_libs/algos_take_helper.pxi.in#L67)

My understanding of the cause is:

1. `uniques` is an empty array in `_factorize_keys` - https://github.com/pandas-dev/pandas/blob/main/pandas/core/reshape/merge.py#L2706
2. The mask set in `safe_sort` assumes that the array being sorted is at least size 1 - https://github.com/pandas-dev/pandas/blob/main/pandas/core/algorithms.py#L1531
3. The masked indices are set to 0.
4. `take_nd` assumes the indexer contains no out-of-bounds indices, but an index of 0 is out of bounds in this case.

I am not familiar with pandas internals but changing the mask on https://github.com/pandas-dev/pandas/blob/main/pandas/core/algorithms.py#L1531 to

```
mask = (codes < min(-len(values), -1)) | (codes >= len(values))
```

avoids this out-of-bounds access. Is this a suitable fix? If so, I can prepare a pull request.

### Expected Behavior

No array bounds access errors, should produce
```
   x   y  z  zz
0  1 NaN  4   4
1  2 NaN  5   5
2  3 NaN  6   6
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 642d2446060afb11f9860c79a7339eb6ec96fea7
python                : 3.11.9
python-bits           : 64
OS                    : Linux
OS-release            : 6.6.15-2rodete2-amd64
Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.6.15-2rodete2 (2024-03-19)
machine               : x86_64
processor             :
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1287.g642d244606.dirty
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : 3.0.11
sphinx                : 8.0.2
IPython               : 8.26.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.4.0
fastparquet           : 2024.5.0
fsspec                : 2024.6.1
html5lib              : 1.1
hypothesis            : 6.108.8
gcsfs                 : 2024.6.1
jinja2                : 3.1.4
lxml.etree            : 5.2.2
matplotlib            : 3.9.0
numba                 : 0.60.0
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 17.0.0
pyreadstat            : 1.2.7
pytest                : 8.3.2
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.6.1
scipy                 : 1.14.0
sqlalchemy            : 2.0.31
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.7.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.23.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Reshaping', 'Needs Info']",2024-08-05 18:06:43,2024-08-12 20:03:25,4,closed
59420,Unit Test/CI failing for multiple PRs,"Looks like there are issues with several of the unit tests. Below are just some examples of PRs affected. 

[https://github.com/pandas-dev/pandas/actions/runs/10222022864](file:///private/var/containers/Bundle/Application/F7D0D6DA-C7B8-499B-ADB0-0D5334DE3F46/Bloomberg_Professional.app/msgweb/mobile_rich_text/mobile_rich_text.html#)
[https://github.com/pandas-dev/pandas/actions/runs/10252584958](file:///private/var/containers/Bundle/Application/F7D0D6DA-C7B8-499B-ADB0-0D5334DE3F46/Bloomberg_Professional.app/msgweb/mobile_rich_text/mobile_rich_text.html#)
[https://github.com/pandas-dev/pandas/actions/runs/10230701585](file:///private/var/containers/Bundle/Application/F7D0D6DA-C7B8-499B-ADB0-0D5334DE3F46/Bloomberg_Professional.app/msgweb/mobile_rich_text/mobile_rich_text.html#)
[https://github.com/pandas-dev/pandas/actions/runs/10239338109](file:///private/var/containers/Bundle/Application/F7D0D6DA-C7B8-499B-ADB0-0D5334DE3F46/Bloomberg_Professional.app/msgweb/mobile_rich_text/mobile_rich_text.html#)",[],2024-08-05 17:47:25,2024-08-05 18:30:44,2,closed
59417,Unable to build pandas from source on Windows,"```
pip install pandas
Collecting pandas
  Using cached pandas-2.2.2.tar.gz (4.4 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [32 lines of output]
      + meson setup C:\Users\ךינשגכהד\AppData\Local\Temp\pip-install-12u0d8nj\pandas_dbfe103136b3466e8b5b75d423059088 C:\Users\ךינשגכהד\AppData\Local\Temp\pip-install-12u0d8nj\pandas_dbfe103136b3466e8b5b75d423059088\.mesonpy-ta2thafr\build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=C:\Users\ךינשגכהד\AppData\Local\Temp\pip-install-12u0d8nj\pandas_dbfe103136b3466e8b5b75d423059088\.mesonpy-ta2thafr\build\meson-python-native-file.ini
      The Meson build system
      Version: 1.2.1
      Source dir: C:\Users\ךינשגכהד\AppData\Local\Temp\pip-install-12u0d8nj\pandas_dbfe103136b3466e8b5b75d423059088
      Build dir: C:\Users\ךינשגכהד\AppData\Local\Temp\pip-install-12u0d8nj\pandas_dbfe103136b3466e8b5b75d423059088\.mesonpy-ta2thafr\build
      Build type: native build
      Project name: pandas
      Project version: 2.2.2
      Activating VS 17.11.0 Preview 5.0
      C compiler for the host machine: sccache cl (msvc 19.41.34117 ""Microsoft (R) C/C++ Optimizing Compiler Version 19.41.34117 for x64"")
      C linker for the host machine: link link 14.41.34117.0
      C++ compiler for the host machine: sccache cl (msvc 19.41.34117 ""Microsoft (R) C/C++ Optimizing Compiler Version 19.41.34117 for x64"")
      C++ linker for the host machine: link link 14.41.34117.0
      Cython compiler for the host machine: cython (cython 3.0.5)
      Host machine cpu family: x86_64
      Host machine cpu: x86_64
      Program python found: YES (C:\Users\ךינשגכהד\scoop\apps\python-alpha\current\python.exe)

      ..\..\pandas\meson.build:1:15: ERROR: Command `C:\Users\ךינשגכהד\scoop\apps\python-alpha\current\python.exe -c ""
      import os
      import numpy as np
      try:
          # Check if include directory is inside the pandas dir
          # e.g. a venv created inside the pandas dir
          # If so, convert it to a relative path
          incdir = os.path.relpath(np.get_include())
      except Exception:
          incdir = np.get_include()
      print(incdir)
           ""` failed with status 3221225477.

      A full log can be found at C:\Users\ךינשגכהד\AppData\Local\Temp\pip-install-12u0d8nj\pandas_dbfe103136b3466e8b5b75d423059088\.mesonpy-ta2thafr\build\meson-logs\meson-log.txt
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

```
python -c ""import numpy;print(numpy.version.version)""
2.1.0.dev0+git20240728.fdcbd0e
```","['Build', 'Windows', 'Closing Candidate']",2024-08-05 14:48:15,2024-08-25 18:17:26,13,closed
59415,"BUG: Series' lt, gt, le, ge, ne, eq methods are raising a downcasting warning even though the Series' dtype is float64.","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np



```python
nan_s = pd.Series([np.nan] * len(s), dtype=""float"")
nan_s.dtype
```


```python
nan_s.lt(0, fill_value=False)
nan_s.gt(0, fill_value=False)
nan_s.le(0, fill_value=False)
nan_s.ge(0, fill_value=False)
nan_s.ne(0, fill_value=False)
nan_s.eq(0, fill_value=False)

```

    /tmp/ipykernel_127420/829981878.py:1: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      nan_s.lt(0, fill_value=False)
    /tmp/ipykernel_127420/829981878.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      nan_s.gt(0, fill_value=False)
    /tmp/ipykernel_127420/829981878.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      nan_s.le(0, fill_value=False)
    /tmp/ipykernel_127420/829981878.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      nan_s.ge(0, fill_value=False)
    /tmp/ipykernel_127420/829981878.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      nan_s.ne(0, fill_value=False)
    /tmp/ipykernel_127420/829981878.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      nan_s.eq(0, fill_value=False)





    0    True
    1    True
    2    True
    3    True
    4    True
    5    True
    6    True
    7    True
    dtype: bool
```


### Issue Description

The lt, gt, le, ge, ne, eq methods are raising a downcasting warning even though the Series' dtype is float64.
This issue only seems to occur when the series is full nans and that fill_value is specified.

### Expected Behavior

The methods should not raise a downcast warning.
This is my first issue so hopefully this is an appropriate expectation. 

### Installed Versions

    INSTALLED VERSIONS
    ------------------
    commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
    python                : 3.11.3.final.0
    python-bits           : 64
    OS                    : Linux
    OS-release            : 6.1.0-23-amd64
    Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.1.99-1 (2024-07-15)
    machine               : x86_64
    processor             : 
    byteorder             : little
    LC_ALL                : None
    LANG                  : en_US.UTF-8
    LOCALE                : en_US.UTF-8
    
    pandas                : 2.2.2
    numpy                 : 1.26.4
    pytz                  : 2024.1
    dateutil              : 2.9.0
    setuptools            : 71.0.4
    pip                   : 23.1.2
    Cython                : None
    pytest                : None
    hypothesis            : None
    sphinx                : None
    blosc                 : None
    feather               : None
    xlsxwriter            : None
    lxml.etree            : None
    html5lib              : None
    pymysql               : None
    psycopg2              : None
    jinja2                : 3.1.3
    IPython               : 8.21.0
    pandas_datareader     : None
    adbc-driver-postgresql: None
    adbc-driver-sqlite    : None
    bs4                   : 4.12.3
    bottleneck            : None
    dataframe-api-compat  : None
    fastparquet           : None
    fsspec                : 2023.6.0
    gcsfs                 : None
    matplotlib            : 3.8.2
    numba                 : 0.58.1
    numexpr               : None
    odfpy                 : None
    openpyxl              : None
    pandas_gbq            : None
    pyarrow               : 15.0.0
    pyreadstat            : None
    python-calamine       : None
    pyxlsb                : None
    s3fs                  : 2023.6.0
    scipy                 : 1.11.4
    sqlalchemy            : None
    tables                : None
    tabulate              : 0.9.0
    xarray                : None
    xlrd                  : None
    zstandard             : None
    tzdata                : 2023.4
    qtpy                  : None
    pyqt5                 : None
","['Bug', 'Missing-data', 'Warnings']",2024-08-05 13:06:04,2024-08-05 20:39:04,2,closed
59411,BUG: DataFrame.astype(dict) causes mypy error if dict includes 'datetime64[ns]' among mixed types,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame({'ID':[], 'col_2':[]})
dtypes = dict(ID=int, col_2='datetime64[ns]')
df2 = df.astype(dtypes)
print(df2.dtypes)
```


### Issue Description

Output is:

    ID                int64
    col_1    datetime64[ns]
    dtype: object

Seems fine. But running mypy we get:

    error: Argument 1 to ""astype"" of ""DataFrame"" has incompatible type ""dict[str, object]""; expected ""Literal['bool', 'boolean', '?', 'b1', 'bool8', 'bool_', 'bool[pyarrow]', 'boolean[pyarrow]'] | type[builtins.bool] | BooleanDtype | type[numpy.bool] | Literal['int', 'Int8', 'Int16', 'Int32', 'Int64', 'b', 'i1', 'int8', 'byte', 'h', 'i2', 'int16', 'short', 'i', 'i4', 'int32', 'intc', 'l', 'i8', 'int64', 'int_', 'long', 'q', 'longlong', 'p', 'intp', 'int0', 'int8[pyarrow]', 'int16[pyarrow]', 'int32[pyarrow]', 'int64[pyarrow]'] | type[int] | Int8Dtype | Int16Dtype | Int32Dtype | Int64Dtype | <6 more items> | Literal['UInt8', 'UInt16', 'UInt32', 'UInt64', 'B', 'u1', 'uint8', 'ubyte', 'H', 'u2', 'uint16', 'ushort', 'I', 'u4', 'uint32', 'uintc', 'L', 'u8', 'uint', 'ulong', 'uint64', 'Q', 'ulonglong', 'P', 'uintp', 'uint0', 'uint8[pyarrow]', 'uint16[pyarrow]', 'uint32[pyarrow]', 'uint64[pyarrow]'] | UInt8Dtype | UInt16Dtype | UInt32Dtype | UInt64Dtype | type[unsignedinteger[Any]] | type[unsignedinteger[Any]] | type[unsignedinteger[Any]] | type[unsignedinteger[Any]] | type[unsignedinteger[Any]] | type[unsignedinteger[Any]] | Literal['str', 'string', 'U', 'str_', 'str0', 'unicode', 'unicode_', 'string[pyarrow]'] | type[str] | StringDtype | type[str_] | Literal['bytes', 'S', 'a', 'bytes_', 'bytes0', 'string_', 'binary[pyarrow]'] | type[bytes] | type[bytes_] | <8 more items> | Mapping[Any, ExtensionDtype | str | dtype[generic] | type[str] | type[complex] | type[builtins.bool] | type[object]] | Series[Any]""  [arg-type]
    Found 1 error in 1 file (checked 1 source file)


Note that if both types are `datetime64[ns]` then mypy is fine with it.

### Expected Behavior

I don't see anything in the docs for astype that would forbid this, and indeed the code seems to work fine. So I would expect mypy not to report an error for this.

### Installed Versions

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-44-generic
Version               : #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 18 14:36:16 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_AU.UTF-8
LOCALE                : en_AU.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 72.1.0
pip                   : 24.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'Needs Triage']",2024-08-05 01:50:07,2024-08-05 20:31:11,3,closed
59405,DOC: std() delta degree of freedom vs. degree of freedom,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.std.html#pandas.core.groupby.DataFrameGroupBy.std

### Documentation problem

I think the ddof parameter should mean ""Delta Degrees of Freedom"" instead of ""Degrees of freedom"", which is a bit confusing since ""Degrees of freedom"" = N - ""Delta Degrees of Freedom""


### Suggested fix for documentation

change the documentation of ddof parameter to ""Delta Degrees of Freedom"". Like that of the numpy documentation: https://numpy.org/doc/stable/reference/generated/numpy.std.html
",['Docs'],2024-08-04 15:02:44,2024-08-06 17:03:56,2,closed
59402, module 'pandas' has no attribute '__version__' ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from io import StringIO
csv_data = """"""col1,col2
1,foo
2,bar
3,baz""""""
data = StringIO(csv_data)
df = pd.read_csv(data)
print(df)
```


### Issue Description

pls solve that issue

### Expected Behavior

im not getting the code instead of solution im getting this error  module 'pandas' has no attribute '__version__'

### Installed Versions

<details>python version 3.11

Replace this line with the output of pd.show_versions()
","['Build', 'Needs Info']",2024-08-04 11:53:57,2024-08-04 14:26:54,7,closed
59390,ENH: Add option for DataFrame.compare and Series.compare to overlook differences involving missing values ,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I'd like to use the compare() method to extract only contradictory data between datasets, not all changes in the dataset.
For instance:
````python
import pandas as np
a = pd.Series([1, pd.NA, pd.NA])
b = pd.Series([2, ""zz"", pd.NA])
a.compare(b)
Out[3]: 
   self other
0     1     2
1  <NA>    zz
````
While the first row is a clear cut difference /contradiction (used to be 1 and is now 2), the second row is a bit more ambiguous and could be interpreted either as:

- used to be a `pd.NA` object ans is now a `str` with a set value (current behavior, completly sound as a default behavior imo)
- used to be something we didn't know but might as well have been ""zz"", and is now ""zz"", hence no difference reported (desired optional feature)

 This behavior has not been discussed in #30852 and its source issue #30429

### Feature Description

Add a new keyword argument such as `skipna` for aggregation functions to compare such that
````python
a.compare(b, skipna=False)
Out: 
   self other
0     1     2
1  <NA>    zz
a.compare(b, skipna=True)
Out: 
   self other
0     1     2
````
skipna=False would be the default value to conform with the current default behavior (which is stricter/safer and thus more sound as default behavior)

### Alternative Solutions

I can get the desired output by first erasing NA-linked differences using `combine_first` as a workaround:
```python
def skipna_compare(a: pd.Series | pd.DataFrame,
                   b: pd.Series | pd.DataFrame
                   ) -> (pd.Series | pd.DataFrame):
    filled_a = a.combine_first(b)
    filled_b = b.combine_first(a)
    return filled_a.compare(filled_b)
skipna_compare(a,b)
Out: 
  self other
0    1     2
```
But I don't think this is really efficient and satisfactory.

### Additional Context

Thanks for the amazing work!","['Enhancement', 'Missing-data', 'Closing Candidate']",2024-08-02 15:34:59,2024-08-12 17:06:53,4,closed
59387,ENH: DataFrame.info() returns a DataFrame rather than printing a table,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could use Pandas to get the information provided by DataFrame.info() in a more usable format. For example, this function currently prints a table describing each column in the DataFrame, info on Index, memory usage, etc. However, since this information is returned as string, it is difficult to use downstream. Perhaps we could add a flag like `return_info` set to False by default, but when True would return a dictionary containing the above information.

### Feature Description
```python
class DataFrame:
...
def info(return_info=False, [other existing params]):
  # existing implementation that calculates the below values
  if return_info:
    return {
      'Column summary': pd.DataFrame({'#': [0, 1, 2], 'Column': [nameCol1, nameCol2, nameCol3], 'Non-Null count': [4,2,5],...}),
      'Memory usage': 248
      'Index type': 'RangeIndex',
      'Index entries': 5,
      ...}  
```

### Alternative Solutions

I suppose you could either gather all the information manually by calling other functions and then make the JSON yourself. Or you could parse the string output of .info().

### Additional Context

_No response_","['Enhancement', 'Closing Candidate']",2024-08-02 07:31:30,2024-08-09 17:17:12,3,closed
59386,DOC: Link to Numpy's Git Resources in developer guide is no longer active,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/dev/development/contributing.html

### Documentation problem

Link to Numpy's Git resources tutorial (https://numpy.org/doc/stable/dev/gitwash/git_resources.html) leads to a 404 ""file not found"" error

Looking at historical versions of numpy documentation, this page used to exist but was removed in version 2.0.

### Suggested fix for documentation

Remove link as it is no longer supported by NumPy, and leave link to existing Git documentation as sufficient.",['Docs'],2024-08-01 22:05:11,2024-08-07 17:07:20,1,closed
59382,BUG: Segmentation Fault in MacOS installed by Pip,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas import DataFrame
```


### Issue Description

I tried to import pandas but it's straight to segmentation fault. Numpy can be imported separately. Installed by pip in venv and no other pandas installation exists. 

Below is the backtrace from lldb.

```shell
(lldb) target create ""python""
Current executable set to 'python' (x86_64).
(lldb) settings set -- target.run-args  ""-c"" ""import pandas as pd""
(lldb) r
Process 57543 launched: '/.venv/bin/python' (x86_64)
Process 57543 stopped
* thread #2, stop reason = exec
    frame #0: 0x0000000100011000 dyld`_dyld_start
dyld`_dyld_start:
->  0x100011000 <+0>: popq   %rdi
    0x100011001 <+1>: pushq  $0x0
    0x100011003 <+3>: movq   %rsp, %rbp
    0x100011006 <+6>: andq   $-0x10, %rsp
Target 0: (Python) stopped.
(lldb) bt
* thread #2, stop reason = exec
  * frame #0: 0x0000000100011000 dyld`_dyld_start
(lldb)
```
System Version: macOS 11.7.6 (20G1231)
Kernel Version: Darwin 20.6.0
Model Name: MacBook Pro
Model Identifier: MacBookPro11,1
Python: 3.12.2

### Expected Behavior

No segmentation fault

### Installed Versions

<details>
I can't even import it but pip said it's 2.2.2
</details>
","['Bug', 'Needs Info', 'OS X']",2024-08-01 13:27:18,2024-08-04 18:03:20,5,closed
59381,delete,delete,[],2024-08-01 12:57:33,2024-08-01 12:58:31,0,closed
59379,BUG: pandas.Series.rolling fails on some versions of SciPy,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.Series().rolling(17, win_type='triang', min_periods=1, center=True).mean()
```


### Issue Description

Before version 2.0 pandas.Series.rolling used to import windowing functions from `scipy.signal`, starting from 2.0 it  [uses](https://github.com/pandas-dev/pandas/blob/bdb509f95a8c0ff16530cedb01c2efc822c0d314/pandas/core/window/rolling.py#L1139C14-L1139C34
) `scipy.signal.windows`.
I suppose the reason is that SciPy has [deprecated](https://github.com/scipy/scipy/blob/f133c27843a36c8fe1c2979b3a072ac4f5a00624/scipy/signal/__init__.py#L362) import of windowing function from this module since [1.13.0](https://github.com/scipy/scipy/commit/bf9c8dfd59eec07c666beefc45c02ea1ba6afe7c).
The issue is that versions of SciPy prior 1.13.0 seem to be supported by pandas and work fine.
I propose making the import to take into account the installed version of SciPy.
Something like this:


```python
from packaging import version

scipy = import_optional_dependency(""scipy"", extra=""Scipy is required to generate window weight."")
scipy_ver = version.parse(scipy.__version__)
signal_module = 'scipy.signal' if scipy_ver < version.Version('1.13.0') else 'scipy.signal.windows'
signal = import_optional_dependency(signal_module, extra=""Scipy is required to generate window weight."")
```

### Expected Behavior

Rolling functions work with different versions of SciPy in different versions of Pandas.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit           : 2e218d10984e9919f0296931d92ea851c6a6faf5
python           : 3.10.12.final.0
python-bits      : 64
OS               : Linux
OS-release       : 6.5.0-1024-aws
Version          : #24~22.04.1-Ubuntu SMP Thu Jul 18 10:43:12 UTC 2024
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.5.3
numpy            : 1.26.4
pytz             : 2024.1
dateutil         : 2.9.0.post0
setuptools       : 59.6.0
pip              : 22.0.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
brotli           : None
fastparquet      : None
fsspec           : None
gcsfs            : None
matplotlib       : None
numba            : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : 1.13.0
snappy           : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
zstandard        : None
tzdata           : 2024.1
</details>
","['Bug', 'Window']",2024-08-01 04:59:47,2024-08-01 16:01:56,7,closed
59378,BUG: Inconsistent bar and line charts,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import matplotlib.pyplot as plt
import pandas as pd

data = {
    'size': [1, 2, 3, 4, 5, 6],
    'Yes': [1100.65, 1185.08, 338.30, 336.70, 58.61, 0.00],
    'No': [1700.32, 1380.81, 546.25, 722.00, 91.73, 139.32]
}

df = pd.DataFrame(data).set_index('size')
print(df)

# Plot data
fig, ax = plt.subplots(figsize=(10, 6))

# Plot stacked bar chart on primary axis
df.plot(kind='bar', stacked=True, ax=ax, legend=False)

# Plot line chart on primary axis (same as bar chart)
df.plot(kind='line', ax=ax, legend=False)

plt.show()
```


### Issue Description

Not sure why the line chart is starting from size=2 and not size=1 as is the case with stacked bar chart.

```
import matplotlib.pyplot as plt
import pandas as pd

data = {
    'xyz': [1, 2, 3, 4, 5, 6],
    'Yes': [1100.65, 1185.08, 338.30, 336.70, 58.61, 0.00],
    'No': [1700.32, 1380.81, 546.25, 722.00, 91.73, 139.32]
}

df = pd.DataFrame(data).set_index('xyz')
print(df)

# Plot data
fig, ax = plt.subplots(figsize=(10, 6))

# Plot stacked bar chart on primary axis
df.plot(kind='bar', stacked=True, ax=ax, legend=False)

# Plot line chart on primary axis (same as bar chart)
df.plot(kind='line', ax=ax, legend=False)

plt.show()
```

<img width=""901"" alt=""Screenshot 2024-07-31 at 6 27 20 PM"" src=""https://github.com/user-attachments/assets/8af3ee65-2fc3-4dc5-929c-327011a618b9"">


### Expected Behavior

The expected behaviour sis that both stacked bar  and lines should start from size=1.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.9.6.final.0
python-bits         : 64
OS                  : Darwin
OS-release          : 23.5.0
Version             : Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8112
machine             : arm64
processor           : arm
byteorder           : little
LC_ALL              : None
LANG                : en_CA.UTF-8
LOCALE              : en_CA.UTF-8

pandas              : 2.1.4
numpy               : 1.26.3
pytz                : 2023.3.post1
dateutil            : 2.8.2
setuptools          : 58.0.4
pip                 : 21.2.4
Cython              : None
pytest              : 8.2.2
hypothesis          : None
sphinx              : None
blosc               : None
feather             : None
xlsxwriter          : None
lxml.etree          : 5.1.1
html5lib            : 1.1
pymysql             : None
psycopg2            : None
jinja2              : 3.1.2
IPython             : 8.18.1
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : None
dataframe-api-compat: None
fastparquet         : None
fsspec              : None
gcsfs               : None
matplotlib          : 3.8.2
numba               : None
numexpr             : None
odfpy               : None
openpyxl            : None
pandas_gbq          : None
pyarrow             : 15.0.0
pyreadstat          : None
pyxlsb              : None
s3fs                : None
scipy               : 1.12.0
sqlalchemy          : None
tables              : None
tabulate            : 0.9.0
xarray              : None
xlrd                : None
zstandard           : None
tzdata              : 2023.4
qtpy                : 2.4.1
pyqt5               : None

</details>
","['Bug', 'Visualization', 'Duplicate Report']",2024-07-31 22:40:38,2024-08-19 21:00:16,2,closed
59374,BUG: Inconsistent plotting behaviour.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

mpg = sns.load_dataset(""mpg"")
tips = sns.load_dataset(""tips"")



# Plot 1: mpg data
fig, ax = plt.subplots(figsize=(3,3))
ax2 = ax.twinx()

k = mpg.pivot_table(index='model_year', columns='cylinders', values='mpg', aggfunc='sum', dropna=True)
k.plot(ax=ax, kind='bar')

k = k.fillna(10) 
k.plot(ax=ax2, kind='line')

plt.show() 

# Plot 2: tips data
fig, ax = plt.subplots(figsize=(3,3))
ax2 = ax.twinx()

k = tips.pivot_table(index='size', columns='smoker', values='total_bill', aggfunc='sum', dropna=True)
k.plot(ax=ax, kind='bar')

k = k.fillna(10)  
k.plot(ax=ax2, kind='line')

plt.show()
```


### Issue Description

I am trying to plot bar chart in primary axis and line chart in secondary axis. The plotting works fine for tips dataset but not for mpg dataset.
<img width=""433"" alt=""Screenshot 2024-07-31 at 2 22 12 PM"" src=""https://github.com/user-attachments/assets/4e724c5a-e1f2-4ce8-b7b1-1c41df38d881"">


### Expected Behavior

I should see both bar chart and line chart irrespective of underlying data source.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.9.6.final.0
python-bits         : 64
OS                  : Darwin
OS-release          : 23.5.0
Version             : Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8112
machine             : arm64
processor           : arm
byteorder           : little
LC_ALL              : None
LANG                : en_CA.UTF-8
LOCALE              : en_CA.UTF-8

pandas              : 2.1.4
numpy               : 1.26.3
pytz                : 2023.3.post1
dateutil            : 2.8.2
setuptools          : 58.0.4
pip                 : 21.2.4
Cython              : None
pytest              : 8.2.2
hypothesis          : None
sphinx              : None
blosc               : None
feather             : None
xlsxwriter          : None
lxml.etree          : 5.1.1
html5lib            : 1.1
pymysql             : None
psycopg2            : None
jinja2              : 3.1.2
IPython             : 8.18.1
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : None
dataframe-api-compat: None
fastparquet         : None
fsspec              : None
gcsfs               : None
matplotlib          : 3.8.2
numba               : None
numexpr             : None
odfpy               : None
openpyxl            : None
pandas_gbq          : None
pyarrow             : 15.0.0
pyreadstat          : None
pyxlsb              : None
s3fs                : None
scipy               : 1.12.0
sqlalchemy          : None
tables              : None
tabulate            : 0.9.0
xarray              : None
xlrd                : None
zstandard           : None
tzdata              : 2023.4
qtpy                : 2.4.1
pyqt5               : None

</details>
","['Bug', 'Needs Triage']",2024-07-31 18:26:44,2024-07-31 21:52:53,2,closed
59372,BUG: Python 3.13 development wheels not available ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

Look at https://anaconda.org/scientific-python-nightly-wheels/pandas/files, there is no cp313 wheels. There were some a few days ago but it seems like you are removing the old wheels on each upload (not 100% sure about this)? Last sucessful wheel build was July 26, see [build log](https://github.com/pandas-dev/pandas/actions/runs/10105253109). 

Issues seem to be related to Python 3.13 see [build log](https://github.com/pandas-dev/pandas/actions/runs/10173576695/job/28137917210) (not Python 3.13 free-threaded in particular just to be clear)


### Issue Description

In scikit-learn CI we have started relying on pandas development wheels for Python 3.13 free-threaded work. Probably this was a bit over-optimistic on our side, see https://github.com/scikit-learn/scikit-learn/pull/29572 for more details.

In case of failures, we would be more than happy to rely on an older wheel. For example, if the wheel build has been failing for a week we would be more than happy to rely on the wheel that was uploaded a week ago in https://anaconda.org/scientific-python-nightly-wheels/pandas/files than not have any wheel available. I think this is what would happen for scikit-learn, numpy and scipy development wheels although I am not 100% sure about this. Of course, you may have very good reasons to do it like this ...

We could well revert https://github.com/scikit-learn/scikit-learn/pull/29572 if you are telling us that things are a bit too early and test against pandas later when things are a bit more smooth.

### Expected Behavior

There are some Python 3.13 wheels

### Installed Versions

N/A
","['Bug', 'Python 3.13']",2024-07-31 15:31:49,2024-08-08 15:20:25,7,closed
59369,DOC: Add automatic exclusion of prompts from the copies using sphinx-copybutton,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

In the docs the sphinx extension `sphinx-copybutton` is added but the output is not stripped. This means, that code is copied with possible outputs.

https://github.com/pandas-dev/pandas/blob/73b5578967bedd1f94b8a54d9047f33364178783/doc/source/conf.py#L59

A possible solution is documented in the section [using regex prompt identifiers](https://sphinx-copybutton.readthedocs.io/en/latest/use.html#using-regexp-prompt-identifiers).

### Documentation problem

The main problem is, that examples in code blocks can have some output and will not work in a plain jupyter notebook cell without touching the copied code.

One possible example is in [`pandas.DataFrame.to_dict`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_dict.html).

### Suggested fix for documentation

The solution is to add

```
copybutton_prompt_text = r"">>> |\.\.\. |\$ |In \[\d*\]: | {2,5}\.{3,}: | {5,8}: ""
copybutton_prompt_is_regexp = True
```

to `conf.py`.","['Docs', 'Needs Triage']",2024-07-31 12:54:42,2024-07-31 17:13:08,1,closed
59362,Code smells in pandas/core/computation/pytables.py,"[pandas/core/computation/pytables.py](https://github.com/pandas-dev/pandas/blob/main/pandas/core/computation/pytables.py) has a few code smells. Here is what I've found.

- Most function names use snake_case, but some function names combine mixedCase and snake_case. PEP8 suggests only using snake_case.
visit_UnaryOp
visit_Index
visit_Assign
visit_Subscript
visit_Attribute
translate_In

- The function 'convert_value' has a lot going on with few comments and a short docstring. I would suggest at least listing what is accepted by pytables (integer, float, bool, etc.) in the docstring.
- In the function 'convert_value', the variable name 'v' is too vague. I assume it means value, but perhaps changing the variable name or just adding a comment to specify this would help.
- The function 'convert_values' doesn't do anything. Can it be removed?

Thank you for reading my issue. I can work on a pull request, if necessary, after receiving feedback.",[],2024-07-31 03:27:37,2024-07-31 12:37:34,1,closed
59359,DOC: Enhance '10 minute to pandas' by providing video walkthrough to help auditory learners and improve accessibility. ,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#min

### Documentation problem

This documentation does a good job of appealing to those with a reading/writing and visual preferred learning styles (the V and R in education theorist Neil Fleming's VARK student learning model). It even appeals to those Kinesthetic(K) learners as it is modeled like a walk-through where the reader may follow along while reading the guide and dissecting real use-cases.  However, this information may be difficult for auditory(A) learners and observational learners.  It also may be difficult for neurodivergent learners who have difficulty with reading comprehension and tend to miss important details in text.

### Suggested fix for documentation

The addition of a video walkthrough of the '10 minutes to pandas' guide would provide another learning option for those who prefer a non-text based way of learning.  This will also promote inclusivity and accessibility, as it will appeal to all learning types as well as users with disabilities.  ","['Docs', 'Needs Discussion', 'Closing Candidate']",2024-07-30 22:48:15,2025-08-05 16:39:03,3,closed
59358,"DOC: Low priority code smells found in ""core/sorting.py""","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/core/sorting.py

### Documentation problem

The documentation for the function get_group_index_sorter in the pandas/core/sorting.py file could be improved. The current documentation does not fully adhere to Section 2 of the documentation guidelines concerning the extended summary. Additionally, there is an issue with the naming convention for internal helper functions, specifically the maybe_lift function, which should be prefixed with an underscore.

### Suggested fix for documentation

Extended Summary Issue:
The current extended summary of the get_group_index_sorter function includes details about the parameters and implementation notes, which should be placed in other sections according to Section 2 of the documentation guidelines (https://pandas.pydata.org/docs/development/contributing_docstring.html). Furthermore, this block of comments is almost as long as the function itself, which adds unnecessary verbosity. The extended summary should focus on why the function is useful and its use cases, providing a clear and concise explanation.

Current extended summary:
https://github.com/pandas-dev/pandas/blob/7acd629fea2a32d1ace93ceab2b62d5f5f9b2d47/pandas/core/sorting.py#L629-L651

Suggested extended summary:
This function sorts group indices efficiently using a combination of counting sort and merge sort. The sorting is stable, ensuring the correctness of groupby operations, which is crucial for operations like transforming the first value of a grouped column.

Naming Convention Issue:
https://github.com/pandas-dev/pandas/blob/7acd629fea2a32d1ace93ceab2b62d5f5f9b2d47/pandas/core/sorting.py#L164C5-L164C6
The helper function maybe_lift should be renamed to _maybe_lift to indicate that it is intended for internal use only. This naming convention aligns with the project's existing practices and supports consistency in the codebase. It is also good for Python coding practices, as it clearly distinguishes internal functions from those meant for public use.","['Docs', 'Needs Triage']",2024-07-30 21:50:10,2024-07-31 12:38:03,1,closed
59355,BUG: Qcut interval not selecting the correct inclusive and exclusive limits,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
pd.__version__
'2.2.2'

cat = pd.qcut([0, 1, 2, 3], 3, precision=0)

cat
[(-1.0, 1.0], (1.0, 2.0], (2.0, 3.0], (2.0, 3.0]]
Categories (3, interval[float64, right]): [(-1.0, 1.0] < (1.0, 2.0] < (2.0, 3.0]]
```

while the expected output should be

```
cat
[(-1.0, 1.0], (-1.0, 1.0], (1.0, 2.0], (2.0, 3.0]]
Categories (3, interval[float64, right]): [(-1.0, 1.0] < (1.0, 2.0] < (2.0, 3.0]]
```
```


### Issue Description

The second element with value 1 should be allocated to the interval (-1.0, 1.0] instead of the erroneous allocation to the interval (1.0, 2.0]

### Expected Behavior

The expected behaviour should be

```
cat
[(-1.0, 1.0], (-1.0, 1.0], (1.0, 2.0], (2.0, 3.0]]
Categories (3, interval[float64, right]): [(-1.0, 1.0] < (1.0, 2.0] < (2.0, 3.0]]
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.4.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 21.6.0
Version               : Darwin Kernel Version 21.6.0: Wed Apr 24 06:02:02 PDT 2024; root:xnu-8020.240.18.708.4~1/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_GB.UTF-8
pandas                : 2.2.2
numpy                 : 1.26.0
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 68.2.0
pip                   : 23.2.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.9.2
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'cut']",2024-07-30 15:35:25,2024-08-09 17:23:24,2,closed
59351,QST: How does pandas ensure backward compatibility with numpy and other deps,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/78808646

### Question about pandas

If I check pandas [dependencies requirements](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#dependencies), I see the following:

```
+-------------------+--------------------------+
|     Package       | Minimum supported version|
+-------------------+--------------------------+
| NumPy             | 1.22.4                   |
+-------------------+--------------------------+
| python-dateutil   | 2.8.2                    |
+-------------------+--------------------------+
| pytz              | 2020.1                   |
+-------------------+--------------------------+
| tzdata            | 2022.7                   |
+-------------------+--------------------------+
```

I'm wondering how does pandas ensure that they are backward compatible with all the versions that were released after those specified in the table? I'm sure there are lots of tests, but it's not feasible to run tests against all possible permutations of those libs.

","['Usage Question', 'Needs Triage']",2024-07-30 10:00:00,2024-07-30 17:11:55,1,closed
59344,DOC: Make usage of `rtol` and `atol` arguments in `pd.testing.assert_frame_equal` clearer,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.testing.assert_frame_equal.html

### Documentation problem

When `atol` and `rtol` are used, the check of equality between two dataframes is of the form `absolute(a - b) <= (atol + rtol * absolute(b))`.

It's not clear that `atol` and `rtol` are used together like this, resulting in it being unclear what exact is being checked.

### Suggested fix for documentation

Expand the explanation of `atol` and `rtol` in the documentation, possibly explicitly including the equation that is testing equality.","['Testing', 'Docs']",2024-07-29 13:18:00,2024-08-08 00:43:41,6,closed
59342,ENH: Need API support and __repr__ to discover the storage used for strings,"Originally raised in https://github.com/pandas-dev/pandas/pull/58551#discussion_r1662680953

### Problem Description

With PDEP-14 there is the need for developers to be aware of the storage used for strings. Indeed, the storage might have a lot of impact of performance, for instance
- `pyarrow` storage
    - pros: compact (optimal memory footprint), fast (vectorization)
    - cons: immutable (so any modification creates a new string pyarrow `ChunkedArray`)
- `python` storage
    - pros: mutable
    - cons: highest memory footprint (each string is a different Python object), slow (no vectorization)
- `numpy` 2.0 strings storage (I don't have a good knowledge of these new strings, and never tested them)
    - pros: compact, vectorization, mutable (my understanding is that is takes more space and is slower than pyarrow strings)
    - cons: different representations depending on a string size, which make understanding performance harder


### Feature Description

I would like to have two way to discover the storage
- `__repr__` goal is to give information on the inner of an object, one option suggested by @jorisvandenbossche is to display `<pandas.StringDtype(storage=...)>` instead of `string[storage]`
- `.get_storage` that returns the storage (not sure what is possible with the current implementation, would be best to have a class, otherwise, a string). The API is useful to check before running a time consuming code that we have the correct storage.

### Alternative Solutions

.

### Additional Context

_No response_","['Enhancement', 'Strings', 'Needs Discussion']",2024-07-29 08:56:03,2025-12-14 10:20:28,11,closed
59334,DOC: Website opens search when I press Caps Lock,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

Every page is affected.

### Documentation problem

I have the Caps Lock key remapped to CTRL on my computer (using the keyboard settings in KDE Plasma), and every time I press the Caps Lock key because I want to use the CTRL key (e.g. CTRL-C to copy text) the search box opens as if I had pressed CTRL-K. This is really annoying because it means I cannot use any shortcuts of the browser.

### Suggested fix for documentation

The issue is that the keyboard event handler of the the web page uses `/k/i.test(event.key)` to test whether the key contains `k` or `K`, and in my case the `event.key` of the browser event is the string `CapsLock`, which contains the letter `k`. And since my Caps Lock is remapped to CTRL the browser also reports that `event.ctrlKey` is `true`. Together this produces a false positive.

A simple fix would be to change the regex to `/^k$/` or to use a string comparison test like `event.key === 'k' || event.key === 'K' `.",['Docs'],2024-07-26 22:51:08,2024-07-29 17:32:48,2,closed
59333,BUG:  pd.Series.duplicated(keep='first'|'last') returns multiple duplicates,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
for keep_val in ['first','last']:
    print(f""{keep_val = }"")
    series = pd.Series([1, 2, 2, 3, 4, 4, 5, 5, 5])
    # Identify duplicates (erroneously finds 5 twice)
    mask = series.duplicated(keep=keep_val)
    print(series[mask])

    data = pd.Series(['1', '2', '2', '3', '4', '4', '5', '5', '5'])
    # Identify duplicates (erroneously finds 5 twice)
    mask = series.duplicated(keep=keep_val)
    print(series[mask])
```


### Issue Description

keep='first'|'last' should only return one instance of each duplicated values.

In the above examples it returns ['2', '4', '5', '5'] not ['2', '4', '5'].

`keep='last'` returns the '5' at index 6, 7
`keep='first'` returns the '5' at index 7, 8
eg.
<img width=""519"" alt=""image"" src=""https://github.com/user-attachments/assets/669b4202-30ef-4f49-bcd1-58f46f516b59"">


### Expected Behavior

series.duplicated(keep='first'|'last') should only return one instance of each duplicated values.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.4.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 21.6.0
Version               : Darwin Kernel Version 21.6.0: Wed Aug 10 14:28:23 PDT 2022; root:xnu-8020.141.5~2/RELEASE_ARM64_T6000
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.25.2
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 68.1.2
pip                   : 23.2.1
Cython                : 3.0.0a10
pytest                : 7.4.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.3
html5lib              : None
pymysql               : None
psycopg2              : 2.9.7
jinja2                : 3.1.2
IPython               : 8.14.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.9.0
gcsfs                 : None
matplotlib            : 3.7.2
numba                 : 0.58.1
numexpr               : 2.8.5
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 13.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2023.9.0
scipy                 : 1.11.2
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : 2023.8.0
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-07-26 20:49:30,2024-07-26 21:24:42,2,closed
59332,BUG:  pandas.read_parquet () dtype_backend argument does not get the default value as documented,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

fields = [
    pa.field('int_column',pa.int32()),
    pa.field('float_column', pa.float32()),
    pa.field('bool_column', pa.bool_()),
]
schema = pa.schema(fields)
data = {'int_column': [1, 2, None, 4],
        'float_column': [1.5, 2.5, 3.5, None],
        'bool_column': [True, True, None, False]}
pa_table = pa.table(raw_data, schema=schema)
pq.write_table(pa_table, './example_table.parquet')

df1 = pd.read_parquet('./example_table.parquet')
df2 = pd.read_parquet('./example_table.parquet', dtype_backend='numpy_nullable')

print(df1.dtypes)
print(df2.dtypes)
```


### Issue Description

According to the documentation of `pd.read_parquet()`, the default value for `dtype_backend` is `'numpy_nullable'`.
Data types differ when not passing any argument from when passing `'numpy_nullable'`.
Specifically, if not passing any argument the data types are the not nullable ones.

### Expected Behavior

not passing an argument should give the same result as passing the default value.

### Installed Versions

<details>

NSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-38-generic
Version               : #38-Ubuntu SMP PREEMPT_DYNAMIC Fri Jun  7 15:25:01 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.1
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 17.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.3.1
scipy                 : 1.13.1
sqlalchemy            : 2.0.30
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2023.6.0
xlrd                  : None
zstandard             : 0.22.0
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Docs', 'IO Data']",2024-07-26 19:23:28,2024-08-11 12:35:11,4,closed
59328,String dtype: overview of breaking behaviour changes,"In context of the new default string dtype in 3.0 (https://github.com/pandas-dev/pandas/issues/54792 / [PDEP-14](https://pandas.pydata.org/pdeps/0014-string-dtype.html)), currently enabled with `pd.options.future.infer_string = True`, there are a bunch of breaking changes that we will have to document. 
In preparation of documenting, I want to use this issue to list all the behaviour changes that we are aware of (or run into) / potentially need to discuss if we actually want those changes.

First, there are a few obvious breaking changes that are also mentioned in the PDEP (and that are the main _goals_ of the change):

- Constructors and IO methods will now infer string data as a `str` dtype, instead of using `object` dtype.
- Code checking for the dtype (e.g. `ser.dtype == object`) assuming object dtype, will break
- The missing value sentinel is now always `NaN`, and for example no longer `None` (we still accept None as input, but it will be converted to NaN)

But additionally, there are some other less obvious changes or secondary consequences (or changes we already had a long time with the existing opt-in `string` dtype but will now be relevant for all). 
Starting to list some of them here (and please add comments with other examples if you think of more).

#### `astype(str)` preserving missing values (no longer converting NaN to a string ""nan"")

This is a long standing ""bug"" (or at least generally agreed undesirable behaviour), as discussed in https://github.com/pandas-dev/pandas/issues/25353. 
Currently something like `pd.Series([""foo"", np.nan]).astype(str)` would essentially convert every element to a string, including the missing values:

```python
>>> ser = pd.Series([""foo"", np.nan], dtype=object)
>>> ser
0    foo
1    NaN
dtype: object
>>> ser.astype(str)
0    foo
1    nan
dtype: object
>>> ser.astype(str).values
array(['foo', 'nan'], dtype=object)
```

Generally we expect missing values to propagate in `astype()`. And as a result of making `str` an alias for the new default string dtype (https://github.com/pandas-dev/pandas/pull/59685), this will now follow a different code path and making use of the general StringDtype construction, which does preserve missing values;

```python
>>> pd.options.future.infer_string = True
>>> ser = pd.Series([""foo"", np.nan], dtype=object)
>>> ser.astype(str)
0    foo
1    NaN
dtype: str
>>> ser.astype(str).values
<StringArrayNumpySemantics>
['foo', nan]
Length: 2, dtype: str
```

Because 

#### Mixed dtype operations

Any working code that previously relied on the object dtype allowing mixed types, where the initial data is now inferred as string dtype. Because the string dtype is now strict about only allowing strings, that means certain workflows will no longer work (unless users explicitly ensure to keep using object dtype).

For example, setitem with a non string:

```
>>> ser = pd.Series([""a"", ""b""], dtype=""object"")
>>> ser[0] = 1
>>> ser
0    1
1    b
dtype: object

>>> pd.options.future.infer_string = True
>>> ser = pd.Series([""a"", ""b""])
>>> ser[0] = 1
...
TypeError: Scalar must be NA or str
```

~The same happens if you try to fill a column of strings and missing values with a non-string:~

```python
>>> pd.Series([""a"", None]).fillna(0)
...
TypeError: Invalid value '0' for dtype string
```

Update: the above is kept working with upcasting to object dtype (see https://github.com/pandas-dev/pandas/pull/60296)

#### Numeric aggregations

With object dtype strings, we do allow `sum` and `prod` in certain cases:

```python
>>> pd.Series([""a"", ""b""], dtype=""object"").sum()
'ab'
>>> pd.Series([""a"", ""b""], dtype=""string"").sum()
...
TypeError: Cannot perform reduction 'sum' with string dtype

# prod only in case of 1 string (can be other missing values)
>>> pd.Series([""a""], dtype=""object"").prod()
'a'
>>> pd.Series([""a""], dtype=""string"").prod()
...
TypeError: Cannot perform reduction 'prod' with string dtype
```

Based on the discussion below, we decided to keep `sum()` working (https://github.com/pandas-dev/pandas/pull/59853 is adding that functionality to string dtype), but `prod()` is fine to start raising.

Note: due to pyarrow implementation limitation, the sum is limited to 2GB result, see https://github.com/pandas-dev/pandas/pull/59853/files#r1794090618 (given this is about the size of a single Python string, that seems very unlikely to happen)

For `any()`/`all()` (which does work for object dtype, but didn't for the already existing StringDtype), we decided to keep this working for now for the new default string dtype, see https://github.com/pandas-dev/pandas/issues/51939, https://github.com/pandas-dev/pandas/pull/54591


#### Invalid unicode input

```python
>>> pd.options.future.infer_string = False
>>> pd.Series([""\ud83d""])
0    \ud83d
dtype: object

>>> pd.options.future.infer_string = True
>>> pd.Series([""\ud83d""])
...
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 0: surrogates not allowed
```

Users that want to keep the previous behaviour can explicitly specify `dtype=object` to keep working with object dtype.","['API Design', 'Strings']",2024-07-26 13:50:35,2025-07-07 11:08:57,19,closed
59315,"BUILD: Pandas never succeeds, the most time consuming part of using pandas","### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

Windows-10-10.0.19045-SP0

### Installation Method

pip install

### pandas Version

pandas==2.0.3

### Python Version

3.12

### Installation Logs

<details>

      dependency C:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include\numpy\ndarraytypes.h won't be automatically included in the manifest: the path must be relative
      dependency C:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include\numpy\ufuncobject.h won't be automatically included in the manifest: the path must be relative
      reading manifest file 'pandas.egg-info\SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      warning: no files found matching 'versioneer.py'
      no previously-included directories found matching 'doc\build'
      warning: no previously-included files matching '*.bz2' found anywhere in distribution
      warning: no previously-included files matching '*.csv' found anywhere in distribution
      warning: no previously-included files matching '*.dta' found anywhere in distribution
      warning: no previously-included files matching '*.feather' found anywhere in distribution
      warning: no previously-included files matching '*.tar' found anywhere in distribution
      warning: no previously-included files matching '*.gz' found anywhere in distribution
      warning: no previously-included files matching '*.h5' found anywhere in distribution
      warning: no previously-included files matching '*.html' found anywhere in distribution
      warning: no previously-included files matching '*.json' found anywhere in distribution
      warning: no previously-included files matching '*.jsonl' found anywhere in distribution
      warning: no previously-included files matching '*.msgpack' found anywhere in distribution
      warning: no previously-included files matching '*.pdf' found anywhere in distribution
      warning: no previously-included files matching '*.pickle' found anywhere in distribution
      warning: no previously-included files matching '*.png' found anywhere in distribution
      warning: no previously-included files matching '*.pptx' found anywhere in distribution
      warning: no previously-included files matching '*.ods' found anywhere in distribution
      warning: no previously-included files matching '*.odt' found anywhere in distribution
      warning: no previously-included files matching '*.orc' found anywhere in distribution
      warning: no previously-included files matching '*.sas7bdat' found anywhere in distribution
      warning: no previously-included files matching '*.sav' found anywhere in distribution
      warning: no previously-included files matching '*.so' found anywhere in distribution
      warning: no previously-included files matching '*.xls' found anywhere in distribution
      warning: no previously-included files matching '*.xlsb' found anywhere in distribution
      warning: no previously-included files matching '*.xlsm' found anywhere in distribution
      warning: no previously-included files matching '*.xlsx' found anywhere in distribution
      warning: no previously-included files matching '*.xpt' found anywhere in distribution
      warning: no previously-included files matching '*.cpt' found anywhere in distribution
      warning: no previously-included files matching '*.xz' found anywhere in distribution
      warning: no previously-included files matching '*.zip' found anywhere in distribution
      warning: no previously-included files matching '*.zst' found anywhere in distribution
      warning: no previously-included files matching '*~' found anywhere in distribution
      warning: no previously-included files matching '.DS_Store' found anywhere in distribution
      warning: no previously-included files matching '.git*' found anywhere in distribution
      warning: no previously-included files matching '#*' found anywhere in distribution
      warning: no previously-included files matching '*.py[ocd]' found anywhere in distribution
      no previously-included directories found matching 'pandas\tests\io\parser\data'
      adding license file 'LICENSE'
      adding license file 'AUTHORS.md'
      writing manifest file 'pandas.egg-info\SOURCES.txt'
      C:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\setuptools\command\build_py.py:215: _Warning: Package 'pandas._libs' is absent from the `packages` configuration.
      !!
     
              ********************************************************************************
              ############################
              # Package would be ignored #
              ############################
              Python recognizes 'pandas._libs' as an importable package[^1],
              but it is absent from setuptools' `packages` configuration.
     
              This leads to an ambiguous overall configuration. If you want to distribute this
              package, please make sure that 'pandas._libs' is explicitly added
              to the `packages` configuration field.
     
              Alternatively, you can also rely on setuptools' discovery methods
              (for example by using `find_namespace_packages(...)`/`find_namespace:`
              instead of `find_packages(...)`/`find:`).
     
              You can read more about ""package discovery"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
     
              If you don't want 'pandas._libs' to be distributed and are
              already explicitly excluding 'pandas._libs' via
              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,
              you can try to use `exclude_package_data`, or `include-package-data=False` in
              combination with a more fine grained `package-data` configuration.
     
              You can read more about ""package data files"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html
     
     
              [^1]: For Python, any directory (with suitable naming) can be imported,
                    even if it does not contain any `.py` files.
                    On the other hand, currently there is no concept of package data
                    directory, all directories are treated like packages.
              ********************************************************************************
     
      !!
        check.warn(importable)
      C:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\setuptools\command\build_py.py:215: _Warning: Package 'pandas._libs.src' is absent from the `packages` configuration.
      !!
     
              ********************************************************************************
              ############################
              # Package would be ignored #
              ############################
              Python recognizes 'pandas._libs.src' as an importable package[^1],
              but it is absent from setuptools' `packages` configuration.
     
              This leads to an ambiguous overall configuration. If you want to distribute this
              package, please make sure that 'pandas._libs.src' is explicitly added
              to the `packages` configuration field.
     
              Alternatively, you can also rely on setuptools' discovery methods
              (for example by using `find_namespace_packages(...)`/`find_namespace:`
              instead of `find_packages(...)`/`find:`).
     
              You can read more about ""package discovery"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
     
              If you don't want 'pandas._libs.src' to be distributed and are
              already explicitly excluding 'pandas._libs.src' via
              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,
              you can try to use `exclude_package_data`, or `include-package-data=False` in
              combination with a more fine grained `package-data` configuration.
     
              You can read more about ""package data files"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html
     
     
              [^1]: For Python, any directory (with suitable naming) can be imported,
                    even if it does not contain any `.py` files.
                    On the other hand, currently there is no concept of package data
                    directory, all directories are treated like packages.
              ********************************************************************************
     
      !!
        check.warn(importable)
      C:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\setuptools\command\build_py.py:215: _Warning: Package 'pandas._libs.src.headers' is absent from the `packages` configuration.
      !!
     
              ********************************************************************************
              ############################
              # Package would be ignored #
              ############################
              Python recognizes 'pandas._libs.src.headers' as an importable package[^1],
              but it is absent from setuptools' `packages` configuration.
     
              This leads to an ambiguous overall configuration. If you want to distribute this
              package, please make sure that 'pandas._libs.src.headers' is explicitly added
              to the `packages` configuration field.
     
              Alternatively, you can also rely on setuptools' discovery methods
              (for example by using `find_namespace_packages(...)`/`find_namespace:`
              instead of `find_packages(...)`/`find:`).
     
              You can read more about ""package discovery"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
     
              If you don't want 'pandas._libs.src.headers' to be distributed and are
              already explicitly excluding 'pandas._libs.src.headers' via
              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,
              you can try to use `exclude_package_data`, or `include-package-data=False` in
              combination with a more fine grained `package-data` configuration.
     
              You can read more about ""package data files"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html
     
     
              [^1]: For Python, any directory (with suitable naming) can be imported,
                    even if it does not contain any `.py` files.
                    On the other hand, currently there is no concept of package data
                    directory, all directories are treated like packages.
              ********************************************************************************
     
      !!
        check.warn(importable)
      C:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\setuptools\command\build_py.py:215: _Warning: Package 'pandas._libs.src.klib' is absent from the `packages` configuration.
      !!
     
              ********************************************************************************
              ############################
              # Package would be ignored #
              ############################
              Python recognizes 'pandas._libs.src.klib' as an importable package[^1],
              but it is absent from setuptools' `packages` configuration.
     
              This leads to an ambiguous overall configuration. If you want to distribute this
              package, please make sure that 'pandas._libs.src.klib' is explicitly added
              to the `packages` configuration field.
     
              Alternatively, you can also rely on setuptools' discovery methods
              (for example by using `find_namespace_packages(...)`/`find_namespace:`
              instead of `find_packages(...)`/`find:`).
     
              You can read more about ""package discovery"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
     
              If you don't want 'pandas._libs.src.klib' to be distributed and are
              already explicitly excluding 'pandas._libs.src.klib' via
              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,
              you can try to use `exclude_package_data`, or `include-package-data=False` in
              combination with a more fine grained `package-data` configuration.
     
              You can read more about ""package data files"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html
     
     
              [^1]: For Python, any directory (with suitable naming) can be imported,
                    even if it does not contain any `.py` files.
                    On the other hand, currently there is no concept of package data
                    directory, all directories are treated like packages.
              ********************************************************************************
     
      !!
        check.warn(importable)
      C:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\setuptools\command\build_py.py:215: _Warning: Package 'pandas._libs.src.parser' is absent from the `packages` configuration.
      !!
     
              ********************************************************************************
              ############################
              # Package would be ignored #
              ############################
              Python recognizes 'pandas._libs.src.parser' as an importable package[^1],
              but it is absent from setuptools' `packages` configuration.
     
              This leads to an ambiguous overall configuration. If you want to distribute this
              package, please make sure that 'pandas._libs.src.parser' is explicitly added
              to the `packages` configuration field.
     
              Alternatively, you can also rely on setuptools' discovery methods
              (for example by using `find_namespace_packages(...)`/`find_namespace:`
              instead of `find_packages(...)`/`find:`).
     
              You can read more about ""package discovery"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
     
              If you don't want 'pandas._libs.src.parser' to be distributed and are
              already explicitly excluding 'pandas._libs.src.parser' via
              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,
              you can try to use `exclude_package_data`, or `include-package-data=False` in
              combination with a more fine grained `package-data` configuration.
     
              You can read more about ""package data files"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html
     
     
              [^1]: For Python, any directory (with suitable naming) can be imported,
                    even if it does not contain any `.py` files.
                    On the other hand, currently there is no concept of package data
                    directory, all directories are treated like packages.
              ********************************************************************************
     
      !!
        check.warn(importable)
      C:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\setuptools\command\build_py.py:215: _Warning: Package 'pandas._libs.src.ujson.lib' is absent from the `packages` configuration.
      !!
     
              ********************************************************************************
              ############################
              # Package would be ignored #
              ############################
              Python recognizes 'pandas._libs.src.ujson.lib' as an importable package[^1],
              but it is absent from setuptools' `packages` configuration.
     
              This leads to an ambiguous overall configuration. If you want to distribute this
              package, please make sure that 'pandas._libs.src.ujson.lib' is explicitly added
              to the `packages` configuration field.
     
              Alternatively, you can also rely on setuptools' discovery methods
              (for example by using `find_namespace_packages(...)`/`find_namespace:`
              instead of `find_packages(...)`/`find:`).
     
              You can read more about ""package discovery"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
     
              If you don't want 'pandas._libs.src.ujson.lib' to be distributed and are
              already explicitly excluding 'pandas._libs.src.ujson.lib' via
              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,
              you can try to use `exclude_package_data`, or `include-package-data=False` in
              combination with a more fine grained `package-data` configuration.
     
              You can read more about ""package data files"" on setuptools documentation page:
     
              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html
 
      pandas\_libs/groupby.c(46782): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_float64_t', possible loss of data
      pandas\_libs/groupby.c(46851): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_float64_t', possible loss of data
      pandas\_libs/groupby.c(52474): warning C4244: '=': conversion from '__pyx_t_5numpy_uint64_t' to '__pyx_t_5numpy_float64_t', possible loss of data
      pandas\_libs/groupby.c(52543): warning C4244: '=': conversion from '__pyx_t_5numpy_uint64_t' to '__pyx_t_5numpy_float64_t', possible loss of data
      pandas\_libs/groupby.c(56665): warning C4244: '=': conversion from '__pyx_t_5numpy_float64_t' to '__pyx_t_5numpy_float32_t', possible loss of data
      pandas\_libs/groupby.c(58009): warning C4244: '=': conversion from 'npy_double' to '__pyx_t_5numpy_float32_t', possible loss of data
      pandas\_libs/groupby.c(58301): warning C4244: 'function': conversion from 'npy_double' to 'float', possible loss of data
      pandas\_libs/groupby.c(58797): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/groupby.c(67749): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/groupby.c(77207): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/groupby.c(91152): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/groupby.c(95027): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/groupby.c(106720): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/groupby.c(110395): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\link.exe"" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\libs /LIBPATH:C:\Python312\libs /LIBPATH:C:\Python312 /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\PCbuild\amd64 ""/LIBPATH:C:\Program 
Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\lib\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\\um\x64"" /EXPORT:PyInit_groupby build\temp.win-amd64-cpython-312\Release\pandas\_libs/groupby.obj /OUT:build\lib.win-amd64-cpython-312\pandas\_libs\groupby.cp312-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-312\Release\pandas\_libs\groupby.cp312-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-312\Release\pandas\_libs\groupby.cp312-win_amd64.lib and object build\temp.win-amd64-cpython-312\Release\pandas\_libs\groupby.cp312-win_amd64.exp
      Generating code
      Finished generating code
      building 'pandas._libs.hashing' extension
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DNPY_NO_DEPRECATED_API=0 
-IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -Id:\GitHub\WORK\JudDoc\.venv\include -IC:\Python312\include -IC:\Python312\Include ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\include"" ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt"" /Tcpandas\_libs/hashing.c /Fobuild\temp.win-amd64-cpython-312\Release\pandas\_libs/hashing.obj
      hashing.c
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\link.exe"" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\libs /LIBPATH:C:\Python312\libs /LIBPATH:C:\Python312 /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\PCbuild\amd64 ""/LIBPATH:C:\Program 
Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\lib\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\\um\x64"" /EXPORT:PyInit_hashing build\temp.win-amd64-cpython-312\Release\pandas\_libs/hashing.obj /OUT:build\lib.win-amd64-cpython-312\pandas\_libs\hashing.cp312-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-312\Release\pandas\_libs\hashing.cp312-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-312\Release\pandas\_libs\hashing.cp312-win_amd64.lib and object build\temp.win-amd64-cpython-312\Release\pandas\_libs\hashing.cp312-win_amd64.exp
      Generating code
      Finished generating code
      building 'pandas._libs.hashtable' extension
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DNPY_NO_DEPRECATED_API=0 
-Ipandas/_libs/src/klib -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -Id:\GitHub\WORK\JudDoc\.venv\include -IC:\Python312\include -IC:\Python312\Include ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\include"" ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt"" /Tcpandas\_libs/hashtable.c /Fobuild\temp.win-amd64-cpython-312\Release\pandas\_libs/hashtable.obj
      hashtable.c
      pandas/_libs/src/klib\khash_python.h(371): warning C4244: 'return': conversion from 'khuint64_t' to 'khuint32_t', possible loss of data
      pandas\_libs/hashtable.c(22002): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(22030): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(23655): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(23750): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(24076): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(27661): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(27689): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(29314): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(29409): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(29735): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(33320): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(33348): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(34973): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(35068): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(35394): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(38979): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(39007): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(40834): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(40929): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(41255): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(45351): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(45379): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(47004): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(47099): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(47425): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(51010): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(51038): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(52663): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(52758): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(53084): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(56669): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(56697): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(58322): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(58417): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(58743): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(62328): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(62356): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(63981): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(64076): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(64402): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(67987): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(68015): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(69640): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(69735): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(70061): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(73646): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(73674): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(75299): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(75394): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(75720): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(79305): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(79333): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(80958): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(81053): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(81379): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(84963): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(84991): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(86616): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(86711): warning C4244: '=': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(87037): warning C4244: '=': conversion from '__pyx_t_5numpy_int64_t' to '__pyx_t_5numpy_int8_t', possible loss of data
      pandas\_libs/hashtable.c(90606): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(90634): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(94679): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(94707): warning C4244: 'function': conversion from '__pyx_t_5numpy_int64_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(98027): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(98496): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(99344): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(99795): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(100264): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(101112): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(101563): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(102032): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(102880): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(103331): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(103800): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(104648): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(105099): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(105568): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(106416): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(106867): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(107336): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(108184): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(108635): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(109104): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(109952): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(110403): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(110872): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(111720): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(112213): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(112648): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(113434): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(113827): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(114296): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(115144): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(115595): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(116064): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(116912): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(117363): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(117832): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(118680): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(119131): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(119600): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(120448): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(121044): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/hashtable.c(124904): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/hashtable.c(128866): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/hashtable.c(132719): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/hashtable.c(140868): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(141375): warning C4244: 'function': conversion from 'Py_ssize_t' to 'khuint_t', possible loss of data
      pandas\_libs/hashtable.c(141518): warning C4244: 'function': conversion from 'Py_ssize_t' to '__pyx_t_5numpy_int32_t', possible loss of data
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\link.exe"" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\libs /LIBPATH:C:\Python312\libs /LIBPATH:C:\Python312 /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\PCbuild\amd64 ""/LIBPATH:C:\Program 
Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\lib\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\\um\x64"" /EXPORT:PyInit_hashtable build\temp.win-amd64-cpython-312\Release\pandas\_libs/hashtable.obj /OUT:build\lib.win-amd64-cpython-312\pandas\_libs\hashtable.cp312-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-312\Release\pandas\_libs\hashtable.cp312-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-312\Release\pandas\_libs\hashtable.cp312-win_amd64.lib and object build\temp.win-amd64-cpython-312\Release\pandas\_libs\hashtable.cp312-win_amd64.exp
      Generating code
      Finished generating code
      building 'pandas._libs.index' extension
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DNPY_NO_DEPRECATED_API=0 
-I.\pandas\_libs\tslibs -Ipandas/_libs/src/klib -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -Id:\GitHub\WORK\JudDoc\.venv\include -IC:\Python312\include -IC:\Python312\Include ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\include"" ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt"" /Tcpandas\_libs/index.c /Fobuild\temp.win-amd64-cpython-312\Release\pandas\_libs/index.obj
      index.c
      pandas/_libs/src/klib\khash_python.h(371): warning C4244: 'return': conversion from 'khuint64_t' to 'khuint32_t', possible loss of data
      pandas\_libs/index.c(6281): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\link.exe"" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\libs /LIBPATH:C:\Python312\libs /LIBPATH:C:\Python312 /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\PCbuild\amd64 ""/LIBPATH:C:\Program 
Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\lib\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\\um\x64"" /EXPORT:PyInit_index build\temp.win-amd64-cpython-312\Release\pandas\_libs/index.obj /OUT:build\lib.win-amd64-cpython-312\pandas\_libs\index.cp312-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-312\Release\pandas\_libs\index.cp312-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-312\Release\pandas\_libs\index.cp312-win_amd64.lib and object build\temp.win-amd64-cpython-312\Release\pandas\_libs\index.cp312-win_amd64.exp
      Generating code
      Finished generating code
      building 'pandas._libs.indexing' extension
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DNPY_NO_DEPRECATED_API=0 
-IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -Id:\GitHub\WORK\JudDoc\.venv\include -IC:\Python312\include -IC:\Python312\Include ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\include"" ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt"" /Tcpandas\_libs/indexing.c /Fobuild\temp.win-amd64-cpython-312\Release\pandas\_libs/indexing.obj
      indexing.c
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\link.exe"" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\libs /LIBPATH:C:\Python312\libs /LIBPATH:C:\Python312 /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\PCbuild\amd64 ""/LIBPATH:C:\Program 
Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\lib\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\\um\x64"" /EXPORT:PyInit_indexing build\temp.win-amd64-cpython-312\Release\pandas\_libs/indexing.obj /OUT:build\lib.win-amd64-cpython-312\pandas\_libs\indexing.cp312-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-312\Release\pandas\_libs\indexing.cp312-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-312\Release\pandas\_libs\indexing.cp312-win_amd64.lib and object build\temp.win-amd64-cpython-312\Release\pandas\_libs\indexing.cp312-win_amd64.exp
      Generating code
      Finished generating code
      building 'pandas._libs.internals' extension
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DNPY_NO_DEPRECATED_API=0 
-IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -Id:\GitHub\WORK\JudDoc\.venv\include -IC:\Python312\include -IC:\Python312\Include ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\include"" ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt"" /Tcpandas\_libs/internals.c /Fobuild\temp.win-amd64-cpython-312\Release\pandas\_libs/internals.obj
      internals.c
      pandas\_libs/internals.c(4829): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data
      pandas\_libs/internals.c(4829): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data
      pandas\_libs/internals.c(4829): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data
      pandas\_libs/internals.c(8046): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data
      pandas\_libs/internals.c(8046): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data
      pandas\_libs/internals.c(8046): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data
      pandas\_libs/internals.c(8212): warning C4244: 'function': conversion from '__pyx_t_5numpy_intp_t' to 'long', possible loss of data
      pandas\_libs/internals.c(10144): warning C4244: '=': conversion from 'npy_intp' to 'long', possible loss of data
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\link.exe"" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\libs /LIBPATH:C:\Python312\libs /LIBPATH:C:\Python312 /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\PCbuild\amd64 ""/LIBPATH:C:\Program 
Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\lib\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\\um\x64"" /EXPORT:PyInit_internals build\temp.win-amd64-cpython-312\Release\pandas\_libs/internals.obj /OUT:build\lib.win-amd64-cpython-312\pandas\_libs\internals.cp312-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-312\Release\pandas\_libs\internals.cp312-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-312\Release\pandas\_libs\internals.cp312-win_amd64.lib and object build\temp.win-amd64-cpython-312\Release\pandas\_libs\internals.cp312-win_amd64.exp
      Generating code
      Finished generating code
      building 'pandas._libs.interval' extension
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DNPY_NO_DEPRECATED_API=0 
-I.\pandas\_libs\tslibs -Ipandas/_libs/src/klib -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -Id:\GitHub\WORK\JudDoc\.venv\include -IC:\Python312\include -IC:\Python312\Include ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\include"" ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt"" /Tcpandas\_libs/interval.c /Fobuild\temp.win-amd64-cpython-312\Release\pandas\_libs/interval.obj        
      interval.c
      pandas/_libs/src/klib\khash_python.h(371): warning C4244: 'return': conversion from 'khuint64_t' to 'khuint32_t', possible loss of data
      pandas\_libs/interval.c(12338): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/interval.c(13992): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\link.exe"" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\libs /LIBPATH:C:\Python312\libs /LIBPATH:C:\Python312 /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\PCbuild\amd64 ""/LIBPATH:C:\Program 
Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\lib\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\\um\x64"" /EXPORT:PyInit_interval build\temp.win-amd64-cpython-312\Release\pandas\_libs/interval.obj /OUT:build\lib.win-amd64-cpython-312\pandas\_libs\interval.cp312-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-312\Release\pandas\_libs\interval.cp312-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-312\Release\pandas\_libs\interval.cp312-win_amd64.lib and object build\temp.win-amd64-cpython-312\Release\pandas\_libs\interval.cp312-win_amd64.exp
      Generating code
      Finished generating code
      building 'pandas._libs.join' extension
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DNPY_NO_DEPRECATED_API=0 
-Ipandas/_libs/src/klib -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -IC:\Users\User\AppData\Local\Temp\pip-build-env-bv409dxw\overlay\Lib\site-packages\numpy\core\include -Id:\GitHub\WORK\JudDoc\.venv\include -IC:\Python312\include -IC:\Python312\Include ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\include"" ""-IC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt"" /Tcpandas\_libs/join.c /Fobuild\temp.win-amd64-cpython-312\Release\pandas\_libs/join.obj
      join.c
      pandas/_libs/src/klib\khash_python.h(371): warning C4244: 'return': conversion from 'khuint64_t' to 'khuint32_t', possible loss of data
      pandas\_libs/join.c(9400): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(16504): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(31115): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(45154): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(63382): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(63967): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(99102): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(99687): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(135332): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      pandas\_libs/join.c(135917): warning C4244: '=': conversion from 'long' to 'char', possible loss of data
      ""C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\bin\HostX86\x64\link.exe"" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\libs /LIBPATH:C:\Python312\libs /LIBPATH:C:\Python312 /LIBPATH:d:\GitHub\WORK\JudDoc\.venv\PCbuild\amd64 ""/LIBPATH:C:\Program 
Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.40.33807\lib\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64"" ""/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\\um\x64"" /EXPORT:PyInit_join build\temp.win-amd64-cpython-312\Release\pandas\_libs/join.obj /OUT:build\lib.win-amd64-cpython-312\pandas\_libs\join.cp312-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-312\Release\pandas\_libs\join.cp312-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-312\Release\pandas\_libs\join.cp312-win_amd64.lib and object build\temp.win-amd64-cpython-312\Release\pandas\_libs\join.cp312-win_amd64.exp
      Generating code
      C:\Users\User\AppData\Local\Temp\pip-install-lhhxr07a\pandas_0a95d37b2025437bb0d6182807066fef\pandas\_libs\join.c(6354) : fatal error C1001: Internal compiler error.    
      (compiler file 'D:\a\_work\1\s\src\vctools\Compiler\Utc\src\p2\main.c', line 242)
       To work around this problem, try simplifying or changing the program near the locations listed above.
      If possible please provide a repro here: https://developercommunity.visualstudio.com
      Please choose the Technical Support command on the Visual C++
       Help menu, or open the Technical Support help file for more information
     
      error: command 'C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.40.33807\\bin\\HostX86\\x64\\link.exe' failed with exit code 3221225477 
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for pandas
  Building wheel for pycountry (pyproject.toml) ... done

</details>
","['Build', 'Needs Triage']",2024-07-25 20:50:05,2024-07-26 09:19:49,2,closed
59314,DOC: Typo in docs for na_values parameter in pandas.read_csv function,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html

### Documentation problem

Hi,
I was looking through the pandas documentation for the `na_values` parameter under `pandas.read_csv` and I noticed that there's a typo in the section:

>   By default the following values are interpreted as NaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”, “1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”, “n/a”, “nan”, “null “.


The `"" ""` is included in the list of default string to NAs starting in pandas 2.1 all the way to the current version of the docs. However, [in the code for the list of default string to NAs](https://github.com/pandas-dev/pandas/blob/main/pandas/_libs/parsers.pyx#L1373-L1393), I'm seeing `""""` instead.

### Suggested fix for documentation

I think the fix should be to remove `"" ""` and replace with `""""` in the  list of default string to NAs in the pandas docs for the versions 2.1 and up to the current version so it's consistent with the code.

Thanks for all your help!","['Docs', 'Needs Triage']",2024-07-25 18:40:37,2024-08-02 16:47:28,1,closed
59312,PERF: Significant Performance Difference in DataFrame.to_csv() with and without Index Reset,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

Below is a toy DataFrame example with 10M rows and 20 columns. The CSV write speed differ significantly between whether the multi-index is dropped first or not, even if the resulting CSV files are essentially the same.

The benchmark for PyArrow is also attached for reference. Notice that the CSV generated from PyArrow has column names and column values additionally double-quoted.
```
import pandas as pd
import pyarrow as pa
import pyarrow.csv as csv
import time

NUM_ROWS = 10000000
NUM_COLS = 20

# Example Multi-Index DataFrame
df = pd.DataFrame(
    {
        f""col_{col_idx}"": range(col_idx * NUM_ROWS, (col_idx + 1) * NUM_ROWS)
        for col_idx in range(NUM_COLS)
    }
)
df = df.set_index([""col_0"", ""col_1""], drop=False)

# Timing Operation A
start_time = time.time()
df.to_csv(""file_A.csv"", index=False)
end_time = time.time()
print(f""Operation A time: {end_time - start_time} seconds"")

# Timing Operation B
start_time = time.time()
df_reset = df.reset_index(drop=True)
df_reset.to_csv(""file_B.csv"", index=False)
end_time = time.time()
print(f""Operation B time: {end_time - start_time} seconds"")

# Timing Operation C
start_time = time.time()
table = pa.Table.from_pandas(df)
csv.write_csv(table, 'file_C.csv')
end_time = time.time()
print(f""Operation C time: {end_time - start_time} seconds"")
```

Output is as below.
```
Operation A time: 1080.621277809143 seconds
Operation B time: 45.777050733566284 seconds
Operation C time: 15.710699558258057 seconds
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.8.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.4.266-178.365.amzn2.x86_64
Version               : #1 SMP Fri Jan 12 12:52:04 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.23.5
pytz                  : 2024.1
dateutil              : 2.8.2
setuptools            : 69.1.1
pip                   : 24.0
Cython                : None
pytest                : 8.0.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.22.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.8.3
numba                 : 0.57.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 15.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.6.1
scipy                 : 1.12.0
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>


### Prior Performance

_No response_","['Performance', 'IO CSV']",2024-07-25 16:16:32,2024-08-27 00:07:25,2,closed
59309,pandas.Series.groupby example is not relevant,"Hello,

On this page, in the first example, we can read `ser.groupby([""a"", ""b"", ""a"", ""b""]).mean()`.

However, `a` and `b` are not part of the index at this point. The index only has `Falcon` and `Parrot`: [pandas/core/series.py L1818](https://github.com/pandas-dev/pandas/blob/63dc1bb4f99d24b46bacb113d740d54459fdbe5e/pandas/core/series.py#L1818).

Values `a` and `b` are relevant for the third example: [pandas/core/series.py L1862](https://github.com/pandas-dev/pandas/blob/63dc1bb4f99d24b46bacb113d740d54459fdbe5e/pandas/core/series.py#L1862).","['Docs', 'good first issue']",2024-07-25 11:14:33,2024-07-31 01:46:49,6,closed
59307,BUG: GroupBy.value_counts doesn't preserve original order for non-grouping rows,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame({""X"": [""B"", ""A"", ""A"", ""B""], ""Y"": [4, 1, 3, -1]})
df.groupby(""X"", sort=False).value_counts(sort=False)  # works as expected
df.groupby(""X"", sort=True).value_counts(sort=False)  # column ""Y"" does not match original order
```


### Issue Description

(possibly related to #55951)

When a GroupBy.value_counts operation is performed, the order of rows within the non-grouping columns does not respect the order of elements in the original frame.

In this example, with value_counts(sort=False) I would expect the row `B 4 1` to appear above `B -1 1`, as that would correspond go their order in the original frame.
```
>>> df = pd.DataFrame({""X"": [""B"", ""A"", ""A"", ""B""], ""Y"": [4, 1, 3, -1]})
>>> df.groupby(""X"", sort=False).value_counts(sort=False)
X  Y 
A   1    1
    3    1
B  -1    1
    4    1
Name: count, dtype: int64
```

When the groupby has sort=False set, this order is respected.
```
>>> df.groupby(""X"", sort=False).value_counts(sort=False)
X  Y 
B   4    1
A   1    1
    3    1
B  -1    1
Name: count, dtype: int64
```

### Expected Behavior

Calling groupby(sort=True).value_counts(sort=False) should preserve the order of members within groups, or documentation should be changed to reflect this.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 67a58cddc2f8c0e30cb0123589947d9b3f073720
python                : 3.11.7
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:14:38 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1239.g67a58cddc2
numpy                 : 2.1.0.dev0+git20240720.d489c83
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 23.3.1
Cython                : None
sphinx                : None
IPython               : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
blosc                 : None
bottleneck            : None
fastparquet           : None
fsspec                : None
html5lib              : None
hypothesis            : None
gcsfs                 : None
jinja2                : None
lxml.etree            : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
psycopg2              : None
pymysql               : None
pyarrow               : None
pyreadstat            : None
pytest                : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
xlsxwriter            : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None
</details>
","['Docs', 'Groupby']",2024-07-24 22:41:33,2024-10-06 17:33:16,13,closed
59303,QST: Is this expected behavior when pd.read_csv() with na_values arguments?,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/46397526/how-to-use-na-values-option-in-the-pd-read-csv-function

### Question about pandas

I have a simple csv file that looks like this:

```
x,y,z
a,-99,100
b,-99,200
c,-99.0,300
d,-99.0,400
```

and when I tried a few different na_values, I got different column y back:

```
import pandas as pd

df1 = pd.read_csv('test.csv', na_values={""y"": -99})
print(""df1 = \n"", df1)

df2 = pd.read_csv('test.csv', na_values={""y"": -99.0})
print(""\ndf2 = \n"", df2)

df3 = pd.read_csv('test.csv', na_values={""y"": [-99.0, -99]})
print(""\ndf3 = \n"", df3)

df4 = pd.read_csv('test.csv', na_values={""y"": [-99, -99.0]})
print(""\ndf4 = \n"", df4)
```

Results:

```
df1 = 
    x   y    z
0  a NaN  100
1  b NaN  200
2  c NaN  300
3  d NaN  400

df2 = 
    x     y    z
0  a -99.0  100
1  b -99.0  200
2  c   NaN  300
3  d   NaN  400

df3 = 
    x     y    z
0  a -99.0  100
1  b -99.0  200
2  c   NaN  300
3  d   NaN  400

df4 = 
    x   y    z
0  a NaN  100
1  b NaN  200
2  c NaN  300
3  d NaN  400
```


I'm not sure if this is a bug or it is by design, so just throwing out a general question here. Thank you!

Pandas version is 2.2.1, just in case needed.","['Bug', 'Missing-data', 'IO CSV']",2024-07-24 04:10:46,2024-09-12 01:15:53,12,closed
59301,DOC: Add Bodo to out-of-core projects in ecosystem,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/community/ecosystem.html

### Documentation problem

 Bodo is a jit compiler that operates on native pandas data frames and supports a large number of APIs https://docs.bodo.ai/latest/api_docs/pandas/, and can be installed as a python package. 

### Suggested fix for documentation

Add bodo to the out of core list of packages in the ecosystem. ",['Docs'],2024-07-23 14:03:53,2024-07-24 16:39:57,4,closed
59298,BUG: pandas.to_datetime reports incorrect index when failing.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
pandas as pd
valid = '2024-01-01'
invalid = 'N'
S = pd.Series([valid]*45_000 + [invalid])
T = pd.to_datetime(S, format='%Y-%m-%d', exact=True) 

################################
# Traceback redacted.
################################
ValueError: time data ""N"" doesn't match format ""%Y-%m-%d"", at position 1. You might want to try:
    - passing `format` if your strings have a consistent format;
    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;
    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.
```


### Issue Description

As you can see a ValueError is raised. That is fine. 

The description says it happens for item at postition 1, however, which is not correct. It happens for item at position 45000.

### Expected Behavior

The correct position would be reported to the user. I'd like to do S[<position>] and be able to see why it raised valueError.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.8.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.9.7-amd64
Version               : #1 SMP PREEMPT_DYNAMIC Debian 6.9.7-1 (2024-06-27)
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.1
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 68.1.2
pip                   : 24.1.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'Error Reporting']",2024-07-22 20:26:26,2024-08-27 00:10:32,9,closed
59294,"Surprising behavior: set_index cannot set a MultiIndex from a tuple, only a list ","I ran into the following surprising behavior:

We can set a MultiIndex with a list of column names:
```python
import pandas as pd
df = pd.DataFrame({'A': [1, 2], 'B': [3, 4], 'C': [5, 6]})
# Works fine
df = df.set_index(['A', 'B'])
```

However, attempting to set a MultiIndex with a tuple of column names raises a KeyError:
```python
import pandas as pd
df = pd.DataFrame({'A': [1, 2], 'B': [3, 4], 'C': [5, 6]})
# Raises KeyError: ""None of [('A', 'B')] are in the columns""
df = df.set_index(('A', 'B'))
```

This is technically consistent with the documentation of the `keys` parameter (my emphasis):

> This parameter can be either a single column key, a single array of the same length as the calling DataFrame, or a **list** containing an arbitrary combination of column keys and arrays.

But nonetheless, this is quite surprising behavior in Python.

(This occurs in the latest release as well as on the main branch)

----

The culprit appears to be this isinstance check at the beginning of the set_index method:

```python
if not isinstance(keys, list):
    keys = [keys]
```

My suggestion would be to use the pandas `is_list_like` helper function instead. 
```python
if not is_list_like(keys):
    keys = [keys]
```

To unambiguously demonstrate what I mean, I've put up a branch for this with tests: https://github.com/pandas-dev/pandas/compare/main...tadamcz:pandas:tadamcz/set-index-multiindex-from-tuple?expand=1.

If you think this would indeed be an improvement, I'd be happy to see this turned into a PR. But since I can't promise I'll have time to shepherd this through to the finish line, I thought I'd hold off on officially opening a PR?","['Indexing', 'Closing Candidate']",2024-07-21 13:47:08,2024-07-22 17:22:18,4,closed
59293,BUG: Couldn't run sql:  'Connection' object has no attribute 'cursor',"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

from sqlalchemy import create_engine
from sqlalchemy.schema import Table, MetaData
from sqlalchemy.sql.expression import select, text
engine = create_engine('starrocks://root:root@183.232.229.27:9030/default_catalog.ssb?charset=utf8')
connection = engine.connect()

print('\n'.join(['%s:%s' % item for item in connection.__dict__.items()]))

# You define a function that takes in a SQL query as a string and returns a pandas dataframe
def run_sql(sql: str) -> pd.DataFrame:
    print(""query sql is :""+sql)
    df = pd.read_sql_query(sql, connection)
    return df
```


### Issue Description

![image](https://github.com/user-attachments/assets/00b2f48e-edf4-4541-9fcd-0fe76ab427c4)


### Expected Behavior

Running SQL normally

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

Name: pandas
Version: 2.2.2

Name: SQLAlchemy
Version: 2.0.31

</details>
","['Bug', 'IO SQL', 'Needs Info']",2024-07-21 06:53:08,2024-08-27 17:00:09,5,closed
59285,BUG: Error on query function when the column name has # symbol,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame((1,2,3), columns=['a#'])
df.query('a# > 2')

-------------------------------
KeyError                                  Traceback (most recent call last)
File d:\Applications\Python\Python311\Lib\site-packages\pandas\core\computation\scope.py:231, in Scope.resolve(self, key, is_local)
    230 if self.has_resolvers:
--> 231     return self.resolvers[key]
    233 # if we're here that means that we have no locals and we also have
    234 # no resolvers

File d:\Applications\Python\Python311\Lib\collections\__init__.py:1006, in ChainMap.__getitem__(self, key)
   1005         pass
-> 1006 return self.__missing__(key)

File d:\Applications\Python\Python311\Lib\collections\__init__.py:998, in ChainMap.__missing__(self, key)
    997 def __missing__(self, key):
--> 998     raise KeyError(key)

KeyError: 'a'

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
File d:\Applications\Python\Python311\Lib\site-packages\pandas\core\computation\scope.py:242, in Scope.resolve(self, key, is_local)
    238 try:
    239     # last ditch effort we look in temporaries
    240     # these are created when parsing indexing expressions
...
    242     return self.temps[key]
    243 except KeyError as err:
--> 244     raise UndefinedVariableError(key, is_local) from err

UndefinedVariableError: name 'a' is not defined
```


### Issue Description

The `query` function seems to treat symbol `#` as a comment, it did not work as expected.

I also try to execute 
```python
df.query('`a#` > 2')
```
it still throws an exception
```python
Traceback (most recent call last):

  File d:\Applications\Python\Python311\Lib\site-packages\pandas\core\computation\parsing.py:192 in tokenize_string
    yield tokenize_backtick_quoted_string(

  File d:\Applications\Python\Python311\Lib\site-packages\pandas\core\computation\parsing.py:167 in tokenize_backtick_quoted_string
    return BACKTICK_QUOTED_STRING, source[string_start:string_end]

UnboundLocalError: cannot access local variable 'string_end' where it is not associated with a value


The above exception was the direct cause of the following exception:

Traceback (most recent call last):

  File d:\Applications\Python\Python311\Lib\site-packages\IPython\core\interactiveshell.py:3553 in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  Cell In[59],   [line 1](vscode-notebook-cell:?execution_count=59&line=1)
    df.query('`a#` > 2')

  File d:\Applications\Python\Python311\Lib\site-packages\pandas\core\frame.py:4823 in query
    res = self.eval(expr, **kwargs)

  File d:\Applications\Python\Python311\Lib\site-packages\pandas\core\frame.py:4949 in eval
...
    raise SyntaxError(f""Failed to parse backticks in '{source}'."") from err

  File <string>
SyntaxError: Failed to parse backticks in '`a#` > 2'.
```

### Expected Behavior

like df[df['a#'] > 2]

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : Chinese (Simplified)_China.936

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 65.5.0
pip                   : 24.0
Cython                : 3.0.8
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.20.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'expressions']",2024-07-20 04:07:26,2024-08-20 20:08:19,1,closed
59277,BUG: Error on to_datetime() after running multiple times in jupyter notebook,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# issue cannot be reproduced directly as this occurs in Juputer Notebook: after parent function main is modified (but not directly related to the error), OR when the cell where main function is located is run several times

import pandas as pd
from datetime import timezone, timedelta
df = pd.DataFrame({'col1': [""2 January 2020 12:39 GMT"", ""3 January 2020 12:39 GMT""], 'col2': [3, 4]}, index=[0, 1])
def create_datetime_object_GMT(row):
    return pd.to_datetime(f'{df.loc[0,""col1""]}', format=""%d %B %Y %H:%M %Z"", exact=True).tz_convert(tz=timezone(timedelta(hours=0)))
    
#in a new cell
def main():
    ...
    df.insert(loc=0, column='Datetime Object GMT', value=df.apply(
                create_datetime_object_GMT, axis=1), allow_duplicates=False)
    ...
print(df)
```


### Issue Description

ValueError: time data ""2 January 2020 12:39 GMT"" doesn't match format ""%d %B %Y %H:%M %Z"", at position 0. You might want to try:
    - passing `format` if your strings have a consistent format;
    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;
    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.

### Expected Behavior

expected behavior is without error correctly convert to datetime

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.4.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : Chinese (Traditional)

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 67.6.1
pip                   : 24.1.2
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : 1.1
pymysql               : 1.4.6
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.5.0
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.31
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-07-19 09:02:38,2024-07-25 09:24:03,1,closed
59276,BUG: ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
No exactly reproduce method
```


### Issue Description

For example i got a DataFrame with null values.
Like this

|    |   value |
|---:|--------:|
|  0 |       1 |
|  1 |     nan |
|  2 |       2 |
|  3 |     nan |
|  4 |     nan |

So after calling `df['value'].ffill(inplace=True)`,it turns out like:

|    |   value |
|---:|--------:|
|  0 |       1 |
|  1 |     1|
|  2 |       2 |
|  3 |     2|
|  4 |     nan |

The last nan hasn't been filled with with 2.

### Expected Behavior

It should be filled as 
|    |   value |
|---:|--------:|
|  0 |       1 |
|  1 |     1 |
|  2 |       2 |
|  3 |     2 |
|  4 |     2 |

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.146.1-microsoft-standard-WSL2
Version               : #1 SMP Thu Jan 11 04:09:03 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : 0.60.0
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None","['Bug', 'Needs Triage']",2024-07-19 08:36:33,2024-07-19 08:51:12,1,closed
59274,ENH: pd.concat with keys and ignore_index=True should raise,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

You can happily pass both of these arguments together, but the ignore_index=True call drops the keys, so it ends up being a moot point:

```python
In [1]: import pandas as pd

In [2]: df1 = pd.DataFrame([[0]])

In [3]: df2 = pd.DataFrame([[42]])

In [4]: pd.concat([df1, df2])
Out[4]: 
    0
0   0
0  42

In [5]: pd.concat([df1, df2], keys=[""df1"", ""df2""])
Out[5]: 
        0
df1 0   0
df2 0  42

In [6]: pd.concat([df1, df2], keys=[""df1"", ""df2""], ignore_index=True)
Out[6]: 
    0
0   0
1  42
```

### Feature Description

pandas can raise that this combination of values does not make sense

### Alternative Solutions

status quo

### Additional Context

_No response_","['Enhancement', 'Reshaping', 'Error Reporting']",2024-07-18 17:47:15,2024-08-10 17:10:40,4,closed
59273,BUG: reset_index() looses the frequency of a DatetimeIndex,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> index = pd.DatetimeIndex(pd.date_range(start=""2000"", freq = 'YS', periods = 10), name = 'Date')
>>> df = pd.DataFrame(data=list(range(10)), index = index)
>>> print(df.index.freq)
<YearBegin: month=1>
>>> print(df.reset_index()['Date']._values.freq)
None
>>> df = df.reset_index().set_index('Date')
>>> print(df.index.freq)
None
```


### Issue Description

When doing reset_index() on a DatetimeIndex this leads to the frequency being lost. Although the newly created column is a DatetimeArray, it does not seem to carry the freq attribute. As a result, when doing reset_index() -> set_index() I cannot restore the original index which potentially creates issues.

### Expected Behavior

I would expect that reset_index().set_index() let's me recover the original index :)

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : bfe5be01fef4eaecf4ab033e74139b0a3cac4a39
python                : 3.10.12
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.153.1-microsoft-standard-WSL2
Version               : #1 SMP Fri Mar 29 23:14:13 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 0+untagged.34794.gbfe5be0
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 22.0.2
Cython                : 3.0.10
sphinx                : 7.3.7
IPython               : 8.23.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.3.8
fastparquet           : 2024.2.0
fsspec                : 2024.3.1
html5lib              : 1.1
hypothesis            : 6.100.1
gcsfs                 : 2024.3.1
jinja2                : 3.1.3
lxml.etree            : 5.2.1
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.2
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 16.0.0
pyreadstat            : 1.2.7
pytest                : 8.1.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.3.1
scipy                 : 1.13.0
sqlalchemy            : 2.0.29
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.3.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'Closing Candidate']",2024-07-18 15:58:36,2025-01-31 10:20:25,9,closed
59271,BUG: DatetimeIndex intersection is empty or garbage,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

index = pd.date_range('2024-01-01', '2024-02-19', unit='s')
period = pd.date_range('2024-01-01', '2024-02-09', unit='s')
index.intersection(period)  # ughh... DatetimeIndex(['2024-01-01', '2739931-01-04'], dtype='datetime64[s]', freq='D')

index = pd.date_range('2024-01-01', '2024-02-19', unit='s')
period = pd.date_range('2024-01-09', '2024-02-09', unit='s')

index.intersection(period)  # empty!
```


### Issue Description

In the first example, the result is obviously buggy with year 2739931
The second example is empty when it shouldn't

### Expected Behavior

both examples are expected to return an index equivalent to `period`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.85+
Version               : #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.25.2
pytz                  : 2023.4
dateutil              : 2.8.2
setuptools            : 67.7.2
pip                   : 23.1.2
Cython                : 3.0.10
pytest                : 7.4.4
hypothesis            : None
sphinx                : 5.0.2
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.4
html5lib              : 1.1
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : 7.34.0
pandas_datareader     : 0.10.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.6.0
gcsfs                 : 2023.6.0
matplotlib            : 3.7.1
numba                 : 0.58.1
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : 0.19.2
pyarrow               : 14.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : 2.0.31
tables                : 3.8.0
tabulate              : 0.9.0
xarray                : 2023.7.0
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-07-18 10:57:04,2024-07-19 19:10:59,4,closed
59270,BUILD: Pandas 1.5.3 is unusable due to incompatibility with Numpy 2.0.0,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

macOS-14.5-arm64-arm-64bit

### Installation Method

pip install

### pandas Version

1.5.3

### Python Version

3.10.13

### Installation Logs

Installing `pandas==1.5.3` with either Pip or Poetry on Python >= 3.9 leads to an error upon the first import of Pandas.

This is fixed by later versions, but some of our workflows are still using version 1.5.3.

Pinning Numpy to be below version 2.0.0 should fix the issue, I'll be happy to open a PR myself if needed 😄

```shell
Python 3.10.13 (main, Sep  4 2023, 17:19:45) [Clang 16.0.6 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<project_folder>/pip_project/.venv_310/lib/python3.10/site-packages/pandas/__init__.py"", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
  File ""<project_folder>/pip_project/.venv_310/lib/python3.10/site-packages/pandas/compat/__init__.py"", line 18, in <module>
    from pandas.compat.numpy import (
  File ""<project_folder>/pip_project/.venv_310/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py"", line 4, in <module>
    from pandas.util.version import Version
  File ""<project_folder>/pip_project/.venv_310/lib/python3.10/site-packages/pandas/util/__init__.py"", line 2, in <module>
    from pandas.util._decorators import (  # noqa:F401
  File ""<project_folder>/pip_project/.venv_310/lib/python3.10/site-packages/pandas/util/_decorators.py"", line 14, in <module>
    from pandas._libs.properties import cache_readonly
  File ""<project_folder>/pip_project/.venv_310/lib/python3.10/site-packages/pandas/_libs/__init__.py"", line 13, in <module>
    from pandas._libs.interval import Interval
  File ""pandas/_libs/interval.pyx"", line 1, in init pandas._libs.interval
ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
```
","['Build', 'Usage Question']",2024-07-18 10:31:16,2024-07-18 17:33:23,3,closed
59267,"DOC: New contributor meeting, clarify or remove","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/development/community.html#new-contributor-meeting

### Documentation problem

The documentation advertises new contributor meeting, but last input in the notes documentation is from November 15, 2023 and the meeting does not seem to be happening, as verified today (July 17, 2024). 

### Suggested fix for documentation

Either remove the entry or clarify that this meeting is hibernated. This would be better than the existing statement which seems to be inaccurate. ","['Docs', 'Closing Candidate']",2024-07-17 22:32:52,2024-07-22 20:51:06,5,closed
59263,ENH: Support right hand side operations with Series and custom classes,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wish I could add a custom class to pandas series and to ""hint"" the series object to use the `__radd__` method of the custom class with the series.

I have a class:
```
class DataWrapper:
  def __init__(data: List[float]):
    self.data=data

  def __radd__(self, other):
    return DataWrapper(self.data + other)
```

I perform the following operation:
```
series = pd.Series([1,2])
data = DataWrapper([3,4])

result = series + data
```

This will call the `__radd__` method per item in the underlying numpy array (1 and 2) and return a `pd.Series`
Adding the `__array_ufunc__ = None` (taken from numpy) will result in a call to `__radd__` with the entire underlying numpy array but still the result will be `pd.Series`.
I would like pandas to either use the `__array_ufunc__ = None` as a hint to return `NotImplemented` from it's `__add__` method and allowing the `__radd__` method to be called with the series.
If using the `__array_ufunc__ = None` will break previous behaviour, there could be another option of doing it.

### Feature Description

A few options:
1. look for `__array_ufunc__` property on the `other` class of the operation, and if it's `None` return `NotImplemented`.
2. add another flag for it with similar behaviour to `__array_ufunc__` that can be used specifically for the pandas series. This can be global for all the operations (like numpy) or flag per operation

### Alternative Solutions

I'm not aware of any alternative solution

### Additional Context

The feature is for operations for units and quantities. I have a `Unit` and a `Quantity` classes, the `Quantity` class holds either a single value or numpy array or pandas series of numbers with a binding of a unit.

Some examples:
* `pd.Series([1,2]) * Units.meter` - should be same as `Quantity(pd.Series([1,2]), Units.meter)`. It works for numpy arrays
* `pd.Series([1,2]) * Quantity(3, Units.meter)` - should be similar to `Quantity(pd.Series([1,2]) * 3, Units.meter)`","['Enhancement', 'Needs Triage']",2024-07-17 13:41:23,2024-07-17 17:14:53,1,closed
59261,BUG: Series.mul silently returns wrong values with `UInt8` dtype when overflowing the max value range,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
s = pd.Series([36], dtype='UInt8')
100 * s # This returns a series with one value: 16 (!!)
s.mul(100) # Also returns a wrong value

# But
7 * s # This returns the correct value

8 * s # This returns the wrong value (overflow)

100. * s # returns the correct value (due to casting I guess)
```


### Issue Description

When multiplying a `UInt8` series by a python integer, for which the result overflows the dtype range, the resulting values are wrong.

AFAICT this only happens with the `UInt8` dtype (I tested `UInt8`, `UInt16`, `UInt32` and `UInt64`).
Other UInt dtypes are properly casted to the ""next"" dtype. 

Anybody computing percentages on UInt8 dtypes will easily hit this issue.

This was introduced in the 2.1.0 Release.

### Expected Behavior

I expect the series to be casted to `UInt16` when the results overflow the `UInt8` range and  return the correct values.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : fd3f57170aa1af588ba877e8e28c158a20a4886d
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.153.1-microsoft-standard-WSL2
Version               : #1 SMP Fri Mar 29 23:14:13 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.0
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.3.0
pip                   : 23.3.1
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : 1.4.6
psycopg2              : None
jinja2                : None
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.5
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.6.1
scipy                 : None
sqlalchemy            : 2.0.31
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Numeric Operations', 'Closing Candidate']",2024-07-17 10:50:43,2025-07-04 14:05:54,5,closed
59260,BUILD: ,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

import pandas as pd

### Installation Method

pip install

### pandas Version

pandas 3.0

### Python Version

3.12.1

### Installation Logs

<details>

Replace this line with the installation logs.
Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),
(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)
but was not found to be installed on your system.
</details>
","['Build', 'Needs Triage']",2024-07-17 08:16:40,2024-07-17 17:08:13,1,closed
59253,"DOC: Draft PDEPs appear as ""Under discussion"" in the roadmap","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/about/roadmap.html

### Documentation problem

`Status: Draft` PDEP should not be in the ""Under discussion"" section.
(There are currently no PDEPs with `Status: Draft`, but the problem exists)

### Suggested fix for documentation

I would suggest not showing draft PDEPs at all in the roadmap, but giving it it's own section is fine as well.","['Docs', 'Needs Triage']",2024-07-16 12:29:12,2024-07-16 16:45:39,0,closed
59252,"BUG: rolling window with `center=True, min_periods=1` is not symmetric at edges","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
pd.Series(np.arange(100)).rolling(21, center=True, min_periods=1).mean().plot()
```


### Issue Description

The `np.arange` gives a simple linear trend that should not be affected by the rolling mean filter.  However at the edges the mean filter pulls values more towards the centre than expected, causing kinks in the curve.  It looks like at the edge of the data Null values creep into the window and these are ignored by the mean filter.  Because Null values only creep into one side of the window the effective centre value gets offset.

### Expected Behavior

I would expect both sides of the window to be shrunk so that the point under examination is the centre of the live data.

### Installed Versions


INSTALLED VERSIONS
------------------
commit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2
python                : 3.12.2.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 3.10.0-1160.36.2.el7.x86_64
Version               : #1 SMP Wed Jul 21 11:57:15 UTC 2021
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.1
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.8.2
setuptools            : 69.1.1
pip                   : 24.0
Cython                : 3.0.10
pytest                : None
hypothesis            : None
sphinx                : 7.2.6
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.22.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.3
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

","['Bug', 'Window']",2024-07-16 11:26:40,2024-11-05 13:13:53,7,closed
59237,ENH: Reduce type requirements for the subset parameter in drop_duplicates/duplicated,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I would like to pass a set to drop_duplicates like so:

```
df = get_some_df()
subset = {""column1"", ""column3""}
df_dropped = df.drop_duplicates(subset=subset)
```
According to type hints, `subset` is a `Hashable | Sequence[Hashable] | None`. The documentation says ""column label or sequence of labels, optional"". 

The problem is, I would like to pass a set of columns. The name `subset` suggests that should be ok. And it does work indeed. **However, a `set` is not a `Sequence`.**

Can the requirements be lowered? Maybe to `Collection`? (or even `Iterable`, but that might come with problems).

Looking at the code: https://github.com/pandas-dev/pandas/blob/v2.2.2/pandas/core/frame.py#L6935

It checks if the subset is `np.iterable`, then a `set` is created and `len(subset)` is called. This could be done with collections as well.

Am I missing anything or could this Sequence be make a Collection?



### Feature Description

If I am not mistaken, Sequence could simply be replaced with Collection in duplicated as well as drop_duplicates + in the docu.

### Alternative Solutions

no alt solution required

### Additional Context

_No response_","['Enhancement', 'duplicated']",2024-07-12 09:30:14,2024-08-13 23:46:21,5,closed
59233,BUG: json_normalize KeyError Key not found ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.json_normalize(data={'meta1':{'meta2':'meta_val'}, 'record1':{'record2':[{'rec1':'rec_val1'},{'rec1':'rec_val2'}]}}, meta=[['meta1','meta2']], record_path=['record1','record2'])
```


### Issue Description

fact behaviour: ` KeyError: ""Key 'meta2' not found.`


### Expected Behavior

expected df = 
       rec1                  meta1.meta2
0  rec_val1  meta_val
1  rec_val2  meta_val

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 16 Model 4 Stepping 2, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : Russian_Russia.1251

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO JSON', 'Needs Info']",2024-07-11 20:03:32,2024-11-10 14:07:16,3,closed
59232,"DOC: ""list"" is not a keyword - .query()","> Column names which are Python keywords (like “list”, “for”, “import”, etc) cannot be used.

- [permalink](https://github.com/pandas-dev/pandas/blob/d96646219618e007f64ee49e0a6e20f4aea761b5/pandas/core/frame.py#L4475-L4476)

`for` and `import` are keywords, but `list` is not.

I just tried it and it's fine:
```python
>>> df = pd.DataFrame({'list': [0, 1, 2]})
>>> df.query('list == 1')
   list
1     1
```

While `import` does fail:
```python
>>> df1 = pd.DataFrame({'import': ['beans', 'corn']})
>>> df1.query('import == ""beans""')
    ...
SyntaxError: Python keyword not valid identifier in numexpr query
>>> # working alternative for reference
>>> df1[df1['import'] == 'beans']
  import
0  beans
```",[],2024-07-11 16:07:16,2024-07-12 10:43:12,1,closed
59231,"DOC: sentence fragment in ""String methods""","https://github.com/pandas-dev/pandas/blob/d96646219618e007f64ee49e0a6e20f4aea761b5/doc/source/user_guide/text.rst?plain=1#L207

Context:

https://github.com/pandas-dev/pandas/blob/d96646219618e007f64ee49e0a6e20f4aea761b5/doc/source/user_guide/text.rst?plain=1#L205-L210

Maybe it wasn't intended this way, but I'm reading it as two independent clauses joined by ""and"":

1. ""The type of the Series is inferred"" - this is fine
2. ""the allowed types (i.e. strings)."" - this is a sentence fragment

I'm not sure what it's trying to say. Is it supposed to be something more like this?

> The type of the Series and the allowed types (i.e. strings) **are** inferred.

Or this?

> The type of the Series is inferred, as well as the allowed types (i.e. strings).",[],2024-07-11 15:45:18,2024-07-22 17:27:30,1,closed
59226,ENH: is it worth fixing a warning from a third party library here,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I wanted to fix a warning in another library:
`FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use ser.iloc[pos]`

As it turns out, these lines are from pandas.

### Feature Description

pandas/tests/extension/base/getitem.py  row --- 276
pandas/tests/extension/base/setitem.py row --- 244

Does this need to be fixed?

I also wanted to run this test directly, but I couldn’t:
`pytest pandas/tests/extension/base/getitem.py::test_getitem_series_integer_with_missing_raises`

It is launched only like this (repeatedly everywhere where it is):
`pytest pandas -k test_getitem_series_integer_with_missing_raises `

But I can't get the warning that I get in tests of another library.

### Alternative Solutions

Use iloc.

pandas/tests/extension/base/getitem.py:
before  `ser[idx]`
after s`ser.iloc[idx]`

pandas/tests/extension/base/setitem.py:
before  `arr[idx] = arr[0]`
after s`arr[idx] = arr.iloc[0]`


### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-07-11 07:54:01,2024-07-12 17:53:54,5,closed
59220,"NON-BUG: `to_csv()` argument `float_format` has no effect, always saves with format of ""%.2g""","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas import Series

series: Series = Series(name=""Bug"", data=[1 / 3])
series.to_csv(""float_format_bug.csv"", index=False, float_format=""%.6f"")
# | Bug  |
# | 0.33 |
```


### Issue Description

Attempting to save `float64` values (and potentially other floating point numbers) to a .csv file with `to_csv()` causes them to forcibly be truncated to two decimal places, no matter what preventative measures are taken to circumvent this behaviour (for example, converting the numbers to strings before saving with `series.astype(str)`). The bug continues to occur if no name is given to the Series, but does not occur when `to_csv()` is given the argument `header=False`, yet this behaviour should be universal.

### Expected Behavior

Floating point numbers should be formatted according to the specification in `float_format` when saving using `to_csv()`.

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.2.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_Australia.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.1.1
Cython                : None
pytest                : 8.2.1
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.1
gcsfs                 : None
matplotlib            : 3.9.1
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.14.0
sqlalchemy            : 2.0.31
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>","['Bug', 'Needs Triage']",2024-07-10 00:02:57,2024-07-10 23:04:00,2,closed
59218,BUG: `pandas.tseries.frequencies.to_offset()` raises `ValueError` when parsing a `LastWeekOfMonth` frequency string,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.tseries.frequencies.to_offset(pd.offsets.LastWeekOfMonth().freqstr)
```


### Issue Description

The code above raises the following exception:
```python-traceback
Traceback (most recent call last):
  File ""offsets.pyx"", line 4776, in pandas._libs.tslibs.offsets._get_offset
KeyError: 'LWOM'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""offsets.pyx"", line 4946, in pandas._libs.tslibs.offsets.to_offset
  File ""offsets.pyx"", line 4782, in pandas._libs.tslibs.offsets._get_offset
ValueError: Invalid frequency: LWOM-MON, failed to parse with error message: KeyError('LWOM')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""offsets.pyx"", line 4791, in pandas._libs.tslibs.offsets.to_offset
  File ""offsets.pyx"", line 4954, in pandas._libs.tslibs.offsets.to_offset
ValueError: Invalid frequency: LWOM-MON, failed to parse with error message: ValueError(""Invalid frequency: LWOM-MON, failed to parse with error message: KeyError('LWOM')"")
```

I've tracked down the issue to `LastWeekOfMonth` not being present here: https://github.com/pandas-dev/pandas/blob/d96646219618e007f64ee49e0a6e20f4aea761b5/pandas/_libs/tslibs/offsets.pyx#L4647.

### Expected Behavior

`pd.tseries.frequencies.to_offset` can parse any `LastWeekOfMonth` frequency string.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.9.7-arch1-1
Version               : #1 SMP PREEMPT_DYNAMIC Fri, 28 Jun 2024 04:32:50 +0000
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
None

</details>
","['Bug', 'Needs Triage']",2024-07-09 23:29:35,2024-07-16 09:10:27,2,closed
59213,BUG: pd.unique() does not accept NumpyExtensionArray,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
arr_int = pd.array([1, 2, 3]) # Int64Dtype() => ExtensionArray
arr_complex = pd.array([1+1j, 2, 3]) # NumpyEADtype('complex128') => NumpyExtensionArray

pd.unique(arr_int)  # OK
pd.unique(arr_complex)  # NOK: TypeError: unique requires a Series, Index, ExtensionArray, or np.ndarray, got NumpyExtensionArray.
```


### Issue Description

This issue is similar to #59177 and is coming from `pint-pandas` where test CI job with nightly builds from pandas was recently introduced ([pint-pandas issue related to skipped tests to pass tests with pandas 3](https://github.com/hgrecco/pint-pandas/issues/240)).

The above example is fine with pandas 2.2.2 but fails with pandas nightlies

The above mentionned `TypeError` is coming from https://github.com/pandas-dev/pandas/blob/ab433af410464f4f5c377c82a3d4f5680bf3c65c/pandas/core/algorithms.py#L222

As explained in #59177 `NumpyExtensionArray` may be used internally when other type of `ExtensionArray` are not available.

### Expected Behavior

for pandas 2.2.2 `pd.unique` returns:

``` python
>>> pd.unique(arr_complex)
array([1.+1.j, 2.+0.j, 3.+0.j])
```

While we expect

```python
>>> pd.unique(arr_complex)
<NumpyExtensionArray>
[(1+1j), (2+0j), (3+0j)]
Length: 3, dtype: complex128
```

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2024-07-08 21:18:03,2024-07-09 16:38:26,2,closed
59204,BUG: Series constructor with category dtype does not raise with unknown categories,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Assignment fails as expected:


>>> cat = pd.CategoricalDtype([""foo"", ""bar"", ""baz""])
>>> ser = pd.Series([""foo"", ""baz"", ""baz""], dtype=cat)
>>> ser.iloc[2] = ""qux""
TypeError: Cannot setitem on a Categorical with a new category (qux), set the categories first


But the constructor just maps this to a missing value:
```python
>>> pd.Series([""foo"", ""baz"", ""baz"", ""qux""], dtype=cat)
Out[13]: 
0    foo
1    baz
2    baz
3    NaN
dtype: category
Categories (3, object): ['foo', 'bar', 'baz']
```
```


### Issue Description

The Series constructor allows you to use values that are not part of the categorical values, mapping them to missing

### Expected Behavior

I think the constructor should fail just like assignment does

### Installed Versions

main
","['Bug', 'Categorical', 'Constructors']",2024-07-07 23:46:55,2024-07-08 13:52:25,5,closed
59203,BUG: Pandas 2.2.2 incompatible with Numpy 2. AttributeError: _ARRAY_API not found ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd


A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File ""/Users/dan/Library/Application Support/JetBrains/PyCharmCE2024.1/scratches/scratch_30.py"", line 4, in <module>
    import pandas as pd
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/__init__.py"", line 49, in <module>
    from pandas.core.api import (
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/api.py"", line 28, in <module>
    from pandas.core.arrays import Categorical
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/__init__.py"", line 1, in <module>
    from pandas.core.arrays.arrow import ArrowExtensionArray
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py"", line 5, in <module>
    from pandas.core.arrays.arrow.array import ArrowExtensionArray
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py"", line 50, in <module>
    from pandas.core import (
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/ops/__init__.py"", line 8, in <module>
    from pandas.core.ops.array_ops import (
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/ops/array_ops.py"", line 56, in <module>
    from pandas.core.computation import expressions
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/computation/expressions.py"", line 21, in <module>
    from pandas.core.computation.check import NUMEXPR_INSTALLED
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/computation/check.py"", line 5, in <module>
    ne = import_optional_dependency(""numexpr"", errors=""warn"")
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/compat/_optional.py"", line 135, in import_optional_dependency
    module = importlib.import_module(name)
  File ""/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/homebrew/lib/python3.11/site-packages/numexpr/__init__.py"", line 24, in <module>
    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__
AttributeError: _ARRAY_API not found

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File ""/Users/dan/Library/Application Support/JetBrains/PyCharmCE2024.1/scratches/scratch_30.py"", line 4, in <module>
    import pandas as pd
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/__init__.py"", line 49, in <module>
    from pandas.core.api import (
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/api.py"", line 28, in <module>
    from pandas.core.arrays import Categorical
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/__init__.py"", line 1, in <module>
    from pandas.core.arrays.arrow import ArrowExtensionArray
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py"", line 5, in <module>
    from pandas.core.arrays.arrow.array import ArrowExtensionArray
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py"", line 64, in <module>
    from pandas.core.arrays.masked import BaseMaskedArray
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/masked.py"", line 60, in <module>
    from pandas.core import (
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/core/nanops.py"", line 52, in <module>
    bn = import_optional_dependency(""bottleneck"", errors=""warn"")
  File ""/opt/homebrew/lib/python3.11/site-packages/pandas/compat/_optional.py"", line 135, in import_optional_dependency
    module = importlib.import_module(name)
  File ""/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/homebrew/lib/python3.11/site-packages/bottleneck/__init__.py"", line 7, in <module>
    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,
AttributeError: _ARRAY_API not found
```



### Issue Description

Unable to import Pandas after upgrading to 2.2.2 with Numpy 2.0.0.

### Expected Behavior

That error should not occur as Numpy 2 is supposed to be supported in Pandas 2.2.2 as per release notes(https://pandas.pydata.org/docs/whatsnew/v2.2.2.html)

### Installed Versions

<details>

Unable to run `pd.show_versions()` as it throws error above when importing pandas

```
> pip list 
...
pandas                    2.2.2
numpy                     2.0.0
pyarrow                   16.1.0
...
```

</details>","['Bug', 'Needs Triage']",2024-07-07 19:43:02,2024-07-08 15:17:52,5,closed
59199,ENH: Follow dict color in pandas plotting,"### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description


Should it not be more intuitive that plot color strictly follows the color dict and not depend on the order in which col appears in the dataframe? 

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Create sample data
np.random.seed(0)
df = pd.DataFrame({
    'x': np.random.rand(50),
    'y': np.random.rand(50),
})

# Create bar plot with x in red and y in green using pandas.plot
ax = df[['y','x']].plot(kind='bar', color={'x':'red', 'y':'green','z':'brown'},stacked=True, figsize=(12, 6))

# Customize the plot
plt.title('Bar Plot with X in Red and Y in Green')
plt.xlabel('Index')
plt.ylabel('Values')
plt.legend(['x', 'y'])

# Adjust layout to prevent cutting off labels
plt.tight_layout()

# Show the plot
plt.show()
```

### Feature Description

Also looks like pandas plot is heavily geared towards wide data format and it can be very useful if the approach changes towards the long data format.

### Alternative Solutions

Make color follow the color dict.

### Additional Context

NA","['Enhancement', 'Needs Triage']",2024-07-07 13:26:43,2024-07-08 21:32:14,2,closed
59196,BUG: ValueError when accessing dataFrame with array attribute,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

attrs = {""A"": ""B"", ""G"": np.array([1.2, 2.4])}

# This one works
arr = np.random.rand(60, 1)
df_named = pd.DataFrame(arr)
df_named.attrs = attrs
print(df_named[0])

# This one works
arr = np.random.rand(61, 1)
df_named = pd.DataFrame(arr)
df_named.attrs = {""A"": ""B"", ""G"": ""A""}
print(df_named[0])

# This one does not works
arr = np.random.rand(61, 1)
df_named = pd.DataFrame(arr)
df_named.attrs = attrs
print(df_named)  # This works
print(df_named[0])  # This does not works
```


### Issue Description

Hello, 

I have a dataFrame of size (61,1) with 2 attributes (one is an array) and I can't print the first Serie of the DataFrame.
I have the following Error : 
```
Traceback (most recent call last):

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
    exec(code, globals, locals)

  File d:\documents\perso\travail\mbda\pandas_extension\h5pandas\tests\debug.py:23
    print(df_named[0])  # This does not works

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\pandas\core\series.py:1784 in __repr__
    return self.to_string(**repr_params)

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\pandas\core\series.py:1871 in to_string
    formatter = fmt.SeriesFormatter(

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\pandas\io\formats\format.py:225 in __init__
    self._chk_truncate()

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\pandas\io\formats\format.py:247 in _chk_truncate
    series = concat((series.iloc[:row_num], series.iloc[-row_num:]))

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\pandas\core\reshape\concat.py:395 in concat
    return op.get_result()

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\pandas\core\reshape\concat.py:650 in get_result
    return result.__finalize__(self, method=""concat"")

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\pandas\core\generic.py:6273 in __finalize__
    have_same_attrs = all(obj.attrs == attrs for obj in other.objs[1:])

  File ~\miniforge-pypy3\envs\h5pandas_dev\Lib\site-packages\pandas\core\generic.py:6273 in <genexpr>
    have_same_attrs = all(obj.attrs == attrs for obj in other.objs[1:])

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

```

However I can print the DataFrame, it does not raise the ValueError.
If the DataFrame hasn't got the array attribute, I do not have ValueError.
If the DataFrame has only 60 rows, I do not have ValueError.

### Expected Behavior

I should not have this ValueError.

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 23 Model 1 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : fr_FR.cp1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 70.1.1
pip                   : 24.0
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : 2.4.1
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-07-06 22:52:44,2025-02-21 07:03:19,5,closed
59192,"BUG: Pyarrow series .any() returns a bool, messing up plotting","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

series = pd.Series([1, 2, 3, 4], dtype='int32[pyarrow]')

series.plot(kind='pie')
```


### Issue Description

The example yields this error:

```
Traceback (most recent call last):
  File ""/home/jon/work/panda.py"", line 5, in <module>
    series.plot(kind='pie')
  File ""/home/jon/.local/lib/python3.11/site-packages/pandas/plotting/_core.py"", line 1030, in __call__
    return plot_backend.plot(data, kind=kind, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jon/.local/lib/python3.11/site-packages/pandas/plotting/_matplotlib/__init__.py"", line 70, in plot
    plot_obj = PLOT_CLASSES[kind](data, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jon/.local/lib/python3.11/site-packages/pandas/plotting/_matplotlib/core.py"", line 2051, in __init__
    if (data < 0).any().any():
       ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'bool' object has no attribute 'any'
```

The error comes from `(data < 0).any()` giving a boolean (`<class 'bool'>`) instead of a numpy boolean (`<class 'numpy.bool_'>`). 

```
>>> series = pd.Series([1, 2, 3, 4], dtype='int32[pyarrow]')
>>> type((series < 0).any())
<class 'bool'>
>>> series = pd.Series([1, 2, 3, 4], dtype='int32')         
>>> type((series < 0).any())                       
<class 'numpy.bool_'>
```

When `.any()` is called on the numpy bool, it just returns the same value, but the AttributeError is raised when `.any()` is called on the boolean.

### Expected Behavior

This code does exactly what I want:

```py
import pandas as pd
import matplotlib.pyplot as plt

series = pd.Series([1, 2, 3, 4], dtype='int32[pyarrow]')

plt.pie(series)
```

and works, but I don't have direct control over the code as I'm using an LLM to write code and it still uses the `series.plot` feature. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.3.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.4.0-187-generic
Version               : #207-Ubuntu SMP Mon Jun 10 08:16:10 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.24.3
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 23.2.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : 2.9.7
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.7.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : 0.21.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Arrow']",2024-07-05 17:46:13,2024-07-15 18:22:00,0,closed
59186,BUG: Difference between calamine and openpyxl readers - columns with mixed data types,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
from io import BytesIO
from datetime import datetime

df = pd.DataFrame({'a': ['abc', np.nan, datetime.now(), 'def']})

out = BytesIO()
df.to_excel(out, index=False)
df_openpyxl = pd.read_excel(out, engine='openpyxl')
df_calamine = pd.read_excel(out, engine='calamine')

In [49]: df_openpyxl['a'].map(type)
Out[49]:
0                  <class 'str'>
1                <class 'float'>
2    <class 'datetime.datetime'>
3                  <class 'str'>
Name: a, dtype: object

In [50]: df_calamine['a'].map(type)
Out[50]:
0                                        <class 'str'>
1                                      <class 'float'>
2    <class 'pandas._libs.tslibs.timestamps.Timesta...
3                                        <class 'str'>
Name: a, dtype: object
```


### Issue Description

When trying to read an excel file that has mixed formats, using the calamine engine - results in an object dtype column where the value is `pandas._libs.tslibs.timestamps.Timestamp` and `pandas._libs.tslibs.timedeltas.Timedelta` whereas with the openpyxl engine, these values are `datetime.datetime` and `datetime.timedelta` instead.

The difference seems to be because the calamine implementation explicitly sets `pd.Timestamp/pd.Timedelta` data types
https://github.com/pandas-dev/pandas/blob/79067a76adc448d17210f2cf4a858b0eb853be4c/pandas/io/excel/_calamine.py#L109-L115

whereas the openpyxl implementation uses whatever the library returns.

### Expected Behavior

The individual item data types should match

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : dd87dd3ef661ac06e30adc55d25a1f03deda3abf
python                : 3.10.14
python-bits           : 64
OS                    : Linux
OS-release            : 4.19.128-microsoft-standard
Version               : #1 SMP Tue Jun 23 12:58:10 UTC 2020
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1113.gdd87dd3ef6
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
pip                   : 24.0
Cython                : 3.0.10
sphinx                : 7.3.7
IPython               : 8.25.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.3.8
fastparquet           : 2024.5.0
fsspec                : 2024.6.0
html5lib              : 1.1
hypothesis            : 6.103.0
gcsfs                 : 2024.6.0
jinja2                : 3.1.4
lxml.etree            : 5.2.2
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.2
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 16.1.0
pyreadstat            : 1.2.7
pytest                : 8.2.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.6.0
scipy                 : 1.13.1
sqlalchemy            : 2.0.30
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.5.0
xlrd                  : 2.0.1
xlsxwriter            : 3.1.9
zstandard             : 0.19.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO Excel', 'good first issue']",2024-07-05 10:22:55,2025-12-11 00:04:18,8,closed
59184,BUG: Timestamp.asm8 has different behavior depending on how it was constructed,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [2]: ts1 = pd.Timestamp(year=2023, month=1, day=1, hour=10, second=15)

In [3]: ts2 = pd.Timestamp('2023-01-01 10:00:15') #  ts1 and ts2 are equal

In [4]: np.int64(ts1.asm8)
Out[4]: 1672567215000000

In [5]: np.int64(ts2.asm8)
Out[5]: 1672567215
```


### Issue Description

In the [timestamp.asm8 doc](https://pandas.pydata.org/docs/dev/reference/api/pandas.Timestamp.asm8.html) it states ""Return numpy datetime64 format in nanoseconds."", but, it has different (wrong) behavior when I construct the timestamp from a string, returning in seconds format. Any thoughts @MarcoGorelli?

### Expected Behavior

I would expect `np.int64(ts2.asm8)` to be equal to `1672567215000000000`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 1b2d39cbfc3db9520e0f5455a4eb3da0f569fa0f
python                : 3.12.1
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
pip                   : 24.0
Cython                : 3.0.10
sphinx                : 7.3.7
IPython               : 8.23.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
blosc                 : None
bottleneck            : 1.3.8
fastparquet           : 2024.2.0
fsspec                : 2024.3.1
html5lib              : 1.1
hypothesis            : 6.100.1
gcsfs                 : 2024.3.1
jinja2                : 3.1.4
lxml.etree            : 5.2.1
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.2
psycopg2              : 2.9.9
pymysql               : 1.4.6
pyarrow               : 16.1.0
pyreadstat            : 1.2.7
pytest                : 8.1.1
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.3.1
scipy                 : 1.13.0
sqlalchemy            : 2.0.29
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.5.0
xlrd                  : 2.0.1
xlsxwriter            : 3.2.0
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-07-04 23:50:34,2024-07-08 15:35:41,5,closed
59180,PERF: to_excel slowing runtime,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

```
import numpy as np 
import pandas as pd 

rng = np.random.default_rng()
df = pd.DataFrame(rng.integers(0, 100, size=(100_000, 500)), columns=np.arange(0,500))

%time df.to_excel(r""\to_excel issues\test.xlsx"")
```


New:  Wall time: 18min 35s, 18min 29s, 15min 27s, 15min 25s  
Old:  Wall time: 14min 2s, 12min 10s,14min 7s  

The differences aren't that large for this size but when you start getting bigger it grows exponentially (100k, 1k) already takes over an hour or over two hours.

### Installed Versions

<details>

New:
INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.11.7.final.0
python-bits         : 64
OS                  : Windows
OS-release          : 10
Version             : 10.0.19045
machine             : AMD64
processor           : AMD64 Family 25 Model 80 Stepping 0, AuthenticAMD
byteorder           : little
LC_ALL              : None
LANG                : None
LOCALE              : English_United States.1252

pandas              : 2.1.4
numpy               : 1.26.4
pytz                : 2023.3.post1
dateutil            : 2.8.2
setuptools          : 68.2.2
pip                 : 23.3.1
Cython              : None
pytest              : 7.4.0
hypothesis          : None
sphinx              : 5.0.2
blosc               : None
feather             : None
xlsxwriter          : None
lxml.etree          : 4.9.3
html5lib            : None
pymysql             : 1.4.6
psycopg2            : None
jinja2              : 3.1.3
IPython             : 8.20.0
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : 1.3.7
dataframe-api-compat: None
fastparquet         : None
fsspec              : 2023.10.0
gcsfs               : None
matplotlib          : 3.8.0
numba               : 0.59.0
numexpr             : 2.8.7
odfpy               : None
openpyxl            : 3.1.2
pandas_gbq          : None
pyarrow             : 14.0.2
pyreadstat          : None
pyxlsb              : None
s3fs                : 2023.10.0
scipy               : 1.11.4
sqlalchemy          : 2.0.25
tables              : 3.9.2
tabulate            : 0.9.0
xarray              : 2023.6.0
xlrd                : None
zstandard           : 0.19.0
tzdata              : 2023.3
qtpy                : 2.4.1
pyqt5               : None

</details>


### Prior Performance

<details>

Old:  
INSTALLED VERSIONS
------------------
Wall time: 14min 2s, 12min 10s,14min 7s  

commit           : 2cb96529396d93b46abab7bbc73a208e708c642e
python           : 3.8.8.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : AMD64 Family 25 Model 80 Stepping 0, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.2.4
numpy            : 1.20.1
pytz             : 2021.1
dateutil         : 2.8.1
pip              : 21.0.1
setuptools       : 52.0.0.post20210125
Cython           : 0.29.23
pytest           : 6.2.3
hypothesis       : None
sphinx           : 4.0.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.8
lxml.etree       : 4.6.3
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.3
IPython          : 7.22.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.9.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.4
numexpr          : 2.7.3
odfpy            : None
openpyxl         : 3.0.7
pandas_gbq       : None
pyarrow          : 9.0.0
pyxlsb           : None
s3fs             : None
scipy            : 1.6.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 2.0.1
xlwt             : 1.3.0
numba            : 0.53.1
</details>","['Performance', 'IO Excel']",2024-07-04 08:44:47,2024-07-04 12:24:01,2,closed
59177,BUG: pd.api.extensions.take does not accept NumpyExtensionArray,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd 
import numpy as np

arr = pd.arrays.NumpyExtensionArray(np.array([1,2,3]))
pd.api.extensions.take(arr,[2])

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\mambaforge\envs\test_py300\Lib\site-packages\pandas\core\algorithms.py"", line 1166, in take
    raise TypeError(
TypeError: pd.api.extensions.take requires a numpy.ndarray, ExtensionArray, Index, or Series, got NumpyExtensionArray.
```


### Issue Description

I'd expect NumpyExtensionArray to be considered an ExtensionArray, so compatable with pd.api.extensions.take.

### Expected Behavior

return
```
<NumpyExtensionArray>
[2]
Length: 1, dtype: int32
```

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Algos']",2024-07-03 19:13:18,2024-07-06 16:14:39,4,closed
59176,ENH: Add Dict type to pd.concat objs,"### Feature Type

- [X] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Pandas append allowed dict type and since deprecating append, concat has not been amended to follow suite

### Feature Description

Add dict-like object to concat method to mimic append functionality

### Alternative Solutions

If you would rather document here the recommended methodology for converting a dict like object to a dataframe

### Additional Context

_No response_","['Enhancement', 'Reshaping']",2024-07-03 16:50:08,2024-07-03 17:20:45,5,closed
59174,BUG: incorrect cummax() results when applied to PyArrow dtypes,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
s = pd.Series([-5, -2, -3, 1, 4, 3.0], dtype=""float64"")

s.cummax()
# 0   -5.0
# 1   -2.0
# 2   -2.0
# 3    1.0
# 4    4.0
# 5    4.0
# dtype: float64


s.astype(""double[pyarrow]"").cummax()
# 0    0.0
# 1    0.0
# 2    0.0
# 3    1.0
# 4    4.0
# 5    4.0
# dtype: double[pyarrow]


s.cummax().iat[0]
# -5.0

s.astype(""double[pyarrow]"").cummax().iat[0]
# 2.2250738585072014e-308

float.hex(s.astype(""double[pyarrow]"").cummax().iat[0])
# '0x1.0000000000000p-1022'
```


### Issue Description

Series.cummax() results are wrong when the Series has a PyArrow floating point dtype (either `double[pyarrow]` or `float[pyarrow]` ) and the first elements are negative.

When the correct cummax value would be negative, cummax actually returns the subnormal number: `2.2250738585072014e-308` (for `double[pyarrow]` dtypes)



### Expected Behavior

cummax() results are the same regardless of the dtype.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.146.1-microsoft-standard-WSL2
Version               : #1 SMP Thu Jan 11 04:09:03 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : C.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 70.1.1
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.26.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Upstream issue']",2024-07-03 15:21:40,2024-07-03 16:44:50,1,closed
59166,QST: Critical Vulnerability detection in pandas Library by Fortify due to read_pickle function,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/78683485/critical-vulnerability-detection-in-pandas-library-by-fortify-due-to-read-pickle

### Question about pandas

Our company utilizes the pandas library extensively in our software. However, **Fortify**, our security analysis tool,**flags pandas as having a critical vulnerability due to the read_pickle function**, which has the potential to execute malicious code. **Note that we do not use the read_pickle function in our codebase, but Fortify still identifies this as a critical issue.**

We have tried to solve this by importing only specific submodules of pandas that we use (to avoid importing the entire library) but Fortify continues to detect the critical vulnerability associated with read_pickle.

```
from pandas import DataFrame, Series, read_csv
However, Fortify continues to flag the vulnerability.
```

We wonder if there is a solution to this issue. **Is there a way to structure our imports or configure pandas to avoid including read_pickle entirely?**

Alternatively, **is it possible to address this concern directly within the pandas library to enhance its security profile?**","['Usage Question', 'Needs Triage']",2024-07-02 08:26:15,2024-07-02 15:54:21,3,closed
59165,"ENH: In pd.cut(), allow bins='auto' (leveraging `np.histogram_bin_edges`)","### Feature Type

- [ ] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

While converting a quantitative variable into a qualitative one, `pd.cut()` comes in clutch. However, it requires the user to specify `bins` as either an integer or a list of bin edges. I wish it was allowed to specify `bins='auto'` similar to how `np.histogram` allows it. It internally leverages `np.histogram_bin_edges` to compute these. Thank you.

### Expectation
Instead of coding 
`pd.cut(df['x1'], bins=np.histogram_bin_edges(df['x1'], bins='auto'))`
Allow for coding
`pd.cut(df['x1'], bins='auto')`

### Additional Context

Calculation of bin edges is already done via `np.histogram_bin_edges`. Reference: https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html#numpy-histogram-bin-edges","['Enhancement', 'cut']",2024-07-02 05:31:40,2024-07-22 20:06:17,4,closed
59160,VOTE: Voting issue for PDEP-14: Dedicated string data type for pandas 3.0 ,"### PDEP number and title

PDEP-14: Dedicated string data type for pandas 3.0

### Pull request with discussion

#58551

### Rendered PDEP for easy reading

https://jorisvandenbossche.github.io/pandas-website-preview/pdeps/0014-string-dtype.html

### Discussion participants

_No response_

### Voting will close in 15 days.

July 16

### Vote

Cast your vote in a comment below.
* +1: approve.
* 0: abstain.
    * Reason: A one sentence reason is required.
* -1: disapprove
    * Reason: A one sentence reason is required.
A disapprove vote requires prior participation in the linked discussion PR.

@pandas-dev/pandas-core
",['Vote'],2024-07-01 18:23:19,2024-07-22 20:34:47,14,closed
59154,BUG: `.dt.microsecond` for pyarrow-backed Series returns 0,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [22]: s = pd.Series(pd.date_range('2000', periods=3, freq='15ms'))

In [23]: s.dt.microsecond
Out[23]:
0        0
1    15000
2    30000
dtype: int32

In [24]: s.convert_dtypes(dtype_backend='pyarrow')
Out[24]:
0           2000-01-01 00:00:00
1    2000-01-01 00:00:00.015000
2    2000-01-01 00:00:00.030000
dtype: timestamp[ns][pyarrow]

In [25]: s.convert_dtypes(dtype_backend='pyarrow').dt.microsecond
Out[25]:
0    0
1    0
2    0
dtype: int64[pyarrow]
```


### Issue Description

For non-pyarrow backed dtype, it returns the total number of microseconds since the last second

For pyarrow-backed, it just returns 0

### Expected Behavior
```python
In [25]: s.convert_dtypes(dtype_backend='pyarrow').dt.microsecond
Out[25]:
0        0
1    15000
2    30000
dtype: int64[pyarrow]
```

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.153.1-microsoft-standard-WSL2
Version               : #1 SMP Fri Mar 29 23:14:13 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 24.0
Cython                : None
pytest                : 8.1.1
hypothesis            : 6.100.1
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.23.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Datetime', 'Arrow']",2024-07-01 12:17:56,2024-07-08 15:21:21,2,closed
59153,BUG: Enlarging multilevel index fails if one or more level keys are None,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

#Create simple multilevel index with two levels (note one entry on level 1 is None):
index = pd.MultiIndex.from_tuples([('A', 'a1'), ('A', 'a2'), ('B', 'b1'), ('B', None)])

#Create dataframe with said index:
pd.DataFrame([(0, 6), (1, 5), (2, 4), (3, 7)], index=index)

#       0  1
#A a1   0  6
#  a2   1  5
#B b1   2  4
#  NaN  3  7

#Now it is possible to enlarge this dataframe with a new index entry provided none of the keys are None:
df.loc[('B', 'b2'),:] = [10, 11]

#           0     1
# A a1    0.0   6.0
#   a2    1.0   5.0
# B b1    2.0   4.0
#   NaN   3.0   7.0
#   b2   10.0  11.0

#However this will throw a KeyError:
df.loc[('A', None),:] = [12, 13]

#Also doesn't work with an index slice:
idx = pd.IndexSlice

#this will throw a KeyError:
df.loc[idx['A', None],:] = [12, 13]
```


### Issue Description

It is possible to enlarge a dataframe with a multilevel indexes by providing the new key as parameters to df.loc[...]

It is also possible to create entries to multilevel indices that have None as the key i.e. df.loc[('A', None),...]

It is *not* possible to enlarge a dataframe with a multilevel index if one or more of the keys is None.

### Expected Behavior

Building on the example above, 
`df.loc[('A', None),:] = [12, 13]`

should result in the following:
```#           0     1
# A a1    0.0   6.0
#   a2    1.0   5.0
#   NaN  12.0  13.0
# B b1    2.0   4.0
#   NaN   3.0   7.0
#   b2   10.0  11.0
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.6.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 63.2.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data', 'MultiIndex', 'good first issue', 'Needs Tests', 'setitem-with-expansion']",2024-07-01 08:25:20,2025-11-08 17:40:07,5,closed
59149,database is locked,"### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/3172929/operationalerror-database-is-locked

### Question about pandas

![Image_1719753762358](https://github.com/pandas-dev/pandas/assets/45912561/07e22cab-0072-40e8-a396-bb4cee3abfa2)

The picture is error log

My program transfers the downloaded data to the database by to_sql.The programe is closed like Ctrl+C and closed terminal ，and when I restart my programe，the datebase is locked.Even though I've tried to delay retrying.I close the cursor and connection of the data through the atexit module when the program is closed.But The problem remains.
I don't know how to solve this problem. She has been bothering me for a long time.Thanks
","['Usage Question', 'Needs Triage']",2024-06-30 13:36:09,2024-07-25 09:07:33,1,closed
59147,BUG: read_html returns empty list,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas
from _io import StringIO

table = '<table><tr><td> </td></tr></table>'
res = pandas.read_html(StringIO(table), flavor='lxml')
print(len(res))
```


### Issue Description

From the `read_html` docstring: 

> This function will *always* return a list of :class:`DataFrame` *or*
    it will fail, i.e., it will *not* return an empty list.

It has something to do with the space in the `<td>` tag in the example. Removing the space causes the function to fail instead.

### Expected Behavior

The function should either fail, or return a list containing a `DataFrame` representing a 1x1 table (either empty or containing the space character in its only cell). Don't know which is more appropriate. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.19.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.0-30-amd64
Version               : #1 SMP Debian 5.10.218-1 (2024-06-01)
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.24.1
pytz                  : 2024.1
dateutil              : 2.8.2
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.5.0
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : 2.0.30
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO HTML']",2024-06-29 14:40:55,2024-07-08 17:06:35,3,closed
59138,ENH: is it possible to save a reference to a method depending on the version,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

I did the [PR.](https://github.com/pmorissette/ffn/pull/235) Based on the version, map or applymap is selected. This requires additional lines of code.
Depending on the version, is it possible to save the link to a variable and use it instead of map, applymap?

### Feature Description

Select map or applymap depending on the version of pandas.

### Alternative Solutions

Pseudocode:

```
PANDAS_VERSION = Version(pd.__version__)
PANDAS_210 = PANDAS_VERSION >= Version(""2.1.0"")

select_map = ''

if PANDAS_210:
   select_map = map
else:
   select_map = applymap
   
df.select_map(lambda x: len(str(x)))
```

### Additional Context

_No response_","['Usage Question', 'Closing Candidate']",2024-06-28 11:34:59,2024-06-28 13:23:09,3,closed
59132,BUG: DataFrame to JSON failed when it with UUID,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import uuid
import pandas as pd

pd.DataFrame({""uuid"": [uuid.uuid4()]}).to_json()
```


### Issue Description

If the DataFrame is with UUID, it will fail when to JSON.
And raise the error with the message `Unsupported UTF-8 sequence length when encoding string` or `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa1 in position 183: invalid start byte`.

### Expected Behavior

It should serialize `uuid.UUID` instances to [RFC 4122](https://tools.ietf.org/html/rfc4122) format, e.g., `f81d4fae-7dec-11d0-a765-00a0c91e6bf6`. 

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.0.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.4.0
Version               : Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:41 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.1.1
pip                   : 24.0
Cython                : None
pytest                : 8.2.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : 1.4.6
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.30
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
None
</details>
","['Bug', 'IO JSON', 'Closing Candidate']",2024-06-28 03:55:17,2024-07-03 02:31:43,4,closed
59131,BUG: rounding datetime in series is broken,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
pd.Series([pd.to_datetime(""2022-01-09T12:00:01"")]).round(""30min"") == pd.Series([pd.to_datetime(""2022-01-09T12:00:01"").round(""30min"")])
```


### Issue Description

Repeat of issue of #57002

### Expected Behavior

example should return true

### Installed Versions

<details>


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.13.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.153.1-microsoft-standard-WSL2
Version               : #1 SMP Fri Mar 29 23:14:13 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 69.1.0
pip                   : 24.0
Cython                : None
pytest                : 8.1.1
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None


</details>
","['Bug', 'Needs Triage']",2024-06-28 02:31:44,2024-06-28 08:31:51,1,closed
59130,"DOC: ""Accelerated operations"" talks about speedup in obsolete versions of Pandas","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#accelerated-operations

Permalink to specific line:

https://github.com/pandas-dev/pandas/blob/a89f20853591516b4ba45a1fbadbf645247d133e/doc/source/user_guide/basics.rst?plain=1#L161

### Documentation problem

0.11.0 is more than 10 years old and, of course, before the first stable release.

### Suggested fix for documentation

If the important thing is the speedup you get from enabling a feature, then just talk about the speedup instead of when the feature first became available.

Something like this:

```rst
    :header: ""Operation"", ""With speedup (ms)"", ""Without speedup (ms)"", ""Ratio""
```","['Docs', 'good first issue']",2024-06-28 00:31:12,2024-08-05 17:41:22,9,closed
59126,BUG: pivot_table chokes on pd.DatetimeTZDtype if there are no rows.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame({'timestamp': [pd.Timestamp('2024-01-01T00:00', tz='UTC')], 'category': ['A'], 'value': [100]})
df[df.category == ""C""].pivot_table(index=""category"", columns=""value"", values=""timestamp"")
```


### Issue Description

This raises an error `TypeError: Cannot interpret 'datetime64[ns, UTC]' as a data type`

### Expected Behavior

It should return an empty DataFrame.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:14:38 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6020
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : fr_FR.UTF-8
LOCALE                : fr_FR.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 8.2.1
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.1
html5lib              : None
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : 8.23.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.5.0
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.0
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.5.0
scipy                 : None
sqlalchemy            : 2.0.30
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2024-06-27 19:32:10,2024-06-29 18:15:40,0,closed
59122,"BUG: 0/0 with arrow backend is not ""NA""","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pyarrow as pa
import pandas as pd

s = pd.Series([0], dtype=pd.ArrowDtype(pa.float64()))
(s / s).isna()
```


### Issue Description

Dividing by zero with the arrow backend produces `float('nan')` which is not detected as NA by pandas when it is inside an arrow series.

```python
In [25]: s / s
Out[25]:
0    NaN
dtype: double[pyarrow]

In [25]: (s / s).isna()
Out[25]:
0    False
dtype: bool

In [26]: (s / s).apply(pd.isna)
Out[26]:
0    True
dtype: bool

In [28]: pd.isna(float('nan'))
Out[28]: True

In [30]: pd.Series([float('nan')]).isna()
Out[30]:
0    True
dtype: bool
```



### Expected Behavior

What is considered a NaN should not be dependent on the dtype backend used. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.10.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.1.1
pip                   : 24.1.1
Cython                : None
pytest                : 8.2.1
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : 8.24.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.5.0
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.5.0
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Missing-data', 'Arrow']",2024-06-27 14:50:16,2024-07-16 11:16:25,4,closed
59121,BUG: random crash / hang when calculating rolling sum,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

def func():
    cols = ['A', 'B', 'C']
    df = pd.DataFrame({c: np.random.randn(500000) for c in cols})
    sec_lst = [60 * m for m in range(1, 10)]
    sec_lst += [60 * m for m in range(10, 30, 5)]
    sec_lst += [60 * m for m in range(30, 60, 10)]
    sec_lst += [60 * m for m in range(60, 120, 15)]
    sec_lst += [60 * m for m in range(120, 180, 20)]
    sec_lst += [60 * m for m in range(180, 600, 30)]
    sec_lst += [3600 * h for h in range(10, 24)]
    df_sum_dict = {}
    for sec in sec_lst:
        for c in cols:
            try:
                df_sum_dict[f'{c}{sec}'] = df[c].rolling(sec).sum()
            except Exception as e:
                print(f""Error processing column {c} with window {sec}: {e}"")
                continue

func()
```


### Issue Description

running with this loop results in the following output (Terminated is the output when I kill it because the script hangs, should run few seconds at most)

```
$ for i in $(seq 1 100); do python test.py $i; if [ $? -ne 0 ]; then echo $i; fi; done
Segmentation fault (core dumped)
6
Terminated
24
Terminated
41
Segmentation fault (core dumped)
42
Segmentation fault (core dumped)
43
Terminated
44
Segmentation fault (core dumped)
45
Terminated
64
Terminated
65
Segmentation fault (core dumped)
74
Segmentation fault (core dumped)
95
```

### Expected Behavior

it should not crash or hang

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.0-35-generic
Version               : #35-Ubuntu SMP PREEMPT_DYNAMIC Mon May 20 15:51:52 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.0.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.0
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : 2024.6.0
matplotlib            : 3.9.0
numba                 : 0.59.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.30
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2024-06-27 14:44:04,2024-06-28 01:45:05,2,closed
59120,BUG: datetime64[s] data changes when put into HDFStore,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

x = pd.Timestamp(""20220101"")                         # datetime64[s]
y = pd.Timestamp(""2022-01-01T00:00:00.000000000"")    # datetime64[ns]

df = pd.DataFrame({""x"":x, ""y"":y}, index=list(range(5)))

store = pd.HDFStore(""store.h5"", 'w')  
store.put('data', df, format='table')
df2 = store.get('data')
store.close()

df.equals(df2)     # <  Error here. df should equal df2
```


### Issue Description

df.equals(df2) = False
Should be True.

When a dataframe containing datetime64[s] (i.e. column ""x"") is saved to HDFstore and retrieved, the values change. 
Specifically, they are 10^9 times smaller.
It looks like the HDFStore assumes datetime data is in nanoseconds (datetime64[ns])


### Expected Behavior

df.equals(df2) = True.

Works with pandas 1.5.3

### Installed Versions

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.11.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.1.1
pip                   : 24.1
Cython                : None
pytest                : 8.2.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.0
dataframe-api-compat  : None
fastparquet           : 2024.5.0
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : 2.10.1
odfpy                 : None
openpyxl              : 3.1.0
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : 3.9.2
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
","['Bug', 'IO HDF5', 'Non-Nano']",2024-06-27 12:13:40,2024-06-27 13:10:53,1,closed
59118,BUG: Unable to import `pandas` when `pyarrow` 16.1.0 is installed,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
In [2]: import pandas as pd
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[2], line 1
----> 1 import pandas as pd

File /opt/python/envs/dev310/lib/python3.10/site-packages/pandas/__init__.py:26
     22 del _hard_dependencies, _dependency, _missing_dependencies
     24 try:
     25     # numpy compat
---> 26     from pandas.compat import (
     27         is_numpy_dev as _is_numpy_dev,  # pyright: ignore[reportUnusedImport] # noqa: F401
     28     )
     29 except ImportError as _err:  # pragma: no cover
     30     _module = _err.name

File /opt/python/envs/dev310/lib/python3.10/site-packages/pandas/compat/__init__.py:27
     25 import pandas.compat.compressors
     26 from pandas.compat.numpy import is_numpy_dev
---> 27 from pandas.compat.pyarrow import (
     28     pa_version_under10p1,
     29     pa_version_under11p0,
     30     pa_version_under13p0,
     31     pa_version_under14p0,
     32     pa_version_under14p1,
     33     pa_version_under16p0,
     34 )
     36 if TYPE_CHECKING:
     37     from pandas._typing import F

File /opt/python/envs/dev310/lib/python3.10/site-packages/pandas/compat/pyarrow.py:10
      7 try:
      8     import pyarrow as pa
---> 10     _palv = Version(Version(pa.__version__).base_version)
     11     pa_version_under10p1 = _palv < Version(""10.0.1"")
     12     pa_version_under11p0 = _palv < Version(""11.0.0"")

AttributeError: module 'pyarrow' has no attribute '__version__'
```


### Issue Description

It appears `pyarrow` no longer has a `__version__` attribute:
```python
In [3]: import pyarrow as pa

In [4]: pa.__version__
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[4], line 1
----> 1 pa.__version__

AttributeError: module 'pyarrow' has no attribute '__version__'

In [6]: import importlib

In [8]: importlib.metadata.version('pyarrow')
Out[8]: '16.1.0'

In [9]: importlib.metadata.version('pandas')
Out[9]: '2.2.2'
```


### Expected Behavior

I can import `pandas` when I have `pyarrow` 16.1.0 installed

### Installed Versions

```
python 3.10.14
pandas 2.2.2
pyarrow 16.1.0
```

> [!NOTE]
> Packages installed via `conda-forge`

","['Bug', 'Needs Triage']",2024-06-27 03:33:38,2024-06-27 11:59:32,5,closed
59110,BUG: HDF support and `show_versions()` broken with pandas 2.2.2 and numpy 2.0,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
>>> import pandas as pd
>>> pd.__version__
'2.2.2'
>>> import numpy as np
>>> np.__version__
'2.0.0'
>>> pd.show_versions()
C:\Anaconda3\envs\pandasstubs\lib\site-packages\_distutils_hack\__init__.py:26: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Anaconda3\envs\pandasstubs\lib\site-packages\pandas\util\_print_versions.py"", line 141, in show_versions
    deps = _get_dependency_info()
  File ""C:\Anaconda3\envs\pandasstubs\lib\site-packages\pandas\util\_print_versions.py"", line 98, in _get_dependency_info
    mod = import_optional_dependency(modname, errors=""ignore"")
  File ""C:\Anaconda3\envs\pandasstubs\lib\site-packages\pandas\compat\_optional.py"", line 135, in import_optional_dependency
    module = importlib.import_module(name)
  File ""C:\Anaconda3\envs\pandasstubs\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 850, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""C:\Anaconda3\envs\pandasstubs\lib\site-packages\tables\__init__.py"", line 72, in <module>
    from .utilsextension import get_hdf5_version as _get_hdf5_version
  File ""tables\\utilsextension.pyx"", line 1, in init tables.utilsextension
ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
```


### Issue Description

numpy 2.0 causes HDF support and `show_versions()` broken because pytables hasn't been updated

See https://github.com/PyTables/PyTables/issues/1172

### Expected Behavior

No error - or at least `show_versions()` should work

### Installed Versions

Can't do this - `show_versions()` doesn't work!!

pytables 3.9.2

","['Bug', 'Needs Triage']",2024-06-26 18:17:42,2024-07-01 16:36:57,0,closed
59107,CI: Don't run the 'trailing-whitespace' check on markdown files.,"To introduce a line break `<br /> tag` in markdown files, you have to end a line with 2 or more spaces
But, the `trim trailing whitespace` check always removes those.

https://daringfireball.net/projects/markdown/syntax#p",['CI'],2024-06-26 11:42:19,2024-06-27 15:15:29,0,closed
59095,DOC: to_sql docs should mention ADBC,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html

### Documentation problem

The to_sql documentation does not mention anything about supporting ADBC connections. By contrast the read_sql function does:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html

### Suggested fix for documentation

Add mention / example of ADBC usage with the to_sql method","['Docs', 'good first issue']",2024-06-25 12:39:47,2024-07-08 15:36:58,2,closed
59093,BUG: doctests fail after release of scipy 1.14.0,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
./ci/code_checks.sh doctests
```


### Issue Description

Doctests CI fails because of mismatches in `__repr__` of sparse arrays between SciPy 1.14.0 and earlier versions.

### Expected Behavior

CI should be passing. No doctest failures should occur.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : 6e79c30960960155a8a9eb3c0a1200fcc2549925
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : None.UTF-8

pandas                : 3.0.0.dev0+1144.g6e79c30960.dirty
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 70.1.0
pip                   : 24.0
Cython                : 3.0.10
pytest                : 8.2.2
hypothesis            : 6.104.0
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : 3.1.9
lxml.etree            : 5.2.2
html5lib              : 1.1
pymysql               : 1.4.6
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.4.0
fastparquet           : 2024.5.0
fsspec                : 2024.6.0
gcsfs                 : 2024.6.0
matplotlib            : 3.8.4
numba                 : 0.60.0
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.4
pyarrow               : 16.1.0
pyreadstat            : 1.2.7
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.6.0
scipy                 : 1.14.0
sqlalchemy            : 2.0.31
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.6.0
xlrd                  : 2.0.1
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'CI']",2024-06-25 11:32:15,2024-06-25 17:20:13,0,closed
59091,"BUG: String methods has no method ""isascii()""","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
series = pd.Series([""a"", ""b"", ""c"", ""あ"", """"])

series.str.isalnum()
""""""
0    True
1    True
2    True
3    True
4    False
dtype: bool
""""""

series.str.isascii()
# Traceback (most recent call last):
#   File ""<stdin>"", line 1, in <module>
# AttributeError: 'StringMethods' object has no attribute 'isascii'
```


### Issue Description

pd.Series.str does not support `isascii()` metod.

### Expected Behavior

The code shown above would look like this.
```python
series.str.isascii()
""""""
0    True
1    True
2    True
3    False
4    True
dtype: bool
""""""
```

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.7.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 151 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : ja_JP.UTF-8
LOCALE                : Japanese_Japan.932

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage']",2024-06-25 09:30:30,2025-01-02 21:47:33,5,closed
59087,ENH: Allow to plot weighted KDEs.,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

The current implementation does not currently allow to plot weighted KDEs.

### Feature Description

Estimation of the PDF is currently done via [scipy.stats.gaussian_kde](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html#scipy.stats.gaussian_kde) which allows for a parameter `weights`.
[pandas.DataFrame.plot.kde](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.kde.html) should accept this parameter as well.

### Alternative Solutions

[Here](https://github.com/pandas-dev/pandas/blob/d9cdd2ee5a58015ef6f4d15c7226110c9aab8140/pandas/plotting/_core.py#L1411-L1522) allow to pass a parameter `weights` to [`scipy.stats.gaussian_kde`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html#scipy.stats.gaussian_kde).

### Additional Context

_No response_","['Enhancement', 'Needs Triage']",2024-06-24 22:08:39,2024-07-31 17:25:38,3,closed
59082,BUG: [pyarrow] Bizarre overflow error when subtracting two identical `Index` objects.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

datetimes = [None, ""2022-01-01T10:00:30"", ""2022-01-01T10:01:00""]
dt = pd.Index(datetimes, dtype=""timestamp[ms][pyarrow]"")

offset = pd.Timestamp(""2022-01-01 10:00:30"")
unit = pd.Index([pd.Timedelta(30, ""s"")], dtype=""duration[ms][pyarrow]"").item()

# %% encode to double[pyarrow]
encoded = (dt - offset) / unit
decoded = (encoded.round().astype(float) * unit + offset).astype(dt.dtype)

# compare original and decoded
pd.testing.assert_index_equal(dt, decoded, exact=True)  # ✅
assert ((dt - dt) == 0).all()  # ✅
assert ((decoded - decoded) == 0).all()  # ✅
assert ((decoded - dt) == 0).all()  # ✅
assert ((dt - decoded) == 0).all()  # ❌ overflow ?!?!
```


### Issue Description

This one is absolutely baffling to me. Two `Index` objects, despite satisfying `assert_index_equal`, raise an exception when subtracting each other. I guess `assert_index_equal` must be omitting some internal differences under the hood?

It occurs after encoding and decoding a `timestamp[ms][pyarrow]` index to floating and back to `timestamp[ms][pyarrow]`, by subtracting and offset and dividing by some frequency.

Even more weird is that it makes a difference how we define the unit:

```python
# %% Try with different units
unit_a = pd.Timedelta(30, ""s"")  # <-- this one works!
unit_b = pd.Index([pd.Timedelta(30, ""s"")], dtype=""duration[ms][pyarrow]"").item()
assert unit_a == unit_b  # ✅
assert hash(unit_a) == hash(unit_b)  # ✅

# encode to double[pyarrow]
encoded_a = (dt - offset) / unit_a
encoded_b = (dt - offset) / unit_b
pd.testing.assert_index_equal(encoded_a, encoded_b, exact=True)  # ✅

# decode
decoded_a = (encoded_a.round().astype(float) * unit_a + offset).astype(dt.dtype)
decoded_b = (encoded_b.round().astype(float) * unit_b + offset).astype(dt.dtype)
pd.testing.assert_index_equal(decoded_a, decoded_b, exact=True)  # ✅
pd.testing.assert_index_equal(dt, decoded_a, exact=True)  # ✅
pd.testing.assert_index_equal(dt, decoded_b, exact=True)  # ✅
assert ((dt - decoded_a) == 0).all()  # ✅
assert ((dt - decoded_b) == 0).all()  # ❌ overflow ?!?!
```

Moreover, manually computing the difference in `pyarrow` works as well:

```python
# %% compute differences in pyarrow:
import pyarrow as pa

x = pa.Array.from_pandas(dt)
y = pa.Array.from_pandas(decoded)
pa.compute.subtract(x, y)  # ✅
pa.compute.subtract(y, x)  # ✅
```




### Expected Behavior

Either `assert_index_equal` should show some discrepancy, or `dt` an `decoded` should behave interchangably.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.7.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-41-generic
Version               : #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8
pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.1.0
pip                   : 24.1
Cython                : None
pytest                : 8.2.2
hypothesis            : 6.103.5
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Numeric Operations', 'Upstream issue', 'Arrow']",2024-06-24 15:19:09,2024-07-08 17:17:41,3,closed
59079,"BUG: read_csv throws TypeError with iterator, nrows","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from io import BytesIO
import pandas

csv = b'a,b\n1,2\n3,4'
with BytesIO(csv) as f:
    it = pd.read_csv(
                f,
                nrows=1,
                iterator=True,
            )
    for df in it:
        pass
```

Behavior:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[82], line 11
      5 with BytesIO(csv) as f:
      6     it = pd.read_csv(
      7                 f,
      8                 nrows=1,
      9                 iterator=True,
     10             )
---> 11     for df in it:
     12         pass

File ~\AppData\Local\anaconda3\Lib\site-packages\pandas\io\parsers\readers.py:1843, in TextFileReader.__next__(self)
   1841 def __next__(self) -> DataFrame:
   1842     try:
-> 1843         return self.get_chunk()
   1844     except StopIteration:
   1845         self.close()

File ~\AppData\Local\anaconda3\Lib\site-packages\pandas\io\parsers\readers.py:1984, in TextFileReader.get_chunk(self, size)
   1982     if self._currow >= self.nrows:
   1983         raise StopIteration
-> 1984     size = min(size, self.nrows - self._currow)
   1985 return self.read(nrows=size)

TypeError: '<' not supported between instances of 'int' and 'NoneType'
```
```


### Issue Description

It seems that `read_csv` throws a `TypeError` when combining `nrows` and `iterable`.


My context: I want to convert a CSV to parquet. The CSV is larger than memory, so I want to use `iterator=True`. The CSV contains a footer. But since `skipfooter` is not supported for the fast engines (c or pyarrow), and `comment` can only be a single character, I want to instead indirectly skip the footer by using `nrows`. (I know in advance the number of rows.) My data contains `\r\n` line endings, although the error happens with normal `\n`.

### Expected Behavior

The script runs without error. `nrows` rows of data are returned (across one chunk in this small example). 

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Australia.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : 0.4.1
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.1
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 14.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.3.1
scipy                 : 1.13.1
sqlalchemy            : 2.0.30
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2023.6.0
xlrd                  : 2.0.1
zstandard             : 0.22.0
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None
</details>
","['Bug', 'IO CSV']",2024-06-24 08:41:58,2024-06-24 17:13:10,2,closed
59078,DOC: clarify how read_csv nrows interacts with header and skiprows argument,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html

### Documentation problem

The documentation for `read_csv`'s `nrows` argument says:

> Number of rows of file to read. Useful for reading pieces of large files.

I want to read a file using `header=1`, and then limit the number of rows. The documentation says this counts the number of rows **of the file**. To me that sounds like it includes the skipped row and the column header row, since pandas still reads those rows from the file. But I've done some testing. The `nrows` argument counts the number of data rows. It excludes the skipped rows, and excludes the column header row. `skiprows` is the same (skipped rows aren't counted towards `nrows`). When I have a row which is a comment, that also doesn't count towards `nrows`.

```
import pandas as pd
csv = """"""extra,
a,b
1,1
#comment,comment
2,2
3,3
footer,blah,yeah
""""""
from io import StringIO
with StringIO(csv) as io:
    df = pd.read_csv(io, header=1, nrows=2, comment='#')
```
For `nrows=2`, it seems to always return 2 rows.



### Suggested fix for documentation

> Number of rows of data to read. Useful for reading pieces of large files. Refers to the number of included data rows. The following rows are not included in the count:
> 
> * the column header
> * rows before the column header, if `header=1` or larger
> * rows which are fully comment rows
> * rows skipped with `skiprows`
",['Docs'],2024-06-24 08:12:05,2024-08-12 17:36:09,3,closed
59077,BUG: index_col in read_csv ignores dtype argument ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.
- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.
- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import io
import numpy as np
import pandas as pd

data = io.StringIO(""345.5 519.5 0\n519.5 726.5 1\n"")


df = pd.read_csv(
    data,
    sep=r""\s+"",
    header=None,
    names=[""start"", ""stop"", ""bin_id""],
    dtype={""start"": np.float32, ""stop"": np.float32, ""bin_id"": np.uint32},
    index_col=""bin_id""
)
print(df.index.dtype)
```

### Issue Description

`df.index.dtype` is `int64` with pandas `3.0.0.dev0+1132.ga5e812d86d`, although the `dtype` parameter already sets it to `np.uint32`. The issue is similar to an old issue #9435.

### Expected Behavior

Pandas 2.x correctly returns `uint32`, which is the expected behavior.

### Installed Versions

<details>
```
INSTALLED VERSIONS
------------------
commit                : a5e812d86deb62872f8d514d894a22931fc84217
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.11-300.fc40.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Mon May 27 14:53:33 UTC 2024
machine               : x86_64
processor             :
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1132.ga5e812d86d
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.0.0
pip                   : 24.0
Cython                : None
pytest                : 8.2.1
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0.dev
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : 2024.5.0
gcsfs                 : None
matplotlib            : 3.10.0.dev202+gd901275d7c
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : 2024.5.1.dev6+g12123be8
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```
</details>
",['Bug'],2024-06-24 01:11:45,2024-07-30 17:20:08,7,closed
59076,BUG: Dataframe's loc method works incorrectly when selecting a sequence from an index that does not exist.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# Dataframe data
people=[
     {""name"":""Juan"",""age"":15}
    ,{""name"":""Jorge"",""age"":25}
    ,{""name"":""Sebas"",""age"":30}
    ]

# Dataframe index
index_df=[""Person1"",""Person2"",""Person3""]

people_df=pd.DataFrame(data=people,index=index_df)

# Example with incorrect result, A non-existent index sequence is selected
print(""Real Result:"",people_df.loc[""Person"":],sep=""\n"")

# Example with correct result
print(""Expected Result:"",people_df.loc[""example"":],sep=""\n"")
```


### Issue Description

### The incorrect behavior (The issue)

There is a problem with the loc method because when you try to select a sequence of elements based on the index and the sequence does not match the index of the data frame, the result should be an empty data frame, but in this case it does not.
``` python 
# Dataframe data
people=[
     {""name"":""Juan"",""age"":15}
    ,{""name"":""Jorge"",""age"":25}
    ,{""name"":""Sebas"",""age"":30}
    ]

# Dataframe index
index_df=[""Person1"",""Person2"",""Person3""]

people_df=pd.DataFrame(data=people,index=index_df)

# Example with incorrect result, A non-existent index sequence is selected
print(""Real Result:"",people_df.loc[""Person"":],sep=""\n"")

#The output:
output=""""""
Real Result:
          name  age
Person1   Juan   15
Person2  Jorge   25
Person3  Sebas   30
""""""
````

Another example:

```python

population_dict={
    'Chuquisaca':626000,
    'La Paz':26,
    'Pando':1500
}

extension_dict={
    'Chuquisaca':626000,
    'La Paz':26
}

population_series=pd.Series(population_dict)

extension_series=pd.Series(extension_dict)

# A dataframe created by two series, The data is about a fake population of bolivia.
bolivia_data=pd.DataFrame({'poblacion':population_series,'extension':extension_series})

# Example with incorrect result, A non-existent index sequence is selected
print(bolivia_data.loc['Cochabamba':'O',])

# The output:
output=""""""
        poblacion  extension
La Paz         26       26.0
""""""
```



### Expected Behavior

### The expected behavior
First example:
``` python 
# Dataframe data
people=[
     {""name"":""Juan"",""age"":15}
    ,{""name"":""Jorge"",""age"":25}
    ,{""name"":""Sebas"",""age"":30}
    ]

# Dataframe index
index_df=[""Person1"",""Person2"",""Person3""]

people_df=pd.DataFrame(data=people,index=index_df)

# Example with correct result, it prints an empty dataframe
print(""Expected Result:"",people_df.loc[""example"":],sep=""\n"")

#The output:
output=""""""
Expected Result:
Empty DataFrame
Columns: [name, age]
Index: []
""""""
````
Another example:


```python

population_dict={
    'Chuquisaca':626000,
    'La Paz':26,
    'Pando':1500
}

extension_dict={
    'Chuquisaca':626000,
    'La Paz':26
}

population_series=pd.Series(population_dict)

extension_series=pd.Series(extension_dict)

# A dataframe created by two series, The data is about a fake population of bolivia.
bolivia_data=pd.DataFrame({'poblacion':population_series,'extension':extension_series})

# Example with correct result, it prints an empty dataframe
print(bolivia_data.loc['another_example':,])


# The output:
output=""""""
Empty DataFrame
Columns: [poblacion, extension]
Index: []
""""""
```



### Installed Versions


### INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22621
machine               : AMD64
processor             : AMD64 Family 23 Model 104 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2024.1
dateutil              : 2.8.2
setuptools            : 65.5.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.21.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

","['Bug', 'Needs Triage']",2024-06-23 18:39:53,2024-07-01 00:36:11,6,closed
59063,BUG: `DataFrame.sparse.from_spmatrix` hard codes an invalid ``fill_value`` for certain subtypes,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.

### Reproducible Example

```python
import pandas as pd
from scipy.sparse import eye

pd.DataFrame.sparse.from_spmatrix(eye(2, dtype=bool))
```

### Issue Description

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/pandas/pandas/core/arrays/sparse/accessor.py"", line 316, in from_spmatrix
    dtype = SparseDtype(array_data.dtype, 0)
  File ""/pandas/pandas/core/dtypes/dtypes.py"", line 1751, in __init__
    self._check_fill_value()
  File ""/pandas/pandas/core/dtypes/dtypes.py"", line 1835, in _check_fill_value
    raise ValueError(
ValueError: fill_value must be a valid value for the SparseDtype.subtype
```

### Expected Behavior

The default argument for ``fill_value`` should be used instead of passing ``0``, which will fix the issue as the default missing value selected for ``bool`` is ``False``. This bug also affects other dtypes like ``float`` and ``complex`` without raising a ``ValueError``, as a ``fill_value`` of ``0.`` or ``np.nan`` and ``0. + 0.j``, ``np.nan + 0.j``, or ``np.nan`` respectively are more appropriate than `0`.

 We can also introduce a ``fill_value`` parameter to the `DataFrame.sparse.from_spmatrix` method, with a default argument of ``None``, to fix the issue whilst giving the user flexibility to select a ``fill_value`` of choice.

https://github.com/pandas-dev/pandas/blob/c46fb76afaf98153b9eef97fc9bbe9077229e7cd/pandas/core/arrays/sparse/accessor.py#L316

https://github.com/pandas-dev/pandas/blob/c46fb76afaf98153b9eef97fc9bbe9077229e7cd/pandas/core/dtypes/missing.py#L638-L641

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit : c46fb76afaf98153b9eef97fc9bbe9077229e7cd
python : 3.10.14.final.0
python-bits : 64
OS : Darwin
OS-release : 23.5.0
Version : Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64
machine : x86_64
processor : i386
byteorder : little
LC_ALL : None
LANG : en_GB.UTF-8
LOCALE : en_GB.UTF-8

pandas : 3.0.0.dev0+1125.gc46fb76afa
numpy : 1.26.4
pytz : 2024.1
dateutil : 2.9.0
setuptools : 70.1.0
pip : 24.0
Cython : 3.0.10
pytest : 8.2.2
hypothesis : 6.103.2
sphinx : 7.3.7
blosc : None
feather : None
xlsxwriter : 3.1.9
lxml.etree : 5.2.2
html5lib : 1.1
pymysql : 1.4.6
psycopg2 : 2.9.9
jinja2 : 3.1.4
IPython : 8.25.0
pandas_datareader : None
adbc-driver-postgresql : None
adbc-driver-sqlite : None
bs4 : 4.12.3
bottleneck : 1.4.0
fastparquet : 2024.5.0
fsspec : 2024.6.0
gcsfs : 2024.6.0
matplotlib : 3.8.4
numba : 0.59.1
numexpr : 2.10.0
odfpy : None
openpyxl : 3.1.2
pyarrow : 16.1.0
pyreadstat : 1.2.7
python-calamine : None
pyxlsb : 1.0.10
s3fs : 2024.6.0
scipy : 1.13.1
sqlalchemy : 2.0.31
tables : 3.9.2
tabulate : 0.9.0
xarray : 2024.6.0
xlrd : 2.0.1
zstandard : 0.22.0
tzdata : 2024.1
qtpy : None
pyqt5 : None

</details>","['Bug', 'Needs Triage']",2024-06-21 10:28:23,2024-06-27 15:13:45,0,closed
59059,BUG: Pandas does not validate some parameters properly when reading CSVs and it causes segmentation faults,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
#######################################################################
# Example 1
#######################################################################

import pandas
import io


string_i_o_0 = io.StringIO('""To98odN""\n-40542\n-5\n-4\n')

pandas.read_csv(string_i_o_0, encoding_errors=string_i_o_0)

#######################################################################
# Example 2
#######################################################################

import pandas
import io


string_i_o_0 = io.StringIO('""To98odN""\n-40542\n-5\n-4\n')
pandas.read_table(string_i_o_0, encoding_errors=string_i_o_0)

#######################################################################
# Example 3
#######################################################################

import io
import pandas


string_i_o_0 = io.StringIO('""H45""\n6\n-1\n8\n')
var_0 = pandas.read_csv(string_i_o_0)
var_1 = pandas.read_csv(
    string_i_o_0,
    skipinitialspace=var_0,
)
```


### Issue Description

I was working on Pynguin, an automated unit test generation tool for Python, and the tool found that Pandas does not validate some parameters properly and the generated tests produce the following output:

```
Segmentation fault (core dumped)
```

First, there is the `encoding_errors` parameter in the `pandas.read_csv` and `pandas.read_table` functions. I don't know why it happens for this one.

Second, there is the `skipinitialspace` parameter in the `pandas.read_csv` function. I think that this one comes from the fact that Pandas does not support that an exception is raised in the `__bool__` method of the `var_0` dataframe.

```
Traceback (most recent call last):
  File ""/home/lucas/Documents/GitHub/pynguin-for-ML-libraries/err13.py"", line 7, in <module>
    print(bool(var_0))
  File ""/home/lucas/.conda/envs/pynguin-for-ML-libraries/lib/python3.10/site-packages/pandas/core/generic.py"", line 1495, in __nonzero__
    raise ValueError(
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
```




### Expected Behavior

Pandas should raise an exception instead of causing a segmentation fault when a wrong value is passed as an argument.

### Installed Versions

commit                : c46fb76afaf98153b9eef97fc9bbe9077229e7cd
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.8.11-200.fc39.x86_64
Version               : #1 SMP PREEMPT_DYNAMIC Sun May 26 20:05:41 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1125.gc46fb76afa
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 68.2.2
pip                   : 23.3.1
Cython                : None
pytest                : 8.2.0
hypothesis            : 6.103.2
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : 2024.2.0
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pyarrow               : 10.0.1
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None","['Bug', 'Needs Triage']",2024-06-20 15:35:14,2024-06-27 17:20:22,0,closed
59057,ENH: Python 3.13 free-threading support,"This aims to be a tracking issue for all work necessary to support the free-threaded build of Python 3.13. A very high-level list of steps is (more details to follow as I investigate more issues):

- [x] Set up CI for free-threading
  - https://github.com/pandas-dev/pandas/pull/59058
- [x] Audit C extension module for thread-safety issues
  - [x] `pandas_datetime` is thread-safe
  - [x] `pandas_parser` is not thread-safe, but cannot be used in multiple threads from within `pandas` Python code. 
- [x] Test Cython extension modules (with cython/cython#6226)
  - Everything appears to be okay (green CI) both with Cython nightly and with the above Cython PR!
- [x] Mark C extension modules as thread-safe with the `Py_mod_gil` slot
  - #59135
  - https://github.com/pandas-dev/pandas/pull/59248
- [x] Upload nightly wheels for the free-threaded build
  - https://github.com/pandas-dev/pandas/pull/59136 



","['Enhancement', 'Build', 'Python 3.13']",2024-06-20 13:40:54,2025-07-14 16:32:39,24,closed
59054,BUG: `DataFrame.to_numpy()` unnecessarily upcasts to `object` dtype.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame({""a"" : [1, 2], ""b"" : [3, 4]}, dtype=""int64[pyarrow]"")
assert df[""a""].to_numpy().dtype == int  # ✅
assert df[""b""].to_numpy().dtype == int  # ✅
assert df.to_numpy().dtype == int  # ❌ object dtype instead of int
```


### Issue Description

All columns convert to `int` dtypes, but the full frame converts to `object` dtype.

### Expected Behavior

One would expect a commutative property:  the dtype of `DataFrame.to_numpy()` should be identical to the dtype of

```python
np.stack([df[col].to_numpy() for col in df], axis=-1)
```

(In fact, this provides a naive way of implementing `DataFrame.to_numpy()`.)

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.7.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-41-generic
Version               : #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 2.0.0
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.1.0
pip                   : 24.0
Cython                : None
pytest                : 8.2.2
hypothesis            : 6.103.2
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-06-19 14:35:58,2024-06-19 19:52:19,1,closed
59052,BUG: Failed to import pandas <2.1.0 witn numpy >=2.0.0,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

In a clean python 3.11 environment install pandas==2.0.3
```python
 py -3.11 -m venv venv
.\venv\Scripts\Activate.ps1
pip install pandas==2.0.3
```

This install pandas==2.0.3 and numpy==2.0.0.

Start python and simply import pandas, throws an error
```
py
import pandas
>>> ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
```
```


### Issue Description

Pandas 2.0.* has incorrect numpy dependency.

### Expected Behavior

Install numpy<2.0.0

### Installed Versions

Cannont import pandas and therefore I cannot run this command.","['Build', 'Usage Question', 'Compat']",2024-06-19 13:39:05,2024-06-19 17:27:46,6,closed
59050,BUG:  Limit param of fillna method does not work for pd.Int64Dtype() ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df_test = pd.DataFrame({'col': [pd._libs.missing.NAType(), pd._libs.missing.NAType(), pd._libs.missing.NAType()]})
df_test.col = df_test.col.astype(pd.Int64Dtype())
df_test.fillna(1, limit=1)
```


### Issue Description

The fillna method called on 3rd row should only replace the first NA value in the dataframe with 1, but it replaces the whole column

![image](https://github.com/pandas-dev/pandas/assets/61687159/59d1f78a-f647-4022-94d9-9ef0f3cf13ff)

Note: if the second code line is remove, it works as expected, so it is something related to dtypes

### Expected Behavior

It should only replace the first row

![image](https://github.com/pandas-dev/pandas/assets/61687159/64a5ffe8-68c4-462a-b856-626dfbab37f4)


### Installed Versions

<details>

/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")


INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.1.85+
Version               : #1 SMP PREEMPT_DYNAMIC Sun Apr 28 14:29:16 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.25.2
pytz                  : 2023.4
dateutil              : 2.8.2
setuptools            : 67.7.2
pip                   : 23.1.2
Cython                : 3.0.10
pytest                : 7.4.4
hypothesis            : None
sphinx                : 5.0.2
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.4
html5lib              : 1.1
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : 7.34.0
pandas_datareader     : 0.10.0
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.6.0
gcsfs                 : 2023.6.0
matplotlib            : 3.7.1
numba                 : 0.58.1
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : 0.19.2
pyarrow               : 14.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : 2.0.30
tables                : 3.8.0
tabulate              : 0.9.0
xarray                : 2023.7.0
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Missing-data', 'ExtensionArray', 'Closing Candidate']",2024-06-19 07:53:27,2024-06-22 21:48:09,2,closed
59048,BUG: ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas, numpy

df = pandas.DataFrame([['s', 1, 2.3, 0]], columns=['str', 'int', 'flt', 'to ignore'])
df['int'] = df['int'].astype('Int64')
df['flt'] = df['flt'].astype('Float64')

columns_to_process = ['str', 'int', 'flt']

df.loc[:, columns_to_process] = df.loc[:, columns_to_process].astype(str)
```


### Issue Description

```
df.loc[:, columns_to_process] = df.loc[:, columns_to_process].astype(str)
```

throws the following error

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
TypeError: len() of unsized object
Exception ignored in: 'pandas._libs.lib.is_string_array'
Traceback (most recent call last):
  File ""C:\Users\evgeniy\AppData\Local\anaconda3\Lib\site-packages\pandas\core\arrays\numeric.py"", line 162, in _coerce_to_data_and_mask
    inferred_type = lib.infer_dtype(values, skipna=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: len() of unsized object
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
TypeError: len() of unsized object
Exception ignored in: 'pandas._libs.lib.is_string_array'
Traceback (most recent call last):
  File ""C:\Users\evgeniy\AppData\Local\anaconda3\Lib\site-packages\pandas\core\arrays\numeric.py"", line 162, in _coerce_to_data_and_mask
    inferred_type = lib.infer_dtype(values, skipna=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: len() of unsized object
```

### Expected Behavior

No error

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.11.7.final.0
python-bits         : 64
OS                  : Windows
OS-release          : 10
Version             : 10.0.22631
machine             : AMD64
processor           : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder           : little
LC_ALL              : None
LANG                : None
LOCALE              : Russian_Russia.1251

pandas              : 2.1.4
numpy               : 1.26.4
pytz                : 2023.3.post1
dateutil            : 2.8.2
setuptools          : 68.2.2
pip                 : 23.3.1
Cython              : None
pytest              : 7.4.0
hypothesis          : None
sphinx              : 5.0.2
blosc               : None
feather             : None
xlsxwriter          : 3.2.0
lxml.etree          : 4.9.3
html5lib            : None
pymysql             : None
psycopg2            : None
jinja2              : 3.1.3
IPython             : 8.20.0
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : 1.3.7
dataframe-api-compat: None
fastparquet         : None
fsspec              : 2023.10.0
gcsfs               : None
matplotlib          : 3.8.0
numba               : 0.59.0
numexpr             : 2.8.7
odfpy               : None
openpyxl            : 3.0.10
pandas_gbq          : None
pyarrow             : 14.0.2
pyreadstat          : None
pyxlsb              : None
s3fs                : 2023.10.0
scipy               : 1.11.4
sqlalchemy          : 2.0.25
tables              : 3.9.2
tabulate            : 0.9.0
xarray              : 2023.6.0
xlrd                : None
zstandard           : 0.19.0
tzdata              : 2023.3
qtpy                : 2.4.1
pyqt5               : None

</details>
","['Bug', 'Closing Candidate', 'Astype']",2024-06-19 01:30:08,2024-06-22 21:50:13,2,closed
59045,BUG: pandas dataframe column definition or mapping does not cater for upper case values.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import psycopg2
from sqlalchemy import create_engine

conn_string = ""postgresql://postgres:password@localhost/database""
engine = create_engine(conn_string)

query = """"""
select 
	p.id as CustomerId
from table
""""""

df = pd.read_sql(query, engine)

dtype_mappings = {
	'CustomerId': int
}

df = df.astype(dtype_mappings)
```


### Issue Description

When running the code we get the following error

.venv\lib\site-packages\pandas\core\generic.py"", line 6605, in astype
    raise KeyError(
KeyError: ""Only a column name can be used for the key in a dtype mappings argument. 'CustomerId' not found in columns.""

If we change the column name to all lower case, the code works.

### Expected Behavior

pandas should support both upper and lower case for the column names

### Installed Versions

<details>

Name: pandas
Version: 2.2.2

</details>
","['Bug', 'Needs Info', 'Astype']",2024-06-18 16:16:33,2024-08-27 16:59:03,2,closed
59043,BUG: eval fails to process expression when one column name starts with a digit or some special characters,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

data = {'A':[True,False,True],'1B':[True,True,False]}
df = pd.DataFrame(data)

expr = 'A & 1B'
result = df.eval(expr)
```


### Issue Description

The example above results in this error:
    A +1 B
         ^
SyntaxError: invalid syntax

### Expected Behavior

Prepending an underscore to the column name in both the dataframe and in the expression fixes the problem.
Since pandas allows column names to start with a digit, the .eval function should be able to process expressions with those columns.

```
import pandas as pd

data = {'A':[True,False,True],'_1B':[True,True,False]}
df = pd.DataFrame(data)

expr = 'A & _1B'
result = df.eval(expr)

print(result)
```

0     True
1    False
2    False
dtype: bool

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.4.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'expressions']",2024-06-18 11:23:50,2024-06-18 16:13:09,1,closed
59042,BUG: `pd.read_excel` gives uninformative error for protected files,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
pd.read_excel(path_protected_xlsx, engine='openpyxl')
```


### Issue Description

Reading protected excel files throws BadZipFile(""File is not a zip file"") error.

### Expected Behavior

The error should indicate that a protected file cannot be processed by read_excel or some other validation message informative to the user.

### Installed Versions

<details>

commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.7.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2023.3.post1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 8.0.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.1.9
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.24.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 14.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : 2.0.25
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO Excel', 'Closing Candidate']",2024-06-18 09:44:04,2024-06-20 09:01:04,2,closed
59041,ENH: .isin() method should use __contains__ rather than __iter__ for user-defined classes to determine presence.,"### Feature Type

- [X] Adding new functionality to pandas

- [X] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Right now, if you would define a user class:

```python
class MyClass:
    def __init__(self):
        self.collection = [1, 2, 3]
        self.another_collection = [4, 5, 6]
    
    def __contains__(self, item):
        return item in self.collection
    
    def __iter__(self):
        yield from self.another_collection
```

and would then initialize a pandas dataframe like this:

```python
example_dataframe = pd.DataFrame(
    {
        'column_name': [3, 1, 4, 6, 13],
        'another_column_name': ['tolly', 'trolly', 'telly', 'belly', 'nelly']
    }
)
```

and would then call the `.isin()` method like this:

```python
class_instance = MyClass()
example_dataframe['column_name'].isin(class_instance)
```

you would actually get this output:

```python
False
False
True
True
False
```

which is if the values from `self.another_collections` specified in `__iter__` are checked, rather than `self.collection` from `__contains__`. I do realize that this might stem from compatibility with other libraries, but this seems counter-intuitive.

### Feature Description

A solution I suggest is either to change the behavior (which might result into ruining some peoples code, I believe), or adding a flag (which would lead to more complexity, I guess).

### Alternative Solutions

See above. 

### Additional Context

_No response_","['Docs', 'good first issue', 'isin']",2024-06-18 05:54:58,2024-07-08 18:15:26,10,closed
59038,BUILD: Pandas 1.2.5 build no longer works,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

macOS-14.5-arm64-arm-64bit

### Installation Method

pip install

### pandas Version

1.2.5

### Python Version

Python 3.11.7

### Installation Logs

```
pip install --no-cache six==1.16.0
pip install --no-cache pytz==2024.1
pip install --no-cache python-dateutil==2.9.0.post0
pip install --no-cache numpy==1.26.4
pip install --no-cache cython==0.29.21
# blows up
pip install --no-cache pandas==1.2.5 
```

<details>

      clang -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/src/ujson/python -Ipandas/_libs/src/ujson/lib -Ipandas/_libs/src/datetime -I/private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include -I/Users/adriano.nobre/dev/test/sample-venv/include -I/Users/adriano.nobre/.pyenv/versions/3.11.7/include/python3.11 -c pandas/_libs/src/ujson/lib/ultrajsonenc.c -o build/temp.macosx-13.6-arm64-cpython-311/pandas/_libs/src/ujson/lib/ultrajsonenc.o -D_GNU_SOURCE -Wno-error=unreachable-code
      clang -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/src/ujson/python -Ipandas/_libs/src/ujson/lib -Ipandas/_libs/src/datetime -I/private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include -I/Users/adriano.nobre/dev/test/sample-venv/include -I/Users/adriano.nobre/.pyenv/versions/3.11.7/include/python3.11 -c pandas/_libs/src/ujson/python/JSONtoObj.c -o build/temp.macosx-13.6-arm64-cpython-311/pandas/_libs/src/ujson/python/JSONtoObj.o -D_GNU_SOURCE -Wno-error=unreachable-code
      pandas/_libs/src/ujson/python/JSONtoObj.c:195:49: warning: incompatible pointer types passing 'PyObject *' (aka 'struct _object *') to parameter of type 'const PyArrayObject *' (aka 'const struct tagPyArrayObject_fields *') [-Wincompatible-pointer-types]
              new_data = PyDataMem_RENEW(PyArray_DATA(ret), i * npyarr->elsize);
                                                      ^~~
      /private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include/numpy/ndarraytypes.h:1508:35: note: passing argument to parameter 'arr' here
      PyArray_DATA(const PyArrayObject *arr)
                                        ^
      pandas/_libs/src/ujson/python/JSONtoObj.c:260:33: error: no member named 'elsize' in 'struct _PyArray_Descr'
              npyarr->elsize = dtype->elsize;
                               ~~~~~  ^
      pandas/_libs/src/ujson/python/JSONtoObj.c:305:53: warning: incompatible pointer types passing 'PyObject *' (aka 'struct _object *') to parameter of type 'const PyArrayObject *' (aka 'const struct tagPyArrayObject_fields *') [-Wincompatible-pointer-types]
                  new_data = PyDataMem_RENEW(PyArray_DATA(npyarr->ret),
                                                          ^~~~~~~~~~~
      /private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include/numpy/ndarraytypes.h:1508:35: note: passing argument to parameter 'arr' here
      PyArray_DATA(const PyArrayObject *arr)
                                        ^
      pandas/_libs/src/ujson/python/JSONtoObj.c:316:18: warning: incompatible pointer types passing 'PyObject *' (aka 'struct _object *') to parameter of type 'const PyArrayObject *' (aka 'const struct tagPyArrayObject_fields *') [-Wincompatible-pointer-types]
          PyArray_DIMS(npyarr->ret)[0] = i + 1;
                       ^~~~~~~~~~~
      /private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include/numpy/ndarraytypes.h:1520:35: note: passing argument to parameter 'arr' here
      PyArray_DIMS(const PyArrayObject *arr)
                                        ^
      pandas/_libs/src/ujson/python/JSONtoObj.c:318:33: warning: incompatible pointer types passing 'PyObject *' (aka 'struct _object *') to parameter of type 'const PyArrayObject *' (aka 'const struct tagPyArrayObject_fields *') [-Wincompatible-pointer-types]
          if ((item = PyArray_GETPTR1(npyarr->ret, i)) == NULL ||
                                      ^~~~~~~~~~~
      /private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include/numpy/ndarrayobject.h:138:57: note: expanded from macro 'PyArray_GETPTR1'
      #define PyArray_GETPTR1(obj, i) ((void *)(PyArray_BYTES(obj) + \
                                                              ^~~
      /private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include/numpy/ndarraytypes.h:1514:36: note: passing argument to parameter 'arr' here
      PyArray_BYTES(const PyArrayObject *arr)
                                         ^
      pandas/_libs/src/ujson/python/JSONtoObj.c:318:33: warning: incompatible pointer types passing 'PyObject *' (aka 'struct _object *') to parameter of type 'const PyArrayObject *' (aka 'const struct tagPyArrayObject_fields *') [-Wincompatible-pointer-types]
          if ((item = PyArray_GETPTR1(npyarr->ret, i)) == NULL ||
                                      ^~~~~~~~~~~
      /private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include/numpy/ndarrayobject.h:139:62: note: expanded from macro 'PyArray_GETPTR1'
                                               (i)*PyArray_STRIDES(obj)[0]))
                                                                   ^~~
      /private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include/numpy/ndarraytypes.h:1526:38: note: passing argument to parameter 'arr' here
      PyArray_STRIDES(const PyArrayObject *arr)
                                           ^
      pandas/_libs/src/ujson/python/JSONtoObj.c:319:25: warning: incompatible pointer types passing 'PyObject *' (aka 'struct _object *') to parameter of type 'PyArrayObject *' (aka 'struct tagPyArrayObject_fields *') [-Wincompatible-pointer-types]
              PyArray_SETITEM(npyarr->ret, item, value) == -1) {
                              ^~~~~~~~~~~
      /private/var/folders/gx/t9hqn79x4wdgbqy8n09y0bww0000gp/T/pip-build-env-74z_mupr/overlay/lib/python3.11/site-packages/numpy/_core/include/numpy/ndarrayobject.h:292:32: note: passing argument to parameter 'arr' here
      PyArray_SETITEM(PyArrayObject *arr, char *itemptr, PyObject *v)
                                     ^
      6 warnings and 1 error generated.
      error: command '/usr/bin/clang' failed with exit code 1
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for pandas
Failed to build pandas
ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects

</details>
","['Build', 'Usage Question']",2024-06-18 01:11:43,2024-06-18 14:00:41,15,closed
59036,"BUG: `DatetimeIndex.union` gives wrong result with ""datetime64[us]""","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
l1 = pd.DatetimeIndex(['2024-05-11', '2024-05-12'], dtype='datetime64[us]', name='Date', freq='D')
l2 = pd.DatetimeIndex(['2024-05-13'], dtype='datetime64[us]', name='Date', freq='D')

print(l1.union(l2))
# Returns DatetimeIndex(['2024-05-11', '2024-05-13', '2027-02-05'], dtype='datetime64[us]', name='Date', freq='D')
```


### Issue Description

`DatetimeIndex.union` is returning an incorrect result. Since this method is used by `MultiIndex.concat`, it leads to unexpected errors when combining several MultiIndex DataFrames/Series that have a `datetime64[us]` level.

### Expected Behavior

`DatetimeIndex(['2024-05-11', '2024-05-12', '2024-05-13'], dtype='datetime64[us]', name='Date', freq=None)`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 141 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 7.4.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : 1.3.8
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : None
numba                 : 0.59.1
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : 1.2.7
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.30
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Index', 'Needs Triage', 'datetime.date']",2024-06-17 22:11:59,2024-06-24 17:55:16,0,closed
59031,ENH/BUG: pd.date_range() still defaults to nanosecond resolution,"After https://github.com/pandas-dev/pandas/pull/55901, `to_datetime` with strings will now infer the resolution from the data, but the related `pd.date_range` to create datetime data still returns nanoseconds:


```python
In [5]: pd.date_range(""2012-01-01"", periods=3, freq=""1min"")
Out[5]: 
DatetimeIndex(['2012-01-01 00:00:00', '2012-01-01 00:01:00',
               '2012-01-01 00:02:00'],
              dtype='datetime64[ns]', freq='min')

In [6]: pd.to_datetime(['2012-01-01 00:00:00', '2012-01-01 00:01:00', '2012-01-01 00:02:00'])
Out[6]: 
DatetimeIndex(['2012-01-01 00:00:00', '2012-01-01 00:01:00',
               '2012-01-01 00:02:00'],
              dtype='datetime64[s]', freq=None)
```

Should we update `pd.date_range` as well to infer the resulting resolution from the start/stop timestamp and freq ?

(I encountered this inconsistency in the pyarrow tests, where we essentially were using both idioms to create a result and expected data, but so that started failing because of a different dtype. I also opened https://github.com/pandas-dev/pandas/issues/58989 for that, but regardless of a possible default resolution, `pd.date_range` would still need to follow that as well)",['Non-Nano'],2024-06-17 13:55:42,2025-11-29 02:53:12,3,closed
59024,BUG: df.all(skipna=False),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

# Sample data
data = {
    'a': [np.nan, True],
}

df = pd.DataFrame(data)
print(df.all(skipna=False))
```


### Issue Description

The `DataFrame.all` method returns `True` when `skipna=False` even if there are `NaN` values in the DataFrame. According to the documentation, `NaN` values should be considered as `False` when `skipna=False`.

### Expected Behavior

  # Expected output: False, but it returns True

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.8.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : Korean_Korea.949

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 68.2.2
pip                   : 23.3.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.10.0
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None

Process finished with exit code 0

</details>
","['Bug', 'Needs Info', 'Closing Candidate']",2024-06-17 04:22:06,2024-06-18 04:35:08,2,closed
59023,"BUG: can't support numpy2.2,how to deal with it","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File """", line 14, in 
import as qi
File ""y"", line 14, in 
import pandas as pd
File ""C:\Python312\Lib\site-packages\pandas_init_.py"", line 62, in 
from pandas.core.api import (
File ""C:\Python312\Lib\site-packages\pandas\core\api.py"", line 28, in 
from pandas.core.arrays import Categorical
File ""C:\Python312\Lib\site-packages\pandas\core\arrays_init_.py"", line 1, in 
from pandas.core.arrays.arrow import ArrowExtensionArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\arrow_init_.py"", line 5, in 
from pandas.core.arrays.arrow.array import ArrowExtensionArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\arrow\array.py"", line 64, in 
from pandas.core.arrays.masked import BaseMaskedArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\masked.py"", line 60, in 
from pandas.core import (
File ""C:\Python312\Lib\site-packages\pandas\core\nanops.py"", line 52, in 
bn = import_optional_dependency(""bottleneck"", errors=""warn"")
File ""C:\Python312\Lib\site-packages\pandas\compat_optional.py"", line 135, in import_optional_dependency
module = importlib.import_module(name)
File ""C:\Python312\Lib\importlib_init_.py"", line 90, in import_module
return _bootstrap.gcd_import(name[level:], package, level)
File ""C:\Python312\Lib\site-packages\bottleneck_init.py"", line 7, in 
from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,
Traceback (most recent call last):
File ""C:\Users\cuili\AppData\Roaming\Python\Python312\site-packages\numpy\core_multiarray_umath.py"", line 44, in getattr
raise ImportError(msg)
ImportError:
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.
```


### Issue Description

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File """", line 14, in 
import q as qi
File """", line 14, in 
import pandas as pd
File ""C:\Python312\Lib\site-packages\pandas_init_.py"", line 62, in 
from pandas.core.api import (
File ""C:\Python312\Lib\site-packages\pandas\core\api.py"", line 28, in 
from pandas.core.arrays import Categorical
File ""C:\Python312\Lib\site-packages\pandas\core\arrays_init_.py"", line 1, in 
from pandas.core.arrays.arrow import ArrowExtensionArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\arrow_init_.py"", line 5, in 
from pandas.core.arrays.arrow.array import ArrowExtensionArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\arrow\array.py"", line 64, in 
from pandas.core.arrays.masked import BaseMaskedArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\masked.py"", line 60, in 
from pandas.core import (
File ""C:\Python312\Lib\site-packages\pandas\core\nanops.py"", line 52, in 
bn = import_optional_dependency(""bottleneck"", errors=""warn"")
File ""C:\Python312\Lib\site-packages\pandas\compat_optional.py"", line 135, in import_optional_dependency
module = importlib.import_module(name)
File ""C:\Python312\Lib\importlib_init_.py"", line 90, in import_module
return _bootstrap.gcd_import(name[level:], package, level)
File ""C:\Python312\Lib\site-packages\bottleneck_init.py"", line 7, in 
from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,
Traceback (most recent call last):
File ""C:\Users\cuili\AppData\Roaming\Python\Python312\site-packages\numpy\core_multiarray_umath.py"", line 44, in getattr
raise ImportError(msg)
ImportError:
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

### Expected Behavior

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File """", line 14, in 
import q as qi
File """", line 14, in 
import pandas as pd
File ""C:\Python312\Lib\site-packages\pandas_init_.py"", line 62, in 
from pandas.core.api import (
File ""C:\Python312\Lib\site-packages\pandas\core\api.py"", line 28, in 
from pandas.core.arrays import Categorical
File ""C:\Python312\Lib\site-packages\pandas\core\arrays_init_.py"", line 1, in 
from pandas.core.arrays.arrow import ArrowExtensionArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\arrow_init_.py"", line 5, in 
from pandas.core.arrays.arrow.array import ArrowExtensionArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\arrow\array.py"", line 64, in 
from pandas.core.arrays.masked import BaseMaskedArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\masked.py"", line 60, in 
from pandas.core import (
File ""C:\Python312\Lib\site-packages\pandas\core\nanops.py"", line 52, in 
bn = import_optional_dependency(""bottleneck"", errors=""warn"")
File ""C:\Python312\Lib\site-packages\pandas\compat_optional.py"", line 135, in import_optional_dependency
module = importlib.import_module(name)
File ""C:\Python312\Lib\importlib_init_.py"", line 90, in import_module
return _bootstrap.gcd_import(name[level:], package, level)
File ""C:\Python312\Lib\site-packages\bottleneck_init.py"", line 7, in 
from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,
Traceback (most recent call last):
File ""C:\Users\cuili\AppData\Roaming\Python\Python312\site-packages\numpy\core_multiarray_umath.py"", line 44, in getattr
raise ImportError(msg)
ImportError:
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

### Installed Versions

<details>

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File """", line 14, in 
import q as qi
File """", line 14, in 
import pandas as pd
File ""C:\Python312\Lib\site-packages\pandas_init_.py"", line 62, in 
from pandas.core.api import (
File ""C:\Python312\Lib\site-packages\pandas\core\api.py"", line 28, in 
from pandas.core.arrays import Categorical
File ""C:\Python312\Lib\site-packages\pandas\core\arrays_init_.py"", line 1, in 
from pandas.core.arrays.arrow import ArrowExtensionArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\arrow_init_.py"", line 5, in 
from pandas.core.arrays.arrow.array import ArrowExtensionArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\arrow\array.py"", line 64, in 
from pandas.core.arrays.masked import BaseMaskedArray
File ""C:\Python312\Lib\site-packages\pandas\core\arrays\masked.py"", line 60, in 
from pandas.core import (
File ""C:\Python312\Lib\site-packages\pandas\core\nanops.py"", line 52, in 
bn = import_optional_dependency(""bottleneck"", errors=""warn"")
File ""C:\Python312\Lib\site-packages\pandas\compat_optional.py"", line 135, in import_optional_dependency
module = importlib.import_module(name)
File ""C:\Python312\Lib\importlib_init_.py"", line 90, in import_module
return _bootstrap.gcd_import(name[level:], package, level)
File ""C:\Python312\Lib\site-packages\bottleneck_init.py"", line 7, in 
from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,
Traceback (most recent call last):
File ""C:\Users\cuili\AppData\Roaming\Python\Python312\site-packages\numpy\core_multiarray_umath.py"", line 44, in getattr
raise ImportError(msg)
ImportError:
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.
</details>
","['Bug', 'Closing Candidate', 'Upstream issue']",2024-06-17 00:49:44,2024-06-18 14:03:28,8,closed
59022,"Potential performance regression with ""BUG: Fix issue with negative labels in `group_cumsum` (#58984)""","PR https://github.com/pandas-dev/pandas/pull/58984

@luke396 
@mroeschke 

`groupby.Cumulative.time_frame_transform`  dtype='Int64', method='cumsum', with_nans=False
http://57.128.112.95:5000/compare/benchmark-results/0666c8806d1470e4800008c2b6c32e9a...0666cf95819673ab800040a42d1661f3/

`groupby.Cumulative.time_frame_transform` dtype='float64', method='cumsum', with_nans=False
http://57.128.112.95:5000/benchmark-results/0666cf957942716a8000ecba8a850e85/

![Screenshot 2024-06-16 at 16 27 43](https://github.com/pandas-dev/pandas/assets/11835246/0e8dd457-5df7-4014-935b-28cc9b02dc1b)

",['Benchmark'],2024-06-16 14:29:43,2024-06-17 17:17:20,0,closed
59020,"QST: This library always worked, except with this specific .sav file, anyone could give me a clue on why?","### Research

- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).


### Link to question on StackOverflow

https://stackoverflow.com/questions/78628592/trying-to-read-this-specific-sav-file-no-success-however

### Question about pandas

Just like the stackoverflow question, i have no idea why","['Usage Question', 'Needs Triage']",2024-06-16 08:11:52,2024-08-27 17:13:35,1,closed
59012,"BUG: `to_parquet(engine=""fastparquet"")` can't handle DataFrame with NumPy array values","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# Run `pip install -U numpy pyarrow fastparquet` first

import pandas as pd
import numpy as np

df = pd.DataFrame({""a"": [np.array([0.0])]})

df.to_parquet(engine=""pyarrow"") # Works
df.to_parquet(engine=""fastparquet"") # Error
```


### Issue Description

`df.to_parquet` with `engine=""fastparquet""` raises an error as below when the DataFrame includes NumPy array values
while `engine=""pyarrow""` (default) works fine.

```
>>> df.to_parquet(engine=""fastparquet"") # Error
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/workspace/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py"", line 333, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/.venv/lib/python3.12/site-packages/pandas/core/frame.py"", line 3113, in to_parquet
    return to_parquet(
           ^^^^^^^^^^^
  File ""/workspace/.venv/lib/python3.12/site-packages/pandas/io/parquet.py"", line 480, in to_parquet
    impl.write(
  File ""/workspace/.venv/lib/python3.12/site-packages/pandas/io/parquet.py"", line 349, in write
    self.api.write(
  File ""/workspace/.venv/lib/python3.12/site-packages/fastparquet/writer.py"", line 1304, in write
    fmd = make_metadata(data, has_nulls=has_nulls, ignore_columns=ignore,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/.venv/lib/python3.12/site-packages/fastparquet/writer.py"", line 904, in make_metadata
    se, type = find_type(data[column], fixed_text=fixed,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/.venv/lib/python3.12/site-packages/fastparquet/writer.py"", line 122, in find_type
    object_encoding = infer_object_encoding(data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/workspace/.venv/lib/python3.12/site-packages/fastparquet/writer.py"", line 357, in infer_object_encoding
    raise ValueError(""Can't infer object conversion type: %s"" % data)
ValueError: Can't infer object conversion type: 0    [0.0]
Name: a, dtype: object
```

### Expected Behavior

The `fastparquet` engine can deal with NumPy array values.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.1.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.4.0
Version               : Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:41 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8103
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 23.2.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : 2024.5.0
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-06-14 07:12:55,2024-06-14 16:49:49,1,closed
59010,DOC: Link to SPEC 0 instead of NEP 29,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/development/policies.html#python-support 

### Documentation problem

NEP 29 has been superseded by SPEC 0

### Suggested fix for documentation

We should link to [SPEC 0](https://scientific-python.org/specs/spec-0000/#description) instead.","['Docs', 'good first issue']",2024-06-14 01:43:45,2024-06-14 22:52:55,2,closed
59006,ENH: Pandas Tensor Data Type,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

Reviewing Arrow docs link from @WillAyd, spotted this 

https://arrow.apache.org/docs/format/CanonicalExtensions.html#variable-shape-tensor 

Tensor is _exactly_ what I'm talking about in Additional Context [1] and would enable Pandas users to have a column datatype for big blocks of some underlying type

### Feature Description

Support Arrow Tensor in Pandas 

Python
https://arrow.apache.org/docs/python/generated/pyarrow.Tensor.html#pyarrow.Tensor

Rust
https://github.com/apache/arrow-rs/blob/3715d5447e468a5a4dc631ae9aafec706c57aa20/arrow/src/tensor.rs#L115 

### Alternative Solutions

just make everything an ""object"":
```
>>> import numpy as np
>>> import pandas as pd
>>> x = {'hello': 'world'}
>>> y = np.ones(3)
>>> df = pd.DataFrame({'X': [x], 'Y': [y]})
>>> df
                    X                Y
0  {'hello': 'world'}  [1.0, 1.0, 1.0]
>>> df.dtypes
X    object
Y    object
dtype: object
```

### Additional Context

[1] https://github.com/pandas-dev/pandas/pull/58455#issuecomment-2161603939 onward","['Enhancement', 'Needs Discussion', 'ExtensionArray', 'Closing Candidate']",2024-06-13 19:30:21,2025-08-27 18:38:46,6,closed
59005,ENH: Pivot Table,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

To create a dynamic table, is it possible to make the generated table editable when saved in an excel file? In other words, when I click on the result of the dynamic table, will the Excel table edit field open?
![pandas](https://github.com/pandas-dev/pandas/assets/25842043/22c88e8b-50fa-44a4-9c47-76707da34cb1)


### Feature Description

Maybe leave a boolean option

### Alternative Solutions

pivot_tab = pd.pivot_table(editable=True,...)

### Additional Context

_No response_","['Enhancement', 'IO Excel']",2024-06-13 19:06:34,2024-06-13 21:53:19,1,closed
59004,BUG: HDFStore doesn't save datetime64[s] right,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# **** This doesn't work

import pandas as pd
df = pd.DataFrame(['2001-01-01', '2002-02-02'], dtype='datetime64[s]')
print(df)
# Prints
#            0
# 0 2001-01-01
# 1 2002-02-02

print(df.dtypes)
# Prints
# 0    datetime64[s]
# dtype: object

with pd.HDFStore('deleteme.h5', mode='w') as store:
    store.put(
        'df',
        df,
    )

with pd.HDFStore('deleteme.h5', mode='r') as store:
    df_fromstore = store.get('df')
print(df_fromstore)
# Prints
#                               0
# 0 1970-01-01 00:00:00.978307200
# 1 1970-01-01 00:00:01.012608000

# Delete created file
import pathlib
pathlib.Path('deleteme.h5').unlink()

# **** However this does work

import pandas as pd
df = pd.DataFrame(['2001-01-01', '2002-02-02'], dtype='datetime64[ns]')
print(df)
# Prints
#            0
# 0 2001-01-01
# 1 2002-02-02

print(df.dtypes)
# Prints
# 0    datetime64[ns]
# dtype: object

with pd.HDFStore('deleteme.h5', mode='w') as store:
    store.put(
        'df',
        df,
    )

with pd.HDFStore('deleteme.h5', mode='r') as store:
    df_fromstore = store.get('df')
print(df_fromstore)
# Prints
#            0
# 0 2001-01-01
# 1 2002-02-02

# Delete created file
pathlib.Path('deleteme.h5').unlink()
```


### Issue Description

When saving to a file using .h5 (HDF5 format), if a column is of type `datetime64[s]` (s not ns) and then loading the file, the date isn't loaded correctly. My intuition tells me that the file gets saved to `datetime64[s]` but when read Pandas parses the date as `datetime64[ns]` and I guess some more digits should be present if parsing to `datetime64[ns]`. I haven't done any tests with other datetime precisions.

### Expected Behavior

When saving a `datetime64[s]` to HDF5 file format and loading that file, the date should be correctly loaded.

### Installed Versions

<details>

```shell
UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 70.0.0
pip                   : 24.0
Cython                : 3.0.8
pytest                : 8.0.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.1.9
lxml.etree            : 5.1.0
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.21.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.2
numba                 : None
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.4
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.12.0
sqlalchemy            : None
tables                : 3.9.2
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'IO HDF5', 'Non-Nano']",2024-06-13 18:39:51,2024-06-18 21:49:21,1,closed
59003,BUG: None value in MultiIndex can't be found,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

indices = [[""one""], [""a""], [""yes""]]
df = pd.DataFrame([0], index=indices).T
df[""one"", None, ""yes""] = 1
print(df)
print()
print(df[""one""])
print()
print(df[""one""][None])
```


### Issue Description

The dataframe has an item at the multiindex of [""one"", None, ""yes""], but attempting to access it fails. It appears to only happen when I use None/nan values for the middle index that I'm adding, and it only happens if I add the value using the df[] = syntax. 

### Expected Behavior

It should return a value when I query df[""one""][None]. If I construct the dataframe with this data in it to begin with, or add it with a df.concat, it works.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.2.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 4.18.0-513.24.1.el8_9.x86_64
Version               : #1 SMP Thu Mar 14 14:20:09 EDT 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.23.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.0.0
pip                   : 23.1
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : 2.0.12
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-06-13 18:29:53,2024-06-25 20:14:42,10,closed
59000,BUG: docker build on windows fails; need eol=lf in .gitattributes,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
``python -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true`` fails when run in a docker container on a windows host.
```


### Issue Description

I followed Option 3 Using Docker instructions to set up a pandas build environment. When I ran the pip install command in my container on a windows host, the build failed because ``generate_version.py`` has CRLF line endings. (I checked, my ``~/.gitconfig`` and it has autocrlf=false.) But CRLF line endings are not allowed in a shebang script. When I added ``eol=lf`` to line 1 of ``.gitattributes`` the problem went away. So, I think [line 1 of .gitattributes](https://github.com/pandas-dev/pandas/blob/main/.gitattributes#L1) should be

```text
* text=auto eol=lf
```


### Expected Behavior

For ``python -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true`` to succeed.

### Installed Versions

2.2.2","['Bug', 'Build', 'Closing Candidate']",2024-06-13 15:35:32,2024-06-13 17:52:54,2,closed
58999,BUG: output of pandas.util.hash_pandas_object() changed in pandas 2.2.x,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

index = pd.Index([0, 1, 2], dtype=""Int64"")  # without `dtype=""Int64""` the bug doesn't appear
df = pd.DataFrame([0, 1, 2], index=index)
pd.util.hash_pandas_object(df)
```


### Issue Description

Under `pandas` 2.1.0 we get:

```
0    17186048303505514001
1    10757116890508915594
2    14391254723460080955
dtype: uint64
```

Under `pandas` 2.2.0 and 2.2.2 we get:

```
0    3713087409444908179
1    7554402398462747209
2    1687604933839263903
dtype: uint64
```

### Expected Behavior

Hashing should always return the same values, independent of the used `pandas` version.

### Installed Versions

<details>

INSTALLED VERSIONS          
------------------          
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64  
OS                    : Linux
OS-release            : 6.5.0-1023-oem
Version               : #24-Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 14:26:31 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_GB.UTF-8
LOCALE                : en_GB.UTF-8
                                           
pandas                : 2.2.2                                                                                                                                         [7/2907]
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 59.6.0
pip                   : 22.0.2
Cython                : None
pytest                : 8.1.1
hypothesis            : None
sphinx                : 5.3.0
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.24.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 15.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2024-06-13 13:49:25,2024-06-13 15:06:11,2,closed
58997,"Potential performance regression with ""BUG: DateTimeIndex.is_year_start unexpected behavior when constructed…""","PR #57494

@mattheeter

{
        ""`tslibs.fields.TimeGetDateField.time_get_date_field` (Python) with field='s', size=1000000"": ""http://57.128.112.95:5000/compare/benchmarks/066639a2084f75ab8000e6252b6a277a...066647f1be1a7f878000e6f25fe386d1"",
        ""`tslibs.fields.TimeGetDateField.time_get_date_field` (Python) with field='M', size=1000000"": ""http://57.128.112.95:5000/compare/benchmarks/066639a205f975a580001752d162969b...066647f1bba577c78000f3cc52b36eca"",
        ""`tslibs.fields.TimeGetDateField.time_get_date_field` (Python) with field='doy', size=1000000"": ""http://57.128.112.95:5000/compare/benchmarks/066639a20a157acf800052022a1021a8...066647f1bfb77cbb8000eeb799121073"",
        ""`tslibs.fields.TimeGetDateField.time_get_date_field` (Python) with field='doy', size=10000"": ""http://57.128.112.95:5000/compare/benchmarks/066639a201ec71228000b75645555199...066647f1b762714e8000123e3c46ad25""
    }

<img width=""727"" alt=""Screenshot 2024-06-13 at 10 55 21"" src=""https://github.com/pandas-dev/pandas/assets/11835246/8577275b-f450-46e7-b881-1bb0e25a340d"">
",['Benchmark'],2024-06-13 08:56:02,2024-06-13 15:19:01,1,closed
58996,BUG: csv date inconsistencies,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from io import BytesIO

csv_data = """"""name,date
Alice,13/06/2024
Bob,11/05/2017""""""

pd.read_csv(
    BytesIO(csv_data.encode('utf-8')), parse_dates=['date'], dtype=str
)
```


### Issue Description

with version 2.1.3 I have:

name,date
Alice,13/06/2024
Bob,11/05/2017

and with 2.2.1:

name,date
Alice, 1718236800000000000
Bob, 1494460800000000000



### Expected Behavior

name,date
Alice,13/06/2024
Bob,11/05/2017

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2
python                : 3.12.1.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 22.6.0
Version               : Darwin Kernel Version 22.6.0: Mon Apr 22 20:54:28 PDT 2024; root:xnu-8796.141.3.705.2~1/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.1
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 7.4.4
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.1.1
lxml.etree            : 5.1.0
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.21.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : 2024.2.0
fsspec                : 2023.12.2
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : None
numexpr               : 2.8.7
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 14.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : 2024.2.0
scipy                 : 1.13.0
sqlalchemy            : 2.0.25
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO CSV', 'Closing Candidate']",2024-06-13 08:14:57,2024-06-15 12:36:08,2,closed
58993,"BUG: pandas.read_parquet fails to read parquet files with ""file://"" prefix on Windows","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
# Place the following files in E:\tmp:
# 
# - ab.parquet
# - a b.parquet


import pandas

p0 = ""E:/tmp/a b.parquet"" 
p1 = ""file://E:/tmp/a b.parquet""
p2 = ""file://E:/tmp/ab.parquet""
p3 = ""file://E:/tmp/a%20b.parquet""

pandas.read_parquet(p0)  # Should succeed
pandas.read_parquet(p1)  # Should succeed
pandas.read_parquet(p2)  # Fails
pandas.read_parquet(p3)  # Fails
```


### Issue Description

When attempting to read Parquet files using pandas.read_parquet with a ""file://"" prefix in the file path, the function raises a FileNotFoundError. This issue occurs specifically when using paths with the ""file://"" prefix, regardless of whether the space character is URL encoded or not.

### Expected Behavior

All paths (p0, p1, p2, and p3) should be read successfully without any exceptions.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 141 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : Korean_Korea.949

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.0.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO Parquet', 'Closing Candidate']",2024-06-13 00:36:19,2024-06-13 01:52:30,2,closed
58991,BUG: `unstack` with `sort=False` scrambles data,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

assert pd.__version__ == ""2.2.2""

df = pd.DataFrame(
    {
        ""index1"": [2, 1],
        ""index2"": [2, 1],
        ""index3"": [2, 1],
        ""value"": [2, 1],
    },
)
df = df.set_index([""index1"", ""index2"", ""index3""])


df.unstack((""index2"", ""index1""), sort=False)
```


### Issue Description

When using triple indices and unstacking two, the data is mapped to the wrong index value when using `sort=False`. In the example the data should be on the diagonal.

### Expected Behavior

The output with `sort=True` seems valid, but with the unsorted index.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.4.0
Version               : Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:54 PST 2024; root:xnu-10063.101.15~2/RELEASE_ARM64_T6031
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : None.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.0.0
pip                   : 24.0
Cython                : None
pytest                : 8.2.2
hypothesis            : None
...
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Reshaping', 'Duplicate Report']",2024-06-12 21:20:59,2024-06-15 10:46:38,3,closed
58990,DOC: punctuation of titles in Titanic data,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html


### Documentation problem

In the example data of the Titanic passengers, the titles are all listed with a period after them.  For Miss and Master, this is incorrect because these titles aren't short for anything.

References:
https://www.scribbr.com/effective-communication/ms-mrs-miss/
https://www.grammarly.com/blog/ms-mrs-miss-difference/
https://www.merriam-webster.com/dictionary/Mrs.

### Suggested fix for documentation

Replace Miss. with Miss and Master. with Master

Places to make changes:
- the tutorial text
- the csv file
- the screenshot of a spreadsheet containing the data

I don't see an option to assign the issue to myself, but I can do it.",['Docs'],2024-06-12 18:27:19,2024-06-13 17:34:25,1,closed
58989,API: timestamp resolution inference - default to one unit (if possible) instead of being data-dependent?,"After https://github.com/pandas-dev/pandas/pull/55901, we now do inference of the best resolution, and so allow to create non-nanosecond data by default (instead of raising for out of bounds data). 

To be clear, it is a _very_ nice improvement to stop raising those OutOfBounds errors while the timestamp would perfectly fit in another resolution. But I do think we could maybe reconsider the exact logic of how to determine the resolution.

With the latest changes you get the following:

```python
>>> pd.to_datetime([""2024-03-22 11:43:01""]).dtype
dtype('<M8[s]')
>>> pd.to_datetime([""2024-03-22 11:43:01.002""]).dtype
dtype('<M8[ms]')
>>> pd.to_datetime([""2024-03-22 11:43:01.002003""]).dtype
dtype('<M8[us]')
```

The resulting dtype instance depends on the exact input value (not type). I do think this has some downsides:

- The result dtype becomes very data dependent (while in general we want to avoid value dependent behavior)
- You can very easily get multiple datetime dtypes in a workflow, causing more casting (to different unit) than necessary

The fact that pandas by default truncates the string repr of datetimes (i.e. we don't show the subsecond parts if they are all zero, regardless of the actual resolution), in contrast to numpy, also means that round-tripping through a text representation (eg CSV) will very often lead to a change in dtype.

As a potential alternative, we could also decide to have a fixed _default resolution_ (e.g. microseconds), and then the logic for inferring the resolution could be: try to use the default resolution, and only if that does not work (either out of bounds or too much precision, i.e. nanoseconds present), use the inferred resolution from the data. 

That still gives some values dependent behaviour, but I think this would make it _a lot_ less common to see. And using a resolution like microseconds is sufficient for by far most use cases (in terms of bounds it supports: [290301 BC, 294241 AD])","['Datetime', 'Non-Nano', 'Timestamp']",2024-06-12 18:26:30,2025-11-19 01:25:39,40,closed
58987,"BUG: pd.read_sql() converts string ""NA"" to None","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import sqlalchemy as sa

con_string = f'mssql+pymssql:...' # fill in as appropriate

sql = sa.text('SELECT * FROM table1')
with engine.connect() as conn:
  df = pd.read_sql(sql, con=conn)
```


### Issue Description

When the database table table1 has NA in a varchar column, the resulting dataframe has None instead of the string NA.

There is no option for na_values = [...] as there is with pd.read_csv, for instance, but there really shouldn't have to be. The database distinguishes NULLs from any string, including the string NA, and so any string, even NA should come across as the string. Very odd to understand there's some conversion going on in the background.

### Expected Behavior

The row and column that has string NA in the database table1 should have NA, not None, in the corresponding location in the dataframe.

### Installed Versions

<details>

version is pandas 2.2.2; pd.show_versions() yields an error (AttributeError: module 'pkgutil' has no attribute 'ImpImporter'.)

</details>
","['Bug', 'Needs Triage']",2024-06-12 14:22:48,2024-06-12 14:33:46,1,closed
58983,BUG: Rounding timedelta to 3 decimal places does not work correctly and is unstable.,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
from pandas import Timedelta

td = Timedelta('0 days 00:00:00.000500')
td
# Timedelta('0 days 00:00:00.000500')

td.round('ms')
# Timedelta('0 days 00:00:00') <- the round is incorrect

td_new = Timedelta('0 days 00:00:00.000501')    # added .000001
td_new.round('ms')
# Timedelta('0 days 00:00:00.001000')

# With builtin round
round(0.000500, 3)
# 0.001

# Another strange situation with pandas
td_strange = Timedelta('0 days 00:00:00.001500')    # <- change 000500 to 001500
td_strange.round('ms')
# Timedelta('0 days 00:00:00.002000') <- looks OK

# but with
td_strange1 = Timedelta('0 days 00:00:00.002500')    # <- change 001500 to 002500
td_strange1.round('ms')
# Timedelta('0 days 00:00:00.002000') <- looks incorrect

# and with builtin round
round(1.000500, 3)
# 1.0
round(2.000500, 3)
# 2.001
```


### Issue Description

Rounding does not work for values ​​ending in 5. Based on the rounding rules, we round up from 5. Pandas does this in some cases and not in others. **Unstable**

### Expected Behavior

Correct application of rounding rules, or an alternative solution for fast rounding of large data (> 1 million).

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 186 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 1.25.2
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 65.5.0
pip                   : 23.2.1
Cython                : None
pytest                : 7.4.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.16.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.7.2
numba                 : None
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.1
sqlalchemy            : None
tables                : 3.9.2
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-06-12 06:25:40,2024-06-12 16:44:43,2,closed
58980,BUG: in agg function,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Bob', 'Charlie'],
    'Subject': ['Math', 'Math', 'Math', 'Science', 'Science', 'Science'],
    'Score': [85, 78, 92, 89, 76, 95]
}

df = pd.DataFrame(data)
df.groupby('Subject').agg(['min','max','mean'])
```


### Issue Description

output:-
agg function failed [how->mean,dtype->object]

In previous version the output is coming now not coming due to object dtype, and also unable to change it to numeric_only = True

### Expected Behavior

It should show a output in the form of for numeric value

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.11.7.final.0
python-bits         : 64
OS                  : Windows
OS-release          : 10
Version             : 10.0.22631
machine             : AMD64
processor           : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder           : little
LC_ALL              : None
LANG                : None
LOCALE              : English_India.1252

pandas              : 2.1.4
numpy               : 1.26.4
pytz                : 2023.3.post1
dateutil            : 2.8.2
setuptools          : 68.2.2
pip                 : 23.3.1
Cython              : None
pytest              : 7.4.0
hypothesis          : None
sphinx              : 5.0.2
blosc               : None
feather             : None
xlsxwriter          : None
lxml.etree          : 4.9.3
html5lib            : None
pymysql             : None
psycopg2            : None
jinja2              : 3.1.3
IPython             : 8.20.0
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : 1.3.7
dataframe-api-compat: None
fastparquet         : None
fsspec              : 2023.10.0
gcsfs               : None
matplotlib          : 3.8.0
numba               : 0.59.0
numexpr             : 2.8.7
odfpy               : None
openpyxl            : 3.0.10
pandas_gbq          : None
pyarrow             : 14.0.2
pyreadstat          : None
pyxlsb              : None
s3fs                : 2023.10.0
scipy               : 1.11.4
sqlalchemy          : 2.0.25
tables              : 3.9.2
tabulate            : 0.9.0
xarray              : 2023.6.0
xlrd                : None
zstandard           : 0.19.0
tzdata              : 2023.3
qtpy                : 2.4.1
pyqt5               : None


</details>
","['Bug', 'Groupby', 'Apply']",2024-06-12 03:18:20,2024-06-13 22:02:31,1,closed
58977,BUG: `Series.combine_first` replaces `None` with `NaN` at index not provided by `other`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

s1 = pd.Series([None, None, None], index=['a', 'b', 'c'])
s2 = pd.Series([None, None, None], index=['b', 'c', 'd'])

result = s1.combine_first(s2)
print(result)
```


### Issue Description

The documentation of `Series.combine_first` states:

> Update null elements with value in the same location in ‘other’.

In the above example, `s2` doesn't provide any value at index `'a'`, so `combine_first` should not affect this index.
However, the result with pandas 2.2.2 is the following, which unexpectedly changes the value at `'a'` from `None` to `NaN`:

```
a     NaN
b    None
c    None
d    None
dtype: object
```

Pandas 2.1.4 behaves as expected, so this looks like a regression.
The behavior was changed with this PR: https://github.com/pandas-dev/pandas/pull/57034

### Expected Behavior

The expected output is:
```
a    None
b    None
c    None
d    None
dtype: object
```

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 6dbeeb4009bbfac5ea1ae2111346f5e9f05b81f4
python                : 3.10.8.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-35-generic
Version               : #35-Ubuntu SMP PREEMPT_DYNAMIC Fri Apr 26 11:23:57 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.0rc0+28.g6dbeeb4009
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 63.2.0
pip                   : 24.0
Cython                : 3.0.10
pytest                : 8.2.0
hypothesis            : 6.100.2
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.1
html5lib              : 1.1
pymysql               : 1.4.6
psycopg2              : 2.9.9
jinja2                : 3.1.3
IPython               : 8.24.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.8
fastparquet           : 2024.2.0
fsspec                : 2024.3.1
gcsfs                 : 2024.3.1
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.2
pyarrow               : 16.0.0
pyreadstat            : 1.2.7
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.3.1
scipy                 : 1.13.0
sqlalchemy            : 2.0.29
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.3.0
xlrd                  : 2.0.1
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage', 'combine/combine_first/update']",2024-06-11 07:25:14,2025-10-31 16:54:15,5,closed
58969,BUG: Invalid Stata file is generated if byteorder='big' is specified and strings are saved as strL type,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame(data={'strings': [""abc"", ""def"", ""ghi""]})
df.to_stata(""test.dta"", byteorder=""big"", version=117, convert_strl=[""strings""]) # <-- invalid file created
```


### Issue Description

Although there is a documented argument defined to set it, the selected  `byteorder` option is not passed to the `StataStrLWriter` ` __init__` function from the `_convert_strls` function. There is also no handling of byteorder when calculating or using `self._o_offet`.

This results in the strL data always being saved in the default byteorder, even if the rest of the file isn't, resulting in following error when attempting to open the saved file, both in Pandas:
```python
>>> import pandas as pd
>>> df = pd.DataFrame(data={'strings': [""abc"", ""def"", ""ghi""]})
>>> df
  strings
0     abc
1     def
2     ghi
>>> df.to_stata(""test.dta"", byteorder=""big"", version=117, convert_strl=[""strings""])
>>> df2 = pd.read_stata(""test.dta"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas\io\stata.py"", line 2082, in read_stata
    return reader.read()
  File ""pandas\io\stata.py"", line 1741, in read
    data = self._insert_strls(data)
  File ""pandas\io\stata.py"", line 1838, in _insert_strls
    data.isetitem(i, [self.GSO[str(k)] for k in data.iloc[:, i]])
  File ""pandas\io\stata.py"", line 1838, in <listcomp>
    data.isetitem(i, [self.GSO[str(k)] for k in data.iloc[:, i]])
KeyError: '4294967298'
```
or within Stata:
```
. use ""test.dta"" 
.dta file corrupt
    The strL component of the file is corrupt.  Either the file was incorrectly constructed or it became
    corrupted since it was written.  In case the file was incorrectly constructed,the technical description of
    the problem is that there was an attempt to crosslink a strl to another strl that had not yet been defined.
r(688);

. dtaverify ""test.dta""
  (file ""test.dta"" is .dta-format 117 from Stata 13)

 1. reading and verifying header
    release is 117
    byteorder MSF
    K (# of vars) is 2
    N (# of obs) is 3
    label length 0 ||
    date length 17 |10 Jun 2024 16:05|

 2. reading and verifying map
    map[ 1] =                    0
    map[ 2] =                  153
    map[ 3] =                  276
    map[ 4] =                  313
    map[ 5] =                  400
    map[ 6] =                  427
    map[ 7] =                  544
    map[ 8] =                  649
    map[ 9] =                  846
    map[10] =                  881
    map[11] =                  930
    map[12] =                 1005
    map[13] =                 1034
    map[14] =                 1046
    verifying map
    1 2 3 4 5 6 7 8 9 10 11 12 13 14

 3. reading and verifying vartypes

 4. reading and verifying varnames
    verifying varnames unique

 5. reading and verifying sort order
    verifying contents

 6. reading and verifying display formats
    verifying formats
    verifying formats correspond to variable type

 7. reading and verifying value-label assignment
    verifying value-label construction
    verifying value label and corresponding variable types

 8. reading and verifying variable-label assignment
    verifying variable labels construction

 9. reading and verifying characteristics
    (0 characteristics in file)

10. reading and verifying data
    (dtaverify_117 cannot verify that values are correct)
    verifying construction
    (1 strL variables)
    verifying strL construction
SERIOUS ERROR: (var,obs)=(2,1) has (v,o)=(1,2); v not strL
               chkijvo():  3301  subscript invalid
      strl_process_obs():     -  function returned error
     read_data_u_strls():     -  function returned error
           read_data_u():     -  function returned error
             read_data():     -  function returned error
              readfile():     -  function returned error
       verify_dta_file():     -  function returned error
                 <istmt>:     -  function returned error
r(3301);
```

### Expected Behavior

I would expect the saved file to open correctly in both Pandas:
```python
>>> import pandas as pd
>>> df = pd.DataFrame(data={'strings': [""abc"", ""def"", ""ghi""]})
>>> df
  strings
0     abc
1     def
2     ghi
>>> df.to_stata(""test.dta"", byteorder=""big"", version=117, convert_strl=[""strings""])
>>> df2 = pd.read_stata(""test.dta"")
>>> df2
   index strings
0      0     abc
1      1     def
2      2     ghi
```

and Stata:
```
. use ""test.dta"" 

. list

     +-----------------+
     | index   strings |
     |-----------------|
  1. |     0       abc |
  2. |     1       def |
  3. |     2       ghi |
     +-----------------+

. dtaverify ""test.dta""
  (file ""test.dta"" is .dta-format 117 from Stata 13)

 1. reading and verifying header
    release is 117
    byteorder MSF
    K (# of vars) is 2
    N (# of obs) is 3
    label length 0 ||
    date length 17 |10 Jun 2024 16:12|

 2. reading and verifying map
    map[ 1] =                    0
    map[ 2] =                  153
    map[ 3] =                  276
    map[ 4] =                  313
    map[ 5] =                  400
    map[ 6] =                  427
    map[ 7] =                  544
    map[ 8] =                  649
    map[ 9] =                  846
    map[10] =                  881
    map[11] =                  930
    map[12] =                 1005
    map[13] =                 1034
    map[14] =                 1046
    verifying map
    1 2 3 4 5 6 7 8 9 10 11 12 13 14

 3. reading and verifying vartypes

 4. reading and verifying varnames
    verifying varnames unique

 5. reading and verifying sort order
    verifying contents

 6. reading and verifying display formats
    verifying formats
    verifying formats correspond to variable type

 7. reading and verifying value-label assignment
    verifying value-label construction
    verifying value label and corresponding variable types

 8. reading and verifying variable-label assignment
    verifying variable labels construction

 9. reading and verifying characteristics
    (0 characteristics in file)

10. reading and verifying data
    (dtaverify_117 cannot verify that values are correct)
    verifying construction
    (1 strL variables)
    verifying strL construction

11. reading and verifying strLs
    verifying construction
    (3 strLs expected)

12. reading and verifying value label definitions
    verifying construction
    (0 labels in file)

no errors or warnings above; .dta file valid
```

### Installed Versions

<details>
miniforge3\envs\pandas-dev\lib\site-packages\_distutils_hack\__init__.py:26: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

INSTALLED VERSIONS
------------------
commit                : da802479fca521b7992fbf76d5984992cd5d495d
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United Kingdom.1252

pandas                : 3.0.0.dev0+629.gda802479fc
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 69.2.0
pip                   : 24.0
Cython                : 3.0.9
pytest                : 8.1.1
hypothesis            : 6.99.13
sphinx                : 7.2.6
blosc                 : None
feather               : None
xlsxwriter            : 3.1.9
lxml.etree            : 5.1.0
html5lib              : 1.1
pymysql               : 1.4.6
psycopg2              : 2.9.9
jinja2                : 3.1.3
IPython               : 8.22.2
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.8
fastparquet           : 2024.2.0
fsspec                : 2024.3.1
gcsfs                 : 2024.3.1
matplotlib            : 3.8.3
numba                 : 0.59.1
numexpr               : 2.9.0
odfpy                 : None
openpyxl              : 3.1.2
pyarrow               : 15.0.2
pyreadstat            : 1.2.7
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.3.1
scipy                 : 1.12.0
sqlalchemy            : 2.0.29
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.2.0
xlrd                  : 2.0.1
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage']",2024-06-10 16:02:47,2024-06-10 20:05:46,0,closed
58965,"Inconsistent ""index""/""name"" property referring to both rows and columns based on iloc selection","Depending on how the Dataframe is selected, Pandas does a transpose of what `name` and `index` refer to. 

Namely given `df.index`, it would naturally follow that selecting the index of the first element, `df.iloc[0].index` would return `df.index[0]`, but this isn't the case. Instead it returns the columns.

```python
print(df.index)
# RangeIndex(start=0, stop=3, step=1)

print(df.iloc[0].index) 
# Index(['a', 'b', 'c'], dtype='object')
```

See a more general example below:

```python
import pandas as pd
import numpy as np

df = pd.DataFrame(np.arange(9).reshape(3,3), columns=['a', 'b', 'c'])

print(df.iloc[:, 0].index)  
# output: RangeIndex(start=0, stop=3, step=1)

print(df.iloc[0, :].index) 
# output: Index(['a', 'b', 'c'], dtype='object')

print(df.iloc[:, 0].name)
# output: a
print(df.iloc[0, :].name)
# output: 0
```

I think one should expect that `index` should always refer to one thing and `name` another regardless of how selection is made. The following may be more inline with expectation:

```python
print(df.iloc[:, 0].index)  
# output: RangeIndex(start=0, stop=3, step=1)

print(df.iloc[0, :].index) 
# output: RangeIndex(start=0, stop=1, step=1)

print(df.iloc[:, 0].name)
# output: Index(['a'], dtype='object')
print(df.iloc[0, :].name)
# output: Index(['a', 'b', 'c'], dtype='object')
```",[],2024-06-09 17:53:38,2024-06-10 16:51:08,2,closed
58964,BUG: `interval_range` ignores type of `start` (regression from pandas 2.2.2),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
import numpy as np

result = pd.interval_range(start=np.float32(0), end=2, freq=1)
print(result)
```


### Issue Description

I'd expect the above code to produce an `IntervalIndex` with dtype `interval[float64, right]`, because I'm giving a float32 as the starting value. This has been the behavior in pandas 2.2.2.

The current main branch produces the following output, though, which ignores the starting value's type:
`IntervalIndex([(0, 1], (1, 2]], dtype='interval[int64, right]')`

The change seems due to https://github.com/pandas-dev/pandas/pull/57399. The PR was meant to preserve the `start`/`end` type, but actually changed the behavior so that it isn't preserved anymore in the above case.

### Expected Behavior

Expected output:
`IntervalIndex([(0.0, 1.0], (1.0, 2.0]], dtype='interval[float64, right]')`

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : 6dbeeb4009bbfac5ea1ae2111346f5e9f05b81f4
python                : 3.10.8.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-35-generic
Version               : #35-Ubuntu SMP PREEMPT_DYNAMIC Fri Apr 26 11:23:57 UTC 2024
machine               : x86_64
processor             : 
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.0rc0+28.g6dbeeb4009
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 63.2.0
pip                   : 24.0
Cython                : 3.0.10
pytest                : 8.2.0
hypothesis            : 6.100.2
sphinx                : 7.3.7
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.1
html5lib              : 1.1
pymysql               : 1.4.6
psycopg2              : 2.9.9
jinja2                : 3.1.3
IPython               : 8.24.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : 1.3.8
fastparquet           : 2024.2.0
fsspec                : 2024.3.1
gcsfs                 : 2024.3.1
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.2
pyarrow               : 16.0.0
pyreadstat            : 1.2.7
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : 2024.3.1
scipy                 : 1.13.0
sqlalchemy            : 2.0.29
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.3.0
xlrd                  : 2.0.1
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Interval', 'Needs Triage']",2024-06-09 16:38:44,2025-11-18 14:18:10,2,closed
58963,PERF: 100% CPU utilization with matplotlib (macosx),"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example

machine: Mac (Intel) Ventura 13.6.6
python version: 3.12.3 (pyenv)
pandas version: 2.2.2 (pip)
matplotlib version: 3.9.0 (pip)  (backend: macosx)

Problem: With matplotlib, pandas consumes 100% CPU.

Below code runs with reasonable CPU utilization;  when the canvas was shown, almost no cpu utilization. 
```
import matplotlib.pyplot as plt
fig = plt.figure()
plt.plot(range(5))
plt.show()
```

After closing the matplotlib window, simply importing pandas drives the CPU utilization 100%.

```
import matplotlib.pyplot as plt
fig = plt.figure()
plt.plot(range(5))
plt.show()
# after closing the window
import pandas              # starting 100% CPU utilization
```

If I start the python interpreter, and running following code also consumes 100% of CPU:

```
import matplotlib.pyplot as plt
import pandas as pd

# No CPU utilization at the moment

fig = plt.figure()
df = pd.DataFrame(range(5))

plt.plot(df[0])           # with this, CPU utilization is 100%.
plt.show()                 # the same

# after closing the window, still 100%
```

The closest hint that I found was this: https://github.com/matplotlib/matplotlib/issues/4092

I'm not sure but, I suspect that pandas' GUI backend initialization code (if any) for matplotlib (macosx backend) somehow creating this problem?

Without pandas, matplotlib's plot() alone does not cause high CPU utilization, so I post this here.


### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 22.6.0
Version               : Darwin Kernel Version 22.6.0: Mon Feb 19 19:48:53 PST 2024; root:xnu-8796.141.3.704.6~1/RELEASE_X86_64
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>


### Prior Performance

_No response_","['Performance', 'Needs Triage']",2024-06-09 05:25:12,2024-10-09 21:16:55,1,closed
58956,PERF: groupby is significantly slower for `DatetimeIndex` with timezone,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.
- [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.
- [ ] I have confirmed this issue exists on the main branch of pandas.


### Reproducible Example
```python
import pandas as pd
for tz in [None, ""UTC+01:00""]:
    dtindex = pd.date_range(""1900-01-01"",""2024-01-01"", freq=""30min"", tz=tz)
    df = pd.DataFrame(index=dtindex)
    df[""values""] = 1
    print(f'deriving stats for tz={tz}')
    dtstart = pd.Timestamp.now()
    wl_count_peryear = df.groupby(pd.PeriodIndex(df.index, freq=""Y""))[""values""].count() # .mean() gives comparable timings
    print(f'{(pd.Timestamp.now()-dtstart).total_seconds():.2f} sec')
```

The above code does a groupby with an arbitrary reduction, both with and without `tz` in the DatetimeIndex, the timings are the following:
```python
deriving stats for tz=None:
0.09 sec
deriving stats for tz=UTC+01:00:
7.72 sec
```
This is a significant performance difference, while the results are equal. Is this expected or a bug? I can workaround this with `df.tz_localize(None)` on my dataframe with timezones, but it still seemed good to report this.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.6.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : en
LOCALE                : Dutch_Netherlands.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2023.3.post1
dateutil              : 2.9.0.post0
setuptools            : 68.2.2
pip                   : 23.3.1
Cython                : None
pytest                : 7.4.3
hypothesis            : None
sphinx                : 7.2.6
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.3
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : 1.3.7
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.12.1
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.58.1
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.4
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : 2024.3.0
xlrd                  : None
zstandard             : None
tzdata                : 2023.3
qtpy                  : 2.4.1
pyqt5                 : None

</details>


### Prior Performance

Tested for pandas 2.1.4 and 2.2.2, both versions behave similar.","['Performance', 'Needs Triage']",2024-06-07 14:07:32,2024-06-07 17:30:49,2,closed
58954,BUG: Column of dtype Categorical in DataFrame encounters error when taking a row that includes nan in the column,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

df2 = pd.DataFrame({'a': [1, 2], 'b': pd.Categorical([3, np.nan])})
df2.dtypes
df2.iloc[0, :] # The series has dtype int
df2.iloc[1, :] # ValueError: cannot convert float NaN to integer

df2 = pd.DataFrame({'a': [1., 2.], 'b': pd.Categorical([3, np.nan])})
df2.dtypes
df2.iloc[0, :] # The series has dtype float
df2.iloc[1, :] # OK, because the first column is float

df2 = pd.DataFrame({'a': [1, 2], 'b': pd.Series([3, np.nan], dtype=object)})
df2.dtypes
df2.iloc[0, :] # The series has dtype object
df2.iloc[1, :] # OK, because the Series of dtype object can hold mixed element type
```


### Issue Description

When columns are created as `pd.Categorical`, taking a row out sometimes encounter strange error, because a row is of type `pd.Series`, which has to take a fixed type for all the elements. If there is `np.nan` in the row, it might throw error if the earlier column is of type `int`. Would it make sense to make the row ALWAYS take dtype `object`, because it is very common to have mixed types as row ALWAYS spans different columns?

### Expected Behavior

Taking a row out of a DataFrame that has a `pd.Categorical` column should not report inconsistent error, depending on what earlier columns are present.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit              : ba1cccd19da778f0c3a7d6a885685da16a072870
python              : 3.11.5.final.0
python-bits         : 64
OS                  : Darwin
OS-release          : 23.4.0
Version             : Darwin Kernel Version 23.4.0: Fri Mar 15 00:10:42 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T6000
machine             : arm64
processor           : arm
byteorder           : little
LC_ALL              : None
LANG                : en_US.UTF-8
LOCALE              : en_US.UTF-8

pandas              : 2.1.0
numpy               : 1.25.2
pytz                : 2023.3
dateutil            : 2.8.2
setuptools          : 65.5.0
pip                 : 24.0
Cython              : None
pytest              : 7.4.0
hypothesis          : None
sphinx              : None
blosc               : None
feather             : None
xlsxwriter          : None
lxml.etree          : 4.9.3
html5lib            : None
pymysql             : None
psycopg2            : None
jinja2              : 3.1.2
IPython             : 8.14.0
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : None
dataframe-api-compat: None
fastparquet         : None
fsspec              : None
gcsfs               : None
matplotlib          : 3.8.2
numba               : None
numexpr             : None
odfpy               : None
openpyxl            : 3.1.2
pandas_gbq          : None
pyarrow             : None
pyreadstat          : None
pyxlsb              : None
s3fs                : None
scipy               : 1.11.2
sqlalchemy          : None
tables              : None
tabulate            : 0.9.0
xarray              : None
xlrd                : None
zstandard           : None
tzdata              : 2023.3
qtpy                : None
pyqt5               : None

</details>
","['Bug', 'Indexing', 'Categorical']",2024-06-07 11:37:26,2025-09-04 22:01:10,5,closed
58952,BUG: FutureWarning when following documentation,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
df.replace({'..': np.nan})
responds with
FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  df.replace({'..': np.nan})
```


### Issue Description

This is directly copied from the latest Pandas documentation, which supports this type of replace behavior. It should not generate a FutureWarning.

Either this code will be deprecated and the documentation needs to be updated so I know what to do...

Or this code is correct and should not generate a warning.

I do not want any false errors, I want to know when I have an error that it is something I need to investigate.

### Expected Behavior

If I directly follow code examples from the latest documentation, I should have no warnings or errors. The documentation should clearly point me to the supported and preferred way to write my code as the first example. The first example should never generate a warning. It makes me worry my code is going to deprecate, and this wastes my time. I have written a lot of code over time, and I do my best to write code as up-to-date as possible to ensure I don't have to revise my code every year because I don't have time for that.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.13.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8112
machine               : x86_64
processor             : i386
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.23.5
pytz                  : 2022.1
dateutil              : 2.8.2
setuptools            : 63.4.1
pip                   : 22.2.2
Cython                : 0.29.32
pytest                : 8.0.2
hypothesis            : None
sphinx                : 5.0.2
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.9.1
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 7.31.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : 1.3.8
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Docs', 'Needs Info', 'PDEP6-related']",2024-06-06 22:24:43,2024-08-27 16:58:32,5,closed
58949,BUG: AttributeError: 'Engine' object has no attribute 'cursor',"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
from os import getcwd, path, getenv
from dotenv import load_dotenv
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base


# Get environment variables
host='localhost'
port=5432
user='postgres'
passw='postgres'
database_name='mydb'

# Connect to the database
db_uri = f'postgresql://{user}:{passw}@{host}:{port}/{database_name}'

# Create the database engine and session maker
engine = create_engine(db_uri)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

df=pd.DataFrame({'a': [1,2], 'b': [3,4]})

columns=[
    'a', 
    'b'
]

# Break the dataframe into chunks
df.to_sql(
    name='teste',
    if_exists='append',
    con=engine,
    index=False
)
```


### Issue Description

I use pandas method to_sql often. On sqlalchemy update 2.2.0 and beyond, I got following error AttributeError: 'Engine' object has no attribute 'cursor'. It seems, the interface has changed compared to previous versions.

### Expected Behavior

I expect to load data on table 'mydb' or append in case it exists already and have compatible columns. 

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22621
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : pt_BR.cp1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.2.0
pip                   : 24.0
Cython                : None
pytest                : 8.1.1
hypothesis            : None
...
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Info']",2024-06-06 18:50:28,2024-06-07 18:52:48,5,closed
58944,BUILD: ,"### Installation check

- [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).


### Platform

vs code

### Installation Method

pip install

### pandas Version

3.0

### Python Version

11.0

### Installation Logs

<details>

Replace this line with the installation logs.

</details>
","['Build', 'Needs Info']",2024-06-06 06:33:38,2024-08-26 19:47:29,2,closed
58943,BUG: Refactor test_dti_cmp_tdi_tzawareness function to reduce redundancy pandas/tests/arithmetic/test_datetime64.py,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
def test_dti_cmp_tdi_tzawareness(self, other):
    # GH#22074
    # Reversion test that ensures we do not call _assert_tzawareness_compat
    # when comparing a DatetimeIndex against a TimedeltaIndex

    # Create a DatetimeIndex with a timezone 'Asia/Tokyo', spanning 10 periods
    dti = date_range(""2000-01-01"", periods=10, tz=""Asia/Tokyo"")

    # Comparison operations and expected results
    comparison_tests = {
        '==': np.array([False] * 10),
        '!=': np.array([True] * 10)
    }

    # Loop through each comparison operation and expected result
    for op, expected in comparison_tests.items():
        # Perform the comparison dynamically
        result = eval(f'dti {op} other')
        # Assert that the result matches the expected array
        tm.assert_numpy_array_equal(result, expected)

    # Define the error message expected for invalid comparisons
    msg = ""Invalid comparison between""
    # List of comparison operators that should raise a TypeError
    invalid_comparisons = ['<', '<=', '>', '>=']

    # Loop through invalid comparisons to assert TypeError is raised
    for op in invalid_comparisons:
        with pytest.raises(TypeError, match=msg):
            eval(f'dti {op} other')
```


### Issue Description

The function had repeated sections for testing equality (`==` and `!=`) and invalid comparisons (`<`, `<=`, `>`, `>=`), which resulted in a lot of duplicated code.

### Expected Behavior

The behavior will remain same 

### Installed Versions

<details>

Replace this line with the output of pd.show_versions()

</details>
","['Bug', 'Needs Triage']",2024-06-06 05:45:02,2024-06-06 15:53:16,2,closed
58941,"DOC: add detailed comments pandas/tests/arithmatic/datetime/ line no: 750, test_dti_cmp_tdi_tzawareness","### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://github.com/pandas-dev/pandas/blob/main/pandas/tests/arithmetic/test_datetime64.py

### Documentation problem

Function's complicated parts are not easy to understand without any comments

### Suggested fix for documentation

Add detailed comments",['Docs'],2024-06-06 05:22:13,2024-06-06 15:01:03,1,closed
58938,BUG: default value for dtype_backend is not applied by default,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
python
# (interactive shell)
import pandas
s = pandas.Series(['150', pandas.NA])
print(pandas.to_numeric(s))
# 0    150.0
# 1      NaN
# dtype: float64
print(pandas.to_numeric(s, dtype_backend='numpy_nullable'))
# 0     150
# 1    <NA>
# dtype: Int64
```


### Issue Description

Documentation says that `numpy_nullable` is the default value for the `dtype_backend` argument, 
yet the funcion works differently whether you pass it explicitly or not.

### Expected Behavior

``` python
# (interactive shell)
import pandas
s = pandas.Series(['150', pandas.NA])
print(pandas.to_numeric(s))
# 0     150
# 1    <NA>
# dtype: Int64
```

### Installed Versions

<details>

``` yaml
/home/denisfranco/.local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.12.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 6.5.0-35-generic
Version               : #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : pt_BR.UTF-8

pandas                : 2.2.2
numpy                 : 1.22.4
pytz                  : 2022.1
dateutil              : 2.8.2
setuptools            : 65.6.3
pip                   : 22.0.2
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 4.8.0
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.4.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```

</details>
","['Docs', 'Missing-data', 'Dtype Conversions']",2024-06-05 20:18:13,2024-06-21 17:59:47,5,closed
58932,BUG: `diff()` returns confusing output when dealing with negative timestamp deltas,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
s = pd.Series(
  [
    pd.Timestamp(2024, 6, 5, 16, 35, 1),
    pd.Timestamp(2024, 6, 5, 16, 35, 4),
    pd.Timestamp(2024, 6, 5, 16, 35, 0), # earlier than previous entry
    pd.Timestamp(2024, 6, 5, 16, 35, 3),
  ]
)
s.diff()
```


### Issue Description

Actual output :
```
0                        NaT
1     0 days 00:00:00.000003
2   -1 days +23:59:56
3     0 days 00:00:00.000003
dtype: timedelta64[ns]
```

I just understood *after* submitting the bug that the output is technically correct (−1 day + (1 day - 4 s)…), but :
1. it’s confusing ;
2. it becomes wrong when you apply `dt.seconds` afterwards :

```
s.diff().dt.seconds
0        NaN
1        0.0
2    86396.0
3        0.0
dtype: float64
```

### Expected Behavior

Expected output :
```
0                        NaT
1     0 days 00:00:03
2    0 days -00:00:04
3     0 days 00:00:03
dtype: timedelta64[ns]
```

### Installed Versions

<details>
INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.2.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22621
machine               : AMD64
processor             : Intel64 Family 6 Model 154 Stepping 3, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : fr_FR.cp1252
pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.0.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : 2.9.9
jinja2                : 3.1.4
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : 0.59.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : 2.0.30
tables                : None
tabulate              : None
xarray                : 2024.5.0
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Needs Triage']",2024-06-05 14:40:13,2024-06-05 14:53:55,1,closed
58929,BUG: float64 convert to int64 get confuse result,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

#It's different
pd.DataFrame({
    'ori':pd.Series([995279394541916024+i for i in range(128)]).astype('int64'),
    'convert':pd.Series([995279394541916024+i for i in range(128)]).astype('float64').astype('int64'),
             }
            )
```


### Issue Description

Unless the number is very small, converting float to int will result in different values.
And there will be no warning for this change.

```
import pandas as pd

pd.DataFrame({
    'ori':pd.Series([995279394541916024+i for i in range(128)]).astype('int64'),
    'convert':pd.Series([995279394541916024+i for i in range(128)]).astype('float64').astype('int64'),
             }
            )
```
![image](https://github.com/pandas-dev/pandas/assets/46467828/e18a4f74-3421-48d6-8b23-73ef5006d52b)


### Expected Behavior


```
#It's the same
pd.DataFrame({
    'ori':pd.Series([995279394541916+i for i in range(128)]).astype('int64'),
    'convert':pd.Series([995279394541916+i for i in range(128)]).astype('float64').astype('int64'),
             }
            )
```
![image](https://github.com/pandas-dev/pandas/assets/46467828/cca698a6-6280-43cc-9a3a-c9f517bec6a2)

### Installed Versions

<details>

python ==3.9
pandas == 2.2.2

</details>
","['Bug', 'Needs Triage']",2024-06-05 12:46:14,2024-06-05 17:07:39,1,closed
58927,BUG: iterrows() on an awkward array with equal-length rows results in a ValueError,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import awkward as ak
import awkward_pandas as akpd
import pandas as pd

# numbers = [[1, 2, 3], [4, 5], [6]]
numbers = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
letters = [""A"", ""B"", ""C""]

numbers_ak = ak.from_iter(numbers)
numbers_akpd = akpd.from_awkward(numbers_ak)

df = pd.DataFrame({""letters"": letters, ""numbers"": numbers_akpd})

for idx, row in df.iterrows():
    print(f""{idx} - {row['letters']}, {row['numbers']}"")

```

```python-traceback
File .venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:2253, in EABackedBlock.get_values(self, dtype)
   [2251](.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:2251)     values = values.astype(object)
   [2252](.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:2252) # TODO(EA2D): reshape not needed with 2D EAs
-> [2253](.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:2253) return np.asarray(values).reshape(self.shape)

ValueError: cannot reshape array of size 9 into shape
```


### Issue Description

I'm not really knowledgeable on the internals of Pandas and array extensions, so I'm not sure if this is the right place to report the bug, but since it occurred in the Pandas core code, I came here first. Please let me know if this is actually an issue with the `awkward` or `awkward_pandas` module.

---

When calling `iterrows()` on a DataFrame which contains an awkward array as a column, a ValueError occurs (see stacktrace example). This error only occurs when all rows of the awkward array are of equal length. In this case the calls to `values.astype(object)` and/or `np.asarray(values)` in the `get_values` function in the `pandas/core/internals/blocks.py` module result in a 2D array, instead of a 1D array with nested lists.
When the awkward array is actually jagged, the call results in the correct format of the array (see commented line in code example) and `iterrows()` works as intended.

### Expected Behavior

I would expect `iterrows()` to iterate over the DataFrame rows without throwing an error, but instead returning a Series with the value of the awkward array at the index of the row set correctly.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.0.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.146.1-microsoft-standard-WSL2
Version               : #1 SMP Thu Jan 11 04:09:03 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 22.3
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.6.0
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-06-05 11:00:24,2024-06-08 09:42:12,3,closed
58926,BUG: .mode(dropna=False) doesn't work with nullable integers,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
series = pd.Series([1, 1, 2, 3]).astype('Int64')
print(series.mode(dropna=False))
```


### Issue Description

This code causes the following error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/REDACTED/lib/python3.12/site-packages/pandas/core/series.py"", line 2333, in mode
    res_values = values._mode(dropna=dropna)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/REDACTED/lib/python3.12/site-packages/pandas/core/arrays/masked.py"", line 1112, in _mode
    result, res_mask = mode(self._data, dropna=dropna, mask=self._mask)
    ^^^^^^^^^^^^^^^^
ValueError: too many values to unpack (expected 2)
```

### Expected Behavior

The code should print the modes of the input series.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.4.0
Version               : Darwin Kernel Version 23.4.0: Fri Mar 15 00:10:42 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.25.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.3
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'NA - MaskedArrays']",2024-06-05 03:49:06,2025-03-17 16:33:42,2,closed
58925,BUG: ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import io
import pandas as pd

df = pd.DataFrame(data={'index':[1,2], 'a': [2,3]})
s = df.to_json(orient=""table"")
df = pd.read_json(io.StringIO(s), orient=""table"")
```



### Issue Description

`read_json` failed with
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py"", line 815, in read_json
    return json_reader.read()
           ^^^^^^^^^^^^^^^^^^
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py"", line 1025, in read
    obj = self._get_object_parser(self.data)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py"", line 1051, in _get_object_parser
    obj = FrameParser(json, **kwargs).parse()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py"", line 1187, in parse
    self._parse()
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py"", line 1427, in _parse
    self.obj = parse_table_schema(json, precise_float=self.precise_float)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/io/json/_table_schema.py"", line 380, in parse_table_schema
    df = df.set_index(table[""schema""][""primaryKey""])
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/core/frame.py"", line 6178, in set_index
    index = ensure_index_from_sequences(arrays, names)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py"", line 7588, in ensure_index_from_sequences
    return Index(sequences[0], name=names)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py"", line 528, in __new__
    return cls(np.asarray(data), dtype=dtype, copy=copy, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ranbi/vsa_cs/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py"", line 570, in __new__
    raise ValueError(""Index data must be 1-dimensional"") from err
ValueError: Index data must be 1-dimensional
```

### Expected Behavior

`read_json` should return the same df as the original one

```
   index  a
0      1  2
1      2  3
```

### Installed Versions

<details>



INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.2.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.1.0
Version               : Darwin Kernel Version 23.1.0: Mon Oct  9 21:33:00 PDT 2023; root:xnu-10002.41.9~7/RELEASE_ARM64_T6031
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.3
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 8.0.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.2
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.12.3
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2024.3.1
gcsfs                 : None
matplotlib            : 3.8.2
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 15.0.2
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'IO JSON']",2024-06-04 23:21:51,2024-07-08 23:33:04,3,closed
58924,BUG: Index containing NA behaves absolutely unpredictably when length exceeds 128,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

# OK:
n, val = 127, pd.NA
idx = pd.Index(range(n), dtype=""Int64"").union(pd.Index([val], dtype=""Int64""))
s = pd.Series(index=idx, data=range(n+1), dtype=""Int64"")
s.drop(0)

# Still OK:
n, val = 128, 128
idx = pd.Index(range(n), dtype=""Int64"").union(pd.Index([val], dtype=""Int64""))
s = pd.Series(index=idx, data=range(n+1), dtype=""Int64"")
s.drop(0)

# But this FAILS:
n, val = 128, pd.NA
idx = pd.Index(range(n), dtype=""Int64"").union(pd.Index([val], dtype=""Int64""))
s = pd.Series(index=idx, data=range(n+1), dtype=""Int64"")
s.drop(0)  # ValueError: 'indices' contains values less than allowed (-128 < -1)
# Expected no error

```

**WORKAROUND.** to filter out elements, use a boolean mask/indexing instead of s.drop():
```python
s[~s.index.isin([0])]
```



### Issue Description

When `NA` is present in `Index` and the length of the Index exceeds 128, it behaves in a completely weird way.

This bug can be narrowed down to `IndexEngine.get_indexer()` or [`MaskedIndexEngine.get_indexer()`](https://github.com/pandas-dev/pandas/blob/58461fef2315d228d08f65f8a9430e9294d65b31/pandas/_libs/index.pyx#L1164-L1166), as these examples suggest:
```python
axis = pd.Index(range(250), dtype='Int64').union(pd.Index([pd.NA], dtype='Int64'))
new_axis = axis.drop(0)
axis.get_indexer(new_axis)[-5:] # array([246, 247, 248, 249,  -6])
# Expected array([246, 247, 248, 249,  250])

axis = pd.Index(range(254), dtype='Int64').union(pd.Index([pd.NA], dtype='Int64'))
new_axis = axis.drop(0)
axis.get_indexer(new_axis)[-5:] # array([250, 251, 252, 253,  -2])
# Expected  array([250, 251, 252, 253,  254])
```
These examples further suggest that the root cause of the bug is in how `NaN` is represented in and is interacting with the hash tables that `Index` uses for its `_engine`.


### Expected Behavior

See above

### Installed Versions

<details>

commit                : 76c7274985215c487248fa5640e12a9b32a06e8c
python                : 3.11.5.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.10.216-204.855.amzn2.x86_64
Version               : #1 SMP Sat May 4 16:53:27 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1067.g76c7274985
numpy                 : 1.26.4
pytz                  : 2023.3.post1
dateutil              : 2.8.2
setuptools            : 65.5.0
pip                   : 23.2.1
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pyarrow               : 15.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2023.4
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Missing-data']",2024-06-04 20:47:27,2025-03-07 00:57:58,2,closed
58918,DOC: Document Flashes White in Dark Mode,"### Pandas version checks

- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/)


### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html



### Documentation problem

When accessing the documentation for the first time in dark mode, the whole screen flashes white briefly.
To induce this even if you have already accessed the page clear all cookies and reload the page

### Suggested fix for documentation

Ideally the website should load in the correct mode first. I'm not sure how this would work.","['Docs', 'Needs Triage']",2024-06-04 16:29:56,2024-06-04 18:02:01,2,closed
58913,"BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+)","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd

X = np.random.default_rng().uniform(size=(10, 2)).astype(np.float32)
df = pd.DataFrame(X)
df_arr = df.to_numpy(dtype=np.float32)

np.shares_memory(X, df.values)
# numpy==1.26.4    , pandas==2.2.2           : True
# numpy==2.1.0.dev0, pandas==2.2.2           : True
# numpy==2.1.0.dev0, pandas==3.0.0.dev0+1067 : False

np.shares_memory(X, df_arr)
# numpy==1.26.4    , pandas==2.2.2           : True
# numpy==2.1.0.dev0, pandas==2.2.2           : True
# numpy==2.1.0.dev0, pandas==3.0.0.dev0+1067 : False
```


### Issue Description

Starting with `pandas==3.0.0`, it appears that `DataFrame(data)` creates a copy when `data` is a `numpy` array.

### Expected Behavior

I expected `df.values` and the result of `df.to_numpy()` (with `copy` argument omitted) to return the same `numpy` array that the `DataFrame` was created from.

I think that that has been the behavior of most combinations of `pandas` and `numpy` for at least the last 2 years. I think that because we've been running a test in LightGBM with similar code, to confirm that `lightgbm` isn't creating unnecessary copies in its `pandas` support since January 2022 (https://github.com/microsoft/LightGBM/pull/4927), and that test is now failing with `pandas>=3.0.0`.

Apologies in advance if this is intentional behavior. I did try to look through the `git` blame, issues, and PRs. Did not see anything in these possibly-related discussions:

* https://github.com/pandas-dev/pandas/issues/58243
* https://github.com/pandas-dev/pandas/issues/56022
* https://github.com/pandas-dev/pandas/issues/52823

### Installed Versions

Observed this in a `conda` environment on an M2 macbook, using Python 3.11.9

<details><summary>output of 'conda info' (click me)</summary>

```text
     active environment : lgb-dev
    active env location : /Users/jlamb/miniforge3/envs/lgb-dev
            shell level : 1
       user config file : /Users/jlamb/.condarc
 populated config files : /Users/jlamb/miniforge3/.condarc
          conda version : 24.3.0
    conda-build version : not installed
         python version : 3.12.3.final.0
                 solver : libmamba (default)
       virtual packages : __archspec=1=m2
                          __conda=24.3.0=0
                          __osx=14.4.1=0
                          __unix=0=0
       base environment : /Users/jlamb/miniforge3  (writable)
      conda av data dir : /Users/jlamb/miniforge3/etc/conda
  conda av metadata url : None
           channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64
                          https://conda.anaconda.org/conda-forge/noarch
          package cache : /Users/jlamb/miniforge3/pkgs
                          /Users/jlamb/.conda/pkgs
       envs directories : /Users/jlamb/miniforge3/envs
                          /Users/jlamb/.conda/envs
               platform : osx-arm64
             user-agent : conda/24.3.0 requests/2.31.0 CPython/3.12.3 Darwin/23.4.0 OSX/14.4.1 solver/libmamba conda-libmamba-solver/24.1.0 libmambapy/1.5.8
                UID:GID : 501:20
             netrc file : None
           offline mode : False
```

</details>

<details><summary>How I installed stable versions of `numpy`, `pandas`, and `pyarrow` (click me)</summary>

```shell
conda install \
    -c conda-forge \
    --yes \
        'numpy' \
        'pandas' \
        'pyarrow'
```

</details>

<details><summary>How I gradually replaced those versions with latest nightlies (click me)</summary>

`numpy` and `pyarrrow`:

```shell
python -m pip install \
    --extra-index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple \
    --prefer-binary \
    --pre \
    --upgrade \
        'numpy>=2.1.0.dev0'

python -m pip install \
    --extra-index-url https://pypi.fury.io/arrow-nightlies/ \
    --prefer-binary \
    --pre \
    --upgrade \
        'pyarrow>=17.0.0.dev227'
```

`pandas`:

```shell
python -m pip install \
    --extra-index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple \
    --prefer-binary \
    --pre \
    --upgrade \
        'pandas>=3.0.0.dev0'
```

</details>

<details><summary>output of 'pd.show_versions()' with all nightlies installed (click me)</summary>

```shell
python -c ""import pandas; pandas.show_versions()""
```

result:

```text
INSTALLED VERSIONS
------------------
commit                : 76c7274985215c487248fa5640e12a9b32a06e8c
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.4.0
Version               : Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : None
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1067.g76c7274985
numpy                 : 2.1.0.dev0+git20240531.da5a779
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : 8.2.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pyarrow               : 17.0.0.dev227
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Needs Discussion', 'Constructors', 'Copy / view semantics']",2024-06-04 04:15:40,2024-06-10 16:46:13,9,closed
58911,BUG: Error when repr-ing nested DataFrames,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6]})
df_outer = pd.DataFrame({""a"": [{""x"": df}]})
print(df_outer)
```


### Issue Description

The above code crashed with the stack trace:
<details><summary>Stack trace</summary>
<p>

```python
---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
Cell In[1], line 5
      3 df = pd.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6]})
      4 df_outer = pd.DataFrame({""a"": [{""x"": df}]})
----> 5 print(df_outer)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/core/frame.py:1214, in DataFrame.__repr__(self)
   1211     return buf.getvalue()
   1213 repr_params = fmt.get_dataframe_repr_params()
-> 1214 return self.to_string(**repr_params)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/util/_decorators.py:333, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)
    327 if len(args) > num_allow_args:
    328     warnings.warn(
    329         msg.format(arguments=_format_argument_list(allow_args)),
    330         FutureWarning,
    331         stacklevel=find_stack_level(),
    332     )
--> 333 return func(*args, **kwargs)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/core/frame.py:1394, in DataFrame.to_string(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)
   1375 with option_context(""display.max_colwidth"", max_colwidth):
   1376     formatter = fmt.DataFrameFormatter(
   1377         self,
   1378         columns=columns,
   (...)
   1392         decimal=decimal,
   1393     )
-> 1394     return fmt.DataFrameRenderer(formatter).to_string(
   1395         buf=buf,
   1396         encoding=encoding,
   1397         line_width=line_width,
   1398     )

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/format.py:962, in DataFrameRenderer.to_string(self, buf, encoding, line_width)
    959 from pandas.io.formats.string import StringFormatter
    961 string_formatter = StringFormatter(self.fmt, line_width=line_width)
--> 962 string = string_formatter.to_string()
    963 return save_to_buffer(string, buf=buf, encoding=encoding)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/string.py:29, in StringFormatter.to_string(self)
     28 def to_string(self) -> str:
---> 29     text = self._get_string_representation()
     30     if self.fmt.should_show_dimensions:
     31         text = f""{text}{self.fmt.dimensions_info}""

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/string.py:44, in StringFormatter._get_string_representation(self)
     41 if self.fmt.frame.empty:
     42     return self._empty_info_line
---> 44 strcols = self._get_strcols()
     46 if self.line_width is None:
     47     # no need to wrap around just print the whole frame
     48     return self.adj.adjoin(1, *strcols)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/string.py:35, in StringFormatter._get_strcols(self)
     34 def _get_strcols(self) -> list[list[str]]:
---> 35     strcols = self.fmt.get_strcols()
     36     if self.fmt.is_truncated:
     37         strcols = self._insert_dot_separators(strcols)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/format.py:476, in DataFrameFormatter.get_strcols(self)
    472 def get_strcols(self) -> list[list[str]]:
    473     """"""
    474     Render a DataFrame to a list of columns (as lists of strings).
    475     """"""
--> 476     strcols = self._get_strcols_without_index()
    478     if self.index:
    479         str_index = self._get_formatted_index(self.tr_frame)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/format.py:740, in DataFrameFormatter._get_strcols_without_index(self)
    736 cheader = str_columns[i]
    737 header_colwidth = max(
    738     int(self.col_space.get(c, 0)), *(self.adj.len(x) for x in cheader)
    739 )
--> 740 fmt_values = self.format_col(i)
    741 fmt_values = _make_fixed_width(
    742     fmt_values, self.justify, minimum=header_colwidth, adj=self.adj
    743 )
    745 max_len = max(*(self.adj.len(x) for x in fmt_values), header_colwidth)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/format.py:754, in DataFrameFormatter.format_col(self, i)
    752 frame = self.tr_frame
    753 formatter = self._get_formatter(i)
--> 754 return format_array(
    755     frame.iloc[:, i]._values,
    756     formatter,
    757     float_format=self.float_format,
    758     na_rep=self.na_rep,
    759     space=self.col_space.get(frame.columns[i]),
    760     decimal=self.decimal,
    761     leading_space=self.index,
    762 )

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/format.py:1161, in format_array(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting, fallback_formatter)
   1145     digits = get_option(""display.precision"")
   1147 fmt_obj = fmt_klass(
   1148     values,
   1149     digits=digits,
   (...)
   1158     fallback_formatter=fallback_formatter,
   1159 )
-> 1161 return fmt_obj.get_result()

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/format.py:1194, in _GenericArrayFormatter.get_result(self)
   1193 def get_result(self) -> list[str]:
-> 1194     fmt_values = self._format_strings()
   1195     return _make_fixed_width(fmt_values, self.justify)

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/format.py:1259, in _GenericArrayFormatter._format_strings(self)
   1257 for i, v in enumerate(vals):
   1258     if (not is_float_type[i] or self.formatter is not None) and leading_space:
-> 1259         fmt_values.append(f"" {_format(v)}"")
   1260     elif is_float_type[i]:
   1261         fmt_values.append(float_format(v))

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/format.py:1239, in _GenericArrayFormatter._format_strings.<locals>._format(x)
   1236     return repr(x)
   1237 else:
   1238     # object dtype
-> 1239     return str(formatter(x))

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/printing.py:219, in pprint_thing(thing, _nest_lvl, escape_chars, default_escapes, quote_strings, max_seq_items)
    215     return str(thing)
    216 elif isinstance(thing, dict) and _nest_lvl < get_option(
    217     ""display.pprint_nest_depth""
    218 ):
--> 219     result = _pprint_dict(
    220         thing, _nest_lvl, quote_strings=True, max_seq_items=max_seq_items
    221     )
    222 elif is_sequence(thing) and _nest_lvl < get_option(""display.pprint_nest_depth""):
    223     result = _pprint_seq(
    224         thing,
    225         _nest_lvl,
   (...)
    228         max_seq_items=max_seq_items,
    229     )

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/printing.py:155, in _pprint_dict(seq, _nest_lvl, max_seq_items, **kwds)
    149     nitems = max_seq_items or get_option(""max_seq_items"") or len(seq)
    151 for k, v in list(seq.items())[:nitems]:
    152     pairs.append(
    153         pfmt.format(
    154             key=pprint_thing(k, _nest_lvl + 1, max_seq_items=max_seq_items, **kwds),
--> 155             val=pprint_thing(v, _nest_lvl + 1, max_seq_items=max_seq_items, **kwds),
    156         )
    157     )
    159 if nitems < len(seq):
    160     return fmt.format(things="", "".join(pairs) + "", ..."")

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/printing.py:223, in pprint_thing(thing, _nest_lvl, escape_chars, default_escapes, quote_strings, max_seq_items)
    219     result = _pprint_dict(
    220         thing, _nest_lvl, quote_strings=True, max_seq_items=max_seq_items
    221     )
    222 elif is_sequence(thing) and _nest_lvl < get_option(""display.pprint_nest_depth""):
--> 223     result = _pprint_seq(
    224         thing,
    225         _nest_lvl,
    226         escape_chars=escape_chars,
    227         quote_strings=quote_strings,
    228         max_seq_items=max_seq_items,
    229     )
    230 elif isinstance(thing, str) and quote_strings:
    231     result = f""'{as_escaped_string(thing)}'""

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/printing.py:120, in _pprint_seq(seq, _nest_lvl, max_seq_items, **kwds)
    118 s = iter(seq)
    119 # handle sets, no slicing
--> 120 r = [
    121     pprint_thing(next(s), _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)
    122     for i in range(min(nitems, len(seq)))
    123 ]
    124 body = "", "".join(r)
    126 if nitems < len(seq):

File ~/Library/Python/3.10/lib/python/site-packages/pandas/io/formats/printing.py:121, in <listcomp>(.0)
    118 s = iter(seq)
    119 # handle sets, no slicing
    120 r = [
--> 121     pprint_thing(next(s), _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)
    122     for i in range(min(nitems, len(seq)))
    123 ]
    124 body = "", "".join(r)
    126 if nitems < len(seq):

StopIteration:
```

</p>
</details> 

My interpretation is that this happens because pandas treats the nested DataFrame as a normal sequence and tries to iterate on it, but for DataFrames `len(df) != len(list(df))` because the former is #rows and the latter is #columns.

This issue is essentially the same as #49195, but that issue was incorrectly triaged as been an issue with an external library.

### Expected Behavior

Any reasonable repr output. Should not crash.

### Installed Versions

<details>

```

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.8.final.0
python-bits           : 64
OS                    : Darwin
OS-release            : 23.5.0
Version               : Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000
machine               : arm64
processor             : arm
byteorder             : little
LC_ALL                : en_US.UTF-8
LANG                  : en_US.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 63.2.0
pip                   : 23.2.1
Cython                : None
pytest                : 7.4.2
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.14.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : 0.9.0
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
```

</details>
","['Bug', 'Needs Triage']",2024-06-03 23:09:07,2024-06-04 02:34:34,3,closed
58909,BUG: `<Framelike>.__contains__(<unhashable>)` errors,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
vals = [{1:2}, {""a"":""b""}]
{1:2} in vals # works, as expected
{1:2} in pd.Series(vals). # TypeError
```


### Issue Description

Related: https://github.com/pandas-dev/pandas/issues/36285

Series and dataframes should support `__contains__` for unhashable needles. It makes sense to disallow using unhashable types as keys in set-like and map-like collections, because the ""identity"" of the object can change between insertion time and query time. However, framelikes are more like python lists, which don't have a hash-map-esque behavior.

Am I missing something here that would cause poorly defined behavior?

### Expected Behavior

the existing fast hash-based implementation should work for hashable types, but we should have a O(n) fallback implementation for unhashable types.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit           : e8093ba372f9adfe79439d90fe74b0b5b6dea9d6
python           : 3.8.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.15.0-1064-azure
Version          : #73~20.04.1-Ubuntu SMP Mon May 6 09:43:44 UTC 2024
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.4.3
numpy            : 1.23.1
pytz             : 2022.1
dateutil         : 2.8.2
setuptools       : 45.2.0
pip              : 20.0.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
brotli           : None
fastparquet      : None
fsspec           : None
gcsfs            : None
markupsafe       : None
matplotlib       : 3.5.2
numba            : None
numexpr          : None
odfpy            : None
openpyxl         : 3.1.2
pandas_gbq       : None
pyarrow          : None
pyreadstat       : None
pyxlsb           : None
s3fs             : None
scipy            : 1.9.0
snappy           : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
zstandard        : None

</details>
","['Bug', 'Index']",2024-06-03 20:53:50,2025-04-16 14:10:06,7,closed
58907,BUG: astype() unexpected mutation of values,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
the whole df usually has got >10 rows x 49 columns

`df = pd.DataFrame.from_dict(dictionary)`
`df = df.astype(DF_DTYPES)` where this particular column is defined `{'is_inco_oper': int}`
```


### Issue Description

original df is `int64` converted to `int32`, and values are mutated in row 7-9

```
0     170914000
1      63275000
2     248049000
3     746273000
4    1067817000
5    1022510000
6    1218091000
7    3415195000
8    3669382000
9    2531232000
Name: is_inco_oper, dtype: int64
0     170914000
1      63275000
2     248049000
3     746273000
4    1067817000
5    1022510000
6    1218091000
7    -879772296
8    -625585296
9   -1763735296
Name: is_inco_oper, dtype: int32
```

### Expected Behavior

keep the original values

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_Switzerland.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 70.0.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : 3.1.3
IPython               : 8.24.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.1
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Closing Candidate', 'Upstream issue', 'Astype']",2024-06-03 20:19:04,2024-06-15 12:36:59,5,closed
58906,BUG:  While  doing read_csv parsing it wrongly {beginner},"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
data = [
   [""Indoor"", ""dining-room-set"", ""https://somewebsite.net/collections/dini..."", ""['https://somewebsite.net/cdn/shop/produ..."", ""Presenting a sophisticated and functional addi..."" ],
   [""Indoor"", ""coffee-end-table"", ""https://somewebsite.net/collections/coff...""	, ""['https://somewebsite.net/cdn/shop/produ..."", ""This contemporary mid century modern nightstan..."" ]
]

pd.DataFrame(data).to_csv('test.csv',index=False,lineterminator=""\n"")

pd.read_csv(""test.csv"")   # << reading it properly but not right format
```


### Issue Description


In Reproducible Example I shown some code.

I did bit of research from my side as well.

import ast
df['3rdcolumn'] = df['3rdcolumn'].apply(ast.literal_eval(x))

everytime to get proper dtype i expected

### Expected Behavior

It should read list of list or dict as naively. 

### Installed Versions

<details>



INSTALLED VERSIONS
------------------
commit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : AMD64 Family 25 Model 68 Stepping 1, AuthenticAMD
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_India.1252

pandas                : 2.2.1

</details>
","['Bug', 'Needs Triage']",2024-06-03 19:11:30,2024-06-22 11:44:17,2,closed
58901,"BUG: potential unexpected side-effects of ""copy-on-write"" in case of multiple imports of pandas","### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Not necessary
```


### Issue Description

If I understand correctly https://pandas.pydata.org/docs/user_guide/copy_on_write.html#copy-on-write-enabling, setting the ""copy-on-write"" mode through `pd.set_option(""mode.copy_on_write"", True)` will activate the mode globally. Am I correct to understand that:
1. If pandas is imported elsewhere, e.g. in another library on which my library depends, that library will also be affected by this setting, with potentially unpredictable effect on that library outputs?
2. In the same spirit, if that library also sets this mode, the mode actually used in my code will depend on the import order?

### Expected Behavior

If I am correct, would there be a way to at least ensure this mode is not overwritten if it has been explicitly set once? So that if pandas is imported twice with conflicting mode setting, an error is raised

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.13.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : French_Switzerland.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : None
pip                   : None
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : 8.18.1
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : 1.3.8
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : 0.59.1
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None


</details>
","['Bug', 'Needs Triage']",2024-06-03 14:13:36,2024-06-03 17:58:37,2,closed
58872,BUG: Pandas Styler HTML not rendering properly,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import numpy as np
import pandas as pd


matrix = np.random.randint(10000, size=(40, 30))

df = pd.DataFrame(matrix)


style = df.style.highlight_between(left=8000, inclusive='neither')
style = style.hide(axis='index')
style = style.format(precision=2)
html_str = style.to_html()

or...

style = df.style.background_gradient(cmap = cm,axis=None)
html_str = style.to_html()


fix...

import textwrap
html_str = textwrap.fill(html_str, width=200)
```


### Issue Description

When using pandas styler objects the HTML they render has lines that extend beyond 1000 characters causing ids to truncate unexpectedly and break formatting styles. This is fine for most browsers but can cause problems for emails - there should be an option to control this from within the styler framework.

I use pandas 1,4 as my daily driver, but this behavior occurs on 2.2 as well.

## Here is an example of the truncation at line column 1000 of an email message.

```html

#T_4858f_row14_col16, #T!
f_row18_col21, #T_4858f_!

```

## Here is the trucation at line column 1

```html
_4858f_row14_col17,
 row18_col22, #T_48

```

As you can see the ids are being cut off, thus breaking the color background formatting.

## Fix 

The temporary fix is using the textwrap module to trucate the lines properly at a lower character threshold - 200 and 250 characters seemed to work for me. 

### Expected Behavior

I would expect every line of the render  html to not extend beyond a certain character limit and truncate each line effectively without breaking mid word.

### Installed Versions

<details>

1.4.4

</details>
","['Bug', 'Needs Discussion', 'Styler', 'Closing Candidate']",2024-05-31 14:27:50,2024-06-04 05:56:05,4,closed
58870,ENH: skiprows after header in read_csv,"### Feature Type

- [X] Adding new functionality to pandas

- [ ] Changing existing functionality in pandas

- [ ] Removing existing functionality in pandas


### Problem Description

There isn't a way to tell `read_csv` to `skiprows` **after** the specified `header` row.

### Feature Description

To avoid breaking existing logic, I think a new argument would be required, perhaps `skiprows_after`?

So one could, parse, for example:

```
ignore,me
foo,bar
ignore,me
123,1
456,2
```

with `read_csv(f, header=1, skiprows_after=1)` and get:

```
   foo  bar
0  123    1
1  456    2
```

### Alternative Solutions

Workaround:

```py
header = read_csv(f, skiprows=1, nrows=1)
names = header.columns.to_list()
read_csv(f, skiprows=4, names=names)
```

### Additional Context

_No response_","['Enhancement', 'IO CSV', 'Needs Discussion']",2024-05-30 22:28:29,2024-09-04 17:02:00,2,closed
58868,BUG: Rolling .apply() with method='table' ignores min_periods,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]], columns=['A', 'B'])
print(df)

# prints this:
#    A   B
# 0  1   2
# 1  3   4
# 2  5   6
# 3  7   8
# 4  9  10

df.rolling(3, min_periods=3).apply(lambda x: bool(print(x, '\n')))

# prints this, so as expected, only batches of 3 are being evaluated
# 0    1.0
# 1    3.0
# 2    5.0
# dtype: float64 
# 
# 1    3.0
# 2    5.0
# 3    7.0
# dtype: float64 
# 
# 2    5.0
# 3    7.0
# 4    9.0
# dtype: float64 
# 
# 0    2.0
# 1    4.0
# 2    6.0
# dtype: float64 
# 
# 1    4.0
# 2    6.0
# 3    8.0
# dtype: float64 
# 
# 2     6.0
# 3     8.0
# 4    10.0
# dtype: float64 
# 
# Out[33]: 
#      A    B
# 0  NaN  NaN
# 1  NaN  NaN
# 2  0.0  0.0
# 3  0.0  0.0
# 4  0.0  0.0

df.rolling(3, method='table', min_periods=3).apply(lambda x: bool(print(x, '\n')), raw=True, engine='numba')

# prints this, where we see that the first two batches of sizes 1 and 2 are also being evaluated
# [[1. 2.]]   <-- batch of size 1, illegal
# 
# [[1. 2.]    <-- batch of size 2, illegal
#  [3. 4.]] 
# 
# [[1. 2.]
#  [3. 4.]
#  [5. 6.]] 
# 
# [[3. 4.]
#  [5. 6.]
#  [7. 8.]] 
# 
# [[ 5.  6.]
#  [ 7.  8.]
#  [ 9. 10.]] 
# Out[34]: 
#      A    B
# 0  NaN  NaN
# 1  NaN  NaN
# 2  0.0  0.0
# 3  0.0  0.0
# 4  0.0  0.0
```


### Issue Description

See example above, which violates the contract imposed by the `min_periods` parameter.

### Expected Behavior

The `min_periods` parameter should be respected.

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.12.3.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 11
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : German_Switzerland.utf8
pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0
setuptools            : 69.5.1
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.2.2
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : 8.24.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.8.4
numba                 : 0.59.1
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : 16.0.0
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'Apply', 'Window', 'numba']",2024-05-30 18:04:01,2024-06-03 18:18:14,2,closed
58865,Test Suite: Expand Test Coverage for `script\tests\test_inconsistent_namespace.py,"Add more edge cases to script\tests\test_inconsistent_namespace.py

Write new tests for empty files, files with only comments, files without pandas references, and files with multiple correct pandas imports.",[],2024-05-29 22:03:06,2024-06-07 17:36:53,2,closed
58860,BUG: pandas.errors.LossySetitemError when backtesting Freqtrade,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
2024-05-29 11:45:19,821 - freqtrade - ERROR - Fatal exception!
Traceback (most recent call last):
  File ""/home/ftuser/.local/lib/python3.12/site-packages/pandas/core/internals/blocks.py"", line 1855, in shift
    casted = np_can_hold_element(
             ^^^^^^^^^^^^^^^^^^^^
  File ""/home/ftuser/.local/lib/python3.12/site-packages/pandas/core/dtypes/cast.py"", line 1936, in np_can_hold_element
    raise LossySetitemError
pandas.errors.LossySetitemError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/freqtrade/freqtrade/main.py"", line 42, in main
    return_code = args['func'](args)
                  ^^^^^^^^^^^^^^^^^^
  File ""/freqtrade/freqtrade/commands/optimize_commands.py"", line 58, in start_backtesting
    backtesting.start()
  File ""/freqtrade/freqtrade/optimize/backtesting.py"", line 1410, in start
    min_date, max_date = self.backtest_one_strategy(strat, data, timerange)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/freqtrade/freqtrade/optimize/backtesting.py"", line 1344, in backtest_one_strategy
    results = self.backtest(
              ^^^^^^^^^^^^^^
  File ""/freqtrade/freqtrade/optimize/backtesting.py"", line 1222, in backtest
    data: Dict = self._get_ohlcv_as_lists(processed)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/freqtrade/freqtrade/optimize/backtesting.py"", line 401, in _get_ohlcv_as_lists
    [nan], [0 if not tag_col else None]).shift(1)
                                         ^^^^^^^^
  File ""/home/ftuser/.local/lib/python3.12/site-packages/pandas/core/generic.py"", line 11225, in shift
    new_data = self._mgr.shift(periods=periods, fill_value=fill_value)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ftuser/.local/lib/python3.12/site-packages/pandas/core/internals/base.py"", line 312, in shift
    return self.apply_with_block(""shift"", periods=periods, fill_value=fill_value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ftuser/.local/lib/python3.12/site-packages/pandas/core/internals/managers.py"", line 363, in apply
    applied = getattr(b, f)(**kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ftuser/.local/lib/python3.12/site-packages/pandas/core/internals/blocks.py"", line 1859, in shift
    nb = self.coerce_to_target_dtype(fill_value)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ftuser/.local/lib/python3.12/site-packages/pandas/core/internals/blocks.py"", line 490, in coerce_to_target_dtype
    raise AssertionError(
AssertionError: Something has gone wrong, please report a bug at https://github.com/pandas-dev/pandas/issues
```


### Issue Description

The issue occurs when running a Backtest with the Freqtrade Trading Bot, attempting to backtest a short futures strategy.

Resources:
https://github.com/ssssi/freqtrade_strs/blob/82baabd2099b5f592458678554ca7162148f1d7a/binance/BinHV27/short/BinHV27_short.py

https://github.com/ssssi/freqtrade_strs/blob/82baabd2099b5f592458678554ca7162148f1d7a/binance/BinHV27/short/BinHV27_short.json

https://github.com/freqtrade/freqtrade

### Expected Behavior

To return backtesting result, as the bot does for spot trading backtests.

### Installed Versions

sudo -H pip3 install pandas
[sudo] password for jpeetz:
Collecting pandas
  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)
     |████████████████████████████████| 12.4 MB 1.1 MB/s
Collecting pytz>=2020.1
  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)
     |████████████████████████████████| 505 kB 33.5 MB/s
Collecting python-dateutil>=2.8.2
  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
     |████████████████████████████████| 229 kB 22.9 MB/s
Collecting numpy>=1.20.3; python_version < ""3.10""
  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)
     |████████████████████████████████| 17.3 MB 36.2 MB/s
Collecting tzdata>=2022.1
  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)
     |████████████████████████████████| 345 kB 30.6 MB/s
Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.14.0)
Installing collected packages: pytz, python-dateutil, numpy, tzdata, pandas
Successfully installed numpy-1.24.4 pandas-2.0.3 python-dateutil-2.9.0.post0 pytz-2024.1 tzdata-2024.1","['Bug', 'Needs Info']",2024-05-29 13:47:25,2024-08-27 16:58:11,3,closed
58859,BUG: to_datetime behaves differently depending of the format of the string provided,"### Pandas version checks

- [ ] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Python 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> pd.to_datetime(""2016-03-11"", dayfirst=True)
Timestamp('2016-11-03 00:00:00')
>>> pd.to_datetime(""20160311"", dayfirst=True)
Timestamp('2016-03-11 00:00:00')
>>>
```


### Issue Description

Dates of the format ""YYYYDDMM"" are not converted correctly by to_datetime() when dayfirst=True. to_datetime() still reads the date characters as the months

### Expected Behavior

Python 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> pd.to_datetime(""2016-03-11"", dayfirst=True)
Timestamp('2016-11-03 00:00:00')
>>> pd.to_datetime(""20160311"", dayfirst=True)
Timestamp('2016-11-03 00:00:00')

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673
python              : 3.11.7.final.0
python-bits         : 64
OS                  : Linux
OS-release          : 3.10.0-1160.114.2.el7.x86_64
Version             : #1 SMP Sun Mar 3 08:18:39 EST 2024
machine             : x86_64
processor           : x86_64
byteorder           : little
LC_ALL              : None
LANG                : en_US.UTF-8
LOCALE              : en_US.UTF-8

pandas              : 2.1.4
numpy               : 1.26.3
pytz                : 2023.3.post1
dateutil            : 2.8.2
setuptools          : 69.0.3
pip                 : 23.3.2
Cython              : 3.0.7
pytest              : 7.4.4
hypothesis          : 6.94.0
sphinx              : 7.2.6
blosc               : None
feather             : None
xlsxwriter          : 3.1.9
lxml.etree          : 5.1.0
html5lib            : 1.1
pymysql             : None
psycopg2            : None
jinja2              : 3.1.3
IPython             : 8.20.0
pandas_datareader   : None
bs4                 : 4.12.2
bottleneck          : 1.3.7
dataframe-api-compat: None
fastparquet         : 2023.10.1
fsspec              : 2023.12.2
gcsfs               : None
matplotlib          : 3.8.2
numba               : 0.58.1
numexpr             : 2.8.8
odfpy               : None
openpyxl            : 3.1.2
pandas_gbq          : None
pyarrow             : 16.0.0
pyreadstat          : None
pyxlsb              : None
s3fs                : None
scipy               : 1.11.4
sqlalchemy          : 2.0.25
tables              : 3.9.2
tabulate            : 0.9.0
xarray              : 2023.7.0
xlrd                : 2.0.1
zstandard           : 0.19.0
tzdata              : 2023.4
qtpy                : 2.4.1
pyqt5               : None

</details>
","['Bug', 'Datetime']",2024-05-29 10:53:10,2024-07-17 17:09:06,3,closed
58858,BUG: Inconsistent types for groupby group names ,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

df = pd.DataFrame({'x': [10, 20, 30], 'y': ['a', 'b', 'c']})

# group by atomic - names and keys are atomic
name, _ = next(iter(df.groupby('x')))
name    # `10` as expected
df.groupby('x').groups  # `{10: [0], 20: [1], 30: [2]}` as expected

# group by list of 2 - names and keys are 2-tuples
name, _ = next(iter(df.groupby(['x', 'y'])))
name  # `(10, 'a')` as expected
df.groupby(['x', 'y']).groups  # `{(10, 'a'): [0], (20, 'b'): [1], (30, 'c'): [2]}` as expected

# group by list of 1 - names are 1-tuples but keys are atomic!?
name, _ = next(iter(df.groupby(['x'])))
name  # `(10,)` as expected
df.groupby(['x']).groups  # `{10: [0], 20: [1], 30: [2]}` UNEXPECTED
```


### Issue Description

In pandas 1.5.x grouping by a single element yielded scalar group names, grouping by a list of more than one element yielded tuples names but grouping by a length-1 list yielded scalar group names again which was considered inconsistent (and I agree).

This was mostly resolved in 2.0 (see #42795) and now when [iterating through groups](https://pandas.pydata.org/docs/user_guide/groupby.html#iterating-through-groups) the group names are 1-tuples when grouped with a length-1 list. However the [groups attribute](https://pandas.pydata.org/docs/user_guide/groupby.html#groupby-object-attributes) still returns scalars instead of 1-tuples which I consider to be a bug or at least requiring enhancement.  

### Expected Behavior

I expect group names / keys to be 1-tuples when grouping with a length-1 list: group by scalar get scalar names, group by list get tuple names. (I would also propose that grouping by an empty list should return a single group of the whole dataframe, with an empty tuple as the key, but thats out of scope for this issue!)

I expect the same group name / key behaviour whether  [iterating through groups](https://pandas.pydata.org/docs/user_guide/groupby.html#iterating-through-groups) or accessing groups via the [groups attribute](https://pandas.pydata.org/docs/user_guide/groupby.html#groupby-object-attributes). 

### Installed Versions

<details>
<summary>latest release (2.2.2)</summary>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-1064-azure
Version               : #73~20.04.1-Ubuntu SMP Mon May 6 09:43:44 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pandas_gbq            : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>

<details>

<summary>dev install (3.0.0.dev0+1024.gb16233155)</summary>

INSTALLED VERSIONS
------------------
commit                : b162331554d7c7f6fd46ddde1ff3908f2dc8bcce
python                : 3.10.14.final.0
python-bits           : 64
OS                    : Linux
OS-release            : 5.15.0-1064-azure
Version               : #73~20.04.1-Ubuntu SMP Mon May 6 09:43:44 UTC 2024
machine               : x86_64
processor             : x86_64
byteorder             : little
LC_ALL                : None
LANG                  : C.UTF-8
LOCALE                : en_US.UTF-8

pandas                : 3.0.0.dev0+1024.gb16233155
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 65.5.0
pip                   : 24.0
Cython                : None
pytest                : None
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : None
html5lib              : None
pymysql               : None
psycopg2              : None
jinja2                : None
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : None
bottleneck            : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : None
numba                 : None
numexpr               : None
odfpy                 : None
openpyxl              : None
pyarrow               : None
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : None
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : None
zstandard             : None
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Groupby', 'Deprecate', 'API - Consistency']",2024-05-29 09:56:41,2024-07-23 20:54:33,2,closed
58851,BUG: matplotlib 3.9.0 has issue with pandas when using `subplots=True` on `df.plot.scatter()`,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd
df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],
                   [6.4, 3.2, 1], [5.9, 3.0, 2]],
                  columns=['length', 'width', 'species'])
df.plot.scatter(x=""length"", y=""width"", subplots=True)
```


### Issue Description

With matplotlib 3.9.0 (released 5/15/24), you get:
```text
\lib\site-packages\pandas\plotting\_matplotlib\core.py:896: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
  ax.legend(loc=""best"")
```

### Expected Behavior

No warning.  It doesn't appear with previous versions of `matplotlib`

### Installed Versions

<details>

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.9.16.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.19045
machine               : AMD64
processor             : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : English_United States.1252

pandas                : 2.2.2
numpy                 : 1.26.4
pytz                  : 2024.1
dateutil              : 2.9.0.post0
setuptools            : 68.2.2
pip                   : 22.3.1
Cython                : None
pytest                : 8.2.0
hypothesis            : None
sphinx                : None
blosc                 : None
feather               : None
xlsxwriter            : 3.2.0
lxml.etree            : 5.2.2
html5lib              : 1.1
pymysql               : None
psycopg2              : None
jinja2                : 3.1.4
IPython               : None
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.3
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : None
gcsfs                 : None
matplotlib            : 3.9.0
numba                 : None
numexpr               : 2.10.0
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 16.1.0
pyreadstat            : 1.2.7
python-calamine       : None
pyxlsb                : 1.0.10
s3fs                  : None
scipy                 : 1.13.0
sqlalchemy            : 2.0.30
tables                : 3.9.2
tabulate              : 0.9.0
xarray                : 2024.5.0
xlrd                  : 2.0.1
zstandard             : 0.22.0
tzdata                : 2024.1
qtpy                  : None
pyqt5                 : None

</details>
","['Bug', 'Needs Triage']",2024-05-28 21:38:51,2024-07-03 16:45:26,1,closed
58849,BUG: to_excel() cuts off list of values when creating Excel file,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
import pandas as pd

a = [-0.004518906585872173, -0.004478906746953726, -0.004878906533122063, -0.005398906767368317, -0.005588906817138195, -0.0051789069548249245, -0.004498906899243593, -0.003698906861245632, -0.0029789067339152098, -0.002688906854018569, -0.0024089068174362183, -0.0017389067215844989, -0.0008989067864604294, -0.0004789067606907338, -0.00043890674714930356, -1.890675957838539e-05, -1.890675957838539e-05, -0.0006889067590236664, -0.0012089067604392767, -0.0018789067398756742, -0.002058906713500619, -0.00273890676908195, -0.004358906764537096, -0.002638906706124544, -0.00025890677352435887, -0.0022389066871255636, -0.0024189066607505083, 0.002521093236282468, 0.005981093272566795, 0.005101093091070652, 0.0032310932874679565, 0.002561093308031559, 0.0015310932649299502, 0.0006310932221822441, 0.0034510933328419924, 0.006621093023568392, 0.004561093170195818, 0.0009310932364314795, 0.0010410932591184974, 0.0029210932552814484, 0.0025310933124274015, 0.0017410932341590524, 0.003841093275696039, 0.005901093129068613, 0.006051093339920044, 0.007461093366146088, 0.009891092777252197, 0.00959109328687191, 0.007211093325167894, 0.007781093008816242, 0.01109109353274107, 0.011471093632280827, 0.008611093275249004, 0.00681109307333827, 0.006511093117296696, 0.005071093328297138, 0.0031810931395739317, 0.0027510931249707937, 0.0015210931887850165, -0.0015189067926257849, -0.0029889068100601435, -0.0014989067567512393, 3.109324097749777e-05, -0.0005389067810028791, -0.0018289067083969712, -0.0019889066461473703, -0.001888906816020608, -0.0024589067324995995, -0.0021789066959172487, -0.0012089067604392767, -0.0018089067889377475, -0.0024989068042486906, -0.0013189067831262946, -0.0008089067414402962, -0.002758906688541174, -0.005638906732201576, -0.007608906831592321, -0.008078906685113907, -0.007858906872570515, -0.007538906764239073, -0.0068589067086577415, -0.00627890694886446, -0.00651890691369772, -0.006238906644284725, -0.004568906966596842, -0.002818906679749489, -0.0015189067926257849, -0.00036890676710754633, 0.000311093230266124, 0.00042109325295314193, 0.0009410932543687522, 0.002351093338802457, 0.002461093245074153, 0.00034109322587028146, -0.0019289067713543773, -0.0036489067133516073, -0.0043189069256186485, -0.003248906694352627, -0.0023489068262279034, -0.0023489068262279034, -0.0023389067500829697, -0.0023089067544788122, -0.001678906730376184, -0.0002689067623578012, 0.0011310932459309697, 0.0023810933344066143, 0.00336109334602952, 0.0033310933504253626, 0.0023210933431982994, 0.001231093192473054, -0.00021890675998292863, -0.0018989067757502198, -0.002328906673938036, -0.0022289068438112736, -0.002748906845226884, -0.003908906597644091, -0.006138906814157963, -0.008458906784653664, -0.009408907033503056, -0.009288907051086426, -0.008368906565010548, -0.006698906887322664, -0.0054589067585766315, -0.005058906972408295, -0.003698906861245632, -0.0006589067634195089, 0.0014510932378470898, 0.001711093238554895, 0.0020110933110117912, 0.0015710932202637196, -0.0007889067637734115, -0.0024189066607505083, -0.0019689067266881466, -0.0022389066871255636, -0.004238906782120466, -0.005308906547725201, -0.004798906855285168, -0.005308906547725201, -0.0064889066852629185, -0.006068906746804714, -0.0046589067205786705, -0.0029289068188518286, -0.0015389067120850086, -0.0001589067542226985, 0.0022810932714492083, 0.003931093029677868, 0.005091093014925718, 0.007461093366146088, 0.009111093357205391, 0.008911093696951866, 0.007621093187481165, 0.00703109335154295, 0.00780109316110611, 0.007791093084961176, 0.0076010930351912975, 0.008151093497872353, 0.007421093061566353, 0.006191093008965254, 0.006191093008965254, 0.006831093225628138, 0.007721093017607927, 0.008831093087792397, 0.010201092809438705, 0.01126109343022108, 0.010531092993915081, 0.009041093289852142, 0.008491093292832375, 0.007871093228459358, 0.00669109309092164, 0.005911093205213547, 0.005011093337088823, 0.002991093322634697, 0.0005910932668484747, -0.0008789067505858839, -0.0014989067567512393, -0.002318906830623746, -0.003658906789496541, -0.005028906743973494, -0.00627890694886446, -0.007308906875550747, -0.007258906960487366, -0.006088906899094582, -0.004988906905055046, -0.004588906653225422, -0.004628906957805157, -0.004448906984180212, -0.0042489068582654, -0.004188906867057085, -0.003978906664997339, -0.00414890656247735, -0.004328906536102295, -0.004568906966596842, -0.005388906691223383, -0.0057189068756997585, -0.005898906849324703, -0.006268906872719526, -0.005508906673640013, -0.004188906867057085, -0.0033589068334549665, -0.0028789066709578037, -0.002378906821832061, -0.001568906707689166, -0.00037890675594098866, 0.0010510932188481092, 0.001761093270033598, 0.0014010932063683867, 0.0006210932624526322, -0.0002889067691285163, -0.0006789067410863936, -0.0008389067370444536, -0.0015389067120850086, -0.0020789068657904863, -0.0023989067412912846, -0.002628906862810254, -0.0024489066563546658, -0.002528906799852848, -0.0025189067237079144, -0.0019489068072289228, -0.001298906747251749, -0.0005789067363366485, -0.0004089067515451461, -0.0007489067502319813, -0.000548906740732491, -0.0004889067495241761, -0.0010589067824184895, -0.001308906706981361, -0.001348906778730452, -0.0021889067720621824, -0.0038489068392664194, -0.005588906817138195, -0.006888906937092543, -0.007588906679302454, -0.007668906822800636, -0.007228906732052565, -0.006758906878530979, -0.0065389066003263, -0.0064889066852629185, -0.006378906778991222, -0.005488906987011433, -0.0041189067997038364, -0.002758906688541174, -0.0011289067333564162, -0.00012890675861854106, -0.0002689067623578012, -0.0007989067817106843, -0.0015289067523553967, -0.0016389067750424147, -0.0010989067377522588, -0.000708906736690551, -0.00016890675760805607, -0.0006889067590236664, -0.002478906651958823, -0.003658906789496541, -0.0037489067763090134, -0.002528906799852848, -0.0008389067370444536, 0.000391093228245154, 0.0016410932876169682, 0.002251093275845051, 0.0018910932121798396, 0.0020510931499302387, 0.0036010933108627796, 0.0058310930617153645, 0.007371093146502972, 0.008421093225479126, 0.009611093439161777, 0.009861093014478683, 0.009721092879772186, 0.011161093600094318, 0.013461093418300152, 0.014711093157529831, 0.01454109326004982, 0.0135310934856534, 0.012531093321740627, 0.012291093356907368, 0.012581093236804008, 0.013131093233823776, 0.013681093230843544, 0.012651093304157257, 0.010821092873811722, 0.009841092862188816, 0.009311093017458916, 0.00894109345972538, 0.008081093430519104, 0.006831093225628138, 0.005441093351691961, 0.0033510932698845863, 0.0009910932276397943, -0.0011489067692309618, -0.003018906805664301, -0.003908906597644091, -0.0037989066913723946, -0.004138906951993704, -0.0051789069548249245, -0.007218906655907631, -0.009948906488716602, -0.010998906567692757, -0.010308906435966492, -0.008808907121419907, -0.007468906696885824, -0.00801890715956688, -0.00907890684902668, -0.009228906594216824, -0.00948890671133995, -0.009838907048106194, -0.010118906386196613, -0.010778906755149364, -0.011298906989395618, -0.010948906652629375, -0.009748906828463078, -0.008518906310200691, -0.007638906594365835, -0.00704890675842762, -0.006718906573951244, -0.006088906899094582, -0.005148906726390123, -0.004578906577080488, -0.003958906978368759, -0.0029989066533744335, -0.0020889067091047764, -0.001078906818293035, -0.0001589067542226985, -0.0003289067535661161, -0.0015989067032933235, -0.0022789067588746548, -0.0019689067266881466, -0.001348906778730452, -0.00010890675912378356, 0.0007710932404734194, 0.0007010932313278317, 0.0011110932100564241, 0.002191093284636736, 0.00337109318934381, 0.00425109313800931, 0.004201093222945929, 0.003271093126386404, 0.00209109322167933, 0.001321093295700848, 0.0010810932144522667, 0.0009310932364314795, 0.00033109323703683913, -0.0006389067857526243, -0.002058906713500619, -0.00361890671774745, -0.004288906697183847, -0.004378906916826963, -0.004528906662017107, -0.004608906805515289, -0.00436890684068203, -0.0038389067631214857, -0.0033789067529141903, -0.0029489067383110523, -0.0025089066475629807, -0.002038906794041395, -0.001728906761854887, -0.0016389067750424147, -0.0012889067875221372, -0.0007489067502319813, -0.00031890676473267376, 0.00015109323430806398, -0.0006389067857526243, -0.0027289066929370165, -0.002258906839415431, -0.0009289067820645869, -0.004928906913846731, -0.010918906889855862, -0.01199890673160553, -0.009468906559050083, -0.005928906612098217, -0.0016089067794382572, 0.0009110932587645948, -0.00016890675760805607, -0.0030089067295193672, -0.004468906670808792, -0.0032589067704975605, -0.0008789067505858839, 0.0018410932971164584, 0.005591093096882105, 0.009901093319058418, 0.012841093353927135, 0.013341093435883522, 0.013221093453466892, 0.013421093113720417, 0.013111093081533909, 0.013911093585193157, 0.01616109348833561, 0.016911093145608902, 0.01478109322488308, 0.012061093002557755, 0.01162109337747097, 0.012531093321740627, 0.012721093371510506, 0.012401092797517776, 0.011641093529760838, 0.009851093403995037, 0.00833109300583601, 0.0092210927978158, 0.011031093075871468, 0.01126109343022108, 0.010921093635261059, 0.010231093503534794, 0.00793109368532896, 0.004641093313694, 0.0013410932151600718, -0.0013189067831262946, -0.003188906703144312, -0.005388906691223383, -0.007968906313180923, -0.010558906942605972, -0.013558906503021717, -0.015348906628787518, -0.014728906564414501, -0.012858906760811806, -0.010268907062709332, -0.00842890702188015, -0.008808907121419907, -0.010488906875252724, -0.011738906614482403, -0.011088906787335873, -0.008878907188773155, -0.006758906878530979, -0.005868906620889902, -0.006378906778991222, -0.007588906679302454, -0.009098907001316547, -0.010508907027542591, -0.010708906687796116, -0.009328906424343586, -0.007528906688094139, -0.006478906609117985, -0.005918906535953283, -0.0052589066326618195, -0.004218906629830599, -0.0023989067412912846, -0.00038890677387826145, 0.001061093294993043, 0.0015810932964086533, 0.001441093278117478, 0.002181093208491802, 0.003321093274280429, 0.0037710932083427906, 0.003701093140989542, 0.0032310932874679565, 0.0029210932552814484, 0.0030210933182388544, 0.0035210931673645973, 0.003951093181967735, 0.0036710931453853846, 0.0034010931849479675, 0.0034610931761562824, 0.0029110931791365147, 0.0017210931982845068, 0.0005110932397656143, -0.0007389067322947085, -0.0014189067296683788, -0.0009189067641273141, 0.000391093228245154, 0.0013910932466387749, 0.0007910932181403041, -0.0011689068051055074, -0.003268906846642494, -0.004378906916826963, -0.003828906686976552, -0.0028689068276435137, -0.002698906697332859, -0.0026689067017287016, -0.002818906679749489, -0.0033689066767692566, -0.0043189069256186485, -0.006048906594514847, -0.007318906951695681, -0.007298906799405813, -0.006378906778991222, -0.004978906828910112, -0.00414890656247735, -0.0044889068230986595, -0.005438906606286764, -0.005658906884491444, -0.004558906890451908, -0.0029789067339152098, -0.0018589067040011287, -0.0012389067560434341, -0.0008889067685231566, -0.0006589067634195089, -0.0005289067630656064, -0.0009689067373983562, -0.0014889067970216274, -0.0014689067611470819, -0.0010489067062735558, -0.00019890675321221352, 0.00042109325295314193, 0.0004610932373907417, 0.0007610932225361466, 0.0016710932832211256, 0.003471093252301216, 0.006011093035340309, 0.00817109365016222, 0.009631093591451645, 0.010701092891395092, 0.011881093494594097, 0.013151093386113644, 0.013881092891097069, 0.014501092955470085, 0.014891093596816063, 0.014161093160510063, 0.012771093286573887, 0.011171093210577965, 0.009311093017458916, 0.007831092923879623, 0.006961093284189701, 0.006591093260794878, 0.0066310930997133255, 0.006701093167066574, 0.00703109335154295, 0.007251093164086342, 0.0067210933193564415, 0.006101093254983425, 0.0054910932667553425, 0.004861093126237392, 0.0045410930179059505, 0.003761093132197857, 0.0019510932033881545, -0.0003489067603368312, -0.002698906697332859, -0.0047789067029953, -0.006548906676471233, -0.00830890703946352, -0.009468906559050083, -0.010188906453549862, -0.011208906769752502, -0.011528906412422657, -0.01078890636563301, -0.00997890718281269, -0.009548907168209553, -0.009788907133042812, -0.010108906775712967, -0.009228906594216824, -0.007878907024860382, -0.0073789069429039955, -0.0070289066061377525, -0.006878906860947609, -0.00755890691652894, -0.008388906717300415, -0.00895890686661005, -0.009148906916379929, -0.00919890683144331, -0.009378906339406967, -0.008638907223939896, -0.0073789069429039955, -0.0070289066061377525, -0.006898906547576189, -0.006098906975239515, -0.004468906670808792, -0.0026089067105203867, -0.0011689068051055074, 0.0003210932482033968, 0.0011110932100564241, 0.0011510932818055153, 0.0019610931631177664, 0.0031910932157188654, 0.004021093249320984, 0.00425109313800931, 0.003941093105822802, 0.0035510931629687548, 0.003121093148365617, 0.003051093313843012, 0.0031110933050513268, 0.0024810931645333767, 0.0014810932334512472, 0.0004610932373907417, 0.0001710932410787791, 0.00011109324259450659, -0.000818906759377569, -0.0008589067729189992, -0.00010890675912378356, 0.0001310932420892641, 0.0006110932445153594, 0.0005010932218283415, 6.109324021963403e-05, -0.00021890675998292863, -0.0016989067662507296, -0.0016689067706465721, 0.0007310932269319892, 0.0008910932228900492, -0.0021889067720621824, -0.0055189067497849464, -0.006888906937092543, -0.007098906673491001, -0.006838906556367874, -0.0046589067205786705, -0.0019689067266881466, -0.0016489067347720265, -0.003228906774893403, -0.004328906536102295, -0.004538906738162041, -0.004538906738162041, -0.002378906821832061, 0.0023910931777209044, 0.006331093143671751, 0.007881092838943005, 0.008081093430519104, 0.008421093225479126, 0.009011093527078629, 0.008911093696951866, 0.00930109340697527, 0.010011093690991402, 0.009081093594431877, 0.0074310931377112865, 0.006731093395501375, 0.007841093465685844, 0.009381093084812164, 0.009311093017458916, 0.008891093544661999, 0.008891093544661999, 0.008771093562245369, 0.009191093035042286, 0.010101092979311943, 0.0106810936704278, 0.010751092806458473, 0.010821092873811722, 0.011021093465387821, 0.010201092809438705, 0.007941093295812607, 0.005711093079298735, 0.003801093203946948, 0.0010410932591184974, -0.0022789067588746548, -0.005438906606286764, -0.007998907007277012, -0.00931890681385994, -0.009458906948566437, -0.008568907156586647, -0.007408906705677509, -0.007228906732052565, -0.007338906638324261, -0.007238906808197498, -0.007268906570971012, -0.006798906717449427, -0.005558906588703394, -0.0042289067059755325, -0.003778906771913171, -0.0043989066034555435, -0.005318906623870134, -0.006248906720429659, -0.007398906629532576, -0.00842890702188015, -0.008798906579613686, -0.008488906547427177, -0.007758906576782465, -0.007138906978070736, -0.006248906720429659, -0.004788906779140234, -0.0035789066459983587, -0.0023989067412912846, -0.0014489067252725363, -0.0011589067289605737, -0.0009689067373983562, -0.0007189067546278238, -0.00011890676250914112, 9.109324309974909e-05, -0.0010389067465439439, -0.002528906799852848, -0.003698906861245632, -0.003998906817287207, -0.0032989068422466516, -0.0026689067017287016, -0.002048906870186329, -0.0017989067127928138, -0.0021889067720621824, -0.0019289067713543773, -0.0014289068058133125, -0.0007789067458361387, 0.00024109323567245156, 0.0005910932668484747, 0.0012110932730138302, 0.001991093158721924, 0.001761093270033598, 0.0015410932246595621, 0.0010910932905972004, 6.109324021963403e-05, -0.0006189067498780787, -0.0011189067736268044, -0.0018689067801460624, -0.002638906706124544, -0.0028789066709578037, -0.002648906782269478, -0.0028489066753536463, -0.0030389067251235247, -0.002968906657770276, -0.0037289068568497896, -0.004168906714767218, -0.0029789067339152098, -0.0013289067428559065, -0.00025890677352435887, -7.890676351962611e-05, -0.0007689067861065269, -0.0017389067215844989, -0.0019689067266881466, -0.0012889067875221372, -0.0007389067322947085, -0.0004589067539200187, -0.0007489067502319813, -0.001678906730376184, -0.0021489067003130913, -0.002488906728103757, -0.0027689067646861076, -0.0022989066783338785, -0.0013689068146049976, 1.1093239663750865e-05, 0.001321093295700848, 0.002181093208491802, 0.00261109322309494, 0.0028110933490097523, 0.003571093315258622, 0.004901093430817127, 0.006271093152463436, 0.0071510933339595795, 0.00780109316110611, 0.00829109363257885, 0.008201093412935734, 0.00841109361499548, 0.008761093020439148, 0.008871093392372131, 0.00935109332203865, 0.009821093641221523, 0.010231093503534794, 0.010791093111038208, 0.010531092993915081, 0.009551092982292175, 0.00910109281539917, 0.00894109345972538, 0.008991093374788761, 0.009781093336641788, 0.010011093690991402, 0.00910109281539917, 0.007881092838943005, 0.0062910933047533035, 0.004491093102842569, 0.0025810932274907827, 0.0005410932353697717, -0.0008989067864604294, -0.0022389066871255636, -0.0042489068582654, -0.005698906723409891, -0.006618906743824482, -0.008088906295597553, -0.009278906509280205, -0.009748906828463078, -0.009808906354010105, -0.009028906933963299, -0.007838906720280647, -0.00767890689894557, -0.00810890644788742, -0.007928906939923763, -0.00789890717715025, -0.00822890643030405, -0.008218906819820404, -0.00789890717715025, -0.00704890675842762, -0.006338906940072775, -0.006628906819969416, -0.007058906834572554, -0.006998906843364239, -0.006498906761407852, -0.005658906884491444, -0.004688906949013472, -0.003658906789496541, -0.003338906681165099, -0.0033589068334549665, -0.002588906791061163, -0.0018189067486673594, -0.0012789067113772035, -0.0006989067769609392, -0.000708906736690551, -0.0007289067725650966, -0.00044890676508657634, -0.0008389067370444536, -0.001358906738460064, -0.0016489067347720265, -0.0022989066783338785, -0.0026689067017287016, -0.002808906836435199, -0.002818906679749489, -0.0021389068569988012, -0.0015789067838340998, -0.0013289067428559065, -0.0009189067641273141, -0.000548906740732491, -3.89067608921323e-05, 0.0005510932533070445, 0.0008210932137444615, 0.0007110932492651045, 0.0007010932313278317, 0.00101109326351434, 0.0010310931829735637, 0.0003610932326409966, -0.0005589067586697638, -0.00113890680950135, -0.001888906816020608, -0.0025589067954570055, -0.0019389067310839891, -0.001028906786814332, -0.0014189067296683788, -0.002588906791061163, -0.003338906681165099, -0.003068906720727682, -0.0020889067091047764, -0.0011289067333564162, 0.00016109323769342154, 0.0019710932392627, 0.0027010932099074125, 0.0018610932165756822, 0.0009310932364314795, 0.0009310932364314795, 0.0014710932737216353, 0.0019810933154076338, 0.0032310932874679565, 0.0046310932375490665, 0.004161093384027481, 0.0028210931923240423, 0.003001093165948987, 0.004221093375235796, 0.00488109327852726, 0.0051510934717953205, 0.005521093029528856, 0.005471093114465475, 0.00488109327852726, 0.005221093073487282, 0.0068910932168364525, 0.008161093108355999, 0.008161093108355999, 0.007461093366146088, 0.006961093284189701, 0.006751093082129955, 0.006481093354523182, 0.006861093454062939, 0.008111093193292618, 0.009021093137562275, 0.0092210927978158, 0.009371093474328518, 0.009051092900335789, 0.007401093374937773, 0.005421093199402094, 0.005431093275547028, 0.006501093041151762, 0.005621093325316906, 0.004021093249320984, 0.0033310933504253626, 0.002191093284636736, 0.0007010932313278317, -0.00029890675796195865, -0.0011989068007096648, -0.0026689067017287016, -0.004638906568288803, -0.00615890696644783, -0.0064889066852629185, -0.006458906922489405, -0.007528906688094139, -0.008738907054066658, -0.008998907171189785, -0.009448906406760216, -0.01021890714764595, -0.010408907197415829, -0.010388907045125961, -0.010418906807899475, -0.009808906354010105, -0.008798906579613686, -0.007968906313180923, -0.007268906570971012, -0.007308906875550747, -0.007788906805217266, -0.007338906638324261, -0.006428906694054604, -0.005358906928449869, -0.004188906867057085, -0.003908906597644091, -0.0037489067763090134, -0.003248906694352627, -0.0027889066841453314, -0.0019089067354798317, -0.0012289067963138223, -0.000818906759377569, 0.00010109323920914903, 0.0010910932905972004, 0.0015210931887850165, 0.0018410932971164584, 0.0023110932670533657, 0.0023310931865125895, 0.002401093253865838, 0.0026610931381583214, 0.0021110931411385536, 0.0015910932561382651, 0.0016210932517424226, 0.0013110932195559144, 0.0008710932452231646, 0.0009210932184942067, 0.0014210932422429323, 0.0016810932429507375, 0.001221093232743442, 0.0008310932316817343, 0.0007310932269319892, 0.0005010932218283415, 0.000391093228245154, 0.0005110932397656143, 0.00041109323501586914, -0.00037890675594098866, -0.0011989068007096648, -0.0017989067127928138, -0.002648906782269478, -0.0032589067704975605, -0.003348906757310033, -0.0032589067704975605, -0.0037989066913723946, -0.004758906550705433, -0.004708906635642052, -0.0042489068582654, -0.0040289065800607204, -0.003458906663581729, -0.0030089067295193672, -0.002638906706124544, -0.0020289067178964615, -0.0017689067171886563, -0.00177890679333359, -0.001348906778730452, -0.0008489067549817264, -0.00044890676508657634, 0.00029109325259923935, 0.0007510932628065348, 0.0008810932631604373, 0.0009710932499729097, 0.0006810932536609471, 0.0007010932313278317, 0.0012610931880772114, 0.001661093207076192, 0.0017510931938886642, 0.0016710932832211256, 0.0018710932927206159, 0.00273109320551157, 0.003851093351840973, 0.0047210934571921825, 0.005011093337088823, 0.0048010931350290775, 0.005021093413233757, 0.005931093357503414, 0.006661093328148127, 0.007081093266606331, 0.007501093205064535, 0.007591093424707651, 0.00703109335154295, 0.006421093363314867, 0.006541093345731497, 0.006521093193441629, 0.006001093424856663, 0.0061810933984816074, 0.006581093184649944, 0.006731093395501375, 0.006951093208044767, 0.006381093058735132, 0.005161093082278967, 0.004151093307882547, 0.00349109317176044, 0.0029810932464897633, 0.0022210932802408934, 0.0013510932913050056, 0.0005610932130366564, -0.00020890675659757107, -0.0007289067725650966, -0.001728906761854887, -0.003338906681165099, -0.0040289065800607204, -0.004298906773328781, -0.004648906644433737, -0.004358906764537096, -0.0043989066034555435, -0.005078906659036875, -0.005438906606286764, -0.005658906884491444, -0.005788906943053007, -0.005678906571120024, -0.005708906799554825, -0.005908906925469637, -0.0061489068903028965, -0.006068906746804714, -0.0055189067497849464, -0.005468906834721565, -0.005808906629681587, -0.005488906987011433, -0.005038906820118427, -0.0048289066180586815, -0.004548906814306974, -0.004348906688392162, -0.004218906629830599, -0.004218906629830599, -0.004378906916826963, -0.004408906679600477, -0.003958906978368759, -0.00360890687443316, -0.003768906695768237, -0.0035789066459983587, -0.003118906868621707, -0.00285890675149858, -0.0024989068042486906, -0.002368906745687127, -0.0024189066607505083, -0.0022989066783338785, -0.0021389068569988012, -0.001358906738460064, -0.00011890676250914112, 0.0006310932221822441, 0.0012910931836813688, 0.002071093302220106, 0.002511093160137534, 0.002881093183532357, 0.0031710932962596416, 0.0033510932698845863, 0.003951093181967735, 0.00425109313800931, 0.003761093132197857, 0.003201093291863799, 0.0024110933300107718, 0.001601093215867877, 0.0013410932151600718, 0.0009410932543687522, 0.0006510932580567896, 0.0008410932496190071, 0.0008510932675562799, 0.0009210932184942067, 0.0010910932905972004, 0.0005710932309739292, -2.8906759325764142e-05, -0.00011890676250914112, 0.00011109324259450659, 0.0005210932577028871, 0.0003810932394117117, -0.00011890676250914112, -0.0005789067363366485, -0.0011289067333564162, -0.0011589067289605737, -0.0009989067912101746, -0.0015489067882299423, -0.0026089067105203867, -0.0028289067558944225, -0.002058906713500619, -0.0015989067032933235, -0.0010489067062735558, -0.00010890675912378356, 0.00023109324683900923, 0.00037109325057826936, 0.0009410932543687522, 0.0015810932964086533, 0.002071093302220106, 0.0022210932802408934, 0.0026810932904481888, 0.0035610932391136885, 0.004081093240529299, 0.004291093442589045, 0.004211093299090862, 0.004191093146800995, 0.004471093416213989, 0.004841093439608812, 0.005781093146651983, 0.00691109336912632, 0.006961093284189701, 0.006751093082129955, 0.006871093064546585, 0.006781093310564756, 0.007181093096733093, 0.007621093187481165, 0.007421093061566353, 0.007271093316376209, 0.006661093328148127, 0.005521093029528856, 0.004851093050092459, 0.003971093334257603, 0.0028410933446139097, 0.0022310931235551834, 0.0014710932737216353, 0.00028109323466196656, -0.001028906786814332, -0.002648906782269478, -0.004238906782120466, -0.004638906568288803, -0.003888906678184867, -0.0039489069022238255, -0.005538906902074814, -0.007138906978070736, -0.00801890715956688, -0.00834890641272068, -0.008038906380534172, -0.007348906714469194, -0.006758906878530979, -0.007128906901925802, -0.008158906362950802, -0.008578906767070293, -0.008568907156586647, -0.008518906310200691, -0.008388906717300415, -0.00806890707463026, -0.007408906705677509, -0.006788906641304493, -0.006558906752616167, -0.006008906755596399, -0.005038906820118427, -0.004238906782120466, -0.003818906843662262, -0.003818906843662262, -0.0036489067133516073, -0.0036389068700373173, -0.0037889068480581045, -0.0031089067924767733, -0.0019789068028330803, -0.0012089067604392767, -0.0010089067509397864, -0.0010589067824184895, -0.0012189067201688886, -0.0012289067963138223, -0.0008489067549817264, -7.890676351962611e-05, 0.0012510932283475995, 0.0024110933300107718, 0.002831093268468976, 0.002671093214303255, 0.0026510932948440313, 0.0034410932566970587, 0.004041093401610851, 0.004141093231737614, 0.0045410930179059505, 0.005171093158423901, 0.00551109341904521, 0.005541093181818724, 0.005701093468815088, 0.005551093257963657, 0.004561093170195818, 0.0036810932215303183, 0.0032810932025313377, 0.002621093299239874, 0.0021510932128876448, 0.0022210932802408934, 0.002461093245074153, 0.0026610931381583214, 0.002401093253865838, 0.0022710931953042746, 0.002201093127951026, 0.0016910932026803493, 0.0020010932348668575, 0.0026410932186990976, 0.002561093308031559, 0.002181093208491802, 0.0010310931829735637, -0.00020890675659757107, -0.0005289067630656064, -0.0009089067461900413, -0.001398906810209155, -0.0014289068058133125, -0.0017089067259803414, -0.002478906651958823, -0.003028906648978591, -0.0029389066621661186, -0.002468906808644533, -0.0020089067984372377, -0.0015789067838340998, -0.0010589067824184895, -0.0004089067515451461, -0.0003489067603368312, -0.0007289067725650966, -0.000308906746795401, 0.000191093233297579, -0.00011890676250914112, -0.00046890677185729146, -0.0004589067539200187, -0.00016890675760805607, 8.109323971439153e-05, 8.109323971439153e-05, 0.00014109324547462165, 0.00022109324345365167, 0.0005010932218283415, 0.0013710932107642293, 0.0022810932714492083, 0.002781093353405595, 0.0032510932069271803, 0.0038310931995511055, 0.003981093410402536, 0.003431093180552125, 0.002871093340218067, 0.003041093237698078, 0.0032410931307822466, 0.0033410931937396526, 0.0037810932844877243, 0.0032410931307822466, 0.0019510932033881545, 0.0008910932228900492, -0.00019890675321221352, -0.00043890674714930356, -0.00020890675659757107, -0.0005289067630656064, -0.0011589067289605737, -0.0019089067354798317, -0.002208906691521406, -0.0023989067412912846, -0.0025089066475629807, -0.0018189067486673594, -0.0012889067875221372, -0.001308906706981361, -0.0015489067882299423, -0.0022489067632704973, -0.0025989068672060966, -0.002758906688541174, -0.0034089067485183477, -0.003988906741142273, -0.004208906553685665, -0.0038589066825807095, -0.0031089067924767733, -0.0028289067558944225, -0.0027089067734777927, -0.0025389068759977818, -0.002648906782269478, -0.002378906821832061, -0.0020289067178964615, -0.0017489067977294326, -0.001088906778022647, -0.0010989067377522588, -0.0015189067926257849, -0.0017989067127928138, -0.0020889067091047764, -0.0024189066607505083, -0.0036489067133516073, -0.004968906752765179, -0.0056289066560566425, -0.005588906817138195, -0.004438906908035278, -0.00426890654489398, -0.004858906846493483, -0.0040589068084955215, -0.003388906829059124, -0.0032989068422466516, -0.0027789068408310413, -0.002058906713500619, -0.0011589067289605737, -0.0002289067633682862, 0.0009410932543687522, 0.0021210932172834873, 0.0021610932890325785, 0.0020410933066159487, 0.002831093268468976, 0.0030710932333022356, 0.0024510931689292192, 0.0020510931499302387, 0.0022210932802408934, 0.0025910933036357164, 0.0022310931235551834, 0.0021110931411385536, 0.0025910933036357164, 0.0027010932099074125, 0.003101093228906393, 0.0037810932844877243, 0.004571093246340752, 0.005521093029528856, 0.005621093325316906, 0.005561093334108591, 0.005821093451231718, 0.005381093360483646, 0.004931093193590641, 0.005051093176007271, 0.004501093178987503, 0.003201093291863799, 0.0020210931543260813, 0.0012610931880772114, 0.0006710932357236743, 5.1093240472255275e-05, -0.00044890676508657634, -0.0007489067502319813, -0.0006989067769609392, -0.0008689067326486111, -0.0012189067201688886, -0.0010589067824184895, -0.0008089067414402962, -0.0004589067539200187, 1.0932398026852752e-06, -9.890676301438361e-05, -0.0002489067555870861, 1.1093239663750865e-05, 0.00010109323920914903, -6.890676013426855e-05, -0.00016890675760805607, -0.0005389067810028791, -0.0008989067864604294, -0.000978906755335629, -0.0013189067831262946, -0.0010689067421481013, -0.0003289067535661161, -0.00011890676250914112, 0.0009210932184942067, 0.0023210933431982994, 0.002441093325614929, 0.002521093236282468, 0.0029810932464897633, 0.003161093220114708, 0.0037810932844877243, 0.004581093322485685, 0.004441093187779188, 0.003901093266904354, 0.0032510932069271803, 0.0021110931411385536, 0.0012610931880772114, 0.0008710932452231646, -8.906759831006639e-06, -0.0011089068138971925, -0.0018989067757502198, -0.0027889066841453314, -0.00360890687443316, -0.00412890687584877, -0.004338906612247229, -0.004588906653225422, -0.0047789067029953, -0.004298906773328781, -0.004088906571269035, -0.0043989066034555435, -0.004218906629830599, -0.0042289067059755325, -0.004718906711786985, -0.0049189068377017975, -0.004668906796723604, -0.004088906571269035, -0.004278906621038914, -0.004818906541913748, -0.004428906831890345, -0.004208906553685665, -0.004198906943202019, -0.0032089068554341793, -0.0022889068350195885, -0.002258906839415431, -0.0019689067266881466, -0.0011589067289605737, -0.0008289067773148417, -0.000708906736690551, -5.89067603868898e-05, 0.000901093240827322, 0.0017210931982845068, 0.0017710932297632098, 0.0011910932371392846, 0.0009510932140983641, 0.0008410932496190071, 0.0004610932373907417, -8.906759831006639e-06, 0.00024109323567245156, 0.001761093270033598, 0.0026810932904481888, 0.002691093133762479, 0.003571093315258622, 0.004081093240529299, 0.003701093140989542, 0.0035510931629687548, 0.0032910932786762714, 0.0031310932245105505, 0.0030910931527614594, 0.0033310933504253626, 0.003861093195155263, 0.003141093300655484, 0.0014510932378470898, -0.00027890675119124353, -0.0017689067171886563, -0.002368906745687127, -0.0024389068130403757, -0.0022889068350195885, -0.0021889067720621824, -0.0031089067924767733, -0.004458906594663858, -0.004328906536102295, -0.0027189068496227264, -0.0014889067970216274, -0.0013189067831262946, -0.001348906778730452, -0.001298906747251749, -0.0010089067509397864, -1.890675957838539e-05, 0.0009710932499729097, 0.0012910931836813688, 0.0008710932452231646, 0.0002110932400682941, 0.0002110932400682941, -0.00016890675760805607, -0.0011089068138971925, -0.0010089067509397864, -0.0008989067864604294, -0.0012189067201688886, -0.000978906755335629, -0.0009089067461900413, -0.0009189067641273141, -0.0004289067583158612, 0.00041109323501586914, 0.0015810932964086533, 0.0028510931879281998, 0.003121093148365617, 0.0023110932670533657, 0.0020010932348668575, 0.002831093268468976, 0.003801093203946948, 0.0038810933474451303, 0.0034510933328419924, 0.002831093268468976, 0.0019310932839289308, 0.0013010932598263025, 0.00133109325543046, 0.0018610932165756822, 0.002081093145534396, 0.0017710932297632098, 0.0015410932246595621, 0.0013410932151600718, 0.0016410932876169682, 0.0026010931469500065, 0.003141093300655484, 0.003001093165948987, 0.002351093338802457, 0.0036610933020710945, 0.008241092786192894, 0.010171093046665192, 0.005931093357503414, -5.89067603868898e-05, -0.003888906678184867, -0.005338906776160002, -0.0041189067997038364, -3.89067608921323e-05, 0.0035210931673645973, 0.0018710932927206159, -0.004558906890451908]

print(len(a)) # 16384

b = pd.DataFrame(columns=['a'])
b.loc[0] = [a]
b.to_excel('test_b.xlsx')

c = pd.read_excel('test_b.xlsx') #a has been stored as a string, since the saved value doesn't even had a final ']'
import ast
d = ast.literal_eval(c['a'].values[0][:-2] + ']') #The last two characters are a space and a comma
print(len(d)) #1425, same result that if we copy the cell from Excel, close the list with a ']' and run len()
```


### Issue Description

When trying to **save** a Dataframe which contains a column whose values are LARGE lists of values (each cell of the column hold a list of 16.384 elements) as **an Excel** with the `to_excel()` method, pandas cuts off each list of the column and save it as a 32766 character-long string. Since each list is cut off, they don't end in a square bracket. When closing them and casting them as list, they hold just 1425 elements.

### Expected Behavior

The resulting Excel column should hold as much values as the original Dataframe from which it has been produced. 

### Installed Versions

<details>

[C:\Program](file:///C:/Program) Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\site-packages\_distutils_hack\__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

INSTALLED VERSIONS
------------------
commit                : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140
python                : 3.11.9.final.0
python-bits           : 64
OS                    : Windows
OS-release            : 10
Version               : 10.0.22631
machine               : AMD64
processor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel
byteorder             : little
LC_ALL                : None
LANG                  : None
LOCALE                : es_ES.cp1252

pandas                : 2.2.2
numpy                 : 1.24.4
pytz                  : 2023.3
dateutil              : 2.8.2
setuptools            : 65.5.0
pip                   : 23.2.1
Cython                : 0.29.36
pytest                : None
hypothesis            : None
sphinx                : 6.2.1
blosc                 : None
feather               : None
xlsxwriter            : None
lxml.etree            : 5.1.0
html5lib              : None
pymysql               : 1.0.3
psycopg2              : None
jinja2                : 3.1.2
IPython               : 8.14.0
pandas_datareader     : None
adbc-driver-postgresql: None
adbc-driver-sqlite    : None
bs4                   : 4.12.2
bottleneck            : None
dataframe-api-compat  : None
fastparquet           : None
fsspec                : 2023.5.0
gcsfs                 : None
matplotlib            : 3.7.1
numba                 : 0.57.1
numexpr               : 2.8.4
odfpy                 : None
openpyxl              : 3.1.2
pandas_gbq            : None
pyarrow               : 12.0.1
pyreadstat            : None
python-calamine       : None
pyxlsb                : None
s3fs                  : None
scipy                 : 1.11.3
sqlalchemy            : None
tables                : None
tabulate              : None
xarray                : None
xlrd                  : 2.0.1
zstandard             : None
tzdata                : 2023.3
qtpy                  : None
pyqt5                 : None
</details>
","['Bug', 'IO Excel', 'Needs Info']",2024-05-28 11:48:31,2024-05-29 07:35:28,4,closed
58829,BUG: Warning when compiling pandas/_libs/algos.c,"### Pandas version checks

- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.

- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.


### Reproducible Example

```python
Build pandas from commit b162331554d7c7f6fd46ddde1ff3908f2dc8bcce and review build logs
```


### Issue Description

```
gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DOPENSSL_NO_SSL3 -fPIC -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/include -I/home/ijtaba/.pyenv/versions/pandas-dev/lib/python3.9/site-packages/numpy/core/include -I/home/ijtaba/.pyenv/versions/pandas-dev/include -I/home/ijtaba/.pyenv/versions/3.9.10/include/python3.9 -c pandas/_libs/algos.c -o build/temp.linux-x86_64-3.9/pandas/_libs/algos.o
    In file included from pandas/_libs/algos.c:1270:
    pandas/_libs/include/pandas/vendored/klib/khash_python.h: In function ‘traced_realloc’:
    pandas/_libs/include/pandas/vendored/klib/khash_python.h:39:7: warning: pointer ‘old_ptr’ may be used after ‘realloc’ [-Wuse-after-free]
       39 |       PyTraceMalloc_Untrack(KHASH_TRACE_DOMAIN, (uintptr_t)old_ptr);
          |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/include/pandas/vendored/klib/khash_python.h:36:15: note: call to ‘realloc’ here
       36 |   void *ptr = realloc(old_ptr, size);
          |               ^~~~~~~~~~~~~~~~~~~~~~
```

### Expected Behavior

No warnings

### Installed Versions

<details>
0+untagged.35052.g5db4dd6
</details>
","['Bug', 'Build', 'Closing Candidate']",2024-05-26 10:10:14,2025-08-05 16:50:36,3,closed
